<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 177]
- [cs.CL](#cs.CL) [Total: 61]
- [cs.HC](#cs.HC) [Total: 12]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DS](#cs.DS) [Total: 10]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.RO](#cs.RO) [Total: 64]
- [cs.LG](#cs.LG) [Total: 103]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.NE](#cs.NE) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions](https://arxiv.org/abs/2512.00042)
*Egemen Sert,Şeyda Ertekin*

Main category: cs.CV

TL;DR: 通过数据中心的监督微调方法，使用高质量多模态数据集（1.614亿token）优化Qwen-2.5VL-32B模型，在标准化考试基准YKSUniform上达到78.6%准确率，仅比Gemini 2.0 Flash低1.0%


<details>
  <summary>Details</summary>
Motivation: 多模态推理已成为现代AI研究的基石，标准化考试问题为此提供了严格的测试平台。虽然近期研究主要集中在强化学习等算法进步上，但视觉语言推理的数据中心基础仍未得到充分探索。本研究旨在证明通过高质量数据进行监督微调可以媲美专有方法。

Method: 收集了1.614亿token的多模态数据集，包含教科书问答对、课程对齐图表和上下文材料。使用优化的推理语法（QMSA）对Qwen-2.5VL-32B模型进行监督微调。发布了新的基准YKSUniform，标准化了309个课程主题下的1,854个多模态考试问题。

Result: 微调后的模型在YKSUniform基准上达到78.6%的准确率，仅比Gemini 2.0 Flash低1.0%。这表明数据组合和表示语法在多模态推理中起着决定性作用。

Conclusion: 数据组合和表示语法在多模态推理中具有决定性作用。本研究为推进开源视觉语言模型建立了数据中心框架，证明精心策划和基于课程的多模态数据可以将监督微调提升到接近最先进的性能水平。

Abstract: Multimodal reasoning has become a cornerstone of modern AI research. Standardized exam questions offer a uniquely rigorous testbed for such reasoning, providing structured visual contexts and verifiable answers. While recent progress has largely focused on algorithmic advances such as reinforcement learning (e.g., GRPO, DPO), the data centric foundations of vision language reasoning remain less explored.
  We show that supervised fine-tuning (SFT) with high-quality data can rival proprietary approaches. To this end, we compile a 161.4 million token multimodal dataset combining textbook question-solution pairs, curriculum aligned diagrams, and contextual materials, and fine-tune Qwen-2.5VL-32B using an optimized reasoning syntax (QMSA). The resulting model achieves 78.6% accuracy, only 1.0% below Gemini 2.0 Flash, on our newly released benchmark YKSUniform, which standardizes 1,854 multimodal exam questions across 309 curriculum topics.
  Our results reveal that data composition and representational syntax play a decisive role in multimodal reasoning. This work establishes a data centric framework for advancing open weight vision language models, demonstrating that carefully curated and curriculum-grounded multimodal data can elevate supervised fine-tuning to near state-of-the-art performance.

中文标题: 弥合差距：视觉语言模型在标准化考试问题上的数据中心微调

中文摘要: 多模态推理已成为现代AI研究的基石。标准化考试问题为此类推理提供了独特的严格测试平台，提供结构化的视觉上下文和可验证的答案。虽然近期进展主要集中在算法进步上（如GRPO、DPO等强化学习方法），但视觉语言推理的数据中心基础仍未得到充分探索。

我们证明，使用高质量数据进行监督微调（SFT）可以媲美专有方法。为此，我们编译了一个1.614亿token的多模态数据集，结合了教科书问答对、课程对齐图表和上下文材料，并使用优化的推理语法（QMSA）对Qwen-2.5VL-32B进行微调。所得模型在我们新发布的基准YKSUniform上达到78.6%的准确率，仅比Gemini 2.0 Flash低1.0%。YKSUniform标准化了309个课程主题下的1,854个多模态考试问题。

我们的结果表明，数据组合和表示语法在多模态推理中起着决定性作用。这项工作为推进开源视觉语言模型建立了数据中心框架，证明精心策划和基于课程的多模态数据可以将监督微调提升到接近最先进的性能水平。

</details>


### [2] [PEFT-DML: Parameter-Efficient Fine-Tuning Deep Metric Learning for Robust Multi-Modal 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2512.00060)
*Abdolazim Rezaei,Mehdi Sookhak*

Main category: cs.CV

TL;DR: PEFT-DML是一个参数高效的深度度量学习框架，用于自动驾驶中的多模态3D目标检测，能够在传感器丢失或新模态组合情况下保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统多模态3D检测模型假设固定传感器可用性，但在实际自动驾驶场景中，传感器可能丢失或出现新的组合，需要更鲁棒的解决方案。

Method: 使用深度度量学习将多种传感器模态映射到共享潜在空间，集成LoRA和适配器层实现参数高效微调，提高训练效率和鲁棒性。

Result: 在nuScenes基准测试上表现出优越的准确性，能够有效处理传感器丢失、快速运动、天气变化和域偏移等挑战。

Conclusion: PEFT-DML为自动驾驶中的多模态3D目标检测提供了参数高效且鲁棒的解决方案，在传感器不确定性和环境变化下保持可靠性能。

Abstract: This study introduces PEFT-DML, a parameter-efficient deep metric learning framework for robust multi-modal 3D object detection in autonomous driving. Unlike conventional models that assume fixed sensor availability, PEFT-DML maps diverse modalities (LiDAR, radar, camera, IMU, GNSS) into a shared latent space, enabling reliable detection even under sensor dropout or unseen modality class combinations. By integrating Low-Rank Adaptation (LoRA) and adapter layers, PEFT-DML achieves significant training efficiency while enhancing robustness to fast motion, weather variability, and domain shifts. Experiments on benchmarks nuScenes demonstrate superior accuracy.

中文标题: PEFT-DML：用于自动驾驶中鲁棒多模态3D目标检测的参数高效微调深度度量学习

中文摘要: 本研究提出了PEFT-DML，一种用于自动驾驶中鲁棒多模态3D目标检测的参数高效深度度量学习框架。与传统模型假设固定传感器可用性不同，PEFT-DML将多种模态（LiDAR、雷达、相机、IMU、GNSS）映射到共享潜在空间中，即使在传感器丢失或未见过的模态类别组合情况下也能实现可靠检测。通过集成低秩适应（LoRA）和适配器层，PEFT-DML在显著提高训练效率的同时，增强了对快速运动、天气变化和域偏移的鲁棒性。在nuScenes基准测试上的实验证明了其优越的准确性。

</details>


### [3] [Satellite to Street : Disaster Impact Estimator](https://arxiv.org/abs/2512.00065)
*Sreesritha Sai,Sai Venkata Suma Sreeja,Deepthi,Nikhil*

Main category: cs.CV

TL;DR: 本文提出了一种名为Satellite-to-Street的深度学习框架，通过联合处理灾前和灾后卫星图像来生成精细的像素级损害地图，改进了传统分割和变化检测模型在细微结构变化和严重类别不平衡方面的不足。


<details>
  <summary>Details</summary>
Motivation: 准确的灾后损害评估对于优先安排应急响应至关重要，但人工解释卫星图像速度慢、主观性强且难以扩展。现有的深度学习分割模型（如U-Net基线和变化检测模型）在处理细微结构变化和严重类别不平衡时表现不佳，导致对高度受损区域的检测效果差。

Method: 提出了一个深度学习框架，采用改进的双输入U-Net架构，通过增强的特征融合机制同时捕捉局部结构变化和更广泛的上下文线索。集成了类别感知的加权损失函数来处理真实灾害数据集中未受损像素占主导地位的问题，从而增强对主要受损和完全破坏类别的敏感性。

Result: 在公开可用的灾害数据集上的实验表明，与传统的分割和基线变化检测模型相比，该模型在结构损害的定位和分类方面都有显著改进。生成的损害地图提供了快速一致的评估机制。

Conclusion: Satellite-to-Street模型能够提供快速、一致的损害评估机制，支持而非替代专家决策，从而实现更高效、数据驱动的灾害管理。

Abstract: Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator. The model uses a modified dual-input U-Net architecture with enhanced feature fusion to capture both the local structural changes as well as the broader contextual cues. Class-aware weighted loss functions are integrated in order to handle the dominance of undamaged pixels in real disaster datasets, thus enhancing sensitivity toward major and destroyed categories. Experimentation on publicly available disaster datasets shows improved localization and classification of structural damage when compared to traditional segmentation and baseline change-detection models. The resulting damage maps provide a rapid and consistent assessment mechanism to support and not replace expert decision-making, thus allowing more efficient, data-driven disaster management.

中文标题: 卫星到街道：灾害影响评估器

中文摘要: 准确的灾后损害评估对于优先安排应急响应至关重要；然而，人工解释卫星图像速度慢、主观性强且难以扩展。虽然用于图像分割的深度学习模型（如基于U-Net的基线和变化检测模型）是有用的基线，但它们往往难以处理细微的结构变化和严重的类别不平衡，导致对高度受损区域的检测效果差。本研究提出了一个深度学习框架，联合处理灾前和灾后卫星图像以获得精细的像素级损害地图：卫星到街道：灾害影响评估器。该模型使用改进的双输入U-Net架构，通过增强的特征融合来捕捉局部结构变化以及更广泛的上下文线索。集成了类别感知的加权损失函数，以处理真实灾害数据集中未受损像素占主导地位的问题，从而增强对主要受损和完全破坏类别的敏感性。在公开可用的灾害数据集上的实验表明，与传统的分割和基线变化检测模型相比，该模型在结构损害的定位和分类方面都有改进。生成的损害地图提供了快速一致的评估机制，以支持而非替代专家决策，从而实现更高效、数据驱动的灾害管理。

</details>


### [4] [ProvRain: Rain-Adaptive Denoising and Vehicle Detection via MobileNet-UNet and Faster R-CNN](https://arxiv.org/abs/2512.00073)
*Aswinkumar Varathakumaran,Nirmala Paramanandham*

Main category: cs.CV

TL;DR: ProvRain是一个用于雨夜车辆检测的管道系统，结合MobileNet-U-Net去噪架构和Faster R-CNN检测器，通过课程学习提高在恶劣天气条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 在夜间车辆检测中，除了车头灯外提取其他特征可以提前检测到车辆，但雨雪等恶劣天气条件会产生大量噪声，影响检测精度。需要一种既能处理噪声又能保持检测准确性的解决方案。

Method: 使用轻量级MobileNet-U-Net架构进行去噪，采用课程学习概念来适应恶劣天气条件。结合合成数据和PVDN数据集进行训练。将去噪后的图像输入Faster R-CNN进行车辆检测。

Result: 在雨夜帧中，车辆检测准确率提高8.94%，召回率提高10.25%。MobileNet-U-Net相比其他Transformer方法，PSNR提高10-15%，SSIM提高5-6%，感知误差(LPIPS)降低高达67%。

Conclusion: ProvRain管道系统通过结合去噪和检测模块，显著提高了雨夜条件下的车辆检测性能，证明了去噪架构对恶劣天气条件下计算机视觉任务的重要性。

Abstract: Provident vehicle detection has a lot of scope in the detection of vehicle during night time. The extraction of features other than the headlamps of vehicles allows us to detect oncoming vehicles before they appear directly on the camera. However, it faces multiple issues especially in the field of night vision, where a lot of noise caused due to weather conditions such as rain or snow as well as camera conditions. This paper focuses on creating a pipeline aimed at dealing with such noise while at the same time maintaining the accuracy of provident vehicular detection. The pipeline in this paper, ProvRain, uses a lightweight MobileNet-U-Net architecture tuned to generalize to robust weather conditions by using the concept of curricula training. A mix of synthetic as well as available data from the PVDN dataset is used for this. This pipeline is compared to the base Faster RCNN architecture trained on the PVDN dataset to see how much the addition of a denoising architecture helps increase the detection model's performance in rainy conditions. The system boasts an 8.94\% increase in accuracy and a 10.25\% increase in recall in the detection of vehicles in rainy night time frames. Similarly, the custom MobileNet-U-Net architecture that was trained also shows a 10-15\% improvement in PSNR, a 5-6\% increase in SSIM, and upto a 67\% reduction in perceptual error (LPIPS) compared to other transformer approaches.

中文标题: ProvRain：基于MobileNet-UNet和Faster R-CNN的雨适应去噪与车辆检测

中文摘要: 预见性车辆检测在夜间车辆检测中具有广泛应用前景。除了车辆头灯外提取特征，使我们能够在车辆直接出现在摄像头前检测到来车。然而，特别是在夜视领域，由于雨雪等天气条件以及相机条件会产生大量噪声，面临多重问题。本文重点创建一个旨在处理此类噪声同时保持预见性车辆检测准确性的管道系统。本文中的管道ProvRain使用轻量级MobileNet-U-Net架构，通过课程学习概念调整以泛化到恶劣天气条件。为此使用了合成数据以及PVDN数据集中的可用数据。将该管道与在PVDN数据集上训练的基准Faster RCNN架构进行比较，以了解添加去噪架构在多大程度上有助于提高检测模型在雨天条件下的性能。该系统在雨夜帧中车辆检测的准确率提高了8.94%，召回率提高了10.25%。同样，训练的自定义MobileNet-U-Net架构相比其他Transformer方法，PSNR提高了10-15%，SSIM提高了5-6%，感知误差(LPIPS)降低了高达67%。

</details>


### [5] [Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation](https://arxiv.org/abs/2512.00075)
*Jun Jia,Hongyi Miao,Yingjie Zhou,Wangqiu Zhou,Jianbo Zhang,Linhan Cao,Dandan Zhu,Hua Yang,Xiongkuo Min,Wei Sun,Guangtao Zhai*

Main category: cs.CV

TL;DR: Adapter Shield是一个内置认证的统一防御框架，通过可逆加密和多目标对抗扰动保护图像免遭未经授权的零样本图像生成滥用，同时允许授权用户正常使用。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型的发展，零样本图像到图像生成技术能够仅用一张肖像或艺术作品就实现高保真身份克隆和风格模仿，这带来了严重的知识产权侵权风险。现有方法缺乏有效的保护机制来防止个人图像被滥用。

Method: 1. 分析零样本方法使用图像编码器提取嵌入并通过交叉注意力输入扩散模型的机制；2. 构建可逆加密系统，根据密钥将原始嵌入映射为加密表示；3. 设计多目标对抗扰动方法，主动将原始嵌入向加密模式偏移；4. 授权用户可通过解密模块和正确密钥恢复真实嵌入。

Result: 广泛评估表明，Adapter Shield在阻止未经授权的零样本图像合成方面超越了现有最先进的防御方法，同时为已验证用户提供灵活且安全的访问控制，确保未经授权用户只能产生扭曲或加密的输出。

Conclusion: Adapter Shield是首个通用且集成认证的解决方案，有效保护个人图像免遭零样本生成场景的滥用，在防御性能和用户体验之间取得了良好平衡，为图像版权保护提供了新思路。

Abstract: With the rapid progress in diffusion models, image synthesis has advanced to the stage of zero-shot image-to-image generation, where high-fidelity replication of facial identities or artistic styles can be achieved using just one portrait or artwork, without modifying any model weights. Although these techniques significantly enhance creative possibilities, they also pose substantial risks related to intellectual property violations, including unauthorized identity cloning and stylistic imitation. To counter such threats, this work presents Adapter Shield, the first universal and authentication-integrated solution aimed at defending personal images from misuse in zero-shot generation scenarios. We first investigate how current zero-shot methods employ image encoders to extract embeddings from input images, which are subsequently fed into the UNet of diffusion models through cross-attention layers. Inspired by this mechanism, we construct a reversible encryption system that maps original embeddings into distinct encrypted representations according to different secret keys. The authorized users can restore the authentic embeddings via a decryption module and the correct key, enabling normal usage for authorized generation tasks. For protection purposes, we design a multi-target adversarial perturbation method that actively shifts the original embeddings toward designated encrypted patterns. Consequently, protected images are embedded with a defensive layer that ensures unauthorized users can only produce distorted or encrypted outputs. Extensive evaluations demonstrate that our method surpasses existing state-of-the-art defenses in blocking unauthorized zero-shot image synthesis, while supporting flexible and secure access control for verified users.

中文标题: Adapter Shield：一种内置认证的统一框架，用于防止未经授权的零样本图像到图像生成

中文摘要: 随着扩散模型的快速发展，图像合成已进入零样本图像到图像生成阶段，仅使用一张肖像或艺术作品即可实现面部身份或艺术风格的高保真复制，而无需修改任何模型权重。尽管这些技术显著增强了创作可能性，但也带来了与知识产权侵权相关的重大风险，包括未经授权的身份克隆和风格模仿。为应对此类威胁，本研究提出了Adapter Shield，这是首个旨在保护个人图像免遭零样本生成场景滥用的通用且集成认证的解决方案。我们首先研究了当前零样本方法如何使用图像编码器从输入图像中提取嵌入表示，随后通过交叉注意力层将这些嵌入输入到扩散模型的UNet中。受此机制启发，我们构建了一个可逆加密系统，根据不同的密钥将原始嵌入映射为不同的加密表示。授权用户可以通过解密模块和正确的密钥恢复真实嵌入，从而支持授权的生成任务正常使用。出于保护目的，我们设计了一种多目标对抗扰动方法，主动将原始嵌入向指定的加密模式偏移。因此，受保护的图像嵌入了防御层，确保未经授权的用户只能产生扭曲或加密的输出。广泛的评估表明，我们的方法在阻止未经授权的零样本图像合成方面超越了现有最先进的防御方法，同时为已验证用户提供灵活且安全的访问控制。

</details>


### [6] [Diffusion-Based Synthetic Brightfield Microscopy Images for Enhanced Single Cell Detection](https://arxiv.org/abs/2512.00078)
*Mario de Jesus da Graca,Jörg Dahlkemper,Peer Stelldinger*

Main category: cs.CV

TL;DR: 使用扩散模型生成合成明场显微镜图像，通过混合合成与真实数据训练目标检测模型，显著提高单细胞检测精度，同时降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 明场显微镜单细胞检测在生物学研究中至关重要，但面临数据稀缺和人工标注成本高的瓶颈，限制了深度学习方法的进一步发展。

Method: 训练基于U-Net的扩散模型生成合成明场显微镜图像，创建不同合成/真实比例的数据集，使用YOLOv8、YOLOv9和RT-DETR三种目标检测模型进行实验评估。

Result: 合成数据训练显著提高检测精度（成本最低），人类专家无法区分合成与真实图像（准确率50%），证明合成图像具有高度真实感。

Conclusion: 基于扩散的合成数据生成是增强显微镜图像分析的有效途径，能减少对大量手动标注的依赖，提高细胞检测模型的鲁棒性和性能。

Abstract: Accurate single cell detection in brightfield microscopy is crucial for biological research, yet data scarcity and annotation bottlenecks limit the progress of deep learning methods. We investigate the use of unconditional models to generate synthetic brightfield microscopy images and evaluate their impact on object detection performance. A U-Net based diffusion model was trained and used to create datasets with varying ratios of synthetic and real images. Experiments with YOLOv8, YOLOv9 and RT-DETR reveal that training with synthetic data can achieve improved detection accuracies (at minimal costs). A human expert survey demonstrates the high realism of generated images, with experts not capable to distinguish them from real microscopy images (accuracy 50%). Our findings suggest that diffusion-based synthetic data generation is a promising avenue for augmenting real datasets in microscopy image analysis, reducing the reliance on extensive manual annotation and potentially improving the robustness of cell detection models.

中文标题: 基于扩散的合成明场显微镜图像用于增强单细胞检测

中文摘要: 明场显微镜中准确的单细胞检测对生物学研究至关重要，但数据稀缺和标注瓶颈限制了深度学习方法的发展。我们研究了使用无条件模型生成合成明场显微镜图像，并评估其对目标检测性能的影响。训练了一个基于U-Net的扩散模型，用于创建具有不同合成与真实图像比例的数据集。使用YOLOv8、YOLOv9和RT-DETR进行的实验表明，使用合成数据训练可以实现更高的检测精度（成本最低）。人类专家调查显示生成图像具有高度真实感，专家无法区分它们与真实显微镜图像（准确率50%）。我们的研究结果表明，基于扩散的合成数据生成是增强显微镜图像分析中真实数据集的有前景途径，减少了对大量手动标注的依赖，并可能提高细胞检测模型的鲁棒性。

</details>


### [7] [Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels](https://arxiv.org/abs/2512.00080)
*André Dehne,Juri Zach,Peer Stelldinger*

Main category: cs.CV

TL;DR: 本文探讨了在加速器隧道环境中使用深度视觉立体里程计（DVSO）替代MARWIN辐射监测机器人当前导航系统的概念性评估，分析了其潜在优势与挑战。


<details>
  <summary>Details</summary>
Motivation: MARWIN机器人在欧洲XFEL加速器隧道中执行自主辐射监测，当前导航系统结合激光雷达边缘检测、轮式/激光雷达里程计和QR码参考，在预定义区域表现稳健，但缺乏应对未知几何结构和障碍物的灵活性。需要探索更灵活、基于视觉的导航方案。

Method: 提出使用深度视觉立体里程计（DVSO）作为替代方案。DVSO基于立体视觉，利用立体视差、光流和自监督学习联合估计深度和自我运动，无需标注数据。为保持全局一致性，DVSO可与绝对参考（如地标）或其他传感器融合。

Result: 概念性评估显示DVSO在加速器隧道环境中的预期优势包括：通过立体视觉减少尺度漂移、低成本传感、可扩展的数据收集。同时识别了挑战：低纹理表面、光照变化、计算负载以及在辐射环境下的鲁棒性。

Conclusion: 深度视觉立体里程计为MARWIN机器人在受限、安全关键的基础设施中实现更自主导航提供了有前景的方向。论文定义了研究议程，旨在解决当前挑战，推动该技术在加速器隧道环境中的应用。

Abstract: The MARWIN robot operates at the European XFEL to perform autonomous radiation monitoring in long, monotonous accelerator tunnels where conventional localization approaches struggle. Its current navigation concept combines lidar-based edge detection, wheel/lidar odometry with periodic QR-code referencing, and fuzzy control of wall distance, rotation, and longitudinal position. While robust in predefined sections, this design lacks flexibility for unknown geometries and obstacles. This paper explores deep visual stereo odometry (DVSO) with 3D-geometric constraints as a focused alternative. DVSO is purely vision-based, leveraging stereo disparity, optical flow, and self-supervised learning to jointly estimate depth and ego-motion without labeled data. For global consistency, DVSO can subsequently be fused with absolute references (e.g., landmarks) or other sensors. We provide a conceptual evaluation for accelerator tunnel environments, using the European XFEL as a case study. Expected benefits include reduced scale drift via stereo, low-cost sensing, and scalable data collection, while challenges remain in low-texture surfaces, lighting variability, computational load, and robustness under radiation. The paper defines a research agenda toward enabling MARWIN to navigate more autonomously in constrained, safety-critical infrastructures.

中文标题: MARWIN辐射监测机器人在加速器隧道中深度视觉立体里程计的概念性评估

中文摘要: MARWIN机器人在欧洲XFEL执行自主辐射监测，工作在长而单调的加速器隧道中，传统定位方法在此类环境中表现不佳。其当前导航概念结合了基于激光雷达的边缘检测、轮式/激光雷达里程计与周期性QR码参考，以及墙壁距离、旋转和纵向位置的模糊控制。虽然在预定义区域表现稳健，但这种设计缺乏应对未知几何结构和障碍物的灵活性。本文探讨了具有3D几何约束的深度视觉立体里程计（DVSO）作为重点替代方案。DVSO完全基于视觉，利用立体视差、光流和自监督学习联合估计深度和自我运动，无需标注数据。为保持全局一致性，DVSO随后可与绝对参考（如地标）或其他传感器融合。我们为加速器隧道环境提供了概念性评估，以欧洲XFEL为案例研究。预期优势包括通过立体视觉减少尺度漂移、低成本传感和可扩展的数据收集，而挑战仍然存在于低纹理表面、光照变化、计算负载以及在辐射环境下的鲁棒性。本文定义了研究议程，旨在使MARWIN能够在受限、安全关键的基础设施中实现更自主的导航。

</details>


### [8] [Exploring Diagnostic Prompting Approach for Multimodal LLM-based Visual Complexity Assessment: A Case Study of Amazon Search Result Pages](https://arxiv.org/abs/2512.00082)
*Divendar Murtadak,Yoon Kim,Trilokya Akula*

Main category: cs.CV

TL;DR: 该研究探索诊断性提示能否提升多模态大语言模型在评估亚马逊搜索结果页面视觉复杂度时的可靠性。通过对比诊断性提示与基于格式塔原则的标准提示，发现诊断性提示显著提升了预测人类复杂度判断的能力（F1分数从0.031提升至0.297），但绝对性能仍有限（Cohen's κ = 0.071）。模型更关注视觉设计元素，而人类更强调内容相似性，两者推理模式存在部分对齐。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉复杂度评估任务中的可靠性有限，特别是在亚马逊搜索结果页面这种复杂视觉界面的评估中。研究旨在探索诊断性提示方法是否能提升MLLM与人类专家判断的一致性，从而为更可靠的人机对齐评估方法提供基础。

Method: 研究使用200个亚马逊搜索结果页面，对比了两种提示方法：基于格式塔原则的标准提示和诊断性提示。诊断性提示通过引导模型分析具体视觉元素（如徽章杂乱度、产品相似性、颜色强度等）来评估复杂度。使用人类专家标注作为基准，通过F1分数、Cohen's κ等指标评估模型性能，并采用决策树分析模型推理模式。

Result: 诊断性提示显著提升了性能：F1分数从0.031提升至0.297（相对提升858%），Cohen's κ从接近0提升至0.071。决策树分析显示模型最重视视觉设计元素（徽章杂乱度重要性38.6%），而人类专家更关注内容相似性。失败案例分析揭示了MLLM在视觉感知方面的持续挑战，特别是在产品相似性和颜色强度评估方面。

Conclusion: 诊断性提示是迈向人机对齐MLLM评估的有希望的第一步，但绝对性能仍有限。模型与人类在推理模式上存在部分对齐，但持续存在的人机分歧表明需要进一步研究提示方法，并需要更大的基准数据集来实现可靠的实践部署。

Abstract: This study investigates whether diagnostic prompting can improve Multimodal Large Language Model (MLLM) reliability for visual complexity assessment of Amazon Search Results Pages (SRP). We compare diagnostic prompting with standard gestalt principles-based prompting using 200 Amazon SRP pages and human expert annotations. Diagnostic prompting showed notable improvements in predicting human complexity judgments, with F1-score increasing from 0.031 to 0.297 (+858\% relative improvement), though absolute performance remains modest (Cohen's $κ$ = 0.071). The decision tree revealed that models prioritize visual design elements (badge clutter: 38.6\% importance) while humans emphasize content similarity, suggesting partial alignment in reasoning patterns. Failure case analysis reveals persistent challenges in MLLM visual perception, particularly for product similarity and color intensity assessment. Our findings indicate that diagnostic prompting represents a promising initial step toward human-aligned MLLM-based evaluation, though failure cases with consistent human-MLLM disagreement require continued research and refinement in prompting approaches with larger ground truth datasets for reliable practical deployment.

中文标题: 探索多模态大语言模型视觉复杂度评估的诊断性提示方法：以亚马逊搜索结果页面为例

中文摘要: 本研究探讨诊断性提示是否能提高多模态大语言模型在评估亚马逊搜索结果页面视觉复杂度时的可靠性。我们使用200个亚马逊搜索结果页面和人类专家标注，比较了诊断性提示与基于格式塔原则的标准提示。诊断性提示在预测人类复杂度判断方面显示出显著改进，F1分数从0.031提升至0.297（相对提升858%），尽管绝对性能仍然有限（Cohen's κ = 0.071）。决策树分析显示模型优先考虑视觉设计元素（徽章杂乱度重要性：38.6%），而人类强调内容相似性，表明推理模式存在部分对齐。失败案例分析揭示了MLLM视觉感知的持续挑战，特别是在产品相似性和颜色强度评估方面。我们的研究结果表明，诊断性提示代表了迈向人机对齐MLLM评估的有希望的第一步，但人类与MLLM持续存在分歧的失败案例需要继续研究并改进提示方法，同时需要更大的基准数据集以实现可靠的实践部署。

</details>


### [9] [A Fast and Efficient Modern BERT based Text-Conditioned Diffusion Model for Medical Image Segmentation](https://arxiv.org/abs/2512.00084)
*Venkata Siddharth Dhara,Pawan Kumar*

Main category: cs.CV

TL;DR: FastTextDiff是一个结合ModernBERT和扩散模型的医学图像分割方法，利用文本注释减少对密集像素标签的依赖，提高分割效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需要密集的像素级标注，这成本高、耗时长且需要专业知识。现有的扩散模型虽然有效，但受限于标注需求。本研究旨在利用医学文本注释来增强语义表示，减少对密集标注的依赖。

Method: 提出FastTextDiff模型，整合ModernBERT处理临床文本注释，通过跨模态注意力机制连接文本和图像特征。使用FlashAttention 2加速训练，在MIMIC-III和MIMIC-IV数据集上进行训练。

Result: ModernBERT作为Clinical BioBERT的替代方案，在扩散分割流程中表现出更高的效率和可扩展性。FastTextDiff相比传统扩散模型在分割准确性和训练效率上都有提升。

Conclusion: 研究验证了ModernBERT在医学图像分割中的有效性，展示了多模态技术（结合文本和图像）在医学图像分析中的潜力，为减少标注依赖提供了可行方案。

Abstract: In recent times, denoising diffusion probabilistic models (DPMs) have proven effective for medical image generation and denoising, and as representation learners for downstream segmentation. However, segmentation performance is limited by the need for dense pixel-wise labels, which are expensive, time-consuming, and require expert knowledge. We propose FastTextDiff, a label-efficient diffusion-based segmentation model that integrates medical text annotations to enhance semantic representations. Our approach uses ModernBERT, a transformer capable of processing long clinical notes, to tightly link textual annotations with semantic content in medical images. Trained on MIMIC-III and MIMIC-IV, ModernBERT encodes clinical knowledge that guides cross-modal attention between visual and textual features. This study validates ModernBERT as a fast, scalable alternative to Clinical BioBERT in diffusion-based segmentation pipelines and highlights the promise of multi-modal techniques for medical image analysis. By replacing Clinical BioBERT with ModernBERT, FastTextDiff benefits from FlashAttention 2, an alternating attention mechanism, and a 2-trillion-token corpus, improving both segmentation accuracy and training efficiency over traditional diffusion-based models.

中文标题: 基于现代BERT的快速高效文本条件扩散模型用于医学图像分割

中文摘要: 近年来，去噪扩散概率模型（DPMs）在医学图像生成、去噪以及作为下游分割任务的特征学习器方面已被证明是有效的。然而，分割性能受到密集像素级标签需求的限制，这些标签昂贵、耗时且需要专业知识。我们提出了FastTextDiff，一种基于标签高效扩散的分割模型，它整合医学文本注释以增强语义表示。我们的方法使用ModernBERT（一种能够处理长临床记录的Transformer模型）来紧密连接文本注释与医学图像中的语义内容。在MIMIC-III和MIMIC-IV数据集上训练的ModernBERT编码了临床知识，指导视觉和文本特征之间的跨模态注意力。本研究验证了ModernBERT作为Clinical BioBERT在基于扩散的分割流程中的快速、可扩展替代方案，并强调了多模态技术在医学图像分析中的前景。通过用ModernBERT替换Clinical BioBERT，FastTextDiff受益于FlashAttention 2、交替注意力机制和2万亿标记的语料库，相比传统基于扩散的模型，在分割准确性和训练效率方面都有所提升。

</details>


### [10] [Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data](https://arxiv.org/abs/2512.00087)
*Ivo Bueno,Ruikun Hou,Babette Bühler,Tim Fütterer,James Drimalla,Jonathan Kyle Foster,Peter Youngs,Peter Gerjets,Ulrich Trautwein,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: 该研究探索使用AI自动识别课堂中的教学活动与话语，通过多模态数据分析为教师提供可扩展的反馈系统。视频分析使用零样本多模态LLM、微调视觉语言模型和自监督视频Transformer识别24种活动标签；文本分析使用微调Transformer分类器和基于提示的LLM识别19种话语标签。微调模型在视频和文本分析中分别达到0.577和0.460的宏F1分数，优于基于提示的方法。


<details>
  <summary>Details</summary>
Motivation: 课堂观察能为教师提供具体反馈，但现有方法依赖人工标注，资源密集且难以扩展。需要探索AI驱动的自动化分析来克服这些限制，为教师提供可扩展的反馈系统。

Method: 使用164小时视频和68节课转录的密集标注数据集，设计并行、模态特定的分析流程。视频分析评估零样本多模态LLM、微调视觉语言模型和自监督视频Transformer；文本分析微调基于Transformer的分类器并与基于提示的LLM比较。采用每标签阈值、上下文窗口和不平衡感知损失函数处理类别不平衡和多标签复杂性。

Result: 微调模型在视频活动识别中达到0.577的宏F1分数，在文本话语识别中达到0.460的宏F1分数，均显著优于基于提示的方法。视频分析中，微调视觉语言模型表现最佳；文本分析中，微调Transformer分类器优于基于提示的LLM。

Conclusion: 研究表明自动化课堂分析的可行性，为可扩展的教师反馈系统奠定了基础。微调模型在两种模态中都优于基于提示的方法，证明了针对特定任务微调的重要性。该工作为未来开发更复杂的多模态融合方法和实时反馈系统提供了基础。

Abstract: Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.

中文标题: 探索基于多模态课堂数据的教学活动与话语自动识别

中文摘要: 课堂互动观察能为教师提供具体反馈，但现有方法依赖人工标注，资源密集且难以扩展。本研究探索AI驱动的课堂录音分析，重点关注多模态教学活动与话语识别，作为可操作反馈的基础。使用包含164小时视频和68节课转录的密集标注数据集，我们设计了并行、模态特定的分析流程。对于视频，我们评估了零样本多模态LLM、微调视觉语言模型和自监督视频Transformer在24个活动标签上的表现。对于转录文本，我们微调了基于Transformer的分类器，并使用上下文化输入，与基于提示的LLM在19个话语标签上进行比较。为处理类别不平衡和多标签复杂性，我们应用了每标签阈值、上下文窗口和不平衡感知损失函数。结果显示，微调模型在视频和转录文本分析中分别达到0.577和0.460的宏F1分数，一致优于基于提示的方法。这些结果证明了自动化课堂分析的可行性，并为可扩展的教师反馈系统奠定了基础。

</details>


### [11] [TeleViT1.0: Teleconnection-aware Vision Transformers for Subseasonal to Seasonal Wildfire Pattern Forecasts](https://arxiv.org/abs/2512.00089)
*Ioannis Prapas,Nikolaos Papadopoulos,Nikolaos-Ioannis Bountos,Dimitrios Michail,Gustau Camps-Valls,Ioannis Papoutsis*

Main category: cs.CV

TL;DR: TeleViT是一种结合局部火灾因子、全球气象场和遥相关指数的视觉变换器模型，用于提前数周至数月的野火预测，在所有预测时效上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提前数周至数月预测野火对于资源规划和燃料管理至关重要，但传统方法主要依赖局部天气条件，难以捕捉影响长期预测的全球遥相关模式。

Method: 提出TeleViT模型，采用非对称标记化策略将精细尺度的局部火灾驱动因子、粗化的全球气象场和遥相关指数融合为异质标记，通过变换器编码器联合处理，再经解码器保持空间结构进行预测。

Result: 在SeasFire数据集上，TeleViT在所有预测时效（包括长达4个月）的AUPRC均优于U-Net++、ViT和气候学基线；在非洲稀树草原等季节性一致区域表现最佳，在北方和干旱地区表现较差；归因分析显示预测主要依赖局部信息，全球场和指数提供上下文。

Conclusion: 显式编码大规模地球系统上下文的架构能够有效扩展野火在次季节到季节时间尺度上的可预测性，为长期野火预测提供了新方法。

Abstract: Forecasting wildfires weeks to months in advance is difficult, yet crucial for planning fuel treatments and allocating resources. While short-term predictions typically rely on local weather conditions, long-term forecasting requires accounting for the Earth's interconnectedness, including global patterns and teleconnections. We introduce TeleViT, a Teleconnection-aware Vision Transformer that integrates (i) fine-scale local fire drivers, (ii) coarsened global fields, and (iii) teleconnection indices. This multi-scale fusion is achieved through an asymmetric tokenization strategy that produces heterogeneous tokens processed jointly by a transformer encoder, followed by a decoder that preserves spatial structure by mapping local tokens to their corresponding prediction patches.
  Using the global SeasFire dataset (2001-2021, 8-day resolution), TeleViT improves AUPRC performance over U-Net++, ViT, and climatology across all lead times, including horizons up to four months. At zero lead, TeleViT with indices and global inputs reaches AUPRC 0.630 (ViT 0.617, U-Net 0.620), at 16x8day lead (around 4 months), TeleViT variants using global input maintain 0.601-0.603 (ViT 0.582, U-Net 0.578), while surpassing the climatology (0.572) at all lead times. Regional results show the highest skill in seasonally consistent fire regimes, such as African savannas, and lower skill in boreal and arid regions. Attention and attribution analyses indicate that predictions rely mainly on local tokens, with global fields and indices contributing coarse contextual information. These findings suggest that architectures explicitly encoding large-scale Earth-system context can extend wildfire predictability on subseasonal-to-seasonal timescales.

中文标题: TeleViT1.0：用于次季节到季节尺度野火模式预测的遥相关感知视觉变换器

中文摘要: 提前数周至数月预测野火具有挑战性，但对于规划燃料处理和分配资源至关重要。短期预测通常依赖于当地天气条件，而长期预测需要考虑地球的相互关联性，包括全球模式和遥相关。我们提出了TeleViT，一种遥相关感知视觉变换器，它整合了：(i)精细尺度的局部火灾驱动因子，(ii)粗化的全球场，以及(iii)遥相关指数。这种多尺度融合通过非对称标记化策略实现，该策略生成异质标记，由变换器编码器联合处理，然后通过解码器将局部标记映射到相应的预测斑块，从而保持空间结构。

使用全球SeasFire数据集（2001-2021年，8天分辨率），TeleViT在所有预测时效（包括长达四个月的预测期）上的AUPRC性能均优于U-Net++、ViT和气候学方法。在零提前期，TeleViT结合指数和全球输入达到AUPRC 0.630（ViT 0.617，U-Net 0.620）；在16x8天提前期（约4个月），使用全球输入的TeleViT变体保持0.601-0.603（ViT 0.582，U-Net 0.578），同时在所有提前期均超过气候学方法（0.572）。区域结果显示，在季节性一致的火灾制度（如非洲稀树草原）中技能最高，而在北方和干旱地区技能较低。注意力和归因分析表明，预测主要依赖于局部标记，而全球场和指数提供粗略的上下文信息。这些发现表明，显式编码大规模地球系统上下文的架构可以扩展次季节到季节时间尺度上的野火可预测性。

</details>


### [12] [Deep Filament Extraction for 3D Concrete Printing](https://arxiv.org/abs/2512.00091)
*Karam Mawas,Mehdi Maboudi,Pedro Achanccaray,Markus Gerke*

Main category: cs.CV

TL;DR: 本文提出了一种用于3D混凝土打印的自动化细丝质量控制系统，适用于挤出式和喷射混凝土打印方法，支持多种传感器数据采集，可用于在线和打印后质量控制。


<details>
  <summary>Details</summary>
Motivation: 3D混凝土打印中细丝作为定义打印对象的基本结构，其几何质量对最终产品的质量至关重要。当前需要一种自动化、传感器无关的质量控制方法来确保打印质量。

Method: 开发了一种独立于传感器的自动化质量控制工作流程，支持相机、结构光系统或地面激光扫描仪等多种数据采集设备，适用于新鲜和固化状态的混凝土材料。

Result: 该方法实现了对挤出式和喷射混凝土3D打印中细丝几何质量的自动化评估，能够进行在线和打印后质量控制。

Conclusion: 提出的深度细丝提取方法为3D混凝土打印提供了一种有效的自动化质量控制解决方案，具有传感器无关性和材料状态适应性，可显著提高打印质量控制的效率和准确性。

Abstract: The architecture, engineering and construction (AEC) industry is constantly evolving to meet the demand for sustainable and effective design and construction of the built environment. In the literature, two primary deposition techniques for large-scale 3D concrete printing (3DCP) have been described, namely extrusion-based (Contour Crafting-CC) and shotcrete 3D printing (SC3DP) methods. The deposition methods use a digitally controlled nozzle to print material layer by layer. The continuous flow of concrete material used to create the printed structure is called a filament or layer. As these filaments are the essential structure defining the printed object, the filaments' geometry quality control is crucial. This paper presents an automated procedure for quality control (QC) of filaments in extrusion-based and SC3DP printing methods. The paper also describes a workflow that is independent of the sensor used for data acquisition, such as a camera, a structured light system (SLS) or a terrestrial laser scanner (TLS). This method can be used with materials in either the fresh or cured state. Thus, it can be used for online and post-printing QC.

中文标题: 用于3D混凝土打印的深度细丝提取方法

中文摘要: 建筑、工程和施工（AEC）行业不断发展，以满足可持续和有效的建筑环境设计与施工需求。文献中描述了两种主要的3D混凝土打印（3DCP）沉积技术，即基于挤出的（轮廓成型-CC）和喷射混凝土3D打印（SC3DP）方法。这些沉积方法使用数字控制的喷嘴逐层打印材料。用于创建打印结构的混凝土材料的连续流动被称为细丝或层。由于这些细丝是定义打印对象的基本结构，因此细丝几何质量的控制至关重要。本文提出了一种用于挤出式和SC3DP打印方法中细丝质量控制的自动化程序。本文还描述了一种独立于数据采集传感器（如相机、结构光系统（SLS）或地面激光扫描仪（TLS））的工作流程。该方法可用于新鲜或固化状态的材料。因此，它可用于在线和打印后质量控制。

</details>


### [13] [TinyViT: Field Deployable Transformer Pipeline for Solar Panel Surface Fault and Severity Screening](https://arxiv.org/abs/2512.00117)
*Ishwaryah Pandiarajan,Mohamed Mansoor Roomi Sindha,Uma Maheswari Pandyan,Sharafia N*

Main category: cs.CV

TL;DR: TinyViT是一个紧凑的深度学习管道，仅使用可见光图像就能检测太阳能电池板的七种表面故障并评估其严重程度，无需昂贵的多模态传感器，适合现场部署。


<details>
  <summary>Details</summary>
Motivation: 太阳能光伏资产需要定期检测表面故障以确保持续运行，但现有的多模态成像方法（如电致发光、红外）存在物流和经济障碍，不适合常规农场级部署。需要一种仅使用可见光图像就能实现准确故障检测和严重程度评估的解决方案。

Method: TinyViT是一个紧凑的管道，集成了三个主要组件：1）基于Transformer的图像分割模块，2）光谱空间特征工程，3）集成回归模型。系统接收消费级彩色相机拍摄的太阳能电池板图像，首先进行分割，然后提取特征，最后分类七种表面故障并评估严重程度。

Result: 在真实公共数据集上的实验表明，TinyViT在分类和回归任务上都取得了与专业方法相竞争的性能。系统能够准确识别七种细微的表面故障，并生成可操作的严重程度等级，同时保持了良好的可解释性。

Conclusion: TinyViT通过结合深度学习和经典机器学习，仅使用可见光图像就能实现太阳能电池板表面故障的准确检测和严重程度评估。该方法消除了对昂贵传感器的依赖，为资源有限的太阳能安装提供了经济实惠、可扩展的维护解决方案，推动了太阳能健康监测向普遍现场可访问性发展。

Abstract: Sustained operation of solar photovoltaic assets hinges on accurate detection and prioritization of surface faults across vast, geographically distributed modules. While multi modal imaging strategies are popular, they introduce logistical and economic barriers for routine farm level deployment. This work demonstrates that deep learning and classical machine learning may be judiciously combined to achieve robust surface anomaly categorization and severity estimation from planar visible band imagery alone. We introduce TinyViT which is a compact pipeline integrating Transformer based segmentation, spectral-spatial feature engineering, and ensemble regression. The system ingests consumer grade color camera mosaics of PV panels, classifies seven nuanced surface faults, and generates actionable severity grades for maintenance triage. By eliminating reliance on electroluminescence or IR sensors, our method enables affordable, scalable upkeep for resource limited installations, and advances the state of solar health monitoring toward universal field accessibility. Experiments on real public world datasets validate both classification and regression sub modules, achieving accuracy and interpretability competitive with specialized approaches.

中文标题: TinyViT：用于太阳能电池板表面故障及严重程度筛查的现场可部署Transformer管道

中文摘要: 太阳能光伏资产的持续运行依赖于在广泛地理分布的模块上准确检测和优先处理表面故障。虽然多模态成像策略很流行，但它们为常规农场级部署引入了物流和经济障碍。这项工作表明，深度学习和经典机器学习可以明智地结合，仅从平面可见光波段图像中实现稳健的表面异常分类和严重程度估计。我们引入了TinyViT，这是一个紧凑的管道，集成了基于Transformer的分割、光谱空间特征工程和集成回归。该系统接收消费级彩色相机拍摄的PV面板马赛克图像，分类七种细微的表面故障，并为维护分类生成可操作的严重程度等级。通过消除对电致发光或红外传感器的依赖，我们的方法为资源有限的安装提供了经济实惠、可扩展的维护方案，并将太阳能健康监测推向普遍现场可访问性。在真实公共世界数据集上的实验验证了分类和回归子模块，实现了与专业方法相竞争的准确性和可解释性。

</details>


### [14] [Mammo-FM: Breast-specific foundational model for Integrated Mammographic Diagnosis, Prognosis, and Reporting](https://arxiv.org/abs/2512.00198)
*Shantanu Ghosh,Vedant Parthesh Joshi,Rayan Syed,Aya Kassem,Abhishek Varshney,Payel Basak,Weicheng Dai,Judy Wawira Gichoya,Hari M. Trivedi,Imon Banerjee,Shyam Visweswaran,Clare B. Poynton,Kayhan Batmanghelich*

Main category: cs.CV

TL;DR: Mammo-FM是首个专门针对乳腺X线摄影的基础模型，在14万患者数据上预训练，统一支持诊断、定位、报告生成和风险预后四大临床任务，性能优于通用基础模型。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性主要死因，现有AI模型多为任务特定型，缺乏统一框架。需要开发专门针对乳腺X线摄影的基础模型，整合诊断、预后、报告等核心临床任务，提高临床实用性和可解释性。

Method: 在四个美国机构的14万患者、82万张乳腺X线照片上进行预训练，构建图像-文本对齐的乳腺特异性基础模型。模型在原生分辨率图像上运行，参数仅为通用基础模型的三分之一，通过统一框架支持多任务学习。

Result: 在多个公共和私人基准测试中，Mammo-FM在诊断、预后和报告生成任务上均优于最先进的通用基础模型，尽管参数更少。模型展示了卓越的分布内和分布外泛化能力，同时提供视觉和文本可解释性。

Conclusion: Mammo-FM证明了领域特定基础模型在临床医学中的价值，通过围绕完整临床任务谱设计，实现了高效的多任务整合。该模型为乳腺成像提供了统一、可解释的AI解决方案，强调了领域对齐评估的重要性。

Abstract: Breast cancer is one of the leading causes of death among women worldwide. We introduce Mammo-FM, the first foundation model specifically for mammography, pretrained on the largest and most diverse dataset to date - 140,677 patients (821,326 mammograms) across four U.S. institutions. Mammo-FM provides a unified foundation for core clinical tasks in breast imaging, including cancer diagnosis, pathology localization, structured report generation, and cancer risk prognosis within a single framework. Its alignment between images and text enables both visual and textual interpretability, improving transparency and clinical auditability, which are essential for real-world adoption. We rigorously evaluate Mammo-FM across diagnosis, prognosis, and report-generation tasks in in- and out-of-distribution datasets. Despite operating on native-resolution mammograms and using only one-third of the parameters of state-of-the-art generalist FMs, Mammo-FM consistently outperforms them across multiple public and private benchmarks. These results highlight the efficiency and value of domain-specific foundation models designed around the full spectrum of tasks within a clinical domain and emphasize the importance of rigorous, domain-aligned evaluation.

中文标题: Mammo-FM：用于集成乳腺X线诊断、预后和报告的乳腺特异性基础模型

中文摘要: 乳腺癌是全球女性死亡的主要原因之一。我们介绍了Mammo-FM，这是首个专门针对乳腺X线摄影的基础模型，在迄今为止最大、最多样化的数据集上进行了预训练——涵盖美国四个机构的140,677名患者（821,326张乳腺X线照片）。Mammo-FM为乳腺成像中的核心临床任务提供了统一的基础，包括癌症诊断、病理定位、结构化报告生成和癌症风险预后，所有这些都在单一框架内完成。其图像和文本之间的对齐实现了视觉和文本的可解释性，提高了透明度和临床可审计性，这对于实际应用至关重要。我们在分布内和分布外数据集上对Mammo-FM在诊断、预后和报告生成任务方面进行了严格评估。尽管在原生分辨率乳腺X线照片上运行且仅使用最先进通用基础模型三分之一参数的情况下，Mammo-FM在多个公共和私人基准测试中始终优于这些模型。这些结果突显了围绕临床领域内完整任务谱设计的领域特定基础模型的效率和价值，并强调了严格、领域对齐评估的重要性。

</details>


### [15] [ReactionMamba: Generating Short &Long Human Reaction Sequences](https://arxiv.org/abs/2512.00208)
*Hajra Anwar Beg,Baptiste Chopin,Hao Tang,Mohamed Daoudi*

Main category: cs.CV

TL;DR: ReactionMamba是一个用于生成长短3D人体反应动作序列的新框架，结合了运动VAE编码和Mamba状态空间模型解码，在真实感、多样性和长序列生成方面表现优异，同时大幅提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成长序列3D人体反应动作时面临挑战，特别是在保持时间一致性和处理复杂动作（如舞蹈、武术）方面存在局限性。需要一种既能生成简单短序列又能处理复杂长序列的高效框架。

Method: ReactionMamba采用双阶段架构：1）使用运动VAE对动作进行高效编码；2）基于Mamba的状态空间模型解码生成时间一致的反应动作序列。这种设计允许框架处理不同长度的动作序列。

Result: 在NTU120-AS、Lindy Hop和InterX三个数据集上的评估显示，ReactionMamba在真实感、多样性和长序列生成方面与InterFormer、ReMoS、Ready-to-React等方法相比具有竞争力，同时在推理速度上实现了显著提升。

Conclusion: ReactionMamba成功解决了长短3D人体反应动作序列生成问题，通过结合运动VAE和Mamba模型，在保持高质量生成的同时大幅提升了计算效率，为复杂人体动作生成提供了有效解决方案。

Abstract: We present ReactionMamba, a novel framework for generating long 3D human reaction motions. Reaction-Mamba integrates a motion VAE for efficient motion encoding with Mamba-based state-space models to decode temporally consistent reactions. This design enables ReactionMamba to generate both short sequences of simple motions and long sequences of complex motions, such as dance and martial arts. We evaluate ReactionMamba on three datasets--NTU120-AS, Lindy Hop, and InterX--and demonstrate competitive performance in terms of realism, diversity, and long-sequence generation compared to previous methods, including InterFormer, ReMoS, and Ready-to-React, while achieving substantial improvements in inference speed.

中文标题: ReactionMamba：生成长短人体反应序列

中文摘要: 我们提出了ReactionMamba，一个用于生成长序列3D人体反应动作的新框架。ReactionMamba集成了用于高效动作编码的运动VAE和基于Mamba的状态空间模型来解码时间一致的反应动作。这种设计使ReactionMamba能够生成简单动作的短序列和复杂动作（如舞蹈和武术）的长序列。我们在三个数据集（NTU120-AS、Lindy Hop和InterX）上评估了ReactionMamba，并在真实感、多样性和长序列生成方面展示了与先前方法（包括InterFormer、ReMoS和Ready-to-React）相比的竞争性能，同时在推理速度上实现了显著提升。

</details>


### [16] [DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation](https://arxiv.org/abs/2512.00226)
*Zirui Wang,Tao Zhang*

Main category: cs.CV

TL;DR: DenseScan是一个通过多视角2D图像和MLLMs自动生成密集语义标注的3D场景理解数据集，提供对象级描述和场景级问题，显著提升3D环境中的理解和问答能力。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景理解数据集主要关注几何和实例信息，缺乏进行复杂视觉语言任务所需的丰富语义标注。高质量标注数据对于推动3D理解社区发展至关重要，但传统手动标注方法成本高且难以扩展。

Method: 开发了一个自动化标注流程：1）利用多视角2D图像捕获场景信息；2）使用多模态大语言模型（MLLMs）生成密集的对象级描述；3）通过场景上下文扩展标注，生成整合对象属性、空间关系和场景上下文的高级问题。

Result: 实验结果表明，与传统标注流程相比，DenseScan显著提升了3D环境中的对象级理解和问答性能。该数据集支持从详细视觉语言导航到交互式问答的多种下游任务。

Conclusion: DenseScan通过将几何细节与语义丰富性相结合，为3D场景理解提供了更全面的标注框架。该数据集和标注流程的发布将促进机器人、增强现实等领域的未来研究和应用，催化3D理解的新发展途径。

Abstract: 3D understanding is a key capability for real-world AI assistance. High-quality data plays an important role in driving the development of the 3D understanding community. Current 3D scene understanding datasets often provide geometric and instance-level information, yet they lack the rich semantic annotations necessary for nuanced visual-language tasks.In this work, we introduce DenseScan, a novel dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs). Our approach enables dense captioning of scene elements, ensuring comprehensive object-level descriptions that capture context-sensitive details. Furthermore, we extend these annotations through scenario-based question generation, producing high-level queries that integrate object properties, spatial relationships, and scene context. By coupling geometric detail with semantic richness, DenseScan broadens the range of downstream tasks, from detailed visual-language navigation to interactive question answering. Experimental results demonstrate that our method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines. We release both the annotated dataset and our annotation pipeline to facilitate future research and applications in robotics, augmented reality, and beyond. Through DenseScan, we aim to catalyze new avenues in 3D scene understanding, allowing researchers and practitioners to tackle the complexities of real-world environments with richer, more contextually aware annotations.

中文标题: DenseScan：通过2D密集标注推进3D场景理解

中文摘要: 3D理解是现实世界人工智能辅助的关键能力。高质量数据在推动3D理解社区发展中起着重要作用。当前的3D场景理解数据集通常提供几何和实例级信息，但缺乏进行细致视觉语言任务所需的丰富语义标注。在这项工作中，我们引入了DenseScan，这是一个通过自动化流程生成详细多级描述的新型数据集，该流程利用多视角2D图像和多模态大语言模型（MLLMs）。我们的方法实现了场景元素的密集标注，确保捕获上下文敏感细节的全面对象级描述。此外，我们通过基于场景的问题生成扩展这些标注，产生整合对象属性、空间关系和场景上下文的高级查询。通过将几何细节与语义丰富性相结合，DenseScan拓宽了下游任务的范围，从详细的视觉语言导航到交互式问答。实验结果表明，与传统标注流程相比，我们的方法显著增强了3D环境中的对象级理解和问答性能。我们发布了标注数据集和我们的标注流程，以促进机器人、增强现实等领域未来的研究和应用。通过DenseScan，我们旨在催化3D场景理解的新途径，让研究人员和从业者能够用更丰富、更具上下文感知的标注来应对现实世界环境的复杂性。

</details>


### [17] [BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud](https://arxiv.org/abs/2512.01366)
*Yunzhe Li,Jiajun Yan,Yuzhou Wei,Kechen Liu,Yize Zhao,Chong Zhang,Hongzi Zhu,Li Lu,Shan Chang,Minyi Guo*

Main category: cs.CV

TL;DR: BlinkBud是一个利用单个耳塞和手机检测用户后方危险物体的系统，通过少量采样图像进行3D物体跟踪，结合卡尔曼滤波和强化学习优化采样策略，在低功耗下实现高精度危险检测。


<details>
  <summary>Details</summary>
Motivation: 行人和骑行者难以察觉从后方高速接近的车辆，这对道路安全构成严重威胁。现有解决方案要么依赖昂贵的传感器设备，要么功耗过高，难以在移动设备上实时运行。

Method: 1. 使用单个耳塞和配对的手机系统架构；2. 基于少量采样相机图像的3D物体跟踪算法；3. 卡尔曼滤波进行轨迹估计；4. 强化学习优化图像采样策略；5. 利用头部姿态估计校正物体深度和坐标系对齐。

Result: 1. 极低功耗：耳塞平均29.8 mW，手机平均702.6 mW；2. 高检测精度：平均误报率4.90%，平均漏报率1.47%；3. 原型系统在真实世界实验中表现良好。

Conclusion: BlinkBud系统成功实现了在低功耗移动设备上准确检测用户后方危险物体的目标，为行人和骑行者的道路安全提供了一种实用且高效的解决方案。

Abstract: Failing to be aware of speeding vehicles approaching from behind poses a huge threat to the road safety of pedestrians and cyclists. In this paper, we propose BlinkBud, which utilizes a single earbud and a paired phone to online detect hazardous objects approaching from behind of a user. The core idea is to accurately track visually identified objects utilizing a small number of sampled camera images taken from the earbud. To minimize the power consumption of the earbud and the phone while guaranteeing the best tracking accuracy, a novel 3D object tracking algorithm is devised, integrating both a Kalman filter based trajectory estimation scheme and an optimal image sampling strategy based on reinforcement learning. Moreover, the impact of constant user head movements on the tracking accuracy is significantly eliminated by leveraging the estimated pitch and yaw angles to correct the object depth estimation and align the camera coordinate system to the user's body coordinate system, respectively. We implement a prototype BlinkBud system and conduct extensive real-world experiments. Results show that BlinkBud is lightweight with ultra-low mean power consumptions of 29.8 mW and 702.6 mW on the earbud and smartphone, respectively, and can accurately detect hazards with a low average false positive ratio (FPR) and false negative ratio (FNR) of 4.90% and 1.47%, respectively.

中文标题: BlinkBud：通过单耳塞采样单目3D检测实现后方危险检测

中文摘要: 未能意识到从后方高速接近的车辆对行人和骑行者的道路安全构成巨大威胁。本文提出BlinkBud系统，利用单个耳塞和配对的手机在线检测用户后方接近的危险物体。核心思想是通过耳塞拍摄的少量采样相机图像准确跟踪视觉识别的物体。为了最小化耳塞和手机的功耗同时保证最佳跟踪精度，设计了一种新颖的3D物体跟踪算法，集成了基于卡尔曼滤波的轨迹估计方案和基于强化学习的最优图像采样策略。此外，通过利用估计的俯仰角和偏航角分别校正物体深度估计并将相机坐标系对齐到用户身体坐标系，显著消除了持续用户头部运动对跟踪精度的影响。我们实现了BlinkBud原型系统并进行了广泛的真实世界实验。结果表明，BlinkBud轻量级，耳塞和智能手机的平均功耗分别为29.8 mW和702.6 mW，能够准确检测危险，平均误报率（FPR）和漏报率（FNR）分别为4.90%和1.47%。

</details>


### [18] [UniDiff: Parameter-Efficient Adaptation of Diffusion Models for Land Cover Classification with Multi-Modal Remotely Sensed Imagery and Sparse Annotations](https://arxiv.org/abs/2512.00261)
*Yuzhen Hu,Saurabh Prasad*

Main category: cs.CV

TL;DR: UniDiff是一个参数高效的框架，通过仅使用目标域数据，将单个ImageNet预训练的扩散模型适配到多种遥感模态（如高光谱和合成孔径雷达），解决了稀疏标注下的多模态遥感分类问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏标注严重限制了多模态遥感的发展。即使像MSFMamba这样的最新监督方法也受限于标注数据的可用性，限制了实际部署。ImageNet预训练模型提供了丰富的视觉表示，但在没有大量标注数据集的情况下，将其适配到异质模态（如高光谱和合成孔径雷达）仍然具有挑战性。

Method: UniDiff结合了：1）基于FiLM的时间步-模态条件调节；2）仅适配约5%参数的参数高效适配；3）伪RGB锚定技术以保持预训练表示并防止灾难性遗忘。该设计能够在稀疏标注下从遥感数据中有效提取特征。

Result: 在两个已建立的多模态基准数据集上的结果表明，预训练扩散模型的无监督适配有效缓解了标注约束，并实现了多模态遥感数据的有效融合。

Conclusion: UniDiff框架通过参数高效的方式将预训练扩散模型适配到多模态遥感数据，在稀疏标注条件下实现了有效的特征提取和多模态融合，为实际部署提供了可行的解决方案。

Abstract: Sparse annotations fundamentally constrain multimodal remote sensing: even recent state-of-the-art supervised methods such as MSFMamba are limited by the availability of labeled data, restricting their practical deployment despite architectural advances. ImageNet-pretrained models provide rich visual representations, but adapting them to heterogeneous modalities such as hyperspectral imaging (HSI) and synthetic aperture radar (SAR) without large labeled datasets remains challenging. We propose UniDiff, a parameter-efficient framework that adapts a single ImageNet-pretrained diffusion model to multiple sensing modalities using only target-domain data. UniDiff combines FiLM-based timestep-modality conditioning, parameter-efficient adaptation of approximately 5% of parameters, and pseudo-RGB anchoring to preserve pre-trained representations and prevent catastrophic forgetting. This design enables effective feature extraction from remote sensing data under sparse annotations. Our results with two established multi-modal benchmarking datasets demonstrate that unsupervised adaptation of a pre-trained diffusion model effectively mitigates annotation constraints and achieves effective fusion of multi-modal remotely sensed data.

中文标题: UniDiff：基于扩散模型的参数高效适配方法，用于稀疏标注下的多模态遥感影像土地覆盖分类

中文摘要: 稀疏标注从根本上限制了多模态遥感：即使是最近最先进的监督方法如MSFMamba也受限于标注数据的可用性，尽管架构有所进步，但实际部署仍然受限。ImageNet预训练模型提供了丰富的视觉表示，但在没有大量标注数据集的情况下，将其适配到异质模态如高光谱成像（HSI）和合成孔径雷达（SAR）仍然具有挑战性。我们提出了UniDiff，一个参数高效的框架，仅使用目标域数据将单个ImageNet预训练的扩散模型适配到多种感知模态。UniDiff结合了基于FiLM的时间步-模态条件调节、约5%参数的参数高效适配以及伪RGB锚定技术，以保持预训练表示并防止灾难性遗忘。该设计能够在稀疏标注下从遥感数据中有效提取特征。我们在两个已建立的多模态基准数据集上的结果表明，预训练扩散模型的无监督适配有效缓解了标注约束，并实现了多模态遥感数据的有效融合。

</details>


### [19] [SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation](https://arxiv.org/abs/2512.01960)
*Zisu Li,Hengye Lyu,Jiaxin Shi,Yufeng Zeng,Mingming Fan,Hanwang Zhang,Chen Liang*

Main category: cs.CV

TL;DR: SpriteHand是一个实时自回归视频生成框架，能够合成各种手物交互效果，支持非刚性物体和复杂交互，在单个GPU上实现18 FPS的实时生成。


<details>
  <summary>Details</summary>
Motivation: 传统物理引擎和模拟方法在建模复杂手物交互方面存在局限性，特别是对于非刚性物体、可变形材料、关节结构等复杂实体。现有方法依赖预定义模型和手势，无法适应多样化的交互场景。

Method: 采用自回归视频生成框架，输入静态物体图像和手部交互视频流，通过因果推理架构生成交互效果。使用混合后训练方法提升视觉真实性和时间连贯性。

Result: 1.3B参数模型在NVIDIA RTX 5090 GPU上实现约18 FPS、640x368分辨率的实时生成，延迟约150毫秒，支持超过1分钟的连续输出。在视觉质量、物理合理性和交互保真度方面优于基线方法。

Conclusion: SpriteHand成功解决了复杂手物交互的实时合成问题，特别是对于传统物理引擎难以处理的非刚性物体和复杂交互场景，为实时交互应用提供了有效的解决方案。

Abstract: Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.

中文标题: SpriteHand：基于自回归视频生成的实时多功能手物交互系统

中文摘要: 建模和合成复杂的手物交互仍然是一个重大挑战，即使对于最先进的物理引擎也是如此。传统的基于模拟的方法依赖于明确定义的刚性物体模型和预定义的手势，这使得它们无法捕捉与非刚性或关节实体（如可变形织物、弹性材料、铰链结构、毛绒表面，甚至生物）的动态交互。在本文中，我们提出了SpriteHand，一个用于实时合成各种物体类型和运动模式的手物交互视频的自回归视频生成框架。SpriteHand以静态物体图像和手部想象与嵌入真实场景中的虚拟物体交互的视频流作为输入，并实时生成相应的手物交互效果。我们的模型采用因果推理架构进行自回归生成，并利用混合后训练方法来增强视觉真实性和时间连贯性。我们的1.3B模型支持在单个NVIDIA RTX 5090 GPU上以约18 FPS和640x368分辨率进行实时流式生成，延迟约为150毫秒，并能连续输出超过一分钟。实验表明，与生成式和基于引擎的基线方法相比，我们的方法在视觉质量、物理合理性和交互保真度方面表现更优。

</details>


### [20] [USB: Unified Synthetic Brain Framework for Bidirectional Pathology-Healthy Generation and Editing](https://arxiv.org/abs/2512.00269)
*Jun Wang,Peirong Liu*

Main category: cs.CV

TL;DR: USB是一个统一的合成大脑框架，能够双向生成和编辑病理与健康脑图像，通过配对扩散机制建模病变与解剖结构的联合分布，解决了配对病理-健康数据难以获取的问题。


<details>
  <summary>Details</summary>
Motivation: 理解病理与健康脑结构之间的关系对神经影像学至关重要，但配对的病理-健康数据极难获取，现有方法多为领域特定且独立处理病理和健康图像建模，缺乏统一框架。

Method: USB采用端到端框架，通过配对扩散机制建模病变与脑解剖结构的联合分布，实现病理和健康图像的双向生成，并引入一致性引导算法在双向编辑中保持解剖一致性和病变对应关系。

Result: 在六个公共脑MRI数据集（包括健康对照、中风和阿尔茨海默病患者）上的广泛实验表明，USB能够产生多样且逼真的结果，并建立了首个统一的脑图像生成和编辑基准。

Conclusion: USB作为首个统一病理与健康脑图像双向生成和编辑的框架，为可扩展的数据集创建和稳健的神经影像分析开辟了新机会。

Abstract: Understanding the relationship between pathological and healthy brain structures is fundamental to neuroimaging, connecting disease diagnosis and detection with modeling, prediction, and treatment planning. However, paired pathological-healthy data are extremely difficult to obtain, as they rely on pre- and post-treatment imaging, constrained by clinical outcomes and longitudinal data availability. Consequently, most existing brain image generation and editing methods focus on visual quality yet remain domain-specific, treating pathological and healthy image modeling independently. We introduce USB (Unified Synthetic Brain), the first end-to-end framework that unifies bidirectional generation and editing of pathological and healthy brain images. USB models the joint distribution of lesions and brain anatomy through a paired diffusion mechanism and achieves both pathological and healthy image generation. A consistency guidance algorithm further preserves anatomical consistency and lesion correspondence during bidirectional pathology-healthy editing. Extensive experiments on six public brain MRI datasets including healthy controls, stroke, and Alzheimer's patients, demonstrate USB's ability to produce diverse and realistic results. By establishing the first unified benchmark for brain image generation and editing, USB opens opportunities for scalable dataset creation and robust neuroimaging analysis. Code is available at https://github.com/jhuldr/USB.

中文标题: USB：用于双向病理-健康生成和编辑的统一合成大脑框架

中文摘要: 理解病理与健康脑结构之间的关系是神经影像学的基础，连接疾病诊断和检测与建模、预测和治疗规划。然而，配对的病理-健康数据极难获取，因为它们依赖于治疗前后的影像，受到临床结果和纵向数据可用性的限制。因此，大多数现有的脑图像生成和编辑方法侧重于视觉质量，但仍保持领域特定性，独立处理病理和健康图像建模。我们引入了USB（统一合成大脑），这是首个端到端框架，统一了病理和健康脑图像的双向生成和编辑。USB通过配对扩散机制建模病变和脑解剖结构的联合分布，并实现病理和健康图像生成。一致性引导算法进一步在双向病理-健康编辑中保持解剖一致性和病变对应关系。在六个公共脑MRI数据集（包括健康对照、中风和阿尔茨海默病患者）上的广泛实验证明了USB产生多样且逼真结果的能力。通过建立首个统一的脑图像生成和编辑基准，USB为可扩展的数据集创建和稳健的神经影像分析开辟了机会。代码可在https://github.com/jhuldr/USB获取。

</details>


### [21] [HIMOSA: Efficient Remote Sensing Image Super-Resolution with Hierarchical Mixture of Sparse Attention](https://arxiv.org/abs/2512.00275)
*Yi Liu,Yi Wan,Xinyi Liu,Qiong Wu,Panwang Xia,Xuejun Huang,Yongjun Zhang*

Main category: cs.CV

TL;DR: HIMOSA是一个轻量级的遥感图像超分辨率框架，通过内容感知稀疏注意力机制和分层窗口扩展，在保持高性能的同时实现快速推理。


<details>
  <summary>Details</summary>
Motivation: 在遥感应用中（如灾害检测和响应），实时效率和模型轻量化至关重要。现有的遥感图像超分辨率方法往往需要在模型性能和计算效率之间进行权衡。

Method: HIMOSA利用遥感图像固有的冗余性，引入内容感知稀疏注意力机制，实现快速推理的同时保持强大的重建性能。此外，通过分层窗口扩展有效利用遥感图像中的多尺度重复模式，并通过调整注意力的稀疏性降低计算复杂度。

Result: 在多个遥感数据集上的大量实验表明，该方法在保持计算效率的同时实现了最先进的性能。

Conclusion: HIMOSA是一个高效的遥感图像超分辨率框架，通过创新的稀疏注意力机制和分层设计，成功解决了性能与效率之间的权衡问题。

Abstract: In remote sensing applications, such as disaster detection and response, real-time efficiency and model lightweighting are of critical importance. Consequently, existing remote sensing image super-resolution methods often face a trade-off between model performance and computational efficiency. In this paper, we propose a lightweight super-resolution framework for remote sensing imagery, named HIMOSA. Specifically, HIMOSA leverages the inherent redundancy in remote sensing imagery and introduces a content-aware sparse attention mechanism, enabling the model to achieve fast inference while maintaining strong reconstruction performance. Furthermore, to effectively leverage the multi-scale repetitive patterns found in remote sensing imagery, we introduce a hierarchical window expansion and reduce the computational complexity by adjusting the sparsity of the attention. Extensive experiments on multiple remote sensing datasets demonstrate that our method achieves state-of-the-art performance while maintaining computational efficiency.

中文标题: HIMOSA：基于分层混合稀疏注意力的高效遥感图像超分辨率

中文摘要: 在遥感应用中，如灾害检测和响应，实时效率和模型轻量化至关重要。因此，现有的遥感图像超分辨率方法往往面临模型性能与计算效率之间的权衡。在本文中，我们提出了一个轻量级的遥感图像超分辨率框架，命名为HIMOSA。具体来说，HIMOSA利用遥感图像固有的冗余性，引入内容感知稀疏注意力机制，使模型能够在保持强大重建性能的同时实现快速推理。此外，为了有效利用遥感图像中的多尺度重复模式，我们引入了分层窗口扩展，并通过调整注意力的稀疏性降低计算复杂度。在多个遥感数据集上的大量实验表明，我们的方法在保持计算效率的同时实现了最先进的性能。

</details>


### [22] [Rethinking Lung Cancer Screening: AI Nodule Detection and Diagnosis Outperforms Radiologists, Leading Models, and Standards Beyond Size and Growth](https://arxiv.org/abs/2512.00281)
*Sylvain Bodard,Pierre Baudot,Benjamin Renoust,Charles Voyton,Gwendoline De Bie,Ezequiel Geremia,Van-Khoa Le,Danny Francis,Pierre-Henri Siot,Yousra Haddou,Vincent Bobin,Jean-Christophe Brisset,Carey C. Thomson,Valerie Bourdes,Benoit Huet*

Main category: cs.CV

TL;DR: AI系统在肺结节检测和恶性诊断上全面超越放射科医生、现有标准和领先AI模型，通过集成浅层深度学习与特征模型，在2.5万+扫描中实现0.98 AUC，假阳性率0.5/扫描，灵敏度99.3%，尤其擅长早期癌症和生长缓慢结节。


<details>
  <summary>Details</summary>
Motivation: 当前肺癌筛查依赖结节大小和生长速度，导致诊断延迟；现有AI模型在数据集规模和可解释性方面存在局限；需要更早、更准确的肺癌检测系统。

Method: 设计集成系统，结合浅层深度学习模型和基于特征的专门模型；在25,709次低剂量CT扫描（含69,449个标注结节）上进行训练和评估；直接进行结节级检测和恶性诊断。

Result: 内部验证AUC 0.98，独立队列0.945；每扫描0.5个假阳性，灵敏度99.3%；在所有结节大小和分期上超越放射科医生，尤其擅长1期癌症；在所有生长指标上超越，包括最不准确的体积倍增时间；在不确定和缓慢生长结节诊断上比放射科医生早达一年。

Conclusion: 该AI系统重新定义了肺癌筛查范式，通过同时进行检测和诊断，克服了传统依赖大小和生长速度的局限性，为早期肺癌检测提供了更准确、更及时的解决方案。

Abstract: Early detection of malignant lung nodules is critical, but its dependence on size and growth in screening inherently delays diagnosis. We present an AI system that redefines lung cancer screening by performing both detection and malignancy diagnosis directly at the nodule level on low-dose CT scans. To address limitations in dataset scale and explainability, we designed an ensemble of shallow deep learning and feature-based specialized models. Trained and evaluated on 25,709 scans with 69,449 annotated nodules, the system outperforms radiologists, Lung-RADS, and leading AI models (Sybil, Brock, Google, Kaggle). It achieves an area under the receiver operating characteristic curve (AUC) of 0.98 internally and 0.945 on an independent cohort. With 0.5 false positives per scan at 99.3\% sensitivity, it addresses key barriers to AI adoption. Critically, it outperforms radiologists across all nodule sizes and stages, excelling in stage 1 cancers, and all growth-based metrics, including the least accurate: Volume-Doubling Time. It also surpasses radiologists by up to one year in diagnosing indeterminate and slow-growing nodules.

中文标题: 重新思考肺癌筛查：AI结节检测与诊断超越放射科医生、领先模型及超越大小和生长的标准

中文摘要: 恶性肺结节的早期检测至关重要，但其在筛查中对大小和生长的依赖本质上延迟了诊断。我们提出了一种AI系统，通过在低剂量CT扫描中直接在结节级别执行检测和恶性诊断，重新定义了肺癌筛查。为了解决数据集规模和可解释性的限制，我们设计了一个集成系统，结合浅层深度学习和基于特征的专门模型。在25,709次扫描（含69,449个标注结节）上进行训练和评估后，该系统超越了放射科医生、Lung-RADS以及领先的AI模型（Sybil、Brock、Google、Kaggle）。它在内部实现了0.98的接收者操作特征曲线下面积（AUC），在独立队列中达到0.945。每扫描0.5个假阳性，灵敏度为99.3%，解决了AI采用的关键障碍。重要的是，它在所有结节大小和分期上都超越了放射科医生，在1期癌症中表现优异，并且在所有基于生长的指标上都超越，包括最不准确的体积倍增时间。在诊断不确定和缓慢生长结节方面，它也比放射科医生早达一年。

</details>


### [23] [TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion](https://arxiv.org/abs/2512.00300)
*Rui Qian,Haozhi Cao,Tianchen Deng,Tianxin Hu,Weixiang Guo,Shenghai Yuan,Lihua Xie*

Main category: cs.CV

TL;DR: TGSFormer是一个用于具身语义场景补全的可扩展时序高斯泼溅框架，通过持久高斯内存和置信度感知融合机制，在保持场景完整性的同时显著减少基元数量，实现更好的可扩展性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯的方法存在两个主要问题：1）依赖随机初始化大量基元导致冗余和可扩展性差；2）深度引导方法仍然是局部性的，随着场景规模增加会产生延迟和内存开销。需要一种能够处理无边界场景、保持长期场景完整性的可扩展解决方案。

Method: TGSFormer采用持久高斯内存进行时序预测，不依赖图像连贯性或帧缓存。核心组件包括：1）双时序编码器，通过置信度感知交叉注意力联合处理当前和历史高斯特征；2）置信度感知体素融合模块，将重叠基元合并为体素对齐表示，调节密度并保持紧凑性。

Result: 在局部和具身SSC基准测试中达到最先进结果，以显著更少的基元实现卓越准确性和可扩展性，同时保持一致的长期场景完整性。

Conclusion: TGSFormer通过持久高斯内存和置信度感知融合机制，解决了现有高斯方法在可扩展性和冗余方面的问题，为具身语义场景补全提供了一个高效、可扩展的解决方案。

Abstract: Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance.

中文标题: TGSFormer：面向具身语义场景补全的可扩展时序高斯泼溅方法

中文摘要: 具身3D语义场景补全（SSC）从连续的自我中心观测中推断密集的几何和语义信息。大多数现有的基于高斯的方法依赖于在预定义空间边界内随机初始化大量基元，导致冗余且对无边界场景的可扩展性差。最近的深度引导方法缓解了这一问题，但仍然是局部性的，随着规模增加会遭受延迟和内存开销。为了克服这些挑战，我们提出了TGSFormer，一个用于具身SSC的可扩展时序高斯泼溅框架。它维护一个持久的高斯内存用于时序预测，不依赖于图像连贯性或帧缓存。对于时序融合，双时序编码器通过置信度感知的交叉注意力联合处理当前和历史高斯特征。随后，置信度感知体素融合模块将重叠的基元合并为体素对齐的表示，调节密度并保持紧凑性。大量实验表明，TGSFormer在局部和具身SSC基准测试中都达到了最先进的结果，以显著更少的基元提供卓越的准确性和可扩展性，同时保持一致的长期场景完整性。代码将在接受后发布。

</details>


### [24] [Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation](https://arxiv.org/abs/2512.00308)
*Xiao Cui,Yulei Qin,Wengang Zhou,Hongsheng Li,Houqiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于最优传输（OT）的数据集蒸馏方法，通过全局和实例级别的细粒度对齐来优化分布几何，显著提升了蒸馏数据集的质量和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模数据集蒸馏方法主要关注匹配全局分布统计量（如均值和方差），但忽视了关键的实例级特征和类内变化，导致泛化性能不佳。需要一种能够同时对齐全局和实例级别分布的方法来更好地保留数据集的几何结构。

Method: 将数据集蒸馏重新表述为最优传输距离最小化问题，包含三个核心组件：1）OT引导的扩散采样，对齐真实图像和蒸馏图像的潜在分布；2）标签-图像对齐的软重标注，根据蒸馏图像分布的复杂性调整标签分布；3）基于OT的logit匹配，对齐学生模型输出与软标签分布。

Result: 在多种架构和大规模数据集上的广泛实验表明，该方法在效率上始终优于现有最先进方法，在IPC=10设置下，每个架构在ImageNet-1K上至少获得4%的准确率提升。

Conclusion: 通过最优传输框架实现全局和实例级别的细粒度对齐，能够有效保留复杂高维分布的几何特征，显著提升数据集蒸馏的性能和泛化能力。

Abstract: Dataset distillation seeks to synthesize a compact distilled dataset, enabling models trained on it to achieve performance comparable to models trained on the full dataset. Recent methods for large-scale datasets focus on matching global distributional statistics (e.g., mean and variance), but overlook critical instance-level characteristics and intraclass variations, leading to suboptimal generalization. We address this limitation by reformulating dataset distillation as an Optimal Transport (OT) distance minimization problem, enabling fine-grained alignment at both global and instance levels throughout the pipeline. OT offers a geometrically faithful framework for distribution matching. It effectively preserves local modes, intra-class patterns, and fine-grained variations that characterize the geometry of complex, high-dimensional distributions. Our method comprises three components tailored for preserving distributional geometry: (1) OT-guided diffusion sampling, which aligns latent distributions of real and distilled images; (2) label-image-aligned soft relabeling, which adapts label distributions based on the complexity of distilled image distributions; and (3) OT-based logit matching, which aligns the output of student models with soft-label distributions. Extensive experiments across diverse architectures and large-scale datasets demonstrate that our method consistently outperforms state-of-the-art approaches in an efficient manner, achieving at least 4% accuracy improvement under IPC=10 settings for each architecture on ImageNet-1K.

中文标题: 基于最优传输的生成式数据集蒸馏分布几何对齐优化

中文摘要: 数据集蒸馏旨在合成一个紧凑的蒸馏数据集，使在其上训练的模型能够达到与在全数据集上训练的模型相当的性能。针对大规模数据集的最新方法主要关注匹配全局分布统计量（如均值和方差），但忽视了关键的实例级特征和类内变化，导致泛化性能不佳。我们通过将数据集蒸馏重新表述为最优传输距离最小化问题来解决这一局限性，在整个流程中实现全局和实例级别的细粒度对齐。最优传输提供了一个几何保真的分布匹配框架，能够有效保留局部模式、类内模式和细粒度变化，这些特征刻画了复杂高维分布的几何结构。我们的方法包含三个专门用于保留分布几何的组件：（1）OT引导的扩散采样，对齐真实图像和蒸馏图像的潜在分布；（2）标签-图像对齐的软重标注，根据蒸馏图像分布的复杂性调整标签分布；（3）基于OT的logit匹配，对齐学生模型输出与软标签分布。在多种架构和大规模数据集上的广泛实验表明，我们的方法在效率上始终优于现有最先进方法，在IPC=10设置下，每个架构在ImageNet-1K上至少获得4%的准确率提升。

</details>


### [25] [Odometry Without Correspondence from Inertially Constrained Ruled Surfaces](https://arxiv.org/abs/2512.00327)
*Chenqi Zhu,Levi Burner,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 提出了一种无需点对应关系的视觉里程计方法，通过分析图像中直线运动形成的直纹曲面，结合IMU约束来估计相机运动和3D场景重建。


<details>
  <summary>Details</summary>
Motivation: 传统视觉里程计依赖图像间的特征点对应关系，计算成本高且精度不稳定。现有方法虽然尝试使用线特征或融合其他传感器，但仍严重依赖对应关系。本文旨在绕过对应关系问题，利用直线运动形成的直纹曲面进行运动估计。

Method: 通过分析相机观察直线运动时在图像空间-时间中形成的直纹曲面，结合IMU测量约束解空间维度。仅需点-线关联的微分计算更新，无需点对点对应关系。特别适合事件相机边缘检测特性。

Result: 开发了一种新颖算法，能够从直纹曲面重建3D场景并计算视觉里程计。通过IMU约束显著降低解空间维度，提高了运动估计的效率和稳定性。

Conclusion: 该方法成功绕过了传统视觉里程计中的对应关系问题，利用直纹曲面分析和IMU约束实现了更高效、更稳定的运动估计，特别适合事件相机等边缘检测优势的传感器。

Abstract: Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow. This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate's quality. Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence. If the camera observes a straight line as it moves, the image of the line sweeps a smooth surface in image-space time. It is a ruled surface and analyzing its shape gives information about odometry. Further, its estimation requires only differentially computed updates from point-to-line associations. Inspired by event cameras' propensity for edge detection, this research presents a novel algorithm to reconstruct 3D scenes and visual odometry from these ruled surfaces. By constraining the surfaces with the inertia measurements from an onboard IMU sensor, the dimensionality of the solution space is greatly reduced.

中文标题: 基于惯性约束直纹曲面的无对应关系里程计

中文摘要: 视觉里程计技术通常依赖于从图像序列中提取特征并随后计算光流。两个连续帧之间的点对点对应关系计算成本高且精度变化大，这会影响里程计估计的质量。已有尝试通过采用线特征和融合其他传感器（事件相机、IMU）来绕过对应关系问题带来的困难，但许多方法仍然严重依赖对应关系。如果相机在移动时观察到直线，该直线在图像空间-时间中的图像会扫过一个光滑曲面。这是一个直纹曲面，分析其形状可以提供里程计信息。此外，其估计仅需要从点-线关联进行微分计算更新。受事件相机边缘检测倾向的启发，本研究提出了一种新颖算法，从这些直纹曲面重建3D场景和视觉里程计。通过使用板载IMU传感器的惯性测量约束这些曲面，解空间的维度大大降低。

</details>


### [26] [MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection](https://arxiv.org/abs/2512.00336)
*Mengxue Hu,Yunfeng Diao,Changtao Miao,Jianshu Li,Zhe Li,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: MVAD是首个专门用于检测AI生成多模态视频-音频内容的综合数据集，填补了现有数据集仅关注视觉模态或局限于面部深度伪造的空白。


<details>
  <summary>Details</summary>
Motivation: AI生成的多模态视频-音频内容快速发展引发了信息安全和内容真实性的严重担忧。现有合成视频数据集大多仅关注视觉模态，少数包含音频的数据集也主要局限于面部深度伪造，这无法应对日益扩大的通用多模态AI生成内容，严重阻碍了可信检测系统的发展。

Method: 创建了MVAD数据集，具有三个关键特征：1) 真实多模态性，包含三种现实视频-音频伪造模式生成的样本；2) 高感知质量，通过多样化的最先进生成模型实现；3) 全面多样性，涵盖现实和动漫视觉风格、四个内容类别（人类、动物、物体、场景）以及四种视频-音频多模态数据类型。

Result: MVAD是首个专门为检测AI生成多模态视频-音频内容设计的综合数据集，填补了该领域的关键空白，为开发可信的多模态检测系统提供了重要资源。

Conclusion: MVAD数据集通过提供真实多模态、高质量和多样性的样本，为AI生成多模态视频-音频内容的检测研究提供了重要基础，将促进可信检测系统的发展。

Abstract: The rapid advancement of AI-generated multimodal video-audio content has raised significant concerns regarding information security and content authenticity. Existing synthetic video datasets predominantly focus on the visual modality alone, while the few incorporating audio are largely confined to facial deepfakes--a limitation that fails to address the expanding landscape of general multimodal AI-generated content and substantially impedes the development of trustworthy detection systems. To bridge this critical gap, we introduce the Multimodal Video-Audio Dataset (MVAD), the first comprehensive dataset specifically designed for detecting AI-generated multimodal video-audio content. Our dataset exhibits three key characteristics: (1) genuine multimodality with samples generated according to three realistic video-audio forgery patterns; (2) high perceptual quality achieved through diverse state-of-the-art generative models; and (3) comprehensive diversity spanning realistic and anime visual styles, four content categories (humans, animals, objects, and scenes), and four video-audio multimodal data types. Our dataset will be available at https://github.com/HuMengXue0104/MVAD.

中文标题: MVAD：用于AIGC检测的全面多模态视频-音频数据集

中文摘要: AI生成的多模态视频-音频内容的快速发展引发了信息安全和内容真实性的严重担忧。现有的合成视频数据集主要仅关注视觉模态，而少数包含音频的数据集也主要局限于面部深度伪造——这一限制无法应对日益扩大的通用多模态AI生成内容，并严重阻碍了可信检测系统的发展。为了填补这一关键空白，我们引入了多模态视频-音频数据集（MVAD），这是首个专门为检测AI生成多模态视频-音频内容设计的综合数据集。我们的数据集展现出三个关键特征：（1）真实多模态性，包含根据三种现实视频-音频伪造模式生成的样本；（2）高感知质量，通过多样化的最先进生成模型实现；（3）全面多样性，涵盖现实和动漫视觉风格、四个内容类别（人类、动物、物体和场景）以及四种视频-音频多模态数据类型。我们的数据集将在https://github.com/HuMengXue0104/MVAD上提供。

</details>


### [27] [Assimilation Matters: Model-level Backdoor Detection in Vision-Language Pretrained Models](https://arxiv.org/abs/2512.00343)
*Zhongqi Wang,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: AMDET提出了一种无需先验知识的模型级后门检测框架，通过分析文本编码器中的特征同化现象来检测视觉语言预训练模型中的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 视觉语言预训练模型（如CLIP）虽然取得了显著成功，但极易受到后门攻击。现有检测方法通常依赖于训练数据集、后门触发器、目标或下游分类器的先验知识，这在现实应用中可能不切实际。因此需要一种无需这些先验知识的检测方法。

Method: AMDET首先揭示了后门文本编码器中的特征同化特性：后门样本中所有token的表示表现出高度相似性。进一步分析表明这种效应源于注意力权重在触发器token上的集中。基于这一洞察，AMDET通过对token嵌入进行基于梯度的反演来恢复能够激活后门行为的隐式特征。此外，通过分析损失景观来过滤OpenAI官方CLIP模型中存在的自然后门特征。

Result: 在3,600个后门和良性微调模型上的广泛实验表明，AMDET检测后门的F1分数达到89.90%。在RTX 4090 GPU上大约5分钟完成一次完整检测，并对自适应攻击表现出强大的鲁棒性。

Conclusion: AMDET是一种有效的模型级后门检测框架，无需任何先验知识即可检测视觉语言预训练模型中的后门攻击，为实际应用提供了实用的解决方案。

Abstract: Vision-language pretrained models (VLPs) such as CLIP have achieved remarkable success, but are also highly vulnerable to backdoor attacks. Given a model fine-tuned by an untrusted third party, determining whether the model has been injected with a backdoor is a critical and challenging problem. Existing detection methods usually rely on prior knowledge of training dataset, backdoor triggers and targets, or downstream classifiers, which may be impractical for real-world applications. To address this, To address this challenge, we introduce Assimilation Matters in DETection (AMDET), a novel model-level detection framework that operates without any such prior knowledge. Specifically, we first reveal the feature assimilation property in backdoored text encoders: the representations of all tokens within a backdoor sample exhibit a high similarity. Further analysis attributes this effect to the concentration of attention weights on the trigger token. Leveraging this insight, AMDET scans a model by performing gradient-based inversion on token embeddings to recover implicit features that capable of activating backdoor behaviors. Furthermore, we identify the natural backdoor feature in the OpenAI's official CLIP model, which are not intentionally injected but still exhibit backdoor-like behaviors. We then filter them out from real injected backdoor by analyzing their loss landscapes. Extensive experiments on 3,600 backdoored and benign-finetuned models with two attack paradigms and three VLP model structures show that AMDET detects backdoors with an F1 score of 89.90%. Besides, it achieves one complete detection in approximately 5 minutes on a RTX 4090 GPU and exhibits strong robustness against adaptive attacks. Code is available at: https://github.com/Robin-WZQ/AMDET

中文标题: 同化至关重要：视觉语言预训练模型中的模型级后门检测

中文摘要: 视觉语言预训练模型（如CLIP）取得了显著成功，但也极易受到后门攻击。给定由不可信第三方微调的模型，确定该模型是否被注入了后门是一个关键且具有挑战性的问题。现有检测方法通常依赖于训练数据集、后门触发器和目标或下游分类器的先验知识，这在现实应用中可能不切实际。为应对这一挑战，我们引入了同化至关重要的检测框架（AMDET），这是一种无需任何此类先验知识的新型模型级检测框架。具体而言，我们首先揭示了后门文本编码器中的特征同化特性：后门样本中所有token的表示表现出高度相似性。进一步分析将这种效应归因于注意力权重在触发器token上的集中。利用这一洞察，AMDET通过对token嵌入进行基于梯度的反演来恢复能够激活后门行为的隐式特征，从而扫描模型。此外，我们识别了OpenAI官方CLIP模型中存在的自然后门特征，这些特征并非有意注入但仍表现出类似后门的行为。然后通过分析其损失景观将它们从真实注入的后门中过滤出来。在3,600个后门和良性微调模型上的广泛实验，涵盖两种攻击范式和三种VLP模型结构，表明AMDET检测后门的F1分数达到89.90%。此外，在RTX 4090 GPU上大约5分钟完成一次完整检测，并对自适应攻击表现出强大的鲁棒性。代码可在以下网址获取：https://github.com/Robin-WZQ/AMDET

</details>


### [28] [mmPred: Radar-based Human Motion Prediction in the Dark](https://arxiv.org/abs/2512.00345)
*Junqiao Fan,Haocong Rao,Jiarui Zhang,Jianfei Yang,Lihua Xie*

Main category: cs.CV

TL;DR: mmPred是首个基于毫米波雷达的人类运动预测扩散模型，通过双域历史运动表示和全局骨骼关系Transformer解决雷达信号噪声问题，在黑暗环境下实现隐私保护的运动预测。


<details>
  <summary>Details</summary>
Motivation: 现有基于RGB-D摄像头的HMP方法对光照条件敏感且存在隐私问题，限制了在消防、医疗等实际场景的应用。毫米波雷达具有鲁棒性和隐私保护特性，但雷达信号存在镜面反射和多径效应等噪声问题。

Method: 提出首个基于扩散模型的雷达HMP框架mmPred：1）双域历史运动表示，包括时域姿态细化分支学习细粒度细节，频域主导运动分支捕捉全局趋势并抑制帧级不一致；2）全局骨骼关系Transformer作为扩散主干，建模关节间全局协作，使受损关节能动态聚合其他关节信息。

Result: 在mmBody和mm-Fi数据集上的实验表明，mmPred实现了最先进的性能，分别比现有方法高出8.6%和22%。

Conclusion: mmPred首次将毫米波雷达引入HMP领域，通过创新的双域表示和全局Transformer架构有效解决了雷达信号噪声问题，为黑暗环境下的隐私保护运动预测提供了可行方案。

Abstract: Existing Human Motion Prediction (HMP) methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.

中文标题: mmPred：基于雷达的黑暗环境下人体运动预测

中文摘要: 现有基于RGB-D摄像头的人类运动预测方法对光照条件敏感且存在隐私问题，限制了其在消防和医疗等实际应用中的使用。受毫米波雷达的鲁棒性和隐私保护特性启发，本研究首次将雷达作为HMP的新型传感模态。然而，雷达信号常受镜面反射和多径效应影响，导致噪声大且时间不一致的测量结果，如身体部位漏检。为解决这些雷达特有的伪影，我们提出了mmPred，这是首个针对雷达HMP的扩散模型框架。mmPred引入双域历史运动表示来指导生成过程，结合时域姿态细化分支学习细粒度细节，以及频域主导运动分支捕捉全局运动趋势并抑制帧级不一致。此外，我们设计了全局骨骼关系Transformer作为扩散主干，建模关节间全局协作，使受损关节能动态聚合其他关节信息。大量实验表明，mmPred实现了最先进的性能，在mmBody和mm-Fi数据集上分别比现有方法高出8.6%和22%。

</details>


### [29] [THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering](https://arxiv.org/abs/2512.00368)
*Jian Zhu*

Main category: cs.CV

TL;DR: THCRL提出了一种可信的层次对比表示学习方法，通过深度对称层次融合模块和平均K近邻对比学习模块解决多视图聚类中的不可信融合问题，在深度多视图聚类任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多视图聚类面临不可信融合的挑战，主要源于两个关键因素：1）现有方法往往忽略单个视图中存在的固有噪声；2）传统基于对比学习的多视图聚类方法通常依赖同一实例的不同视图进行相似性计算，而忽略了同一聚类内最近邻的结构信息，导致多视图融合方向错误。

Method: THCRL包含两个关键模块：1）深度对称层次融合（DSHF）模块，利用UNet架构集成多种去噪机制实现多视图数据的可信融合；2）平均K近邻对比学习（AKCL）模块，将融合表示与视图特定表示对齐，通过增强同一聚类内样本的表示相似性来强化融合表示的可信度。

Result: 大量实验表明，THCRL在深度多视图聚类任务中实现了最先进的性能。

Conclusion: THCRL通过可信的层次对比表示学习有效解决了多视图聚类中的不可信融合问题，为多视图数据融合提供了可靠的方法。

Abstract: Multi-View Clustering (MVC) has garnered increasing attention in recent years. It is capable of partitioning data samples into distinct groups by learning a consensus representation. However, a significant challenge remains: the problem of untrustworthy fusion. This problem primarily arises from two key factors: 1) Existing methods often ignore the presence of inherent noise within individual views; 2) In traditional MVC methods using Contrastive Learning (CL), similarity computations typically rely on different views of the same instance, while neglecting the structural information from nearest neighbors within the same cluster. Consequently, this leads to the wrong direction for multi-view fusion. To address this problem, we present a novel Trusted Hierarchical Contrastive Representation Learning (THCRL). It consists of two key modules. Specifically, we propose the Deep Symmetry Hierarchical Fusion (DSHF) module, which leverages the UNet architecture integrated with multiple denoising mechanisms to achieve trustworthy fusion of multi-view data. Furthermore, we present the Average K-Nearest Neighbors Contrastive Learning (AKCL) module to align the fused representation with the view-specific representation. Unlike conventional strategies, AKCL enhances representation similarity among samples belonging to the same cluster, rather than merely focusing on the same sample across views, thereby reinforcing the confidence of the fused representation. Extensive experiments demonstrate that THCRL achieves the state-of-the-art performance in deep MVC tasks.

中文标题: THCRL：用于多视图聚类的可信层次对比表示学习

中文摘要: 多视图聚类近年来受到越来越多的关注。它能够通过学习共识表示将数据样本划分为不同的组。然而，一个重大挑战仍然存在：不可信融合问题。这个问题主要源于两个关键因素：1）现有方法往往忽略单个视图中存在的固有噪声；2）在使用对比学习的传统多视图聚类方法中，相似性计算通常依赖于同一实例的不同视图，而忽略了同一聚类内最近邻的结构信息。因此，这导致了多视图融合的错误方向。为了解决这个问题，我们提出了一种新颖的可信层次对比表示学习（THCRL）。它包含两个关键模块。具体来说，我们提出了深度对称层次融合（DSHF）模块，该模块利用集成多种去噪机制的UNet架构实现多视图数据的可信融合。此外，我们提出了平均K近邻对比学习（AKCL）模块，将融合表示与视图特定表示对齐。与常规策略不同，AKCL增强了属于同一聚类的样本之间的表示相似性，而不仅仅是关注跨视图的同一样本，从而强化了融合表示的可信度。大量实验表明，THCRL在深度多视图聚类任务中实现了最先进的性能。

</details>


### [30] [POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models](https://arxiv.org/abs/2512.00369)
*Wenshuo Chen,Haosen Li,Shaofeng Liang,Lei Wang,Haozhe Jia,Kaishen Yuan,Jieming Wu,Bowen Tian,Yutao Yue*

Main category: cs.CV

TL;DR: POLARIS通过将引导尺度ω作为步长变量并推导最小化反演误差的数学公式，解决了扩散模型反演中的噪声近似误差累积问题，仅需一行代码即可显著提升反演质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的反演-去噪范式存在重建质量下降的问题，研究发现这是由于噪声近似误差的累积造成的——用前一步的预测来近似当前步的噪声，导致误差在整个反演过程中不断积累。

Method: POLARIS将反演问题从误差补偿重新定义为误差溯源问题，将引导尺度ω作为步长变量，通过投影正交最小二乘法推导出数学公式来最小化每一步的反演误差，而不是优化嵌入或潜在代码。

Result: POLARIS仅需一行代码即可显著提高反演潜在质量，在可忽略的性能开销下大幅减轻噪声近似误差，持续提升下游任务的准确性。

Conclusion: POLARIS通过数学上严谨的方法解决了扩散模型反演中的噪声近似误差累积问题，提供了一种高效、鲁棒且自适应的反演解决方案，显著提升了图像编辑和修复任务的质量。

Abstract: The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale ω as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.

中文标题: POLARIS：用于扩散模型中鲁棒自适应反演的投影正交最小二乘法

中文摘要: 基于扩散模型的反演-去噪范式在多种图像编辑和修复任务中表现出色。我们重新审视其机制，揭示了导致重建质量下降的一个关键但被忽视的因素：近似噪声误差。这种误差源于用第t-1步的预测来近似第t步的噪声，导致在整个反演过程中严重的误差累积。我们提出了投影正交最小二乘法用于鲁棒自适应反演（POLARIS），将反演问题从误差补偿问题重新表述为误差溯源问题。POLARIS不是通过优化嵌入或潜在代码来抵消累积漂移，而是将引导尺度ω视为步长变量，并推导出数学上严谨的公式来最小化每一步的反演误差。值得注意的是，POLARIS仅用一行代码就能提高反演潜在质量。在性能开销可忽略不计的情况下，它显著减轻了噪声近似误差，并持续提高了下游任务的准确性。

</details>


### [31] [Pore-scale Image Patch Dataset and A Comparative Evaluation of Pore-scale Facial Features](https://arxiv.org/abs/2512.00381)
*Dong Li,HuaLiang Lin,JiaYu Li*

Main category: cs.CV

TL;DR: 本文提出了PorePatch数据集，这是首个高质量毛孔尺度图像补丁数据集，并建立了评估基准。通过数据-模型协同进化框架生成数据集，实验显示深度学习描述符在匹配任务上表现优异（FPR95 1.91% vs PSIFT 22.41%），但在3D重建任务中优势不明显，表明深度学习描述符在面部弱纹理区域仍存在局限性。


<details>
  <summary>Details</summary>
Motivation: 面部皮肤区域的弱纹理特性给局部描述符匹配带来了重大挑战，特别是在面部运动分析和3D人脸重建等应用中。虽然基于深度学习的描述符在许多应用中表现出优于传统手工描述符的性能，但毛孔尺度图像补丁数据集的稀缺阻碍了其在面部领域的进一步发展。

Method: 提出了PorePatch数据集，这是首个高质量毛孔尺度图像补丁数据集。引入了数据-模型协同进化（DMCE）框架，从高分辨率面部图像中生成逐步细化的高质量数据集。在数据集上训练现有的SOTA模型，并进行广泛的实验评估。

Result: 实验结果显示，SOTA模型在匹配任务上取得了1.91%的FPR95值，显著优于PSIFT的22.41%（提升20.5%）。然而，在3D重建任务中，其优势减弱，整体性能并未显著优于传统描述符。

Conclusion: 深度学习描述符在面部弱纹理区域的挑战处理上仍存在局限性，该领域还有许多工作要做。PorePatch数据集为相关研究提供了重要资源，但需要进一步改进描述符在3D重建等实际应用中的性能。

Abstract: The weak-texture nature of facial skin regions presents significant challenges for local descriptor matching in applications such as facial motion analysis and 3D face reconstruction. Although deep learning-based descriptors have demonstrated superior performance to traditional hand-crafted descriptors in many applications, the scarcity of pore-scale image patch datasets has hindered their further development in the facial domain. In this paper, we propose the PorePatch dataset, a high-quality pore-scale image patch dataset, and establish a rational evaluation benchmark. We introduce a Data-Model Co-Evolution (DMCE) framework to generate a progressively refined, high-quality dataset from high-resolution facial images. We then train existing SOTA models on our dataset and conduct extensive experiments. Our results show that the SOTA model achieves a FPR95 value of 1.91% on the matching task, outperforming PSIFT (22.41%) by a margin of 20.5%. However, its advantage is diminished in the 3D reconstruction task, where its overall performance is not significantly better than that of traditional descriptors. This indicates that deep learning descriptors still have limitations in addressing the challenges of facial weak-texture regions, and much work remains to be done in this field.

中文标题: 毛孔尺度图像补丁数据集及毛孔尺度面部特征的比较评估

中文摘要: 面部皮肤区域的弱纹理特性给局部描述符匹配带来了重大挑战，特别是在面部运动分析和3D人脸重建等应用中。虽然基于深度学习的描述符在许多应用中表现出优于传统手工描述符的性能，但毛孔尺度图像补丁数据集的稀缺阻碍了其在面部领域的进一步发展。本文提出了PorePatch数据集，这是一个高质量的毛孔尺度图像补丁数据集，并建立了合理的评估基准。我们引入了数据-模型协同进化（DMCE）框架，从高分辨率面部图像中生成逐步细化的高质量数据集。然后我们在数据集上训练现有的SOTA模型并进行广泛实验。结果显示，SOTA模型在匹配任务上取得了1.91%的FPR95值，显著优于PSIFT的22.41%（提升20.5%）。然而，在3D重建任务中，其优势减弱，整体性能并未显著优于传统描述符。这表明深度学习描述符在解决面部弱纹理区域的挑战方面仍存在局限性，该领域还有许多工作要做。

</details>


### [32] [WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing](https://arxiv.org/abs/2512.00387)
*Kaihang Pan,Weile Chen,Haiyi Qiu,Qifan Yu,Wendong Bu,Zehan Wang,Yun Zhu,Juncheng Li,Siliang Tang*

Main category: cs.CV

TL;DR: WiseEdit是一个知识密集型基准测试，用于全面评估认知和创造力驱动的图像编辑能力，包含1,220个测试案例，涵盖三个认知步骤和三种知识类型。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑基准测试评估范围过于狭窄，无法全面评估当前图像编辑模型的高级认知和创造力能力。需要一个新的基准来更全面地评估这些能力。

Method: WiseEdit将图像编辑类比人类认知创作过程，分解为三个级联步骤：感知、解释和想象，每个步骤对应一个具体任务。同时包含三种基本知识类型：陈述性知识、程序性知识和元认知知识。

Result: WiseEdit包含1,220个测试案例，客观揭示了当前最先进的图像编辑模型在基于知识的认知推理和创造性构图能力方面的局限性。

Conclusion: WiseEdit为评估认知和创造力驱动的图像编辑提供了一个全面、知识密集型的基准测试，有助于揭示现有模型的局限性并推动该领域的发展。

Abstract: Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.

中文标题: WiseEdit：认知与创造力驱动的图像编辑基准测试

中文摘要: 最近的图像编辑模型拥有下一代智能能力，促进了认知和创造力驱动的图像编辑。然而，现有的基准测试评估范围过于狭窄，无法全面评估这些高级能力。为了解决这个问题，我们引入了WiseEdit，这是一个知识密集型基准测试，用于全面评估认知和创造力驱动的图像编辑，具有深度的任务深度和广泛的知识广度。借鉴人类认知创作的类比，WiseEdit将图像编辑分解为三个级联步骤，即感知、解释和想象，每个步骤对应一个模型在特定步骤中需要完成的挑战性任务。它还包含复杂任务，其中三个步骤中的任何一个都不容易完成。此外，WiseEdit包含三种基本知识类型：陈述性知识、程序性知识和元认知知识。最终，WiseEdit包含1,220个测试案例，客观揭示了最先进的图像编辑模型在基于知识的认知推理和创造性构图能力方面的局限性。基准测试、评估代码和每个模型生成的图像将很快公开提供。项目页面：https://qnancy.github.io/wiseedit_project_page/。

</details>


### [33] [Better, Stronger, Faster: Tackling the Trilemma in MLLM-based Segmentation with Simultaneous Textual Mask Prediction](https://arxiv.org/abs/2512.00395)
*Jiazhen Liu,Mingkuan Feng,Long Chen*

Main category: cs.CV

TL;DR: STAMP通过全掩码预测范式解决了MLLM分割的三难困境，在保持对话能力的同时实现高分割性能和快速推理。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM分割方法面临三难困境：无法同时保持对话能力、实现高分割性能和快速推理。现有方法要么牺牲对话能力（嵌入预测），要么在分割性能与推理速度之间权衡（下一词元预测）。

Method: 提出STAMP模型，采用全掩码预测范式：1）将自回归对话生成与非自回归掩码预测解耦；2）先生成文本响应；3）然后将分割掩码视为图像块的并行"填空"任务，在单次前向传播中预测整个掩码。

Result: STAMP在多个分割基准测试中显著优于最先进方法，同时保持了优秀的对话能力，并实现了快速推理速度。

Conclusion: STAMP通过全掩码预测范式成功解决了MLLM分割的三难困境，实现了对话能力、分割性能和推理速度的同步优化，为MLLM分割任务提供了无需妥协的解决方案。

Abstract: Integrating segmentation into Multimodal Large Language Models (MLLMs) presents a core trilemma: simultaneously preserving dialogue ability, achieving high segmentation performance, and ensuring fast inference. Prevailing paradigms are forced into a compromise. Embedding prediction methods introduce a conflicting pixel-level objective that degrades the MLLM's general dialogue abilities. The alternative, next-token prediction, reframes segmentation as an autoregressive task, which preserves dialogue but forces a trade-off between poor segmentation performance with sparse outputs or prohibitive inference speeds with rich ones. We resolve this trilemma with all-mask prediction, a novel paradigm that decouples autoregressive dialogue generation from non-autoregressive mask prediction. We present STAMP: Simultaneous Textual All-Mask Prediction, an MLLM that embodies this paradigm. After generating a textual response, STAMP predicts an entire segmentation mask in a single forward pass by treating it as a parallel "fill-in-the-blank" task over image patches. This design maintains the MLLM's dialogue ability by avoiding conflicting objectives, enables high segmentation performance by leveraging rich, bidirectional spatial context for all mask tokens, and achieves exceptional speed. Extensive experiments show that STAMP significantly outperforms state-of-the-art methods across multiple segmentation benchmarks, providing a solution that excels in dialogue, segmentation, and speed without compromise.

中文标题: 更好、更强、更快：通过同步文本掩码预测解决MLLM分割的三难困境

中文摘要: 将分割任务集成到多模态大语言模型（MLLMs）中面临一个核心的三难困境：同时保持对话能力、实现高分割性能以及确保快速推理。现有范式被迫做出妥协。嵌入预测方法引入了冲突的像素级目标，会降低MLLM的通用对话能力。另一种方法——下一词元预测——将分割重构为自回归任务，虽然保留了对话能力，但被迫在稀疏输出的低分割性能与丰富输出的高推理速度之间做出权衡。我们通过全掩码预测这一新颖范式解决了这一三难困境，该范式将自回归对话生成与非自回归掩码预测解耦。我们提出了STAMP：同步文本全掩码预测，一个体现这一范式的MLLM。在生成文本响应后，STAMP通过将分割掩码视为图像块的并行"填空"任务，在单次前向传播中预测整个分割掩码。这种设计通过避免冲突目标保持了MLLM的对话能力，通过利用所有掩码标记的丰富双向空间上下文实现了高分割性能，并获得了卓越的速度。大量实验表明，STAMP在多个分割基准测试中显著优于最先进的方法，提供了一个在对话、分割和速度方面均表现出色且无需妥协的解决方案。

</details>


### [34] [Low-Bitrate Video Compression through Semantic-Conditioned Diffusion](https://arxiv.org/abs/2512.00408)
*Lingdong Wang,Guan-Ming Su,Divya Kothandaraman,Tsung-Wei Huang,Mohammad Hajiesmaili,Ramesh K. Sitaraman*

Main category: cs.CV

TL;DR: DiSCo是一个语义视频压缩框架，通过将视频分解为文本、降质视频和可选草图/姿态三种紧凑模态，利用条件扩散模型重建高质量视频，在低比特率下显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统视频编解码器在超低比特率下会崩溃并产生严重伪影，这是因为它们过度关注像素级保真度，而忽略了人类感知更关注语义内容。需要一种新的压缩范式，能够在极低比特率下保持视频的感知质量。

Method: 提出DiSCo框架：1) 将源视频分解为三种紧凑模态：文本描述（语义）、时空降质视频（外观）、可选草图/姿态（运动）；2) 使用条件视频扩散模型从这些模态重建高质量视频；3) 引入时间前向填充、令牌交错和模态特定编解码器来优化多模态生成和压缩效率。

Result: 实验表明，在低比特率下，DiSCo在感知指标上优于基线语义编解码器和传统编解码器2-10倍，能够在极低比特率下保持视频的感知质量，同时显著减少伪影。

Conclusion: DiSCo通过语义条件扩散方法实现了有效的低比特率视频压缩，将视频分解为紧凑的语义、外观和运动表示，利用生成模型重建高质量视频，为超低比特率视频压缩提供了新的解决方案。

Abstract: Traditional video codecs optimized for pixel fidelity collapse at ultra-low bitrates and produce severe artifacts. This failure arises from a fundamental misalignment between pixel accuracy and human perception. We propose a semantic video compression framework named DiSCo that transmits only the most meaningful information while relying on generative priors for detail synthesis. The source video is decomposed into three compact modalities: a textual description, a spatiotemporally degraded video, and optional sketches or poses that respectively capture semantic, appearance, and motion cues. A conditional video diffusion model then reconstructs high-quality, temporally coherent videos from these compact representations. Temporal forward filling, token interleaving, and modality-specific codecs are proposed to improve multimodal generation and modality compactness. Experiments show that our method outperforms baseline semantic and traditional codecs by 2-10X on perceptual metrics at low bitrates.

中文标题: 基于语义条件扩散的低比特率视频压缩

中文摘要: 传统视频编解码器以像素保真度优化，在超低比特率下会崩溃并产生严重伪影。这种失败源于像素精度与人类感知之间的根本性错位。我们提出了一个名为DiSCo的语义视频压缩框架，该框架仅传输最有意义的信息，同时依赖生成先验进行细节合成。源视频被分解为三种紧凑模态：文本描述、时空降质视频以及可选的草图或姿态，分别捕获语义、外观和运动线索。然后，条件视频扩散模型从这些紧凑表示中重建高质量、时间连贯的视频。提出了时间前向填充、令牌交错和模态特定编解码器来改进多模态生成和模态紧凑性。实验表明，在低比特率下，我们的方法在感知指标上优于基线语义和传统编解码器2-10倍。

</details>


### [35] [SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control](https://arxiv.org/abs/2512.00413)
*Ji Gan,Lingxu Chen,Jiaxu Leng,Xinbo Gao*

Main category: cs.CV

TL;DR: SplatFont3D是一个创新的文本到3D艺术字体生成框架，使用3D高斯溅射技术实现结构感知的字体生成和部件级风格控制，在风格一致性、视觉质量和渲染效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前艺术字体生成研究主要集中在2D平面设计，3D艺术字体生成领域尚未充分探索。3D字体不仅能在沉浸式3D环境中应用，还能增强2D字体生成。然而，3D字体具有精确的语义结构和强烈的结构约束，需要细粒度的部件级风格控制，这是现有方法难以解决的挑战。

Method: 1. Glyph2Cloud模块：逐步增强2D字形（组件）的形状和风格，生成对应的3D点云用于高斯初始化；2. 3D高斯优化：通过与预训练的2D扩散模型使用分数蒸馏采样进行交互优化；3. 动态组件分配策略：利用3D高斯的几何先验划分组件，减轻优化过程中的漂移引起的纠缠。

Result: SplatFont3D在风格-文本一致性、视觉质量和渲染效率方面优于现有的3D-AFG模型。与NeRF相比，提供更明确和有效的部件级风格控制，并实现更快的渲染效率。

Conclusion: SplatFont3D成功解决了3D艺术字体生成中的结构约束和部件级风格控制挑战，通过创新的3D高斯溅射框架实现了高质量的3D字体生成，为沉浸式3D环境应用和2D字体增强提供了有效解决方案。

Abstract: Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts. However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored. 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views. Moreover, unlike general 3D objects, 3D fonts exhibit precise semantics with strong structural constraints and also demand fine-grained part-level style control. To address these challenges, we propose SplatFont3D, a novel structure-aware text-to-3D AFG framework with 3D Gaussian splatting, which enables the creation of 3D artistic fonts from diverse style text prompts with precise part-level style control. Specifically, we first introduce a Glyph2Cloud module, which progressively enhances both the shapes and styles of 2D glyphs (or components) and produces their corresponding 3D point clouds for Gaussian initialization. The initialized 3D Gaussians are further optimized through interaction with a pretrained 2D diffusion model using score distillation sampling. To enable part-level control, we present a dynamic component assignment strategy that exploits the geometric priors of 3D Gaussians to partition components, while alleviating drift-induced entanglement during 3D Gaussian optimization. Our SplatFont3D provides more explicit and effective part-level style control than NeRF, attaining faster rendering efficiency. Experiments show that our SplatFont3D outperforms existing 3D models for 3D-AFG in style-text consistency, visual quality, and rendering efficiency.

中文标题: SplatFont3D：具有部件级风格控制的结构感知文本到3D艺术字体生成

中文摘要: 艺术字体生成（AFG）可以帮助人类设计师创造创新的艺术字体。然而，先前的研究主要关注平面设计中的2D艺术字体，个性化的3D-AFG在很大程度上尚未得到充分探索。3D-AFG不仅能够在视频游戏和动画等沉浸式3D环境中应用，还可以通过渲染新颖视角的2D字体来增强2D-AFG。此外，与一般的3D对象不同，3D字体表现出具有强烈结构约束的精确语义，并且需要细粒度的部件级风格控制。为了解决这些挑战，我们提出了SplatFont3D，这是一种新颖的结构感知文本到3D AFG框架，采用3D高斯溅射技术，能够从多样化的风格文本提示中创建3D艺术字体，并实现精确的部件级风格控制。具体来说，我们首先引入Glyph2Cloud模块，该模块逐步增强2D字形（或组件）的形状和风格，并生成相应的3D点云用于高斯初始化。初始化的3D高斯通过与预训练的2D扩散模型使用分数蒸馏采样进行交互进一步优化。为了实现部件级控制，我们提出了一种动态组件分配策略，该策略利用3D高斯的几何先验来划分组件，同时减轻3D高斯优化过程中漂移引起的纠缠。我们的SplatFont3D比NeRF提供更明确和有效的部件级风格控制，并实现更快的渲染效率。实验表明，我们的SplatFont3D在风格-文本一致性、视觉质量和渲染效率方面优于现有的3D-AFG模型。

</details>


### [36] [PhysGen: Physically Grounded 3D Shape Generation for Industrial Design](https://arxiv.org/abs/2512.00422)
*Yingxuan You,Chen Zhao,Hantao Zhang,Mingda Xu,Pascal Fua*

Main category: cs.CV

TL;DR: PhysGen是一个用于工业设计的物理基础3D形状生成框架，通过结合流匹配模型和物理指导，在生成过程中交替进行基于速度的更新和基于物理的细化，从而生成既视觉逼真又物理有效的形状。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状生成模型虽然能合成高保真和视觉合理的形状，但对于经过工程设计过程的形状类别（如汽车），其真实性与底层物理属性（如空气动力学效率）紧密相关。由于现有方法缺乏此类物理知识，无法利用这些知识来增强形状生成的真实性。

Method: 提出统一的基于物理的3D形状生成流程，包括：1）引入具有显式物理指导的新流匹配模型，采用交替更新过程（基于速度的更新和基于物理的细化）；2）在基于速度的更新步骤中加入物理感知正则化项以增强物理有效性；3）构建形状-物理变分自编码器（SP-VAE），将形状和物理信息联合编码到统一的潜在空间中。

Result: 在三个基准测试上的实验表明，这种协同公式超越了单纯的视觉合理性，提高了形状的真实性。

Conclusion: PhysGen通过将物理知识整合到生成过程中，为工业设计应用提供了更真实、物理有效的3D形状生成解决方案。

Abstract: Existing generative models for 3D shapes can synthesize high-fidelity and visually plausible shapes. For certain classes of shapes that have undergone an engineering design process, the realism of the shape is tightly coupled with the underlying physical properties, e.g., aerodynamic efficiency for automobiles. Since existing methods lack knowledge of such physics, they are unable to use this knowledge to enhance the realism of shape generation. Motivated by this, we propose a unified physics-based 3D shape generation pipeline, with a focus on industrial design applications. Specifically, we introduce a new flow matching model with explicit physical guidance, consisting of an alternating update process. We iteratively perform a velocity-based update and a physics-based refinement, progressively adjusting the latent code to align with the desired 3D shapes and physical properties. We further strengthen physical validity by incorporating a physics-aware regularization term into the velocity-based update step. To support such physics-guided updates, we build a shape-and-physics variational autoencoder (SP-VAE) that jointly encodes shape and physics information into a unified latent space. The experiments on three benchmarks show that this synergistic formulation improves shape realism beyond mere visual plausibility.

中文标题: PhysGen：面向工业设计的物理基础3D形状生成

中文摘要: 现有的3D形状生成模型可以合成高保真和视觉合理的形状。对于某些经过工程设计过程的形状类别，形状的真实性与底层物理属性（如汽车的空气动力学效率）紧密相关。由于现有方法缺乏此类物理知识，它们无法利用这些知识来增强形状生成的真实性。基于此，我们提出了一个统一的基于物理的3D形状生成流程，专注于工业设计应用。具体来说，我们引入了一个具有显式物理指导的新流匹配模型，包含一个交替更新过程。我们迭代执行基于速度的更新和基于物理的细化，逐步调整潜在代码以与期望的3D形状和物理属性对齐。我们通过在基于速度的更新步骤中加入物理感知正则化项来进一步增强物理有效性。为了支持这种物理指导的更新，我们构建了一个形状-物理变分自编码器（SP-VAE），将形状和物理信息联合编码到统一的潜在空间中。在三个基准测试上的实验表明，这种协同公式超越了单纯的视觉合理性，提高了形状的真实性。

</details>


### [37] [Recovering Origin Destination Flows from Bus CCTV: Early Results from Nairobi and Kigali](https://arxiv.org/abs/2512.00424)
*Nthenya Kyatha,Jay Taneja*

Main category: cs.CV

TL;DR: 利用公交车上已有的CCTV监控，开发了一个从视频中恢复乘客起讫点（OD）流量的自动化系统，在低密度、光照良好条件下表现良好，但在拥挤、单色转换等现实压力下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 撒哈拉以南非洲地区的公共交通经常过度拥挤，现有自动化系统无法可靠捕捉乘客流量数据。利用已部署用于安全的车载CCTV监控，开发一个自动化系统来恢复公交起讫点流量数据。

Method: 结合YOLOv12检测、BotSORT跟踪、OSNet嵌入、基于OCR的时间戳和基于遥测的站点分类的基线流程，从公交车CCTV视频中恢复OD流量。

Result: 在低密度、光照良好条件下获得高计数准确率（召回率≈95%，精确率≈91%，F1≈93%），生成的OD矩阵与人工计数高度匹配。但在拥挤、单色转换、姿势变化和非标准门使用等现实压力下，性能显著下降（如高峰时段登车计数低估约40%，单色片段召回率下降约17个百分点）。

Conclusion: 系统在理想条件下表现良好，但在现实压力下性能显著下降，揭示了部署特定的失败模式，需要为SSA交通开发更鲁棒、面向部署的Re-ID方法。

Abstract: Public transport in sub-Saharan Africa (SSA) often operates in overcrowded conditions where existing automated systems fail to capture reliable passenger flow data. Leveraging onboard CCTV already deployed for security, we present a baseline pipeline that combines YOLOv12 detection, BotSORT tracking, OSNet embeddings, OCR-based timestamping, and telematics-based stop classification to recover bus origin--destination (OD) flows. On annotated CCTV segments from Nairobi and Kigali buses, the system attains high counting accuracy under low-density, well-lit conditions (recall $\approx$95\%, precision $\approx$91\%, F1 $\approx$93\%). It produces OD matrices that closely match manual tallies. Under realistic stressors such as overcrowding, color-to-monochrome shifts, posture variation, and non-standard door use, performance degrades sharply (e.g., $\sim$40\% undercount in peak-hour boarding and a $\sim$17 percentage-point drop in recall for monochrome segments), revealing deployment-specific failure modes and motivating more robust, deployment-focused Re-ID methods for SSA transit.

中文标题: 从公交车CCTV恢复起讫点流量：内罗毕和基加利的初步结果

中文摘要: 撒哈拉以南非洲地区的公共交通经常在过度拥挤的条件下运行，现有自动化系统无法捕捉可靠的乘客流量数据。利用已部署用于安全的车载CCTV监控，我们提出了一个基线流程，结合YOLOv12检测、BotSORT跟踪、OSNet嵌入、基于OCR的时间戳和基于遥测的站点分类来恢复公交车起讫点流量。在内罗毕和基加利公交车的标注CCTV片段上，系统在低密度、光照良好条件下获得了高计数准确率（召回率≈95%，精确率≈91%，F1≈93%）。它生成的OD矩阵与人工计数高度匹配。在拥挤、彩色到单色转换、姿势变化和非标准门使用等现实压力下，性能显著下降（例如，高峰时段登车计数低估约40%，单色片段召回率下降约17个百分点），揭示了部署特定的失败模式，并激励为SSA交通开发更鲁棒、面向部署的Re-ID方法。

</details>


### [38] [Recognizing Pneumonia in Real-World Chest X-rays with a Classifier Trained with Images Synthetically Generated by Nano Banana](https://arxiv.org/abs/2512.00428)
*Jiachuan Peng,Kyle Lam,Jianing Qiu*

Main category: cs.CV

TL;DR: 使用Nano Banana生成的合成胸部X光图像训练的分类器，在真实世界数据上表现良好，证明了合成数据在医学AI开发中的潜力，但仍存在多样性和对齐等挑战。


<details>
  <summary>Details</summary>
Motivation: 探索使用AI生成的合成医学图像训练分类器的可行性，解决医学图像数据稀缺、隐私保护和标注成本高的问题，评估合成数据在真实世界应用中的效果。

Method: 使用Google发布的Nano Banana AI模型生成合成胸部X光图像，仅用这些合成数据训练肺炎分类器，然后在两个真实世界数据集（RSNA肺炎检测数据集和胸部X光数据集）上进行外部验证。

Result: 在RSNA数据集上：AUROC 0.923 (95% CI: 0.919-0.927)，AUPR 0.900 (95% CI: 0.894-0.907)；在胸部X光数据集上：AUROC 0.824 (95% CI: 0.810-0.836)，AUPR 0.913 (95% CI: 0.904-0.922)。

Conclusion: 该方法证明了仅用合成数据训练的分类器在真实世界医学图像分析中的可行性，为医学AI开发提供了新途径，但仍需解决数据多样性控制、后处理对齐等挑战，并需要充分的验证、监管和伦理监督。

Abstract: We trained a classifier with synthetic chest X-ray (CXR) images generated by Nano Banana, the latest AI model for image generation and editing, released by Google. When directly applied to real-world CXRs having only been trained with synthetic data, the classifier achieved an AUROC of 0.923 (95% CI: 0.919 - 0.927), and an AUPR of 0.900 (95% CI: 0.894 - 0.907) in recognizing pneumonia in the 2018 RSNA Pneumonia Detection dataset (14,863 CXRs), and an AUROC of 0.824 (95% CI: 0.810 - 0.836), and an AUPR of 0.913 (95% CI: 0.904 - 0.922) in the Chest X-Ray dataset (5,856 CXRs). These external validation results on real-world data demonstrate the feasibility of this approach and suggest potential for synthetic data in medical AI development. Nonetheless, several limitations remain at present, including challenges in prompt design for controlling the diversity of synthetic CXR data and the requirement for post-processing to ensure alignment with real-world data. However, the growing sophistication and accessibility of medical intelligence will necessitate substantial validation, regulatory approval, and ethical oversight prior to clinical translation.

中文标题: 使用Nano Banana生成的合成图像训练的分类器识别真实世界胸部X光中的肺炎

中文摘要: 我们使用Google发布的最新AI图像生成和编辑模型Nano Banana生成的合成胸部X光图像训练了一个分类器。当仅使用合成数据训练后直接应用于真实世界胸部X光时，该分类器在2018年RSNA肺炎检测数据集（14,863张胸部X光）中识别肺炎的AUROC为0.923（95% CI：0.919-0.927），AUPR为0.900（95% CI：0.894-0.907）；在胸部X光数据集（5,856张胸部X光）中，AUROC为0.824（95% CI：0.810-0.836），AUPR为0.913（95% CI：0.904-0.922）。这些在真实世界数据上的外部验证结果证明了该方法的可行性，并显示了合成数据在医学AI开发中的潜力。然而，目前仍存在一些局限性，包括控制合成胸部X光数据多样性的提示设计挑战，以及确保与真实世界数据对齐的后处理需求。不过，随着医学智能的日益复杂和可及性，在临床转化之前需要进行充分的验证、监管批准和伦理监督。

</details>


### [39] [FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based Reward Signal](https://arxiv.org/abs/2512.00438)
*Hang Xu,Linjiang Huang,Feng Zhao*

Main category: cs.CV

TL;DR: FR-TTS是一种针对NTP图像生成的新型测试时缩放方法，通过填充中间令牌序列来估计未来图像质量，解决了中间样本奖励与最终输出相关性低的问题，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放（TTS）在图像生成中效果显著，但难以应用于下一令牌预测（NTP）范式，因为中间令牌序列解码的图像奖励与最终生成图像的奖励相关性低，导致中间样本无法有效指导剪枝决策。

Method: 提出基于填充的奖励（FR），通过寻找合理填充方案完成中间令牌序列，估计样本的未来轨迹；在此基础上开发FR-TTS策略，结合多样性奖励和动态加权调度，全面评估中间样本质量。

Result: 实验验证表明，FR-TTS在多个基准测试和不同奖励模型上都表现出优越性能，中间样本与最终样本奖励的相关系数显著提升，证明了FR作为中间样本质量评估指标的有效性。

Conclusion: FR-TTS成功解决了NTP图像生成中测试时缩放的关键挑战，通过填充式奖励估计和多样性平衡策略，实现了对中间样本的准确评估，为NTP范式下的高质量图像生成提供了有效解决方案。

Abstract: Test-time scaling (TTS) has become a prevalent technique in image generation, significantly boosting output quality by expanding the number of parallel samples and filtering them using pre-trained reward models. However, applying this powerful methodology to the next-token prediction (NTP) paradigm remains challenging. The primary obstacle is the low correlation between the reward of an image decoded from an intermediate token sequence and the reward of the fully generated image. Consequently, these incomplete intermediate representations prove to be poor indicators for guiding the pruning direction, a limitation that stems from their inherent incompleteness in scale or semantic content. To effectively address this critical issue, we introduce the Filling-Based Reward (FR). This novel design estimates the approximate future trajectory of an intermediate sample by finding and applying a reasonable filling scheme to complete the sequence. Both the correlation coefficient between rewards of intermediate samples and final samples, as well as multiple intrinsic signals like token confidence, indicate that the FR provides an excellent and reliable metric for accurately evaluating the quality of intermediate samples. Building upon this foundation, we propose FR-TTS, a sophisticated scaling strategy. FR-TTS efficiently searches for good filling schemes and incorporates a diversity reward with a dynamic weighting schedule to achieve a balanced and comprehensive evaluation of intermediate samples. We experimentally validate the superiority of FR-TTS over multiple established benchmarks and various reward models. Code is available at \href{https://github.com/xuhang07/FR-TTS}{https://github.com/xuhang07/FR-TTS}.

中文标题: FR-TTS：基于有效填充奖励信号的NTP图像生成测试时缩放方法

中文摘要: 测试时缩放（TTS）已成为图像生成中的一项流行技术，通过扩展并行样本数量并使用预训练奖励模型进行过滤，显著提升了输出质量。然而，将这一强大方法应用于下一令牌预测（NTP）范式仍然具有挑战性。主要障碍在于从中间令牌序列解码的图像奖励与完全生成图像的奖励之间的相关性较低。因此，这些不完整的中间表示被证明是指引剪枝方向的较差指标，这一局限性源于它们在尺度或语义内容上的固有不完备性。为有效解决这一关键问题，我们引入了基于填充的奖励（FR）。这一新颖设计通过寻找并应用合理的填充方案来完成序列，从而估计中间样本的近似未来轨迹。中间样本与最终样本奖励之间的相关系数，以及令牌置信度等多个内在信号均表明，FR为准确评估中间样本质量提供了优秀且可靠的度量标准。基于此基础，我们提出了FR-TTS，一种复杂的缩放策略。FR-TTS高效搜索良好的填充方案，并结合具有动态加权调度的多样性奖励，以实现对中间样本的平衡和全面评估。我们通过实验验证了FR-TTS相对于多个已建立基准和各种奖励模型的优越性。代码可在\href{https://github.com/xuhang07/FR-TTS}{https://github.com/xuhang07/FR-TTS}获取。

</details>


### [40] [CausalAffect: Causal Discovery for Facial Affective Understanding](https://arxiv.org/abs/2512.00456)
*Guanyu Hu,Tangzheng Lian,Dimitrios Kollias,Oya Celiktutan,Xinyu Yang*

Main category: cs.CV

TL;DR: CausalAffect是首个用于面部情感分析的因果图发现框架，通过两级因果层次结构建模面部动作单元之间的依赖关系，无需联合标注数据集或手工因果先验，在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有面部情感分析方法通常缺乏对驱动肌肉激活和表情表达之间潜在因果依赖关系的结构化推理。虽然动作单元（AUs）一直是情感计算的基础，但现有方法很少解决如何直接从数据中推断出心理学上合理的AUs与表情之间的因果关系。

Method: 提出CausalAffect框架，通过两级极性感知和方向感知的因果层次结构建模AU-AU和AU-Expression依赖关系，整合群体级规律性与样本自适应结构。采用特征级反事实干预机制，增强真实因果效应同时抑制虚假相关性。

Result: 在六个基准测试上的广泛实验表明，CausalAffect在AU检测和表情识别方面均达到了最先进的性能。该框架恢复了与既定心理学理论一致的因果结构，同时揭示了新的抑制性和先前未表征的依赖关系。

Conclusion: CausalAffect建立了因果发现与可解释面部行为之间的原则性连接，为面部情感理解提供了首个因果图发现框架，无需联合标注数据集或手工因果先验即可恢复心理学上合理的因果结构。

Abstract: Understanding human affect from facial behavior requires not only accurate recognition but also structured reasoning over the latent dependencies that drive muscle activations and their expressive outcomes. Although Action Units (AUs) have long served as the foundation of affective computing, existing approaches rarely address how to infer psychologically plausible causal relations between AUs and expressions directly from data. We propose CausalAffect, the first framework for causal graph discovery in facial affect analysis. CausalAffect models AU-AU and AU-Expression dependencies through a two-level polarity and direction aware causal hierarchy that integrates population-level regularities with sample-adaptive structures. A feature-level counterfactual intervention mechanism further enforces true causal effects while suppressing spurious correlations. Crucially, our approach requires neither jointly annotated datasets nor handcrafted causal priors, yet it recovers causal structures consistent with established psychological theories while revealing novel inhibitory and previously uncharacterized dependencies. Extensive experiments across six benchmarks demonstrate that CausalAffect advances the state of the art in both AU detection and expression recognition, establishing a principled connection between causal discovery and interpretable facial behavior. All trained models and source code will be released upon acceptance.

中文标题: CausalAffect：面部情感理解的因果发现

中文摘要: 从面部行为理解人类情感不仅需要准确的识别，还需要对驱动肌肉激活及其表达结果的潜在依赖关系进行结构化推理。虽然动作单元（AUs）长期以来一直是情感计算的基础，但现有方法很少解决如何直接从数据中推断出心理学上合理的AUs与表情之间的因果关系。我们提出了CausalAffect，这是首个用于面部情感分析的因果图发现框架。CausalAffect通过两级极性感知和方向感知的因果层次结构建模AU-AU和AU-Expression依赖关系，整合群体级规律性与样本自适应结构。特征级反事实干预机制进一步增强了真实因果效应，同时抑制了虚假相关性。重要的是，我们的方法既不需要联合标注的数据集，也不需要手工制作的因果先验，却能恢复与既定心理学理论一致的因果结构，同时揭示新的抑制性和先前未表征的依赖关系。在六个基准测试上的广泛实验表明，CausalAffect在AU检测和表情识别方面均推进了最先进水平，建立了因果发现与可解释面部行为之间的原则性连接。所有训练模型和源代码将在接受后发布。

</details>


### [41] [RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards](https://arxiv.org/abs/2512.00473)
*Junyan Ye,Leiqi Zhu,Yuncheng Guo,Dongzhi Jiang,Zilong Huang,Yifan Zhang,Zhiyuan Yan,Haohuan Fu,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: RealGen是一个通过检测器引导奖励实现逼真文本到图像生成的框架，包含提示优化、扩散模型生成和检测器奖励机制，显著提升了图像真实感和细节


<details>
  <summary>Details</summary>
Motivation: 当前先进的文本到图像生成模型（如GPT-Image-1和Qwen-Image）虽然在文本一致性和世界知识方面表现优异，但在逼真图像生成方面仍存在不足，容易产生明显的AI伪影，如"过度光滑的皮肤"和"油光满面"等问题。为了重新实现"与现实无法区分"的生成目标，需要开发专门针对逼真度的生成框架。

Method: RealGen框架包含三个核心组件：1）LLM组件用于提示优化；2）扩散模型用于逼真图像生成；3）创新的"检测器奖励"机制，该机制使用语义级和特征级合成图像检测器来量化伪影并评估真实感。通过GRPO算法利用奖励信号优化整个生成流程。此外，还提出了RealBench自动评估基准，采用检测器评分和竞技场评分实现无需人工参与的逼真度评估。

Result: 实验表明，RealGen在真实感、细节和美学方面显著优于通用模型（如GPT-Image-1和Qwen-Image）以及专门的逼真模型（如FLUX-Krea）。RealBench评估基准能够提供更准确、更符合真实用户体验的评估结果。

Conclusion: RealGen通过检测器引导奖励机制成功提升了文本到图像生成的逼真度，实现了更接近现实世界的图像生成效果。该框架为解决当前生成模型在逼真度方面的不足提供了有效方案，并通过自动化评估基准为逼真度评估提供了可靠工具。

Abstract: With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce " fake" images with distinct AI artifacts, often characterized by "overly smooth skin" and "oily facial sheens". To recapture the original goal of "indistinguishable-from-reality" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a "Detector Reward" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.

中文标题: RealGen：通过检测器引导奖励实现逼真的文本到图像生成

中文摘要: 随着图像生成技术的不断进步，GPT-Image-1和Qwen-Image等先进模型在文本到图像一致性和世界知识方面取得了显著成就。然而，这些模型在逼真图像生成方面仍然存在不足。即使在简单的T2I任务上，它们也倾向于产生带有明显AI伪影的"虚假"图像，通常表现为"过度光滑的皮肤"和"油光满面"的特征。为了重新实现"与现实无法区分"的生成目标，我们提出了RealGen，一个逼真的文本到图像生成框架。RealGen集成了用于提示优化的LLM组件和用于逼真图像生成的扩散模型。受对抗生成的启发，RealGen引入了"检测器奖励"机制，该机制使用语义级和特征级合成图像检测器来量化伪影并评估真实感。我们利用GRPO算法中的奖励信号来优化整个生成流程，显著提升了图像的真实感和细节。此外，我们提出了RealBench，这是一个采用检测器评分和竞技场评分的自动评估基准。它实现了无需人工参与的逼真度评估，产生的结果更准确且更符合真实用户体验。实验表明，RealGen在真实感、细节和美学方面显著优于GPT-Image-1和Qwen-Image等通用模型，以及FLUX-Krea等专门的逼真模型。代码可在https://github.com/yejy53/RealGen获取。

</details>


### [42] [CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration](https://arxiv.org/abs/2512.00493)
*Boshi Tang,Henry Zheng,Rui Huang,Gao Huang*

Main category: cs.CV

TL;DR: CC-FMO提出了一种零样本、相机条件的单图像到3D场景生成方法，通过结合语义感知的向量集表示和细节丰富的结构化潜在表示，以及相机条件的尺度求解算法，实现了高质量的3D场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有的单图像3D场景生成方法存在泛化能力差的问题，主要原因是依赖在小型数据集上训练的专业模型。虽然大规模3D基础模型在实例级生成方面取得了进展，但场景级生成仍面临挑战，包括物体姿态估计不准确和空间不一致性。需要一种能够同时保持输入图像物体布局和实例保真度的解决方案。

Method: CC-FMO采用混合实例生成器，结合语义感知的向量集表示和细节丰富的结构化潜在表示。通过相机条件的尺度求解算法，将基础姿态估计模型应用于场景生成任务，确保场景级一致性。整个流程是零样本的，无需额外训练。

Result: 实验表明，CC-FMO能够一致地生成高保真度、相机对齐的组合场景，在所有最先进方法中表现最佳。该方法在保持物体布局和实例保真度方面都表现出色。

Conclusion: CC-FMO通过创新的混合表示方法和相机条件尺度求解算法，成功解决了单图像到3D场景生成中的关键挑战，为AR/VR和具身AI应用提供了高质量的3D场景生成解决方案。

Abstract: High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.

中文标题: CC-FMO：基于基础模型编排的相机条件零样本单图像到3D场景生成

中文摘要: 从单图像生成高质量3D场景对于AR/VR和具身AI应用至关重要。早期方法由于依赖在精选小数据集上训练的专业模型而难以泛化。虽然大规模3D基础模型的最新进展显著增强了实例级生成，但连贯的场景生成仍然是一个挑战，其性能受到不准确的每物体姿态估计和空间不一致性的限制。为此，本文提出了CC-FMO，一种零样本、相机条件的单图像到3D场景生成流程，能够同时符合输入图像中的物体布局并保持实例保真度。CC-FMO采用混合实例生成器，将语义感知的向量集表示与细节丰富的结构化潜在表示相结合，产生既语义合理又高质量的物体几何。此外，CC-FMO通过简单而有效的相机条件尺度求解算法，使基础姿态估计模型能够应用于场景生成任务，以增强场景级一致性。大量实验表明，CC-FMO能够一致地生成高保真度的相机对齐组合场景，优于所有最先进的方法。

</details>


### [43] [Terrain Sensing with Smartphone Structured Light: 2D Dynamic Time Warping for Grid Pattern Matching](https://arxiv.org/abs/2512.00514)
*Tanaka Nobuaki*

Main category: cs.CV

TL;DR: 本文提出了一种基于智能手机结构光的地形感知系统，通过投影网格图案并分析其变形来检测地面不平度。核心创新是开发了拓扑约束的二维动态时间规整算法，用于在透视变形和遮挡下匹配网格图案。


<details>
  <summary>Details</summary>
Motivation: 低成本移动机器人在不平坦地形上运行时，微小的地面凹凸或倾斜难以通过传统视觉方法检测，但这些细微变化会显著影响机器人的运动稳定性。需要一种简单、低成本且能在资源受限平台上运行的地形感知方案。

Method: 1. 使用智能手机结构光系统将网格图案投射到地面；2. 提出拓扑约束的二维动态时间规整算法，在全局网格一致性约束下进行列向对齐，解决透视变形和部分遮挡下的网格匹配问题；3. 系统设计简单，适合资源有限平台。

Result: 成功开发了基于智能手机的地形感知系统，能够从单个手持设备重建局部地形不平度。提出的2D-DTW算法不仅能有效用于地形感知，还可作为图像处理中匹配结构化网格图案的通用工具。

Conclusion: 该研究证明了智能手机结构光系统可用于低成本地形感知，提出的2D-DTW算法解决了二维网格模式匹配的关键技术挑战，为资源受限平台上的地形感知提供了实用解决方案。

Abstract: Low-cost mobile rovers often operate on uneven terrain where small bumps or tilts are difficult to perceive visually but can significantly affect locomotion stability. To address this problem, we explore a smartphone-based structured-light system that projects a grid pattern onto the ground and reconstructs local terrain unevenness from a single handheld device. The system is inspired by face-recognition projectors, but adapted for ground sensing. A key technical challenge is robustly matching the projected grid with its deformed observation under perspective distortion and partial occlusion. Conventional one-dimensional dynamic time warping (1D-DTW) is not directly applicable to such two-dimensional grid patterns. We therefore propose a topology-constrained two-dimensional dynamic time warping (2D-DTW) algorithm that performs column-wise alignment under a global grid consistency constraint. The proposed method is designed to be simple enough to run on resource limited platforms while preserving the grid structure required for accurate triangulation. We demonstrate that our 2D-DTW formulation can be used not only for terrain sensing but also as a general tool for matching structured grid patterns in image processing scenarios. This paper describes the overall system design as well as the 2D-DTW extension that emerged from this application.

中文标题: 基于智能手机结构光的地形感知：用于网格模式匹配的二维动态时间规整

中文摘要: 低成本移动机器人在不平坦地形上运行时，微小的凹凸或倾斜难以通过视觉感知，但会显著影响运动稳定性。为解决这一问题，我们探索了一种基于智能手机的结构光系统，该系统将网格图案投射到地面上，并从单个手持设备重建局部地形不平度。该系统受面部识别投影仪的启发，但适用于地面感知。一个关键技术挑战是在透视变形和部分遮挡下，稳健地匹配投影网格与其变形观测。传统的一维动态时间规整（1D-DTW）不直接适用于此类二维网格图案。因此，我们提出了一种拓扑约束的二维动态时间规整（2D-DTW）算法，在全局网格一致性约束下执行列向对齐。所提出的方法设计得足够简单，可以在资源有限的平台上运行，同时保持精确三角测量所需的网格结构。我们证明，我们的2D-DTW公式不仅可用于地形感知，还可作为图像处理场景中匹配结构化网格图案的通用工具。本文描述了整体系统设计以及从该应用中产生的2D-DTW扩展。

</details>


### [44] [Image Generation as a Visual Planner for Robotic Manipulation](https://arxiv.org/abs/2512.00532)
*Ye Pang*

Main category: cs.CV

TL;DR: 本文探索了使用预训练图像生成模型作为机器人视觉规划器，通过LoRA微调实现文本条件或轨迹条件的机器人操作视频生成，验证了图像生成器具有可转移的时间先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型需要大量领域特定数据且泛化能力有限，而基于语言-图像语料库训练的现代图像生成模型展现出强大的组合能力，包括合成时间连贯的网格图像，这表明即使没有显式的时间建模，它们也具有视频生成潜力。本文旨在探索这些模型是否可以作为机器人的视觉规划器。

Method: 提出一个两部分框架：1) 文本条件生成：使用语言指令和第一帧图像；2) 轨迹条件生成：使用2D轨迹覆盖图和相同初始帧。通过LoRA轻量微调适配预训练图像生成模型，在Jaco Play、Bridge V2和RT1数据集上进行实验。

Result: 实验表明两种模式都能生成平滑、连贯的机器人视频，且与各自条件对齐。预训练图像生成器编码了可转移的时间先验知识，能够在最小监督下作为视频式机器人规划器。

Conclusion: 预训练图像生成模型具有作为机器人视觉规划器的潜力，通过轻量微调即可实现时间连贯的视频生成，为统一感知、规划和行动的具身智能体提供了新方向。

Abstract: Generating realistic robotic manipulation videos is an important step toward unifying perception, planning, and action in embodied agents. While existing video diffusion models require large domain-specific datasets and struggle to generalize, recent image generation models trained on language-image corpora exhibit strong compositionality, including the ability to synthesize temporally coherent grid images. This suggests a latent capacity for video-like generation even without explicit temporal modeling.
  We explore whether such models can serve as visual planners for robots when lightly adapted using LoRA finetuning. We propose a two-part framework that includes: (1) text-conditioned generation, which uses a language instruction and the first frame, and (2) trajectory-conditioned generation, which uses a 2D trajectory overlay and the same initial frame. Experiments on the Jaco Play dataset, Bridge V2, and the RT1 dataset show that both modes produce smooth, coherent robot videos aligned with their respective conditions.
  Our findings indicate that pretrained image generators encode transferable temporal priors and can function as video-like robotic planners under minimal supervision. Code is released at \href{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}.

中文标题: 图像生成作为机器人操作的视觉规划器

中文摘要: 生成真实的机器人操作视频是实现具身智能体中感知、规划和行动统一的重要一步。虽然现有的视频扩散模型需要大量领域特定数据集且难以泛化，但最近在语言-图像语料库上训练的图像生成模型展现出强大的组合能力，包括合成时间连贯的网格图像的能力。这表明即使没有显式的时间建模，它们也具有类似视频生成的潜在能力。

我们探索了当通过LoRA微调轻量适配时，这类模型是否可以作为机器人的视觉规划器。我们提出了一个包含两部分的框架：(1) 文本条件生成，使用语言指令和第一帧图像；(2) 轨迹条件生成，使用2D轨迹覆盖图和相同的初始帧。在Jaco Play数据集、Bridge V2和RT1数据集上的实验表明，两种模式都能生成平滑、连贯的机器人视频，并与各自的条件对齐。

我们的发现表明，预训练图像生成器编码了可转移的时间先验知识，可以在最小监督下作为视频式机器人规划器。代码发布于\href{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}。

</details>


### [45] [Cross-Temporal 3D Gaussian Splatting for Sparse-View Guided Scene Update](https://arxiv.org/abs/2512.00534)
*Zeyuan An,Yanghang Xiao,Zhiying Leng,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: 提出跨时间3D高斯泼溅框架，利用稀疏视图和历史先验高效更新3D场景，支持非连续捕获和时间变化检测。


<details>
  <summary>Details</summary>
Motivation: 解决3D场景随时间一致性的挑战，特别是在城市规划、灾害评估等实际应用中，密集扫描不可行，需要从稀疏视图更新场景。

Method: 三阶段框架：1）跨时间相机对齐；2）基于干扰的置信度初始化识别未变化区域；3）渐进式跨时间优化集成历史先验。

Result: 实验显示在重建质量和数据效率上显著优于基线方法，支持非连续捕获和时间变化检测。

Conclusion: 该方法为场景版本控制、跨时间数字孪生和长期空间文档化提供了有前景的解决方案。

Abstract: Maintaining consistent 3D scene representations over time is a significant challenge in computer vision. Updating 3D scenes from sparse-view observations is crucial for various real-world applications, including urban planning, disaster assessment, and historical site preservation, where dense scans are often unavailable or impractical. In this paper, we propose Cross-Temporal 3D Gaussian Splatting (Cross-Temporal 3DGS), a novel framework for efficiently reconstructing and updating 3D scenes across different time periods, using sparse images and previously captured scene priors. Our approach comprises three stages: 1) Cross-temporal camera alignment for estimating and aligning camera poses across different timestamps; 2) Interference-based confidence initialization to identify unchanged regions between timestamps, thereby guiding updates; and 3) Progressive cross-temporal optimization, which iteratively integrates historical prior information into the 3D scene to enhance reconstruction quality. Our method supports non-continuous capture, enabling not only updates using new sparse views to refine existing scenes, but also recovering past scenes from limited data with the help of current captures. Furthermore, we demonstrate the potential of this approach to achieve temporal changes using only sparse images, which can later be reconstructed into detailed 3D representations as needed. Experimental results show significant improvements over baseline methods in reconstruction quality and data efficiency, making this approach a promising solution for scene versioning, cross-temporal digital twins, and long-term spatial documentation.

中文标题: 用于稀疏视图引导场景更新的跨时间3D高斯泼溅技术

中文摘要: 在计算机视觉中，保持3D场景表示随时间的一致性是一个重大挑战。从稀疏视图观测中更新3D场景对于各种现实世界应用至关重要，包括城市规划、灾害评估和历史遗址保护，在这些应用中密集扫描通常不可用或不切实际。在本文中，我们提出了跨时间3D高斯泼溅（Cross-Temporal 3DGS），这是一个新颖的框架，用于使用稀疏图像和先前捕获的场景先验，高效地重建和更新不同时间段的3D场景。我们的方法包括三个阶段：1）跨时间相机对齐，用于估计和对齐不同时间戳的相机姿态；2）基于干扰的置信度初始化，用于识别时间戳之间未变化的区域，从而指导更新；以及3）渐进式跨时间优化，迭代地将历史先验信息集成到3D场景中以增强重建质量。我们的方法支持非连续捕获，不仅能够使用新的稀疏视图来优化现有场景，还能在当前捕获的帮助下从有限数据中恢复过去场景。此外，我们展示了这种方法仅使用稀疏图像实现时间变化的潜力，这些图像随后可以根据需要重建为详细的3D表示。实验结果显示，在重建质量和数据效率方面相比基线方法有显著改进，使这种方法成为场景版本控制、跨时间数字孪生和长期空间文档化的有前景解决方案。

</details>


### [46] [SAIDO: Generalizable Detection of AI-Generated Images via Scene-Aware and Importance-Guided Dynamic Optimization in Continual Learning](https://arxiv.org/abs/2512.00539)
*Yongkang Hu,Yu Cheng,Yushuo Zhang,Yuan Xie,Zhaoxia Yin*

Main category: cs.CV

TL;DR: SAIDO是一个通过场景感知和重要性引导动态优化的持续学习框架，用于检测AI生成图像，解决了现有方法在泛化到新兴生成方法和内容类型方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 图像生成技术的滥用引发了安全问题，但现有AI生成图像检测方法在泛化方面面临挑战：难以适应现实场景中不断涌现的新生成方法和内容类型。

Method: 提出场景感知和重要性引导动态优化检测框架（SAIDO）：1）设计基于场景感知的专家模块（SAEM），利用VLLM动态识别和整合新场景，为每个场景分配独立专家模块；2）引入重要性引导动态优化机制（IDOM），通过重要性引导的梯度投影策略优化每个神经元，平衡模型可塑性和稳定性。

Result: 在持续学习任务中，SAIDO在稳定性和可塑性方面均优于当前SOTA方法，平均检测错误率和遗忘率分别相对降低44.22%和40.57%；在开放世界数据集上，平均检测准确率比当前SOTA方法提高9.47%。

Conclusion: SAIDO通过场景感知和重要性引导动态优化，有效解决了AI生成图像检测中的泛化问题，在持续学习环境中表现出优越的性能。

Abstract: The widespread misuse of image generation technologies has raised security concerns, driving the development of AI-generated image detection methods. However, generalization has become a key challenge and open problem: existing approaches struggle to adapt to emerging generative methods and content types in real-world scenarios. To address this issue, we propose a Scene-Aware and Importance-Guided Dynamic Optimization detection framework with continual learning (SAIDO). Specifically, we design Scene-Awareness-Based Expert Module (SAEM) that dynamically identifies and incorporates new scenes using VLLMs. For each scene, independent expert modules are dynamically allocated, enabling the framework to capture scene-specific forgery features better and enhance cross-scene generalization. To mitigate catastrophic forgetting when learning from multiple image generative methods, we introduce Importance-Guided Dynamic Optimization Mechanism (IDOM), which optimizes each neuron through an importance-guided gradient projection strategy, thereby achieving an effective balance between model plasticity and stability. Extensive experiments on continual learning tasks demonstrate that our method outperforms the current SOTA method in both stability and plasticity, achieving 44.22\% and 40.57\% relative reductions in average detection error rate and forgetting rate, respectively. On open-world datasets, it improves the average detection accuracy by 9.47\% compared to the current SOTA method.

中文标题: SAIDO：通过场景感知和重要性引导动态优化在持续学习中实现AI生成图像的泛化检测

中文摘要: 图像生成技术的广泛滥用引发了安全担忧，推动了AI生成图像检测方法的发展。然而，泛化已成为一个关键挑战和开放问题：现有方法难以适应现实场景中新兴的生成方法和内容类型。为解决这一问题，我们提出了一个具有持续学习的场景感知和重要性引导动态优化检测框架（SAIDO）。具体而言，我们设计了基于场景感知的专家模块（SAEM），利用VLLM动态识别和整合新场景。对于每个场景，动态分配独立的专家模块，使框架能够更好地捕捉场景特定的伪造特征并增强跨场景泛化能力。为了缓解从多种图像生成方法学习时的灾难性遗忘，我们引入了重要性引导动态优化机制（IDOM），通过重要性引导的梯度投影策略优化每个神经元，从而在模型可塑性和稳定性之间实现有效平衡。在持续学习任务上的大量实验表明，我们的方法在稳定性和可塑性方面均优于当前SOTA方法，平均检测错误率和遗忘率分别相对降低了44.22%和40.57%。在开放世界数据集上，与当前SOTA方法相比，平均检测准确率提高了9.47%。

</details>


### [47] [Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions](https://arxiv.org/abs/2512.00547)
*Sandika Biswas,Qianyi Wu,Biplab Banerjee,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 提出了一种混合方法，结合3D生成模型、语义感知变形和3D高斯溅射优化，用于多人物多物体动态场景的语义重建，在严重遮挡下保持结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的人类环境高度动态，涉及多个人物与周围物体的复杂交互。虽然3D几何建模对AR/VR、游戏和具身AI等应用至关重要，但由于多样的运动模式和频繁遮挡等挑战，这一领域仍未被充分探索。现有基于3D高斯溅射的方法很少处理多人物多物体场景，特别是在单目设置下，严重遮挡时难以保持结构一致性。

Method: 提出混合方法：1) 使用3D生成模型生成场景元素的高保真网格；2) 语义感知变形，包括刚性物体的刚性变换和人物的基于线性混合蒙皮(LBS)的变形，并将变形后的高保真网格映射到动态场景中；3) 基于3D高斯溅射的优化，进一步细化场景中各元素的对齐。

Result: 在HOI-M3数据集上评估，这是目前唯一包含动态场景中多人物多物体交互的数据集。该方法在产生更好的表面重建方面优于最先进的方法，即使在严重遮挡下也能保持物体结构，并产生多视角和时间一致的几何。

Conclusion: 提出的混合方法有效解决了多人物多物体动态场景重建的挑战，结合了3D生成模型、语义感知变形和3D高斯溅射优化的优势，在严重遮挡下仍能保持结构一致性，为AR/VR、游戏和具身AI等应用提供了高质量的3D几何建模解决方案。

Abstract: Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.

中文标题: 基于资产驱动的多人物-物体交互动态场景语义重建

中文摘要: 现实世界的人类环境高度动态，涉及多个人物与周围物体的复杂交互。虽然此类场景的3D几何建模对AR/VR、游戏和具身AI等应用至关重要，但由于多样的运动模式和频繁遮挡等挑战，这一领域仍未被充分探索。除了新颖视角渲染外，3D高斯溅射(GS)在通过快速优化底层结构产生详细、高质量表面几何方面取得了显著进展。然而，很少有基于GS的方法处理多人物、多物体场景，这主要是由于上述固有挑战。在单目设置中，这些挑战进一步加剧，因为当场景仅基于GS渲染损失进行优化时，在严重遮挡下保持结构一致性变得困难。为了解决这种多人物、多物体动态场景的挑战，我们提出了一种混合方法，有效结合了以下优势：1) 3D生成模型用于生成场景元素的高保真网格；2) 语义感知变形，即刚性物体的刚性变换和人物的基于线性混合蒙皮(LBS)的变形，以及变形后的高保真网格在动态场景中的映射；3) 基于GS的各个元素优化，以进一步细化它们在场景中的对齐。这种混合方法有助于即使在严重遮挡下也能保持物体结构，并能产生多视角和时间一致的几何。我们选择HOI-M3进行评估，因为据我们所知，这是唯一包含动态场景中多人物、多物体交互的数据集。我们的方法在产生更好的此类场景表面重建方面优于最先进的方法。

</details>


### [48] [NeuroVolve: Evolving Visual Stimuli toward Programmable Neural Objectives](https://arxiv.org/abs/2512.00557)
*Haomiao Chen,Keith W Jamison,Mert R. Sabuncu,Amy Kuceyeski*

Main category: cs.CV

TL;DR: NeuroVolve是一个生成框架，通过优化预训练视觉语言模型嵌入空间中的神经目标函数，实现大脑引导的图像合成。它可以编程化地激活或抑制单个或多个脑区，生成满足复杂神经约束的连贯场景。


<details>
  <summary>Details</summary>
Motivation: 先前的研究使用生成模型来复制孤立脑区（如FFA中的面孔）的已知类别选择性，但这些方法对复杂自然视觉过程中脑区如何相互作用提供有限见解。需要一种能够探索脑区交互并生成满足复杂多区域约束的视觉刺激的方法。

Method: NeuroVolve在预训练视觉语言模型的嵌入空间中优化神经目标函数来生成图像。神经目标可编程化地控制单个或多个脑区的激活或抑制。通过跟踪优化步骤，揭示嵌入空间中的语义轨迹，将大脑引导的图像编辑和偏好刺激生成统一在一个过程中。

Result: NeuroVolve能够恢复单个脑区的已知选择性，同时扩展到合成满足复杂多区域约束的连贯场景。它可以生成针对单个ROI的低级和语义特征特定刺激，以及与定制神经目标对齐的刺激。框架捕捉了受试者特定偏好，支持个性化大脑驱动合成。

Conclusion: NeuroVolve提供了一个统一的框架，用于映射、分析和探测视觉信息的神经表征。它揭示了脑区间的协作和拮抗调谐关系，为理解分布式神经模式如何组合创建神经表征提供了新见解。

Abstract: What visual information is encoded in individual brain regions, and how do distributed patterns combine to create their neural representations? Prior work has used generative models to replicate known category selectivity in isolated regions (e.g., faces in FFA), but these approaches offer limited insight into how regions interact during complex, naturalistic vision. We introduce NeuroVolve, a generative framework that provides brain-guided image synthesis via optimization of a neural objective function in the embedding space of a pretrained vision-language model. Images are generated under the guidance of a programmable neural objective, i.e., activating or deactivating single regions or multiple regions together. NeuroVolve is validated by recovering known selectivity for individual brain regions, while expanding to synthesize coherent scenes that satisfy complex, multi-region constraints. By tracking optimization steps, it reveals semantic trajectories through embedding space, unifying brain-guided image editing and preferred stimulus generation in a single process. We show that NeuroVolve can generate both low-level and semantic feature-specific stimuli for single ROIs, as well as stimuli aligned to curated neural objectives. These include co-activation and decorrelation between regions, exposing cooperative and antagonistic tuning relationships. Notably, the framework captures subject-specific preferences, supporting personalized brain-driven synthesis and offering interpretable constraints for mapping, analyzing, and probing neural representations of visual information.

中文标题: NeuroVolve：向可编程神经目标进化的视觉刺激

中文摘要: 哪些视觉信息编码在个体脑区中，分布式模式如何组合创建它们的神经表征？先前研究使用生成模型来复制孤立区域（如FFA中的面孔）的已知类别选择性，但这些方法对复杂自然视觉过程中区域如何相互作用提供有限见解。我们引入NeuroVolve，这是一个通过优化预训练视觉语言模型嵌入空间中的神经目标函数，实现大脑引导图像合成的生成框架。图像在可编程神经目标（即激活或抑制单个区域或多个区域）的指导下生成。NeuroVolve通过恢复单个脑区的已知选择性得到验证，同时扩展到合成满足复杂多区域约束的连贯场景。通过跟踪优化步骤，它揭示嵌入空间中的语义轨迹，将大脑引导的图像编辑和偏好刺激生成统一在单个过程中。我们展示NeuroVolve可以为单个ROI生成低级和语义特征特定刺激，以及与定制神经目标对齐的刺激。这些包括区域间的共激活和解相关，揭示了协作和拮抗调谐关系。值得注意的是，该框架捕捉受试者特定偏好，支持个性化大脑驱动合成，并为映射、分析和探测视觉信息的神经表征提供可解释约束。

</details>


### [49] [Integrating Skeleton Based Representations for Robust Yoga Pose Classification Using Deep Learning Models](https://arxiv.org/abs/2512.00572)
*Mohammed Mohiuddin,Syed Mohammod Minhaz Hossain,Sumaiya Khanam,Prionkar Barua,Aparup Barua,MD Tamim Hossain*

Main category: cs.CV

TL;DR: 该研究创建了Yoga-16数据集，系统评估了三种深度学习架构和三种输入模态，发现基于骨架的表示优于原始图像输入，VGG16结合MediaPipe Pose骨架输入达到96.09%的最高准确率，并通过Grad-CAM提供可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 瑜伽作为全球流行的锻炼方式，错误姿势可能导致受伤。自动化的瑜伽姿势分类可以减少对专家从业者的依赖，但现有研究在系统基准测试方面有限，通常只关注原始图像或单一姿势提取模型。

Method: 创建了Yoga-16数据集，系统评估了三种深度学习架构（VGG16、ResNet50和Xception）和三种输入模态（直接图像、MediaPipe Pose骨架图像和YOLOv8 Pose骨架图像），使用Grad-CAM进行可解释性分析。

Result: 基于骨架的表示优于原始图像输入，VGG16结合MediaPipe Pose骨架输入达到96.09%的最高准确率，Grad-CAM分析提供了模型决策的见解。

Conclusion: 骨架表示在瑜伽姿势分类中表现优异，VGG16与MediaPipe Pose骨架组合效果最佳，该研究为瑜伽姿势识别提供了系统基准和可解释性分析框架。

Abstract: Yoga is a popular form of exercise worldwide due to its spiritual and physical health benefits, but incorrect postures can lead to injuries. Automated yoga pose classification has therefore gained importance to reduce reliance on expert practitioners. While human pose keypoint extraction models have shown high potential in action recognition, systematic benchmarking for yoga pose recognition remains limited, as prior works often focus solely on raw images or a single pose extraction model. In this study, we introduce a curated dataset, 'Yoga-16', which addresses limitations of existing datasets, and systematically evaluate three deep learning architectures (VGG16, ResNet50, and Xception) using three input modalities (direct images, MediaPipe Pose skeleton images, and YOLOv8 Pose skeleton images). Our experiments demonstrate that skeleton-based representations outperform raw image inputs, with the highest accuracy of 96.09% achieved by VGG16 with MediaPipe Pose skeleton input. Additionally, we provide interpretability analysis using Grad-CAM, offering insights into model decision-making for yoga pose classification with cross validation analysis.

中文标题: 基于深度学习模型集成骨架表示进行鲁棒的瑜伽姿势分类

中文摘要: 瑜伽因其精神和身体健康益处而成为全球流行的锻炼方式，但错误的姿势可能导致受伤。因此，自动化的瑜伽姿势分类变得重要，以减少对专家从业者的依赖。虽然人体姿势关键点提取模型在动作识别中显示出巨大潜力，但瑜伽姿势识别的系统基准测试仍然有限，因为先前的工作通常只关注原始图像或单一姿势提取模型。在本研究中，我们引入了一个精心策划的数据集"Yoga-16"，解决了现有数据集的局限性，并系统评估了三种深度学习架构（VGG16、ResNet50和Xception）使用三种输入模态（直接图像、MediaPipe Pose骨架图像和YOLOv8 Pose骨架图像）。我们的实验表明，基于骨架的表示优于原始图像输入，VGG16结合MediaPipe Pose骨架输入达到96.09%的最高准确率。此外，我们使用Grad-CAM提供可解释性分析，为瑜伽姿势分类的模型决策提供见解，并进行交叉验证分析。

</details>


### [50] [SatireDecoder: Visual Cascaded Decoupling for Enhancing Satirical Image Comprehension](https://arxiv.org/abs/2512.00582)
*Yue Jiang,Haiwei Xue,Minghao Han,Mingcheng Li,Xiaolu Hou,Dingkang Yang,Lihua Zhang,Xu Zheng*

Main category: cs.CV

TL;DR: SatireDecoder是一个无需训练的框架，通过视觉级联解耦和多智能体系统来增强讽刺图像理解，将图像分解为细粒度的局部和全局语义表示，并使用不确定性引导的思维链推理来减少误解和幻觉。


<details>
  <summary>Details</summary>
Motivation: 讽刺作为一种结合幽默与隐性批判的艺术表达形式，具有重要的社会价值。然而，当前视觉语言模型在理解纯视觉讽刺方面存在困难，需要同时检测讽刺、解读其微妙含义并识别相关实体。现有模型往往无法有效整合局部实体关系与全局上下文，导致误解、理解偏差和幻觉。

Method: 提出SatireDecoder框架，采用无需训练的方法：1）多智能体系统执行视觉级联解耦，将图像分解为细粒度的局部和全局语义表示；2）引入基于不确定性分析的思维链推理策略，将复杂的讽刺理解过程分解为顺序子任务以最小化不确定性。

Result: 实验结果表明，SatireDecoder在理解视觉讽刺方面显著优于现有基线方法，提高了解释准确性，同时减少了幻觉现象。

Conclusion: SatireDecoder为视觉语言推理在微妙、高级语义任务中提供了有前景的方向，通过视觉级联解耦和不确定性引导的推理策略有效增强了讽刺图像的理解能力。

Abstract: Satire, a form of artistic expression combining humor with implicit critique, holds significant social value by illuminating societal issues. Despite its cultural and societal significance, satire comprehension, particularly in purely visual forms, remains a challenging task for current vision-language models. This task requires not only detecting satire but also deciphering its nuanced meaning and identifying the implicated entities. Existing models often fail to effectively integrate local entity relationships with global context, leading to misinterpretation, comprehension biases, and hallucinations. To address these limitations, we propose SatireDecoder, a training-free framework designed to enhance satirical image comprehension. Our approach proposes a multi-agent system performing visual cascaded decoupling to decompose images into fine-grained local and global semantic representations. In addition, we introduce a chain-of-thought reasoning strategy guided by uncertainty analysis, which breaks down the complex satire comprehension process into sequential subtasks with minimized uncertainty. Our method significantly improves interpretive accuracy while reducing hallucinations. Experimental results validate that SatireDecoder outperforms existing baselines in comprehending visual satire, offering a promising direction for vision-language reasoning in nuanced, high-level semantic tasks.

中文标题: SatireDecoder：用于增强讽刺图像理解的视觉级联解耦框架

中文摘要: 讽刺作为一种结合幽默与隐性批判的艺术表达形式，通过揭示社会问题具有重要的社会价值。尽管讽刺具有文化和社会意义，但讽刺理解，特别是纯视觉形式的讽刺理解，对当前视觉语言模型来说仍然是一项具有挑战性的任务。这项任务不仅需要检测讽刺，还需要解读其微妙含义并识别所涉及的实体。现有模型往往无法有效整合局部实体关系与全局上下文，导致误解、理解偏差和幻觉。为了解决这些限制，我们提出了SatireDecoder，一个无需训练的框架，旨在增强讽刺图像理解。我们的方法提出了一个多智能体系统，执行视觉级联解耦，将图像分解为细粒度的局部和全局语义表示。此外，我们引入了基于不确定性分析的思维链推理策略，将复杂的讽刺理解过程分解为顺序子任务以最小化不确定性。我们的方法显著提高了解释准确性，同时减少了幻觉。实验结果表明，SatireDecoder在理解视觉讽刺方面优于现有基线方法，为视觉语言推理在微妙、高级语义任务中提供了有前景的方向。

</details>


### [51] [Scaling Down to Scale Up: Towards Operationally-Efficient and Deployable Clinical Models via Cross-Modal Low-Rank Adaptation for Medical Vision-Language Models](https://arxiv.org/abs/2512.00597)
*Thuraya Alzubaidi,Farhad R. Nezami,Muzammil Behzad*

Main category: cs.CV

TL;DR: MedCT-VLM通过低秩适配（LoRA）仅微调0.38%参数，将CT基础模型适配到医学影像任务，在18种胸部病理零样本分类中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言基础模型在多个领域展现出强大的零样本能力，但在体积医学影像（如CT）中的应用仍然有限。医学影像领域通常面临标记数据稀缺的问题，需要参数高效的方法来适配大规模预训练模型到下游临床任务。

Method: 提出MedCT-VLM框架，使用低秩适配（LoRA）方法在CT-CLIP模型的视觉和文本编码器的注意力层中插入低秩分解矩阵。仅训练167万个参数（占总参数4.4亿的0.38%），而不是直接微调整个模型。

Result: 在18种胸部病理的零样本分类任务中，LoRA微调显著提升性能：平均AUROC从61.3%提高到68.9%（+7.6pp），准确率从67.2%提高到73.6%（+6.4pp），宏观F1分数从32.1%提高到36.9%（+4.8pp）。

Conclusion: 参数高效方法（如LoRA）能够有效将大规模预训练模型迁移到医学影像任务中，特别是在标记数据稀缺的零样本场景下，为临床部署提供了可操作高效的解决方案。

Abstract: Foundation models trained via vision-language pretraining have demonstrated strong zero-shot capabilities across diverse image domains, yet their application to volumetric medical imaging remains limited. We introduce MedCT-VLM: Medical CT Vision-Language Model, a parameter-efficient vision-language framework designed to adapt large-scale CT foundation models for downstream clinical tasks. MedCT-VLM uses a parameter-efficient approach to adapt CT-CLIP, a contrastive vision-language model trained on 25,692 chest CT volumes, for multi-label pathology classification using Low-Rank Adaptation (LoRA). Rather than fine-tuning the model's 440 M parameters directly, we insert low-rank decomposition matrices into attention layers of both vision and text encoders, training only 1.67M parameters (0.38\% of total). We evaluate on zero-shot classification across 18 thoracic pathologies, where the model must align CT embeddings with unseen text prompts at inference without task-specific training. LoRA fine-tuning improves mean AUROC from 61.3\% to 68.9\% (+7.6 pp), accuracy from 67.2\% to 73.6\% (+6.4 pp), and macro-F1 from 32.1\% to 36.9\% (+4.8 pp). These results demonstrate that parameter-efficient methods can effectively transfer large-scale pretraining to downstream medical imaging tasks, particularly for zero-shot scenarios where labeled data is scarce.

中文标题: 缩小规模以扩大规模：通过跨模态低秩适配实现可操作高效且可部署的临床医学视觉语言模型

中文摘要: 通过视觉语言预训练的基础模型在不同图像领域展现出强大的零样本能力，但在体积医学影像中的应用仍然有限。我们介绍了MedCT-VLM：医学CT视觉语言模型，这是一个参数高效的视觉语言框架，旨在将大规模CT基础模型适配到下游临床任务中。MedCT-VLM采用参数高效的方法，使用低秩适配（LoRA）将CT-CLIP（一个在25,692个胸部CT体积上训练的对比视觉语言模型）适配到多标签病理分类任务中。我们没有直接微调模型的4.4亿参数，而是在视觉和文本编码器的注意力层中插入低秩分解矩阵，仅训练167万个参数（占总参数的0.38%）。我们在18种胸部病理的零样本分类上进行了评估，模型必须在推理时对齐CT嵌入与未见过的文本提示，而无需任务特定训练。LoRA微调将平均AUROC从61.3%提高到68.9%（+7.6个百分点），准确率从67.2%提高到73.6%（+6.4个百分点），宏观F1分数从32.1%提高到36.9%（+4.8个百分点）。这些结果表明，参数高效方法可以有效地将大规模预训练迁移到下游医学影像任务中，特别是在标记数据稀缺的零样本场景中。

</details>


### [52] [Automatic Pith Detection in Tree Cross-Section Images Using Deep Learning](https://arxiv.org/abs/2512.00625)
*Tzu-I Liao,Mahmoud Fakhry,Jibin Yesudas Varghese*

Main category: cs.CV

TL;DR: 本研究评估了五种深度学习模型（YOLOv9、U-Net、Swin Transformer、DeepLabV3和Mask R-CNN）在树木横截面髓心检测中的性能。Swin Transformer在分割精度上表现最佳（准确率0.94），而Mask R-CNN经过NMS处理后IoU从0.45提升至0.80。研究通过数据增强和超参数调优解决了边界精度和泛化能力问题。


<details>
  <summary>Details</summary>
Motivation: 树木横截面髓心检测对林业和木材质量分析至关重要，但目前仍依赖人工操作且容易出错。本研究旨在通过深度学习技术实现自动化髓心检测，提高检测效率和准确性。

Method: 使用582张标注图像的数据集，通过动态数据增强提升模型泛化能力。评估了五种深度学习模型：YOLOv9（边界框检测）、U-Net（结构化模式）、Swin Transformer（精细分割）、DeepLabV3（多尺度特征）和Mask R-CNN（实例分割）。针对Mask R-CNN的重叠检测问题应用了非极大值抑制（NMS）。使用俄勒冈州立大学树木年轮实验室的11张橡树图像测试泛化能力，并额外使用64张标注图像训练表现最差的模型以改善泛化性能。

Result: Swin Transformer表现最佳，准确率达到0.94，在精细分割方面表现突出。YOLOv9在边界框检测中表现良好但边界精度不足。U-Net对结构化模式有效，DeepLabV3能捕获多尺度特征但边界略有不准。Mask R-CNN初始IoU仅为0.45，应用NMS后提升至0.80。在橡树数据集上的泛化测试显示模型具有较好的迁移能力。通过额外数据训练最差模型改善了其泛化性能。

Conclusion: 深度学习在树木横截面髓心检测中具有巨大潜力，模型选择应根据数据集特征和应用需求而定。Swin Transformer在精细分割任务中表现最优，而Mask R-CNN经过NMS处理后性能显著改善。数据增强和超参数调优是解决边界不一致和泛化问题的有效方法。

Abstract: Pith detection in tree cross-sections is essential for forestry and wood quality analysis but remains a manual, error-prone task. This study evaluates deep learning models -- YOLOv9, U-Net, Swin Transformer, DeepLabV3, and Mask R-CNN -- to automate the process efficiently. A dataset of 582 labeled images was dynamically augmented to improve generalization. Swin Transformer achieved the highest accuracy (0.94), excelling in fine segmentation. YOLOv9 performed well for bounding box detection but struggled with boundary precision. U-Net was effective for structured patterns, while DeepLabV3 captured multi-scale features with slight boundary imprecision. Mask R-CNN initially underperformed due to overlapping detections, but applying Non-Maximum Suppression (NMS) improved its IoU from 0.45 to 0.80. Generalizability was next tested using an oak dataset of 11 images from Oregon State University's Tree Ring Lab. Additionally, for exploratory analysis purposes, an additional dataset of 64 labeled tree cross-sections was used to train the worst-performing model to see if this would improve its performance generalizing to the unseen oak dataset. Key challenges included tensor mismatches and boundary inconsistencies, addressed through hyperparameter tuning and augmentation. Our results highlight deep learning's potential for tree cross-section pith detection, with model choice depending on dataset characteristics and application needs.

中文标题: 基于深度学习的树木横截面髓心自动检测

中文摘要: 树木横截面髓心检测对林业和木材质量分析至关重要，但目前仍是一项手动且容易出错的任务。本研究评估了深度学习模型——YOLOv9、U-Net、Swin Transformer、DeepLabV3和Mask R-CNN——以实现高效自动化检测。使用582张标注图像的数据集，通过动态数据增强提升泛化能力。Swin Transformer取得了最高准确率（0.94），在精细分割方面表现突出。YOLOv9在边界框检测中表现良好但在边界精度方面存在困难。U-Net对结构化模式有效，而DeepLabV3能捕获多尺度特征但边界略有不准。Mask R-CNN最初因重叠检测问题表现不佳，但应用非极大值抑制（NMS）后其IoU从0.45提升至0.80。随后使用俄勒冈州立大学树木年轮实验室的11张橡树图像测试了泛化能力。此外，为探索性分析目的，使用额外的64张标注树木横截面图像训练了表现最差的模型，以观察其是否能够改善对未见橡树数据集的泛化性能。主要挑战包括张量不匹配和边界不一致问题，通过超参数调优和数据增强得以解决。我们的结果突显了深度学习在树木横截面髓心检测中的潜力，模型选择应基于数据集特征和应用需求。

</details>


### [53] [Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation](https://arxiv.org/abs/2512.00639)
*Mahmoud El Hussieni*

Main category: cs.CV

TL;DR: 该研究使用YOLOv5实例分割算法对甲状腺结节进行分割，发现包含多普勒图像能显著提升分割性能，YOLOv5-Large模型在包含多普勒图像的数据集上获得最佳性能（Dice分数91%，mAP 0.87）。


<details>
  <summary>Details</summary>
Motivation: 全球甲状腺癌发病率上升，需要开发计算机辅助检测方法。甲状腺结节的准确分割是开发AI辅助临床决策支持系统的关键第一步。

Method: 使用YOLOv5实例分割算法（包括Nano、Small、Medium、Large和XLarge变体）在超声图像上进行甲状腺结节分割。评估了两个数据集版本：包含多普勒图像和不包含多普勒图像。

Result: YOLOv5-Large算法在包含多普勒图像的数据集上获得最佳性能：Dice分数91%，mAP 0.87。包含多普勒图像显著提升了所有模型变体的性能（例如，YOLOv5-Small模型从不包含多普勒时的79% Dice分数提升）。

Conclusion: YOLOv5实例分割为甲状腺结节检测提供了有效的实时方法，多普勒图像（通常被医生排除）能显著改善分割性能，具有在自动诊断系统中临床应用的潜力。

Abstract: The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.

中文标题: 多普勒增强深度学习：使用YOLOv5实例分割改进甲状腺结节分割

中文摘要: 全球甲状腺癌患病率的增加推动了各种计算机辅助检测方法的发展。甲状腺结节的准确分割是开发AI辅助临床决策支持系统的关键第一步。本研究专注于使用YOLOv5算法在超声图像上进行甲状腺结节的实例分割。我们评估了多个YOLOv5变体（Nano、Small、Medium、Large和XLarge）在两个数据集版本上的表现，包括包含和不包含多普勒图像的数据集。YOLOv5-Large算法在包含多普勒图像的数据集上获得了最高性能，Dice分数为91%，mAP为0.87。值得注意的是，我们的结果表明，通常被医生排除的多普勒图像可以显著改善分割性能。当排除多普勒图像时，YOLOv5-Small模型获得79%的Dice分数，而包含多普勒图像则提高了所有模型变体的性能。这些发现表明，使用YOLOv5进行实例分割为甲状腺结节检测提供了一种有效的实时方法，在自动诊断系统中具有潜在的临床应用。

</details>


### [54] [MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba](https://arxiv.org/abs/2512.00647)
*Shanhui Liu,Rui Xu,Yunke Wang*

Main category: cs.CV

TL;DR: CF-ViM提出了一种从粗到细的自适应视觉Mamba框架，通过动态分辨率分配根据图像复杂性调整计算，在保持准确性的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉Mamba的效率受限于输入token数量，传统token缩减方法（剪枝/合并）会导致信息丢失，且对所有图像统一应用细粒度处理不够高效。研究发现简单图像可以在粗分辨率下处理，只有复杂图像需要细化。

Method: 提出CF-ViM框架：1）粗粒度推理阶段：将图像划分为大块减少token数量；2）置信度评估：当预测置信度低时触发细化；3）细粒度处理：仅对选定区域进行高分辨率重处理；4）动态分辨率分配策略根据图像复杂性自适应调整计算资源。

Result: 在ImageNet数据集上的实验表明，CF-ViM在准确性和效率方面均优于基线视觉Mamba和现有最先进的token缩减技术，实现了更好的计算-精度权衡。

Conclusion: CF-ViM通过从粗到细的自适应处理策略，有效解决了视觉Mamba的效率瓶颈问题，在保持视觉信息完整性的同时显著提升推理效率，为高效视觉模型设计提供了新思路。

Abstract: Vision Mamba has emerged as a promising and efficient alternative to Vision Transformers, yet its efficiency remains fundamentally constrained by the number of input tokens. Existing token reduction approaches typically adopt token pruning or merging to reduce computation. However, they inherently lead to information loss, as they discard or compress token representations. This problem is exacerbated when applied uniformly to fine-grained token representations across all images, regardless of visual complexity. We observe that not all inputs require fine-grained processing. Simple images can be effectively handled at coarse resolution, while only complex ones may warrant refinement. Based on this insight, we propose \textit{Coarse-to-Fine Vision Mamba (CF-ViM)}, an adaptive framework for efficient inference. CF-ViM first performs coarse-grained inference by dividing the input image into large patches, significantly reducing the token length and computation. When the model's prediction confidence is low, selected regions are re-processed at a finer resolution to recover critical visual details with minimal additional cost. This dynamic resolution assignment strategy allows CF-ViM to allocate computation adaptively according to image complexity, ensuring efficient processing without compromising essential visual information. Experiments on ImageNet demonstrate that CF-ViM outperforms both the baseline Vision Mamba and state-of-the-art token reduction techniques in terms of accuracy and efficiency.

中文标题: MambaScope：从粗到细的视觉Mamba高效范围界定

中文摘要: 视觉Mamba已成为视觉Transformer的一个有前景且高效的替代方案，但其效率仍受输入token数量的根本限制。现有的token缩减方法通常采用token剪枝或合并来减少计算量。然而，这些方法本质上会导致信息丢失，因为它们丢弃或压缩了token表示。当统一应用于所有图像的细粒度token表示时，无论视觉复杂性如何，这个问题都会加剧。我们观察到并非所有输入都需要细粒度处理。简单图像可以在粗分辨率下有效处理，而只有复杂图像可能需要细化。基于这一洞察，我们提出了\textit{从粗到细的视觉Mamba（CF-ViM）}，这是一个用于高效推理的自适应框架。CF-ViM首先通过将输入图像划分为大块来执行粗粒度推理，显著减少token长度和计算量。当模型的预测置信度较低时，选定区域会以更精细的分辨率重新处理，以最小的额外成本恢复关键的视觉细节。这种动态分辨率分配策略使CF-ViM能够根据图像复杂性自适应地分配计算资源，确保高效处理而不损害必要的视觉信息。在ImageNet上的实验表明，CF-ViM在准确性和效率方面都优于基线视觉Mamba和最先进的token缩减技术。

</details>


### [55] [Realistic Handwritten Multi-Digit Writer (MDW) Number Recognition Challenges](https://arxiv.org/abs/2512.00676)
*Kiri L. Wagstaff*

Main category: cs.CV

TL;DR: 该论文创建了更真实的多数字书写者（MDW）基准数据集，用于评估手写多数字识别系统，发现孤立数字分类器在多数字识别中表现不佳，需要新的进展来解决实际问题。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，数字通常以多个数字的形式出现，且由同一人书写（如邮政编码、支票金额、预约时间等）。传统的孤立数字分类研究无法准确反映真实的多数字识别需求，需要更贴近实际的基准数据集。

Method: 利用NIST数字图像中书写者的知识，创建了更真实的多数字书写者（MDW）基准数据集。这些数据集配备了特定任务的性能指标，超越了传统的错误计算，更贴近实际应用影响。

Result: 研究发现，在孤立数字上表现良好的分类器在多数字识别任务中表现不佳。MDW基准为开发能够利用任务特定知识的方法创造了机会，这些方法可以显著超越单个数字分类方法的性能。

Conclusion: 要解决真实的数字识别问题，需要超越孤立数字分类的额外进展。MDW基准提供了更贴近实际应用的评估框架，并促进了能够利用任务特定知识的方法开发。

Abstract: Isolated digit classification has served as a motivating problem for decades of machine learning research. In real settings, numbers often occur as multiple digits, all written by the same person. Examples include ZIP Codes, handwritten check amounts, and appointment times. In this work, we leverage knowledge about the writers of NIST digit images to create more realistic benchmark multi-digit writer (MDW) data sets. As expected, we find that classifiers may perform well on isolated digits yet do poorly on multi-digit number recognition. If we want to solve real number recognition problems, additional advances are needed. The MDW benchmarks come with task-specific performance metrics that go beyond typical error calculations to more closely align with real-world impact. They also create opportunities to develop methods that can leverage task-specific knowledge to improve performance well beyond that of individual digit classification methods.

中文标题: 真实手写多数字书写者（MDW）数字识别挑战

中文摘要: 孤立数字分类作为机器学习研究的激励问题已有数十年历史。在真实场景中，数字通常以多个数字的形式出现，且都由同一人书写，例如邮政编码、手写支票金额和预约时间。在本工作中，我们利用NIST数字图像中书写者的知识，创建了更真实的多数字书写者（MDW）基准数据集。正如预期，我们发现分类器可能在孤立数字上表现良好，但在多数字识别中表现不佳。如果我们想要解决真实的数字识别问题，需要额外的进展。MDW基准配备了特定任务的性能指标，超越了典型的错误计算，更贴近实际世界的影响。它们还为开发能够利用任务特定知识的方法创造了机会，这些方法可以显著超越单个数字分类方法的性能。

</details>


### [56] [Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer](https://arxiv.org/abs/2512.00677)
*Dong In Lee,Hyungjun Doh,Seunggeun Chi,Runlin Duan,Sangpil Kim,Karthik Ramani*

Main category: cs.CV

TL;DR: Dynamic-eDiTor是一个无需训练、基于文本驱动的4D场景编辑框架，利用多模态扩散Transformer和4D高斯溅射技术，通过时空子网格注意力和上下文令牌传播机制，实现了高质量、时空一致的动态场景编辑。


<details>
  <summary>Details</summary>
Motivation: 当前4D场景编辑面临的主要挑战是如何在编辑过程中保持跨空间和时间的多视角一致性。现有方法依赖2D扩散模型独立编辑各帧，导致运动失真、几何漂移和不完整编辑等问题，需要一种能够同时保证时空一致性的编辑方案。

Method: 采用多模态扩散Transformer（MM-DiT）和4D高斯溅射（4DGS）框架，包含两个核心组件：1）时空子网格注意力（STGA）用于局部一致的跨视角和时间融合；2）上下文令牌传播（CTP）通过令牌继承和光流引导令牌替换实现全局传播。整个系统无需额外训练，可直接优化预训练的源4DGS。

Result: 在DyNeRF多视角视频数据集上的实验表明，该方法在编辑保真度、多视角一致性和时间一致性方面均优于现有方法，能够实现高质量、无缝的动态场景编辑。

Conclusion: Dynamic-eDiTor成功解决了文本驱动4D场景编辑中的时空一致性问题，通过创新的STGA和CTP机制，实现了无需训练的、高质量的动态场景编辑，为4D内容创作提供了有效的技术方案。

Abstract: Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: https://di-lee.github.io/dynamic-eDiTor/

中文标题: Dynamic-eDiTor：基于多模态扩散Transformer的无训练文本驱动4D场景编辑

中文摘要: 最近在4D表示方面的进展，如动态NeRF和4D高斯溅射（4DGS），已经实现了动态4D场景重建。然而，文本驱动的4D场景编辑仍然探索不足，因为在编辑过程中确保跨空间和时间的多视角和时间一致性具有挑战性。现有研究依赖于独立编辑帧的2D扩散模型，通常会导致运动失真、几何漂移和不完整编辑。我们提出了Dynamic-eDiTor，一个基于多模态扩散Transformer（MM-DiT）和4DGS的无训练文本驱动4D编辑框架。该机制包括用于局部一致跨视角和时间融合的时空子网格注意力（STGA），以及通过令牌继承和光流引导令牌替换实现全局传播的上下文令牌传播（CTP）。这些组件共同使Dynamic-eDiTor能够执行无缝、全局一致的多视角视频编辑，无需额外训练，并直接优化预训练的源4DGS。在DyNeRF多视角视频数据集上的大量实验表明，我们的方法在编辑保真度以及多视角和时间一致性方面优于先前方法。项目页面包含结果和代码：https://di-lee.github.io/dynamic-eDiTor/

</details>


### [57] [Silhouette-based Gait Foundation Model](https://arxiv.org/abs/2512.00691)
*Dingqiang Ye,Chao Fan,Kartik Narayan,Bingzhe Wu,Chengwen Luo,Jianqiang Li,Vishal M. Patel*

Main category: cs.CV

TL;DR: FoundationGait是首个基于轮廓的可扩展步态基础模型，通过自监督预训练在12个数据集上学习通用步态表示，在多种任务和数据集上实现强大性能


<details>
  <summary>Details</summary>
Motivation: 当前步态识别模型存在两个主要问题：1）可扩展性不足，未能遵循缩放定律；2）泛化能力有限，不同任务需要独立模型。需要构建一个统一的步态基础模型来解决这些问题

Method: 提出FoundationGait框架，采用自监督预训练方法，在12个公共步态数据集（超过200万个行走序列）上训练，最大版本有近1.3亿参数。模型能够处理多种输入模态和任务

Result: 在多种步态数据集、条件和任务上表现优异，包括：在野外Gait3D数据集上实现48.0%的零样本Rank-1准确率（1000个测试对象），在实验室OU-MVLP数据集上达到64.5%（5000+测试对象），在人体识别、脊柱侧弯筛查、抑郁预测等任务上都有良好表现

Conclusion: FoundationGait是首个可扩展的步态基础模型，成功解决了步态模型的可扩展性和泛化性问题，为步态理解建立了新的里程碑，代码和模型已开源

Abstract: Gait patterns play a critical role in human identification and healthcare analytics, yet current progress remains constrained by small, narrowly designed models that fail to scale or generalize. Building a unified gait foundation model requires addressing two longstanding barriers: (a) Scalability. Why have gait models historically failed to follow scaling laws? (b) Generalization. Can one model serve the diverse gait tasks that have traditionally been studied in isolation? We introduce FoundationGait, the first scalable, self-supervised pretraining framework for gait understanding. Its largest version has nearly 0.13 billion parameters and is pretrained on 12 public gait datasets comprising over 2 million walking sequences. Extensive experiments demonstrate that FoundationGait, with or without fine-tuning, performs robustly across a wide spectrum of gait datasets, conditions, tasks (e.g., human identification, scoliosis screening, depression prediction, and attribute estimation), and even input modality. Notably, it achieves 48.0% zero-shot rank-1 accuracy on the challenging in-the-wild Gait3D dataset (1,000 test subjects) and 64.5% on the largest in-the-lab OU-MVLP dataset (5,000+ test subjects), setting a new milestone in robust gait recognition. Coming code and model: https://github.com/ShiqiYu/OpenGait.

中文标题: 基于轮廓的步态基础模型

中文摘要: 步态模式在人体识别和医疗健康分析中起着关键作用，但当前进展仍受限于小型、设计狭窄的模型，这些模型无法扩展或泛化。构建统一的步态基础模型需要解决两个长期存在的障碍：(a) 可扩展性：为什么步态模型历史上未能遵循缩放定律？(b) 泛化性：一个模型能否服务于传统上孤立研究的多样化步态任务？我们引入了FoundationGait，这是首个用于步态理解的可扩展自监督预训练框架。其最大版本拥有近1.3亿参数，并在12个公共步态数据集上预训练，包含超过200万个行走序列。大量实验表明，无论是否微调，FoundationGait都能在广泛的步态数据集、条件、任务（如人体识别、脊柱侧弯筛查、抑郁预测和属性估计）甚至输入模态上表现稳健。值得注意的是，它在具有挑战性的野外Gait3D数据集（1000个测试对象）上实现了48.0%的零样本Rank-1准确率，在最大的实验室OU-MVLP数据集（5000+测试对象）上达到64.5%，为稳健步态识别设立了新的里程碑。即将发布的代码和模型：https://github.com/ShiqiYu/OpenGait。

</details>


### [58] [Affordance-First Decomposition for Continual Learning in Video-Language Understanding](https://arxiv.org/abs/2512.00694)
*Mengzhu Xu,Hanzhi Liu,Ningkang Peng,Qianyu Chen,Canran Xiao*

Main category: cs.CV

TL;DR: AFD是一种持续学习方法，通过将视频映射到缓慢变化的可供性令牌作为共享的时间对齐基底，结合轻量级查询路由调度器进行针对性适应，在视频-语言理解任务中实现稳定性和可塑性的明确分离。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法存在以下问题：1) 模糊了应该保持稳定与应该适应的部分；2) 依赖静态路由/容量分配；3) 需要回放过去视频，存在内存和隐私限制。需要一种能够在现实内存和隐私约束下，明确指定稳定性所在和可塑性重点的方法。

Method: AFD方法包含三个核心组件：1) 将视频映射到缓慢变化的可供性令牌，形成共享的时间对齐基底；2) 轻量级查询路由、冲突感知调度器，仅在需要时集中适应和扩展容量；3) 通过弱对齐和教师一致性稳定基底，使用仅问题回放进行训练。

Result: AFD在多个协议上达到最先进水平：领域增量VideoQA平均准确率51.6%，遗忘率-1.8%；ViLCo R@1@0.5为29.6%(MQ)和20.7%(NLQ)，18.4% stAP@0.25(VQ)；时间增量iVQA准确率39.5%，遗忘率-1.6%。

Conclusion: AFD提供了一种明确、可解释的分离方法，在稳定的交互中心基底和针对性适应之间建立清晰界限，为视频-语言理解的持续学习提供了有效解决方案。

Abstract: Continual learning for video--language understanding is increasingly important as models face non-stationary data, domains, and query styles, yet prevailing solutions blur what should stay stable versus what should adapt, rely on static routing/capacity, or require replaying past videos. We aim to explicitly specify where stability lives and where plasticity should be focused under realistic memory and privacy constraints. We introduce Affordance-First Decomposition (AFD): videos are mapped to slowly varying affordance tokens that form a shared, time-aligned substrate, while a lightweight, query-routed, conflict-aware scheduler concentrates adaptation and grows capacity only when needed. The substrate is stabilized via weak alignment and teacher consistency, and training uses question-only replay. AFD achieves state-of-the-art across protocols: 51.6% average accuracy with -1.8% forgetting on domain-incremental VideoQA, ViLCo R@1@0.5 of 29.6% (MQ) and 20.7% (NLQ) with 18.4% stAP@0.25 (VQ), and 39.5% accuracy with -1.6% forgetting on time-incremental iVQA. Overall, AFD offers an explicit, interpretable split between a stable interaction-centered substrate and targeted adaptation.

中文标题: 面向视频-语言理解持续学习的可供性优先分解

中文摘要: 随着模型面临非平稳数据、领域和查询风格，视频-语言理解的持续学习变得越来越重要，但现有解决方案模糊了应该保持稳定与应该适应的部分，依赖静态路由/容量分配，或需要回放过去视频。我们的目标是在现实内存和隐私约束下，明确指定稳定性所在和可塑性重点。我们引入了可供性优先分解(AFD)：视频被映射到缓慢变化的可供性令牌，形成共享的时间对齐基底，而轻量级、查询路由、冲突感知调度器仅在需要时集中适应和扩展容量。基底通过弱对齐和教师一致性稳定，训练使用仅问题回放。AFD在多个协议上达到最先进水平：领域增量VideoQA平均准确率51.6%，遗忘率-1.8%；ViLCo R@1@0.5为29.6%(MQ)和20.7%(NLQ)，18.4% stAP@0.25(VQ)；时间增量iVQA准确率39.5%，遗忘率-1.6%。总体而言，AFD提供了稳定交互中心基底和针对性适应之间的明确、可解释分离。

</details>


### [59] [Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation](https://arxiv.org/abs/2512.00706)
*Chengzhi Yu,Yifan Xu,Yifan Chen,Wenyi Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于策略内数据和幻觉分类器的LVLM幻觉缓解方法，通过迭代DPO算法显著降低幻觉率，使开源模型性能超越GPT-4V。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态任务中表现出色，但幻觉问题严重阻碍其实际应用。现有方法使用策略外数据效果有限，且标注过程可能引入额外幻觉，需要更有效的数据选择和标注策略。

Method: 1. 分析策略内数据优于策略外数据；2. 训练二元幻觉分类器提供干净标注；3. 设计动态样本重加权的迭代DPO算法；4. 在三个基准测试上验证方法有效性。

Result: 1. LLaVA-1.5-7B在MMHalBench上幻觉率降低50.8%；2. 在Object HalBench上平均幻觉率降低79.5%；3. LLaVA-1.5-13B性能超越GPT-4V；4. 在三个基准测试中优于8个SOTA基线。

Conclusion: 策略内数据结合幻觉分类器和迭代DPO算法能有效缓解LVLM幻觉问题，显著提升模型性能，使开源模型达到甚至超越商业模型的水平。

Abstract: Recently, large vision-language models (LVLMs) have risen to be a promising approach for multimodal tasks. However, principled hallucination mitigation remains a critical challenge.In this work, we first analyze the data generation process in LVLM hallucination mitigation and affirm that on-policy data significantly outperforms off-policy data, which thus calls for efficient and reliable preference annotation of on-policy data. We then point out that, existing annotation methods introduce additional hallucination in training samples, which may enhance the model's hallucination patterns, to address this problem, we propose training a hallucination classifier giving binary annotations, which guarantee clean chosen samples for the subsequent alignment. To further harness of the power of on-policy data, we design a robust iterative direct preference optimization (DPO) algorithm adopting a dynamic sample reweighting scheme. We conduct comprehensive experiments on three benchmarks with comparison to 8 state-of-the-art baselines. In particular, our approach reduces the hallucination rate of LLaVA-1.5-7B on MMHalBench by 50.8% and the average hallucination rate on Object HalBench by 79.5%; more significantly, our method fully taps into the potential of open-source models, enabling LLaVA-1.5-13B to even surpass the performance of GPT-4V.

中文标题: 基于策略内数据优化LVLMs实现有效幻觉缓解

中文摘要: 最近，大型视觉语言模型（LVLMs）已成为多模态任务的一种有前景的方法。然而，原则性的幻觉缓解仍然是一个关键挑战。在这项工作中，我们首先分析了LVLM幻觉缓解中的数据生成过程，并确认策略内数据显著优于策略外数据，这因此需要高效可靠的策略内数据偏好标注。然后我们指出，现有的标注方法在训练样本中引入了额外的幻觉，这可能会增强模型的幻觉模式；为了解决这个问题，我们提出训练一个给出二元标注的幻觉分类器，为后续对齐保证干净的选定样本。为了进一步利用策略内数据的力量，我们设计了一个采用动态样本重加权方案的鲁棒迭代直接偏好优化（DPO）算法。我们在三个基准测试上进行了全面的实验，并与8个最先进的基线进行了比较。特别是，我们的方法将LLaVA-1.5-7B在MMHalBench上的幻觉率降低了50.8%，在Object HalBench上的平均幻觉率降低了79.5%；更重要的是，我们的方法充分挖掘了开源模型的潜力，使LLaVA-1.5-13B甚至超越了GPT-4V的性能。

</details>


### [60] [RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images](https://arxiv.org/abs/2512.00718)
*Deliang Wang,Peng Liu*

Main category: cs.CV

TL;DR: RS-ISRefiner是一个专门为遥感图像设计的交互式分割框架，通过适配器调优策略保留视觉基础模型能力，结合混合注意力机制处理尺度变化和复杂场景，改进交互历史整合，在多个遥感数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有交互式分割方法主要针对自然图像设计，难以适应遥感图像的特殊挑战，包括尺度变化大、边界不规则、背景复杂、标注数据有限和计算开销大等问题。需要专门针对遥感图像特点设计的交互式分割框架。

Method: 1. 基于适配器的调优策略：保留视觉基础模型的通用表示，同时高效学习遥感特定特征；2. 混合注意力机制：集成卷积局部建模和Transformer全局推理，增强对尺度多样性和复杂场景的鲁棒性；3. 改进的概率图调制方案：有效整合历史用户交互，实现稳定迭代优化和边界保真。

Result: 在六个遥感数据集（iSAID、ISPRS Potsdam、SandBar、NWPU、LoveDA Urban、WHUBuilding）上的实验表明，RS-ISRefiner在分割精度、效率和交互成本方面持续优于现有最先进的交互式分割方法。

Conclusion: RS-ISRefiner框架有效解决了遥感图像交互式分割的挑战，通过适配器调优、混合注意力机制和改进的交互整合方案，实现了高质量的分割性能，具有良好的有效性和泛化能力，适用于实际遥感应用场景。

Abstract: Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.

中文标题: RS-ISRefiner：面向遥感图像交互式分割的视觉基础模型适配优化

中文摘要: 交互式图像分割在遥感影像精确标注中起着关键作用，因为遥感对象通常表现出尺度变化、不规则边界和复杂背景。然而，现有主要针对自然图像设计的交互式分割方法由于标注数据有限和计算开销大，难以推广到遥感领域。为解决这些挑战，我们提出了RS-ISRefiner，一种专门针对遥感图像的基于点击的交互式分割框架。该框架采用基于适配器的调优策略，既保留了视觉基础模型的通用表示能力，又能高效学习遥感特定的空间和边界特征。通过集成卷积局部建模与基于Transformer的全局推理的混合注意力机制，增强了对尺度多样性和场景复杂性的鲁棒性。此外，改进的概率图调制方案有效整合了历史用户交互，实现了更稳定的迭代优化和更高的边界保真度。在六个遥感数据集（包括iSAID、ISPRS Potsdam、SandBar、NWPU、LoveDA Urban和WHUBuilding）上的综合实验表明，RS-ISRefiner在分割精度、效率和交互成本方面持续优于最先进的交互式分割方法。这些结果证实了我们框架的有效性和泛化能力，使其非常适用于实际遥感场景中的高质量实例分割。

</details>


### [61] [TrajDiff: End-to-end Autonomous Driving without Perception Annotation](https://arxiv.org/abs/2512.00723)
*Xingtai Gui,Jianbo Zhao,Wencheng Han,Jikai Wang,Jiahao Gong,Feiyang Tan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: TrajDiff是一个无需感知标注的端到端自动驾驶框架，使用轨迹导向的BEV条件扩散模型直接从传感器输入生成驾驶轨迹，在NAVSIM基准上达到87.5 PDMS的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶系统虽然能有效提取环境特征，但通常依赖辅助感知任务，而手动感知标注成本高昂。因此，开发无需感知标注的规划范式变得至关重要，以降低系统开发成本并提高可扩展性。

Method: 1. 提出轨迹导向的BEV条件扩散框架(TrajDiff)，仅需原始传感器输入和未来轨迹
2. 构建高斯BEV热图目标，自然捕捉驾驶模态
3. 设计轨迹导向BEV编码器，无需感知监督提取TrajBEV特征
4. 引入轨迹导向BEV扩散Transformer(TB-DiT)，利用自车状态和TrajBEV特征直接生成多样轨迹
5. 消除对手工运动先验的需求

Result: 1. 在NAVSIM基准测试中达到87.5 PDMS，在所有无需标注方法中实现最先进性能
2. 通过数据扩展进一步提升到88.5 PDMS
3. 性能与先进的基于感知方法相当
4. 证明了在无需标注设置下数据扩展的有效性

Conclusion: TrajDiff成功建立了一个完全无需感知标注的端到端自动驾驶生成框架，通过创新的轨迹导向BEV编码和扩散Transformer设计，实现了与基于感知方法相当的性能，为降低自动驾驶系统开发成本提供了有效解决方案。

Abstract: End-to-end autonomous driving systems directly generate driving policies from raw sensor inputs. While these systems can extract effective environmental features for planning, relying on auxiliary perception tasks, developing perception annotation-free planning paradigms has become increasingly critical due to the high cost of manual perception annotation. In this work, we propose TrajDiff, a Trajectory-oriented BEV Conditioned Diffusion framework that establishes a fully perception annotation-free generative method for end-to-end autonomous driving. TrajDiff requires only raw sensor inputs and future trajectory, constructing Gaussian BEV heatmap targets that inherently capture driving modalities. We design a simple yet effective trajectory-oriented BEV encoder to extract the TrajBEV feature without perceptual supervision. Furthermore, we introduce Trajectory-oriented BEV Diffusion Transformer (TB-DiT), which leverages ego-state information and the predicted TrajBEV features to directly generate diverse yet plausible trajectories, eliminating the need for handcrafted motion priors. Beyond architectural innovations, TrajDiff enables exploration of data scaling benefits in the annotation-free setting. Evaluated on the NAVSIM benchmark, TrajDiff achieves 87.5 PDMS, establishing state-of-the-art performance among all annotation-free methods. With data scaling, it further improves to 88.5 PDMS, which is comparable to advanced perception-based approaches. Our code and model will be made publicly available.

中文标题: TrajDiff：无需感知标注的端到端自动驾驶系统

中文摘要: 端到端自动驾驶系统直接从原始传感器输入生成驾驶策略。虽然这些系统能够提取有效的环境特征用于规划，但由于手动感知标注成本高昂，开发无需感知标注的规划范式变得越来越重要。在本工作中，我们提出了TrajDiff，一种轨迹导向的BEV条件扩散框架，为端到端自动驾驶建立了完全无需感知标注的生成方法。TrajDiff仅需要原始传感器输入和未来轨迹，构建了固有捕捉驾驶模态的高斯BEV热图目标。我们设计了一个简单而有效的轨迹导向BEV编码器，无需感知监督即可提取TrajBEV特征。此外，我们引入了轨迹导向BEV扩散Transformer（TB-DiT），它利用自车状态信息和预测的TrajBEV特征直接生成多样且合理的轨迹，消除了对手工运动先验的需求。除了架构创新，TrajDiff使得在无需标注的设置下探索数据扩展优势成为可能。在NAVSIM基准测试中，TrajDiff实现了87.5 PDMS，在所有无需标注方法中建立了最先进的性能。通过数据扩展，它进一步提升到88.5 PDMS，这与先进的基于感知的方法相当。我们的代码和模型将公开提供。

</details>


### [62] [Generalized Medical Phrase Grounding](https://arxiv.org/abs/2512.01085)
*Wenjun Zhang,Shekhar S. Chandra,Aaron Nicolson*

Main category: cs.CV

TL;DR: 本文提出了广义医学短语定位（GMPG）任务和MedGrounder模型，解决了传统医学短语定位只能返回单个边界框的局限性，能够处理多区域发现、非诊断性文本和不可定位短语，实现了更灵活和实用的医学报告定位。


<details>
  <summary>Details</summary>
Motivation: 现有医学短语定位系统大多遵循指代表达理解范式，每个短语只返回一个边界框，但真实医学报告经常包含多区域发现、非诊断性文本和不可定位短语（如否定句、正常解剖描述），这限制了现有系统的实用性。

Method: 提出了MedGrounder模型，采用两阶段训练：1）在报告句子-解剖框对齐数据集上进行预训练；2）在报告句子-人工标注框数据集上进行微调。模型能够将每个句子映射到零个、一个或多个评分区域。

Result: 在PadChest-GR和MS-CXR数据集上的实验表明，MedGrounder实现了强大的零样本迁移能力，在多区域和不可定位短语方面优于REC风格和基础报告生成基线，同时使用更少的人工框标注。

Conclusion: MedGrounder是第一个广义医学短语定位模型，能够处理真实医学报告中的复杂情况，并且可以与现有报告生成器组合使用，无需重新训练即可生成定位报告，提高了医学报告解释的实用性和灵活性。

Abstract: Medical phrase grounding (MPG) maps textual descriptions of radiological findings to corresponding image regions. These grounded reports are easier to interpret, especially for non-experts. Existing MPG systems mostly follow the referring expression comprehension (REC) paradigm and return exactly one bounding box per phrase. Real reports often violate this assumption. They contain multi-region findings, non-diagnostic text, and non-groundable phrases, such as negations or descriptions of normal anatomy. Motivated by this, we reformulate the task as generalised medical phrase grounding (GMPG), where each sentence is mapped to zero, one, or multiple scored regions. To realise this formulation, we introduce the first GMPG model: MedGrounder. We adopted a two-stage training regime: pre-training on report sentence--anatomy box alignment datasets and fine-tuning on report sentence--human annotated box datasets. Experiments on PadChest-GR and MS-CXR show that MedGrounder achieves strong zero-shot transfer and outperforms REC-style and grounded report generation baselines on multi-region and non-groundable phrases, while using far fewer human box annotations. Finally, we show that MedGrounder can be composed with existing report generators to produce grounded reports without retraining the generator.

中文标题: 广义医学短语定位

中文摘要: 医学短语定位（MPG）将放射学发现的文本描述映射到相应的图像区域。这些定位报告更容易解释，特别是对于非专家而言。现有的MPG系统大多遵循指代表达理解（REC）范式，每个短语返回一个边界框。真实报告经常违反这一假设，包含多区域发现、非诊断性文本以及不可定位的短语，如否定句或正常解剖结构描述。受此启发，我们将任务重新表述为广义医学短语定位（GMPG），其中每个句子被映射到零个、一个或多个评分区域。为实现这一表述，我们引入了第一个GMPG模型：MedGrounder。我们采用了两阶段训练方案：在报告句子-解剖框对齐数据集上进行预训练，然后在报告句子-人工标注框数据集上进行微调。在PadChest-GR和MS-CXR上的实验表明，MedGrounder实现了强大的零样本迁移能力，在多区域和不可定位短语方面优于REC风格和基础报告生成基线，同时使用更少的人工框标注。最后，我们展示了MedGrounder可以与现有报告生成器组合使用，无需重新训练生成器即可生成定位报告。

</details>


### [63] [Multi-GRPO: Multi-Group Advantage Estimation for Text-to-Image Generation with Tree-Based Trajectories and Multiple Rewards](https://arxiv.org/abs/2512.00743)
*Qiang Lyu,Zicong Chen,Chongxiao Wang,Haolin Shi,Shibo Gao,Ran Piao,Youwei Zeng,Jianlou Si,Fei Ding,Jing Li,Chun Pong Lau,Weiqiang Wang*

Main category: cs.CV

TL;DR: Multi-GRPO提出了一种多组优势估计框架，通过树基轨迹和奖励分组机制解决现有GRPO方法中的共享信用分配和奖励混合问题，在文本到图像生成任务中实现了更稳定的多目标对齐。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法存在两个关键限制：1）共享信用分配问题：轨迹级优势从组归一化的稀疏终端奖励中推导，均匀应用于所有时间步，无法准确估计早期去噪步骤的潜力；2）奖励混合问题：预定义权重组合多目标奖励（如文本准确性、视觉质量、文本颜色）导致梯度不稳定和冲突更新。

Method: 提出Multi-GRPO框架，包含两个正交分组机制：1）基于蒙特卡洛树搜索启发的树基轨迹，在选定的早期去噪步骤分支形成时间组，通过后代叶子准确估计早期步骤优势，同时通过共享前缀分摊计算；2）基于奖励的分组，在聚合前独立计算每个奖励函数的优势，解耦冲突信号。还构建了OCR-Color-10数据集用于多目标对齐评估。

Result: 在单奖励PickScore-25k和多目标OCR-Color-10基准测试中，Multi-GRPO实现了优越的稳定性和对齐性能，有效平衡了冲突目标。

Conclusion: Multi-GRPO通过创新的树基轨迹和奖励分组机制，解决了GRPO方法中的信用分配和奖励混合问题，为文本到图像生成的多目标对齐提供了更有效的解决方案。

Abstract: Recently, Group Relative Policy Optimization (GRPO) has shown promising potential for aligning text-to-image (T2I) models, yet existing GRPO-based methods suffer from two critical limitations. (1) \textit{Shared credit assignment}: trajectory-level advantages derived from group-normalized sparse terminal rewards are uniformly applied across timesteps, failing to accurately estimate the potential of early denoising steps with vast exploration spaces. (2) \textit{Reward-mixing}: predefined weights for combining multi-objective rewards (e.g., text accuracy, visual quality, text color)--which have mismatched scales and variances--lead to unstable gradients and conflicting updates. To address these issues, we propose \textbf{Multi-GRPO}, a multi-group advantage estimation framework with two orthogonal grouping mechanisms. For better credit assignment, we introduce tree-based trajectories inspired by Monte Carlo Tree Search: branching trajectories at selected early denoising steps naturally forms \emph{temporal groups}, enabling accurate advantage estimation for early steps via descendant leaves while amortizing computation through shared prefixes. For multi-objective optimization, we introduce \emph{reward-based grouping} to compute advantages for each reward function \textit{independently} before aggregation, disentangling conflicting signals. To facilitate evaluation of multiple objective alignment, we curate \textit{OCR-Color-10}, a visual text rendering dataset with explicit color constraints. Across the single-reward \textit{PickScore-25k} and multi-objective \textit{OCR-Color-10} benchmarks, Multi-GRPO achieves superior stability and alignment performance, effectively balancing conflicting objectives. Code will be publicly available at \href{https://github.com/fikry102/Multi-GRPO}{https://github.com/fikry102/Multi-GRPO}.

中文标题: Multi-GRPO：基于树基轨迹和多奖励的文本到图像生成多组优势估计

中文摘要: 最近，组相对策略优化（GRPO）在文本到图像（T2I）模型对齐方面显示出有前景的潜力，但现有的基于GRPO的方法存在两个关键限制。（1）共享信用分配：从组归一化稀疏终端奖励推导的轨迹级优势均匀应用于所有时间步，无法准确估计具有广阔探索空间的早期去噪步骤的潜力。（2）奖励混合：用于组合多目标奖励（如文本准确性、视觉质量、文本颜色）的预定义权重（这些奖励具有不匹配的尺度和方差）导致梯度不稳定和冲突更新。为解决这些问题，我们提出了Multi-GRPO，一个具有两个正交分组机制的多组优势估计框架。为了更好的信用分配，我们引入了受蒙特卡洛树搜索启发的树基轨迹：在选定的早期去噪步骤分支轨迹自然形成时间组，通过后代叶子准确估计早期步骤的优势，同时通过共享前缀分摊计算。对于多目标优化，我们引入了基于奖励的分组，在聚合前独立计算每个奖励函数的优势，解耦冲突信号。为了促进多目标对齐的评估，我们策划了OCR-Color-10，一个具有明确颜色约束的视觉文本渲染数据集。在单奖励PickScore-25k和多目标OCR-Color-10基准测试中，Multi-GRPO实现了优越的稳定性和对齐性能，有效平衡了冲突目标。代码将在https://github.com/fikry102/Multi-GRPO公开提供。

</details>


### [64] [SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead](https://arxiv.org/abs/2512.00903)
*Chaojun Ni,Cheng Chen,Xiaofeng Wang,Zheng Zhu,Wenzhao Zheng,Boyuan Wang,Tianrun Chen,Guosheng Zhao,Haoyun Li,Zhehao Dong,Qiang Zhang,Yun Ye,Yang Wang,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: SwiftVLA是一种创新的轻量级视觉-语言-动作模型架构，通过引入4D视觉几何变换器、融合标记和掩码重构策略，在保持高效设计的同时显著提升了时空推理能力，使紧凑模型能够与大型模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型虽然潜力巨大，但参数庞大导致实用性受限。轻量级VLM虽然参数少，但牺牲了关键的时空推理能力。现有方法要么依赖大型模型融合多模态输入，要么缺乏时间理解，无法在保持高效的同时实现强大的时空推理。

Method: 1. 使用预训练的4D视觉几何变换器配合时间缓存，从2D图像中提取4D时空特征；2. 引入融合标记——一组可学习标记，通过未来预测目标训练，生成统一的动作表示；3. 提出掩码重构策略，训练时对4D输入进行掩码并要求VLA重构，使VLM学习4D表示，推理时可丢弃4D分支。

Result: 在真实和模拟环境中的实验表明：1. 性能超越轻量级基线；2. 与参数规模达7倍的大型VLA模型性能相当；3. 在边缘设备上实现18倍速度提升；4. 内存占用减少12倍；5. 推理时可丢弃4D分支而性能损失最小。

Conclusion: SwiftVLA成功解决了轻量级VLA模型时空推理能力不足的问题，通过创新的架构设计在保持高效性的同时显著提升了性能，为边缘设备上的智能体应用提供了实用解决方案。

Abstract: Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.

中文标题: SwiftVLA：以最小开销解锁轻量级VLA模型的时空动态能力

中文摘要: 基于预训练视觉语言模型（VLM）构建的视觉-语言-动作（VLA）模型显示出强大潜力，但由于参数数量庞大，其实际应用受到限制。为缓解此问题，使用轻量级VLM已被探索，但这会牺牲时空推理能力。虽然某些方法建议引入额外的3D输入可能有所帮助，但它们通常依赖大型VLM来融合3D和2D输入，并且仍然缺乏时间理解。因此，我们提出SwiftVLA，一种在保持设计效率的同时增强紧凑模型4D理解能力的架构。具体而言，我们的方法采用具有时间缓存的预训练4D视觉几何变换器，从2D图像中提取4D特征。然后，为增强VLM利用2D图像和4D特征的能力，我们引入了融合标记——一组通过未来预测目标训练的可学习标记，用于生成统一的动作生成表示。最后，我们提出了一种掩码重构策略，将4D输入对VLM进行掩码，并训练VLA重构它们，使VLM能够学习有效的4D表示，并允许在推理时丢弃4D分支而性能损失最小。在真实和模拟环境中的实验表明，SwiftVLA优于轻量级基线，并与参数规模达7倍大的VLA相媲美，在边缘设备上实现相当性能的同时，速度提升18倍，内存占用减少12倍。

</details>


### [65] [Probabilistic Modeling of Multi-rater Medical Image Segmentation for Diversity and Personalization](https://arxiv.org/abs/2512.00748)
*Ke Liu,Shangde Gao,Yichao Fu,Shangqi Gao,Chunhua Shen*

Main category: cs.CV

TL;DR: ProSeg是一个概率建模框架，用于多标注者医学图像分割，能够同时实现多样化和个性化分割，通过建模专家偏好和图像边界模糊性来生成既多样又个性化的分割结果。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临数据不确定性的挑战，包括医学扫描中的模糊边界和诊断中的观察者间变异性。现有模型要么只能生成缺乏专家特异性的多样化分割，要么只能复制单个标注者的个性化输出，无法同时满足多样化和个性化需求。

Method: 提出ProSeg概率建模框架，引入两个潜在变量分别建模专家标注偏好和图像边界模糊性。通过变分推断获得它们的条件概率分布，然后从这些分布中采样生成分割输出。

Result: 在鼻咽癌数据集(NPC)和肺结节数据集(LIDC-IDRI)上的广泛实验表明，ProSeg实现了新的最先进性能，提供了既多样化又专家个性化的分割结果。

Conclusion: ProSeg能够同时实现医学图像分割的多样化和个性化，有效建模数据不确定性，为多标注者医学图像分割提供了有效的概率建模框架。

Abstract: Medical image segmentation is inherently influenced by data uncertainty, arising from ambiguous boundaries in medical scans and inter-observer variability in diagnosis. To address this challenge, previous works formulated the multi-rater medical image segmentation task, where multiple experts provide separate annotations for each image. However, existing models are typically constrained to either generate diverse segmentation that lacks expert specificity or to produce personalized outputs that merely replicate individual annotators. We propose Probabilistic modeling of multi-rater medical image Segmentation (ProSeg) that simultaneously enables both diversification and personalization. Specifically, we introduce two latent variables to model expert annotation preferences and image boundary ambiguity. Their conditional probabilistic distributions are then obtained through variational inference, allowing segmentation outputs to be generated by sampling from these distributions. Extensive experiments on both the nasopharyngeal carcinoma dataset (NPC) and the lung nodule dataset (LIDC-IDRI) demonstrate that our ProSeg achieves a new state-of-the-art performance, providing segmentation results that are both diverse and expert-personalized. Code can be found in https://github.com/AI4MOL/ProSeg.

中文标题: 多标注者医学图像分割的概率建模：多样性与个性化

中文摘要: 医学图像分割本质上受到数据不确定性的影响，这种不确定性源于医学扫描中的模糊边界和诊断中的观察者间变异性。为了解决这一挑战，先前的工作制定了多标注者医学图像分割任务，其中多位专家为每张图像提供独立的标注。然而，现有模型通常要么局限于生成缺乏专家特异性的多样化分割，要么局限于产生仅复制单个标注者的个性化输出。我们提出了多标注者医学图像分割的概率建模(ProSeg)，能够同时实现多样化和个性化。具体而言，我们引入了两个潜在变量来建模专家标注偏好和图像边界模糊性。然后通过变分推断获得它们的条件概率分布，允许通过从这些分布中采样来生成分割输出。在鼻咽癌数据集(NPC)和肺结节数据集(LIDC-IDRI)上的广泛实验表明，我们的ProSeg实现了新的最先进性能，提供了既多样化又专家个性化的分割结果。代码可在https://github.com/AI4MOL/ProSeg找到。

</details>


### [66] [MM-ACT: Learn from Multimodal Parallel Generation to Act](https://arxiv.org/abs/2512.00975)
*Haotian Liang,Xinyi Chen,Bin Wang,Mingkang Chen,Yitian Liu,Yuhao Zhang,Zanxin Chen,Tianshuo Yang,Yilun Chen,Jiangmiao Pang,Dong Liu,Xiaokang Yang,Yao Mu,Wenqi Shao,Ping Luo*

Main category: cs.CV

TL;DR: MM-ACT是一个统一的视觉-语言-动作模型，通过共享令牌空间集成文本、图像和动作，采用重新掩码并行解码策略生成文本和图像，一步并行解码策略生成动作，实现多模态并行生成学习。


<details>
  <summary>Details</summary>
Motivation: 通用机器人策略需要语义理解进行任务规划，同时需要预测能力与环境交互。现有方法通常单独处理这些能力，缺乏统一的跨模态学习框架。

Method: 提出MM-ACT统一模型，在共享令牌空间中集成文本、图像和动作；采用重新掩码并行解码策略生成文本和图像，一步并行解码策略生成动作；引入上下文共享多模态学习训练范式，从共享上下文监督所有三种模态的生成。

Result: 在LIBERO仿真中成功率96.3%，真实Franka机器人三个任务中72.0%，RoboTwin2.0八个双手任务中52.38%，跨模态学习带来额外9.25%的性能提升。

Conclusion: MM-ACT通过统一的多模态并行生成框架，实现了语义理解和预测能力的有效结合，跨模态学习显著提升了动作生成性能，为通用机器人策略提供了有效解决方案。

Abstract: A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.

中文标题: MM-ACT：从多模态并行生成中学习动作

中文摘要: 通用机器人策略需要语义理解进行任务规划，同时需要预测能力与环境交互。为此，我们提出了MM-ACT，一个统一的视觉-语言-动作模型，在共享令牌空间中集成文本、图像和动作，并在所有三种模态上执行生成。MM-ACT采用重新掩码并行解码策略生成文本和图像，并采用一步并行解码策略生成动作以提高效率。我们引入了上下文共享多模态学习，这是一种统一的训练范式，从共享上下文监督所有三种模态的生成，通过跨模态学习增强动作生成。实验在LIBERO仿真和Franka真实机器人设置以及RoboTwin2.0上进行，分别评估域内和域外性能。我们的方法在LIBERO上达到96.3%的成功率，在真实Franka的三个任务中达到72.0%，在RoboTwin2.0的八个双手任务中达到52.38%，跨模态学习带来额外9.25%的性能提升。我们在https://github.com/HHYHRHY/MM-ACT发布了代码、模型和数据。

</details>


### [67] [Charts Are Not Images: On the Challenges of Scientific Chart Editing](https://arxiv.org/abs/2512.00752)
*Shawn Li,Ryan Rossi,Sungchul Kim,Sunav Choudhary,Franck Dernoncourt,Puneet Mathur,Zhengzhong Tu,Yue Zhao*

Main category: cs.CV

TL;DR: 本文指出将自然图像编辑模型直接应用于科学图表存在根本性缺陷，因为图表是结构化数据的视觉表示而非像素排列。为此，作者创建了包含3万多个样本的FigEdit基准测试，涵盖10种图表类型和5种编辑任务，揭示了现有模型在结构化转换方面的不足，并展示了传统评估指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在自然图像编辑方面表现出色，但应用于科学图表时存在根本性不匹配：图表不是像素排列，而是遵循图形语法的结构化数据可视化。现有方法将图表视为图像进行像素级编辑，忽略了其结构化本质，导致编辑结果在语义上无效。

Method: 作者创建了FigEdit大规模基准测试，包含30,000多个样本，基于真实世界数据构建。基准涵盖10种图表类型和丰富的复杂编辑指令，组织为5个渐进式挑战任务：单次编辑、多次编辑、对话式编辑、视觉引导编辑和风格迁移。通过评估多种最先进模型在该基准上的表现，分析其结构化转换能力。

Result: 评估显示现有模型在科学图表编辑上表现不佳，无法处理所需的结构化转换。传统评估指标（如SSIM、PSNR）在捕捉图表编辑的语义正确性方面存在局限性。基准测试揭示了像素级操作的根本限制，为开发结构感知模型提供了坚实基础。

Conclusion: 科学图表编辑需要结构感知方法而非像素级操作。FigEdit基准为结构感知图表编辑提供了系统进展的基础，建立了公平比较的共同标准，并鼓励未来研究同时理解科学图表的视觉和语义层面。

Abstract: Generative models, such as diffusion and autoregressive approaches, have demonstrated impressive capabilities in editing natural images. However, applying these tools to scientific charts rests on a flawed assumption: a chart is not merely an arrangement of pixels but a visual representation of structured data governed by a graphical grammar. Consequently, chart editing is not a pixel-manipulation task but a structured transformation problem. To address this fundamental mismatch, we introduce \textit{FigEdit}, a large-scale benchmark for scientific figure editing comprising over 30,000 samples. Grounded in real-world data, our benchmark is distinguished by its diversity, covering 10 distinct chart types and a rich vocabulary of complex editing instructions. The benchmark is organized into five distinct and progressively challenging tasks: single edits, multi edits, conversational edits, visual-guidance-based edits, and style transfer. Our evaluation of a range of state-of-the-art models on this benchmark reveals their poor performance on scientific figures, as they consistently fail to handle the underlying structured transformations required for valid edits. Furthermore, our analysis indicates that traditional evaluation metrics (e.g., SSIM, PSNR) have limitations in capturing the semantic correctness of chart edits. Our benchmark demonstrates the profound limitations of pixel-level manipulation and provides a robust foundation for developing and evaluating future structure-aware models. By releasing \textit{FigEdit} (https://github.com/adobe-research/figure-editing), we aim to enable systematic progress in structure-aware figure editing, provide a common ground for fair comparison, and encourage future research on models that understand both the visual and semantic layers of scientific charts.

中文标题: 图表不是图像：论科学图表编辑的挑战

中文摘要: 生成模型（如扩散和自回归方法）在编辑自然图像方面展示了令人印象深刻的能力。然而，将这些工具应用于科学图表基于一个错误的假设：图表不仅仅是像素排列，而是遵循图形语法的结构化数据的视觉表示。因此，图表编辑不是像素操作任务，而是结构化转换问题。为解决这一根本性不匹配，我们引入了FigEdit，这是一个包含30,000多个样本的科学图表编辑大规模基准测试。基于真实世界数据，我们的基准以其多样性为特色，涵盖10种不同的图表类型和丰富的复杂编辑指令词汇。该基准组织为五个不同且渐进具有挑战性的任务：单次编辑、多次编辑、对话式编辑、视觉引导编辑和风格迁移。我们对一系列最先进模型在该基准上的评估揭示了它们在科学图表上的糟糕表现，因为它们始终无法处理有效编辑所需的基础结构化转换。此外，我们的分析表明传统评估指标（如SSIM、PSNR）在捕捉图表编辑的语义正确性方面存在局限性。我们的基准展示了像素级操作的根本限制，并为开发和评估未来结构感知模型提供了坚实基础。通过发布FigEdit，我们旨在促进结构感知图表编辑的系统进展，提供公平比较的共同基础，并鼓励未来研究同时理解科学图表的视觉和语义层面的模型。

</details>


### [68] [Real-Time On-the-Go Annotation Framework Using YOLO for Automated Dataset Generation](https://arxiv.org/abs/2512.01165)
*Mohamed Abdallah Salem,Ahmed Harb Rabia*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLO的实时标注框架，可在边缘设备上实现图像采集时的即时标注，显著减少数据集准备时间并保持高质量标注。


<details>
  <summary>Details</summary>
Motivation: 在农业等需要快速决策的实际应用中，高效准确的数据集标注仍然是部署YOLO等目标检测模型的主要挑战。传统标注方法劳动密集，需要在数据收集后进行大量手动标注。

Method: 提出了一种新颖的实时标注方法，利用部署在边缘设备上的YOLO模型，在图像采集过程中实现即时标注。使用三种主流YOLO架构（YOLOv5、YOLOv8、YOLOv12）在不同配置下进行综合比较分析：单类别与多类别标注，以及预训练与从头开始训练。

Result: 通过详细的统计测试和学习动态分析，证明了预训练和单类别配置在模型收敛、性能和鲁棒性方面的显著优势。结果强有力地验证了实时标注框架的可行性和有效性。

Conclusion: 该实时标注框架能够大幅减少数据集准备时间，同时保持高标注质量，为实际应用中的快速数据集生成提供了有效解决方案。

Abstract: Efficient and accurate annotation of datasets remains a significant challenge for deploying object detection models such as You Only Look Once (YOLO) in real-world applications, particularly in agriculture where rapid decision-making is critical. Traditional annotation techniques are labor-intensive, requiring extensive manual labeling post data collection. This paper presents a novel real-time annotation approach leveraging YOLO models deployed on edge devices, enabling immediate labeling during image capture. To comprehensively evaluate the efficiency and accuracy of our proposed system, we conducted an extensive comparative analysis using three prominent YOLO architectures (YOLOv5, YOLOv8, YOLOv12) under various configurations: single-class versus multi-class annotation and pretrained versus scratch-based training. Our analysis includes detailed statistical tests and learning dynamics, demonstrating significant advantages of pretrained and single-class configurations in terms of model convergence, performance, and robustness. Results strongly validate the feasibility and effectiveness of our real-time annotation framework, highlighting its capability to drastically reduce dataset preparation time while maintaining high annotation quality.

中文标题: 基于YOLO的实时移动标注框架用于自动化数据集生成

中文摘要: 高效准确的数据集标注仍然是部署YOLO等目标检测模型在实际应用中的重大挑战，特别是在需要快速决策的农业领域。传统标注技术劳动密集，需要在数据收集后进行大量手动标注。本文提出了一种新颖的实时标注方法，利用部署在边缘设备上的YOLO模型，在图像采集过程中实现即时标注。为了全面评估我们提出的系统的效率和准确性，我们使用三种主流YOLO架构（YOLOv5、YOLOv8、YOLOv12）在不同配置下进行了广泛的比较分析：单类别与多类别标注以及预训练与从头开始训练。我们的分析包括详细的统计测试和学习动态，证明了预训练和单类别配置在模型收敛、性能和鲁棒性方面的显著优势。结果强有力地验证了我们实时标注框架的可行性和有效性，突显了其能够大幅减少数据集准备时间同时保持高标注质量的能力。

</details>


### [69] [Seeing the Wind from a Falling Leaf](https://arxiv.org/abs/2512.00762)
*Zhiyuan Gao,Jiageng Mao,Hong-Xing Yu,Haozhe Lou,Emily Yue-Ting Jia,Jernej Barbic,Jiajun Wu,Yue Wang*

Main category: cs.CV

TL;DR: 本文提出了一种从视频中恢复不可见物理力（如风场）的方法，通过可微分逆图形框架联合建模物体几何、物理属性和相互作用，实现从物体运动推断力场。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉长期关注视频中的运动建模，但运动背后的物理相互作用（导致物体变形和移动的不可见力）很少被研究。本文旨在从视觉观测中恢复这些不可见力，填补视觉与物理之间的鸿沟。

Method: 提出端到端可微分逆图形框架，联合建模物体几何、物理属性和相互作用，通过反向传播从视频中的物体运动恢复力表示。

Result: 在合成和真实世界场景中验证了方法的有效性，能够从视频中推断出合理的力场，并展示了在基于物理的视频生成和编辑方面的应用潜力。

Conclusion: 该方法为理解和建模像素背后的物理过程提供了新途径，有助于弥合视觉与物理之间的差距，在物理感知的视频分析、生成和编辑方面具有应用前景。

Abstract: A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our \href{https://chaoren2357.github.io/seeingthewind/}{project page}.

中文标题: 从落叶中看见风：通过视觉观测恢复不可见物理力的研究

中文摘要: 计算机视觉的一个长期目标是建模视频中的运动，但运动背后的表示——即导致物体变形和移动的不可见物理相互作用——在很大程度上仍未得到探索。在本文中，我们研究如何从视觉观测中恢复不可见力，例如通过观察叶子落到地面来估计风场。我们的关键创新是一个端到端的可微分逆图形框架，直接从视频中联合建模物体几何、物理属性和相互作用。通过反向传播，我们的方法能够从物体运动中恢复力表示。我们在合成和真实世界场景中验证了我们的方法，结果表明它能够从视频中推断出合理的力场。此外，我们展示了我们方法的潜在应用，包括基于物理的视频生成和编辑。我们希望我们的方法能为理解和建模像素背后的物理过程提供启示，弥合视觉与物理之间的鸿沟。更多视频结果请查看我们的项目页面。

</details>


### [70] [SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge](https://arxiv.org/abs/2512.01629)
*Yumeng He,Ying Jiang,Jiayin Lu,Yin Yang,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: SPARK是一个从单张RGB图像重建仿真就绪关节化3D对象的框架，结合VLM提取结构参数、扩散模型生成部件形状，并通过可微分优化精调URDF参数。


<details>
  <summary>Details</summary>
Motivation: 关节化3D对象对具身AI和机器人应用至关重要，但现有方法创建仿真就绪资产需要大量人工和专家知识，限制了可扩展性和实用性。

Method: 1. 使用VLM从单张RGB图像提取粗略URDF参数和生成部件参考图像；2. 将部件图像指导和结构图集成到生成扩散变换器中合成一致部件形状；3. 通过可微分前向运动学和可微分渲染优化关节参数。

Result: SPARK能够生成高质量、物理一致的仿真就绪关节资产，支持多种类别对象，在机器人操作和交互建模等下游应用中表现出色。

Conclusion: SPARK框架成功解决了从单张图像重建仿真就绪关节化对象的挑战，结合了VLM的语义理解能力和生成模型的几何建模能力，为具身AI和机器人应用提供了实用的资产创建方案。

Abstract: Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further refine the URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering to optimize joint types, axes, and origins under VLM-generated open-state supervision. Extensive experiments show that SPARK produces high-quality, simulation-ready articulated assets across diverse categories, enabling downstream applications such as robotic manipulation and interaction modeling.

中文标题: SPARK：基于VLM知识的仿真就绪部件级关节重建

中文摘要: 关节化3D对象对于具身AI、机器人和交互式场景理解至关重要，但创建仿真就绪资产仍然劳动密集，需要专家对部件层次结构和运动结构进行建模。我们引入了SPARK，这是一个从单个RGB图像重建物理一致、运动学部件级关节对象的框架。给定输入图像，我们首先利用视觉语言模型提取粗略的URDF参数并生成部件级参考图像。然后，我们将部件图像指导和推断的结构图集成到生成扩散变换器中，以合成关节对象的一致部件和完整形状。为了进一步优化URDF参数，我们结合了可微分前向运动学和可微分渲染，在VLM生成的开状态监督下优化关节类型、轴和原点。大量实验表明，SPARK能够在不同类别中产生高质量、仿真就绪的关节资产，支持机器人操作和交互建模等下游应用。

</details>


### [71] [Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching](https://arxiv.org/abs/2512.01850)
*Yue Pan,Tao Sun,Liyuan Zhu,Lucas Nunes,Iro Armeni,Jens Behley,Cyrill Stachniss*

Main category: cs.CV

TL;DR: RAP将点云注册重新定义为条件生成问题，使用学习的速度场将噪声点传输到已注册场景，直接生成注册结果而非通过对应匹配，在低重叠情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统点云注册方法依赖对应匹配和变换估计，在低重叠场景中效果不佳。本文旨在开发一种更鲁棒、可扩展的注册方法，能够直接生成注册结果，避免复杂的对应匹配过程。

Method: 将注册视为条件生成问题，使用连续点式速度场将噪声点传输到注册场景；采用轻量级局部特征提取器；实施测试时刚性约束；直接生成注册点云而非估计变换。

Result: 在成对和多视角注册基准测试中取得最先进结果，特别是在低重叠情况下；能够跨尺度和传感器模态泛化；支持下游任务如重定位、多机器人SLAM和地图合并。

Conclusion: RAP通过将注册重新定义为生成问题，提供了一种更鲁棒、可扩展的点云注册方法，在挑战性场景中表现优异，并具有广泛的下游应用潜力。

Abstract: Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.

中文标题: 注册任意点：通过流匹配实现三维点云注册的规模化

中文摘要: 点云注册将多个未定位的点云对齐到共同坐标系中，是三维重建和机器人定位的核心步骤。在这项工作中，我们将注册视为条件生成：一个学习到的连续、逐点速度场将噪声点传输到已注册的场景中，从中恢复每个视角的位姿。与先前通过对应匹配估计点云对之间的变换然后优化成对变换来实现多视角注册的方法不同，我们的模型直接生成已注册的点云。通过轻量级局部特征提取器和测试时刚性约束，我们的方法在成对和多视角注册基准测试中取得了最先进的结果，特别是在低重叠情况下，并且能够跨尺度和传感器模态泛化。它进一步支持下游任务，包括重定位、多机器人SLAM和多会话地图合并。源代码可在 https://github.com/PRBonn/RAP 获取。

</details>


### [72] [StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos](https://arxiv.org/abs/2512.01707)
*Daeun Lee,Subhojyoti Mukherjee,Branislav Kveton,Ryan A. Rossi,Viet Dac Lai,Seunghyun Yoon,Trung Bui,Franck Dernoncourt,Mohit Bansal*

Main category: cs.CV

TL;DR: StreamGaze是首个评估多模态大语言模型在流式视频中利用人类注视信号进行时序推理和主动理解的基准测试，包含注视引导的过去、现在和主动任务，揭示了当前模型在注视引导推理方面的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有的流式视频理解基准主要评估时序推理能力，但缺乏评估模型是否能够解释或利用人类注视信号。在AR眼镜等现实应用中，理解用户注视对于预测意图至关重要，因此需要专门的基准来评估模型在流式视频中利用注视进行时序和主动推理的能力。

Method: 开发了注视-视频问答生成流水线，通过注视点提取、区域特定视觉提示和扫描路径构建，将自我中心视频与原始注视轨迹对齐。该流水线生成时空基础的问答对，紧密反映人类感知动态，创建了包含注视引导的过去、现在和主动任务的StreamGaze基准。

Result: 在所有StreamGaze任务中，最先进的多模态大语言模型与人类性能之间存在显著差距，揭示了模型在基于注视的时序推理、意图建模和主动预测方面的根本局限性。研究还提供了注视提示策略、推理行为和任务特定失败模式的详细分析。

Conclusion: StreamGaze基准揭示了当前多模态大语言模型在利用注视信号进行流式视频理解方面的重大挑战，为未来模型开发提供了重要见解。所有数据和代码将公开以支持注视引导流式视频理解的持续研究。

Abstract: Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.

中文标题: StreamGaze：流式视频中的注视引导时序推理与主动理解

中文摘要: 流式视频理解不仅要求模型处理时序输入的帧，还需要预测用户意图以支持AR眼镜等现实应用。虽然先前的流式基准评估了时序推理，但都没有衡量多模态大语言模型是否能在流式设置中解释或利用人类注视信号。为填补这一空白，我们引入了StreamGaze，这是首个旨在评估多模态大语言模型在流式视频中利用注视进行时序和主动推理效果的基准。StreamGaze引入了注视引导的过去、现在和主动任务，全面评估流式视频理解能力。这些任务评估模型是否能够使用实时注视来跟踪注意力转移，并仅从过去和当前观察到的帧推断用户意图。为构建StreamGaze，我们开发了一个注视-视频问答生成流水线，通过注视点提取、区域特定视觉提示和扫描路径构建，将自我中心视频与原始注视轨迹对齐。该流水线生成时空基础的问答对，紧密反映人类感知动态。在所有StreamGaze任务中，我们观察到最先进的多模态大语言模型与人类性能之间存在显著差距，揭示了基于注视的时序推理、意图建模和主动预测方面的根本局限性。我们进一步提供了注视提示策略、推理行为和任务特定失败模式的详细分析，为当前多模态大语言模型为何困难以及未来模型必须开发哪些能力提供了更深入的见解。所有数据和代码将公开发布，以支持注视引导流式视频理解的持续研究。

</details>


### [73] [GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment](https://arxiv.org/abs/2512.01952)
*Haoyang He,Jay Patrikar,Dong-Ki Kim,Max Smith,Daniel McGann,Ali-akbar Agha-mohammadi,Shayegan Omidshafiei,Sebastian Scherer*

Main category: cs.CV

TL;DR: GrndCtrl是一个通过自监督奖励对齐来增强世界模型几何基础的后训练框架，使用物理可验证的几何和感知奖励来提升导航任务中的空间一致性和长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频世界模型虽然视觉逼真度高，但缺乏几何基础，限制了它们在需要空间一致性和长期稳定性的导航任务中的应用。

Method: 提出了强化学习世界基础化(RLWG)框架，使用自监督后训练方法，通过姿态循环一致性、深度重投影和时间一致性等物理可验证的几何和感知奖励来对齐预训练的世界模型。具体实现为基于组相对策略优化(GRPO)的GrndCtrl奖励对齐适应方法。

Result: GrndCtrl能够生成保持稳定轨迹、一致几何和可靠rollout的世界模型，在室外环境中实现了比监督微调更优越的空间一致性和导航稳定性。

Conclusion: 类似于大语言模型的后训练对齐，GrndCtrl通过可验证奖励桥接了生成式预训练和基础行为，为具身导航提供了具有几何基础的世界模型。

Abstract: Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.

中文标题: GrndCtrl：通过自监督奖励对齐实现世界模型的基础化

中文摘要: 视频世界建模的最新进展使得大规模生成模型能够以高视觉保真度模拟具身环境，为预测、规划和控制提供了强大的先验。然而，尽管这些模型具有逼真性，但它们通常缺乏几何基础，限制了它们在需要空间一致性和长期稳定性的导航任务中的应用。我们引入了强化学习世界基础化(RLWG)，这是一个自监督后训练框架，通过几何和感知奖励将预训练的世界模型与物理可验证的结构对齐。类似于语言模型中的可验证反馈强化学习(RLVR)，RLWG可以使用测量姿态循环一致性、深度重投影和时间一致性的多种奖励。我们通过GrndCtrl实例化这一框架，这是一种基于组相对策略优化(GRPO)的奖励对齐适应方法，产生的世界模型能够保持稳定轨迹、一致几何和可靠的具身导航rollout。就像大语言模型中的后训练对齐一样，GrndCtrl利用可验证奖励桥接了生成式预训练和基础行为，在室外环境中实现了比监督微调更优越的空间一致性和导航稳定性。

</details>


### [74] [PolarGS: Polarimetric Cues for Ambiguity-Free Gaussian Splatting with Accurate Geometry Recovery](https://arxiv.org/abs/2512.00794)
*Bo Guo,Sijia Wen,Yifan Zhao,Jia Li,Zhiming Zheng*

Main category: cs.CV

TL;DR: PolarGS利用偏振信息解决3D高斯泼溅在反射和无纹理表面的几何重建模糊问题，通过偏振引导的光度校正和偏振增强的高斯致密化机制，显著提升重建精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于3D高斯泼溅的表面重建方法在反射和无纹理等光度模糊区域性能下降，因为不可靠的光度线索破坏了重建一致性。偏振信息能够揭示表面方向，可作为解决光度模糊的光学补充。

Method: 提出两个互补模块：1）偏振引导的光度校正策略，利用线性偏振度识别反射区域，通过颜色细化图优化反射高斯；2）偏振增强的高斯致密化机制，将角度和线性偏振度集成到PatchMatch深度补全中，实现新高斯的反向投影和融合。

Result: PolarGS实现了框架无关的卓越几何精度，在反射和无纹理表面等挑战性区域显著优于现有最先进方法，获得更完整和准确的重建结果。

Conclusion: 偏振信息作为光学先验能有效解决3D高斯泼溅在光度模糊区域的重建问题，PolarGS通过偏振引导的校正和致密化机制显著提升了重建精度和完整性。

Abstract: Recent advances in surface reconstruction for 3D Gaussian Splatting (3DGS) have enabled remarkable geometric accuracy. However, their performance degrades in photometrically ambiguous regions such as reflective and textureless surfaces, where unreliable cues disrupt photometric consistency and hinder accurate geometry estimation. Reflected light is often partially polarized in a manner that reveals surface orientation, making polarization an optic complement to photometric cues in resolving such ambiguities. Therefore, we propose PolarGS, an optics-aware extension of RGB-based 3DGS that leverages polarization as an optical prior to resolve photometric ambiguities and enhance reconstruction accuracy. Specifically, we introduce two complementary modules: a polarization-guided photometric correction strategy, which ensures photometric consistency by identifying reflective regions via the Degree of Linear Polarization (DoLP) and refining reflective Gaussians with Color Refinement Maps; and a polarization-enhanced Gaussian densification mechanism for textureless area geometry recovery, which integrates both Angle and Degree of Linear Polarization (A/DoLP) into a PatchMatch-based depth completion process. This enables the back-projection and fusion of new Gaussians, leading to more complete reconstruction. PolarGS is framework-agnostic and achieves superior geometric accuracy compared to state-of-the-art methods.

中文标题: PolarGS：利用偏振线索实现无歧义的高斯泼溅与精确几何恢复

中文摘要: 最近在3D高斯泼溅（3DGS）表面重建方面的进展实现了显著的几何精度。然而，在反射和无纹理表面等光度模糊区域，它们的性能会下降，其中不可靠的线索会破坏光度一致性并阻碍准确的几何估计。反射光通常以部分偏振的方式揭示表面方向，使得偏振成为解决此类模糊性的光度线索的光学补充。因此，我们提出了PolarGS，这是基于RGB的3DGS的光学感知扩展，利用偏振作为光学先验来解决光度模糊性并提高重建精度。具体来说，我们引入了两个互补模块：偏振引导的光度校正策略，通过线性偏振度（DoLP）识别反射区域，并使用颜色细化图优化反射高斯；以及偏振增强的高斯致密化机制用于无纹理区域的几何恢复，该机制将角度和线性偏振度（A/DoLP）集成到基于PatchMatch的深度补全过程中。这使得新高斯能够进行反向投影和融合，从而实现更完整的重建。PolarGS是框架无关的，与最先进的方法相比，实现了卓越的几何精度。

</details>


### [75] [CircleFlow: Flow-Guided Camera Blur Estimation using a Circle Grid Target](https://arxiv.org/abs/2512.00796)
*Jiajian He,Enjie Hu,Shiqi Chen,Tianchen Qiu,Huajun Feng,Zhihai Xu,Yueting Chen*

Main category: cs.CV

TL;DR: CircleFlow是一种高保真PSF估计框架，通过使用圆形网格靶标和光流引导的边缘定位来精确表征相机模糊，在解耦图像和核估计的同时实现物理一致的PSF校准。


<details>
  <summary>Details</summary>
Motivation: 点扩散函数（PSF）作为连接真实场景与捕获信号的基本描述符，表现为相机模糊。准确的PSF估计对于光学表征和计算视觉都至关重要，但由于固有的模糊性和基于强度的反卷积的病态性质，这仍然具有挑战性。现有方法难以精确表征局部各向异性和空间变化的PSF。

Method: 1. 使用圆形网格靶标进行结构化捕获，编码局部各向异性和空间变化的PSF；2. 利用靶标的二值亮度先验解耦图像和核估计；3. 通过光流引导的亚像素对齐重建潜在清晰图像；4. 将PSF建模为能量约束的隐式神经表示；5. 在去马赛克感知的可微分框架中联合优化两个组件。

Result: 在模拟和真实世界数据上的大量实验表明，CircleFlow实现了最先进的准确性和可靠性，验证了其在实际PSF校准中的有效性。该方法能够精确表征复杂的相机模糊模式。

Conclusion: CircleFlow通过结合结构化靶标捕获、光流引导的边缘定位和隐式神经表示，提供了一种高保真PSF估计框架，解决了传统强度反卷积方法的局限性，为实际相机校准应用提供了可靠解决方案。

Abstract: The point spread function (PSF) serves as a fundamental descriptor linking the real-world scene to the captured signal, manifesting as camera blur. Accurate PSF estimation is crucial for both optical characterization and computational vision, yet remains challenging due to the inherent ambiguity and the ill-posed nature of intensity-based deconvolution. We introduce CircleFlow, a high-fidelity PSF estimation framework that employs flow-guided edge localization for precise blur characterization. CircleFlow begins with a structured capture that encodes locally anisotropic and spatially varying PSFs by imaging a circle grid target, while leveraging the target's binary luminance prior to decouple image and kernel estimation. The latent sharp image is then reconstructed through subpixel alignment of an initialized binary structure guided by optical flow, whereas the PSF is modeled as an energy-constrained implicit neural representation. Both components are jointly optimized within a demosaicing-aware differentiable framework, ensuring physically consistent and robust PSF estimation enabled by accurate edge localization. Extensive experiments on simulated and real-world data demonstrate that CircleFlow achieves state-of-the-art accuracy and reliability, validating its effectiveness for practical PSF calibration.

中文标题: CircleFlow：使用圆形网格靶标的光流引导相机模糊估计

中文摘要: 点扩散函数（PSF）作为连接真实世界场景与捕获信号的基本描述符，表现为相机模糊。准确的PSF估计对于光学表征和计算视觉都至关重要，但由于固有的模糊性和基于强度的反卷积的病态性质，这仍然具有挑战性。我们提出了CircleFlow，一种高保真PSF估计框架，采用光流引导的边缘定位进行精确模糊表征。CircleFlow从结构化捕获开始，通过成像圆形网格靶标编码局部各向异性和空间变化的PSF，同时利用靶标的二值亮度先验解耦图像和核估计。然后通过光流引导的初始化二值结构的亚像素对齐重建潜在清晰图像，而PSF被建模为能量约束的隐式神经表示。两个组件在去马赛克感知的可微分框架内联合优化，确保通过准确边缘定位实现物理一致且稳健的PSF估计。在模拟和真实世界数据上的大量实验表明，CircleFlow实现了最先进的准确性和可靠性，验证了其在实际PSF校准中的有效性。

</details>


### [76] [Data-Centric Visual Development for Self-Driving Labs](https://arxiv.org/abs/2512.02018)
*Anbang Liu,Guanzhong Hu,Jiayi Wang,Ping Guo,Han Liu*

Main category: cs.CV

TL;DR: 本文提出了一种数据中心的视觉开发方法，用于自动驾驶实验室的移液操作检测。通过融合真实数据和虚拟数据生成，解决了训练数据稀缺问题，特别是负样本不足的挑战，实现了高精度的气泡检测。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶实验室在生物科学中具有减少人工、提高可重复性的潜力，但其高精度要求需要大量标注数据来训练鲁棒模型。然而在实际操作中，特别是移液这种关键且精度敏感的操作，获取足够的训练数据（尤其是负样本）非常困难。

Method: 构建了一个混合管道，融合真实和虚拟数据生成。真实数据采用人机协同方案，结合自动采集和选择性人工验证，以最小努力最大化准确性。虚拟数据通过参考条件、提示引导的图像生成来增强真实数据，并经过筛选和验证确保可靠性。两种途径共同产生类别平衡的数据集，用于鲁棒的气泡检测训练。

Result: 在保留的真实测试集上，仅使用自动采集的真实图像训练的模型达到99.6%的准确率。混合真实和生成数据进行训练时，准确率维持在99.4%，同时减少了数据收集和审查的工作量。

Conclusion: 该方法为SDL工作流程提供了一种可扩展且经济高效的视觉反馈数据供应策略，为罕见事件检测和更广泛的视觉任务中的数据稀缺问题提供了实用解决方案。

Abstract: Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.

中文标题: 面向自动驾驶实验室的数据中心视觉开发

中文摘要: 自动驾驶实验室为减少生物科学中劳动密集型、耗时且通常不可重复的工作流程提供了一条有希望的途径。然而，其严格的精度要求需要高度鲁棒的模型，这些模型的训练依赖于大量标注数据。然而，这类数据在常规实践中难以获得，尤其是负样本。在这项工作中，我们专注于移液操作，这是SDL中最关键且精度敏感的动作。为了克服训练数据的稀缺性，我们构建了一个融合真实和虚拟数据生成的混合管道。真实数据采用人机协同方案，将自动采集与选择性人工验证相结合，以最小努力最大化准确性。虚拟数据通过参考条件、提示引导的图像生成来增强真实数据，并经过筛选和验证以确保可靠性。这两种途径共同产生类别平衡的数据集，从而实现鲁棒的气泡检测训练。在保留的真实测试集上，仅使用自动采集的真实图像训练的模型达到99.6%的准确率，而混合真实和生成数据进行训练时，准确率维持在99.4%，同时减少了收集和审查的工作量。我们的方法为SDL工作流程提供了一种可扩展且经济高效的视觉反馈数据供应策略，并为罕见事件检测和更广泛的视觉任务中的数据稀缺问题提供了实用解决方案。

</details>


### [77] [Thinking with Drafts: Speculative Temporal Reasoning for Efficient Long Video Understanding](https://arxiv.org/abs/2512.00805)
*Pengfei Hu,Meng Cao,Yingyao Wang,Yi Wang,Jiahua Dong,Jun Song,Yu Cheng,Bo Zheng,Xiaodan Liang*

Main category: cs.CV

TL;DR: SpecTemp是一个基于强化学习的推测性时序推理框架，使用轻量级草稿模型快速筛选显著帧，再由强大目标模型进行验证和推理，显著提升了长视频理解的效率。


<details>
  <summary>Details</summary>
Motivation: 现有"思维与帧"范式虽然提升了视频MLLMs的推理能力，但由于需要处理大量冗余的多模态上下文，存在严重的效率瓶颈。长视频理解需要更高效的时序推理方法。

Method: 提出SpecTemp框架：1）轻量级草稿MLLM快速探索密集采样区域并提出显著帧；2）强大目标MLLM验证提议并进行时序推理；3）迭代优化注意力直至收敛；4）构建SpecTemp-80K数据集支持训练。

Result: 在多个视频理解基准测试中，SpecTemp保持了与现有方法相当的准确性，同时显著加速了推理过程，比传统的"思维与帧"方法更高效。

Conclusion: SpecTemp通过解耦时序感知与推理的协作双模型设计，有效平衡了长视频理解的效率与准确性，为人脑启发的视频理解系统提供了新思路。

Abstract: Long video understanding is essential for human-like intelligence, enabling coherent perception and reasoning over extended temporal contexts. While the emerging thinking-with-frames paradigm, which alternates between global temporal reasoning and local frame examination, has advanced the reasoning capabilities of video multi-modal large language models (MLLMs), it suffers from a significant efficiency bottleneck due to the progressively growing and redundant multi-modal context. To address this, we propose SpecTemp, a reinforcement learning-based Speculative Temporal reasoning framework that decouples temporal perception from reasoning via a cooperative dual-model design. In SpecTemp, a lightweight draft MLLM rapidly explores and proposes salient frames from densely sampled temporal regions, while a powerful target MLLM focuses on temporal reasoning and verifies the draft's proposals, iteratively refining its attention until convergence. This design mirrors the collaborative pathways of the human brain, balancing efficiency with accuracy. To support training, we construct the SpecTemp-80K dataset, featuring synchronized dual-level annotations for coarse evidence spans and fine-grained frame-level evidence. Experiments across multiple video understanding benchmarks demonstrate that SpecTemp not only maintains competitive accuracy but also significantly accelerates inference compared with existing thinking-with-frames methods.

中文标题: 思维与草稿：用于高效长视频理解的推测性时序推理

中文摘要: 长视频理解对于类人智能至关重要，能够实现对扩展时序上下文的连贯感知和推理。虽然新兴的"思维与帧"范式（在全局时序推理和局部帧检查之间交替）提升了视频多模态大语言模型（MLLMs）的推理能力，但由于逐步增长和冗余的多模态上下文，它存在显著的效率瓶颈。为解决这一问题，我们提出了SpecTemp，一种基于强化学习的推测性时序推理框架，通过协作双模型设计将时序感知与推理解耦。在SpecTemp中，轻量级草稿MLLM从密集采样的时序区域快速探索并提出显著帧，而强大的目标MLLM专注于时序推理并验证草稿的提议，迭代地优化其注意力直至收敛。这种设计模仿了人脑的协作通路，平衡了效率与准确性。为支持训练，我们构建了SpecTemp-80K数据集，包含粗粒度证据跨度和细粒度帧级证据的同步双级标注。在多个视频理解基准上的实验表明，SpecTemp不仅保持了有竞争力的准确性，而且相比现有的"思维与帧"方法显著加速了推理。

</details>


### [78] [IRPO: Boosting Image Restoration via Post-training GRPO](https://arxiv.org/abs/2512.00814)
*Haoxuan Xu. Yi Liu,Boyuan Jiang,Jinlong Peng,Donghao Luo,Xiaobin Hu,Shuicheng Yan,Haoang Li*

Main category: cs.CV

TL;DR: IRPO提出了一种基于GRPO的后训练范式，通过选择预训练阶段表现不佳的样本进行数据构建，并设计包含通用奖励、专家奖励和恢复奖励的三重奖励系统，在图像恢复任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像恢复方法主要依赖像素级的硬拟合，存在过度平滑和泛化能力差的问题。后训练范式在高级生成任务中取得了显著成功，但在低层视觉任务中尚未得到充分探索。因此，需要开发专门针对低层视觉任务的后训练方法。

Method: IRPO采用GRPO（梯度奖励策略优化）后训练范式，包含两个核心创新：1）数据构建原则：选择预训练阶段表现不佳的样本进行后训练，实现最佳性能和效率；2）奖励建模系统：包含通用奖励（结构保真度）、专家奖励（利用Qwen-VL进行感知对齐）和恢复奖励（任务特定的低层质量）三个互补组件。

Result: 在6个域内和5个域外低层基准测试中，IRPO在多种退化类型上都取得了最先进的结果。相比AdaIR基线，在域内任务上提升了0.83 dB，在域外设置上提升了3.43 dB，表现出优异的泛化能力。

Conclusion: IRPO成功将后训练范式应用于低层视觉任务，通过创新的数据选择和奖励建模策略，显著提升了图像恢复的性能和泛化能力，为低层视觉任务的后训练方法提供了新的思路。

Abstract: Recent advances in post-training paradigms have achieved remarkable success in high-level generation tasks, yet their potential for low-level vision remains rarely explored. Existing image restoration (IR) methods rely on pixel-level hard-fitting to ground-truth images, struggling with over-smoothing and poor generalization. To address these limitations, we propose IRPO, a low-level GRPO-based post-training paradigm that systematically explores both data formulation and reward modeling. We first explore a data formulation principle for low-level post-training paradigm, in which selecting underperforming samples from the pre-training stage yields optimal performance and improved efficiency. Furthermore, we model a reward-level criteria system that balances objective accuracy and human perceptual preference through three complementary components: a General Reward for structural fidelity, an Expert Reward leveraging Qwen-VL for perceptual alignment, and a Restoration Reward for task-specific low-level quality. Comprehensive experiments on six in-domain and five out-of-domain (OOD) low-level benchmarks demonstrate that IRPO achieves state-of-the-art results across diverse degradation types, surpassing the AdaIR baseline by 0.83 dB on in-domain tasks and 3.43 dB on OOD settings. Our code can be shown in https://github.com/HaoxuanXU1024/IRPO.

中文标题: IRPO：通过后训练GRPO提升图像恢复性能

中文摘要: 后训练范式的最新进展在高级生成任务中取得了显著成功，但其在低层视觉中的潜力却很少被探索。现有的图像恢复方法依赖于像素级硬拟合到真实图像，存在过度平滑和泛化能力差的问题。为了解决这些限制，我们提出了IRPO，一种基于低层GRPO的后训练范式，系统地探索了数据构建和奖励建模。我们首先探索了低层后训练范式的数据构建原则，其中选择预训练阶段表现不佳的样本可获得最佳性能和改进的效率。此外，我们建模了一个奖励级标准系统，通过三个互补组件平衡客观准确性和人类感知偏好：用于结构保真度的通用奖励、利用Qwen-VL进行感知对齐的专家奖励，以及用于任务特定低层质量的恢复奖励。在六个域内和五个域外低层基准测试上的综合实验表明，IRPO在多种退化类型上都取得了最先进的结果，在域内任务上超过AdaIR基线0.83 dB，在域外设置上超过3.43 dB。我们的代码可在https://github.com/HaoxuanXU1024/IRPO查看。

</details>


### [79] [PanFlow: Decoupled Motion Control for Panoramic Video Generation](https://arxiv.org/abs/2512.00832)
*Cheng Zhang,Hanwen Liang,Donny Y. Chen,Qianyi Wu,Konstantinos N. Plataniotis,Camilo Cruz Gambardella,Jianfei Cai*

Main category: cs.CV

TL;DR: PanFlow是一种全景视频生成方法，通过解耦相机旋转与光流条件来实现精确的大动态运动控制，并利用球面噪声扭曲确保边界一致性。


<details>
  <summary>Details</summary>
Motivation: 全景视频生成在VR和沉浸式媒体中应用广泛，但现有方法缺乏明确的运动控制能力，难以处理大而复杂的运动场景，需要更精确的运动控制机制。

Method: 1. 利用全景图的球面特性，将相机旋转从输入光流条件中解耦；2. 引入球面噪声扭曲策略确保全景边界运动的循环一致性；3. 构建大规模运动丰富的全景视频数据集进行训练。

Result: 实验表明PanFlow在运动保真度、视觉质量和时间一致性方面显著优于现有方法，并在运动迁移和视频编辑等应用中表现出色。

Conclusion: PanFlow通过解耦运动控制机制和球面噪声扭曲策略，有效解决了全景视频生成中的大动态运动控制问题，为全景视频生成提供了更精确的控制能力。

Abstract: Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of panoramas to decouple the highly dynamic camera rotation from the input optical flow condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence. Our code, dataset, and models are available at https://github.com/chengzhag/PanFlow.

中文标题: PanFlow：全景视频生成的解耦运动控制

中文摘要: 全景视频生成因其在虚拟现实和沉浸式媒体中的应用而受到越来越多的关注。然而，现有方法缺乏明确的运动控制，难以生成具有大而复杂运动的场景。我们提出了PanFlow，一种新颖的方法，利用全景图的球面特性将高度动态的相机旋转从输入光流条件中解耦，从而实现对大而动态运动的更精确控制。我们进一步引入了球面噪声扭曲策略，以促进全景边界上运动的循环一致性。为了支持有效的训练，我们策划了一个大规模、运动丰富的全景视频数据集，包含帧级姿态和光流标注。我们还展示了我们的方法在各种应用中的有效性，包括运动迁移和视频编辑。大量实验表明，PanFlow在运动保真度、视觉质量和时间一致性方面显著优于先前的方法。我们的代码、数据集和模型可在https://github.com/chengzhag/PanFlow获取。

</details>


### [80] [Smol-GS: Compact Representations for Abstract 3D Gaussian Splatting](https://arxiv.org/abs/2512.00850)
*Haishan Wang,Mohammad Hassan Vali,Arno Solin*

Main category: cs.CV

TL;DR: Smol-GS提出了一种用于3D高斯泼溅的紧凑表示方法，通过递归体素层次结构学习空间和语义信息，实现数量级压缩同时保持高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)在神经渲染中表现出色，但需要大量内存存储高斯参数。现有方法在压缩表示方面存在不足，需要更紧凑的表示来支持下游3D场景理解任务。

Method: 采用递归体素层次结构捕获泼溅坐标，同时使用泼溅级特征存储抽象信息（颜色、透明度、变换和材质属性），将空间和语义信息集成到高效编码中。

Result: 在标准基准测试中实现了最先进的压缩效果，同时保持高渲染质量，能够将3D场景压缩数个数量级而不损失灵活性。

Conclusion: Smol-GS为3D高斯泼溅提供了紧凑表示，不仅保持视觉保真度，还为导航、规划和3D场景理解等下游任务奠定了基础。

Abstract: We present Smol-GS, a novel method for learning compact representations for 3D Gaussian Splatting (3DGS). Our approach learns highly efficient encodings in 3D space that integrate both spatial and semantic information. The model captures the coordinates of the splats through a recursive voxel hierarchy, while splat-wise features store abstracted cues, including color, opacity, transformation, and material properties. This design allows the model to compress 3D scenes by orders of magnitude without loss of flexibility. Smol-GS achieves state-of-the-art compression on standard benchmarks while maintaining high rendering quality. Beyond visual fidelity, the discrete representations could potentially serve as a foundation for downstream tasks such as navigation, planning, and broader 3D scene understanding.

中文标题: Smol-GS：抽象3D高斯泼溅的紧凑表示

中文摘要: 我们提出了Smol-GS，一种用于学习3D高斯泼溅(3DGS)紧凑表示的新方法。我们的方法在3D空间中学习高效编码，集成了空间和语义信息。该模型通过递归体素层次结构捕获泼溅坐标，而泼溅级特征存储抽象线索，包括颜色、透明度、变换和材质属性。这种设计使模型能够将3D场景压缩数个数量级而不损失灵活性。Smol-GS在标准基准测试中实现了最先进的压缩效果，同时保持高渲染质量。除了视觉保真度外，离散表示可能为下游任务（如导航、规划和更广泛的3D场景理解）奠定基础。

</details>


### [81] [TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models](https://arxiv.org/abs/2512.00872)
*Tim Veenboer,George Yiasemis,Eric Marcus,Vivien Van Veldhuizen,Cees G. M. Snoek,Jonas Teuwen,Kevin B. W. Groot Lipman*

Main category: cs.CV

TL;DR: TAP-CT提出了一种用于3D CT扫描的任务无关预训练方法，通过改进ViT和DINOv2架构，直接在3D CT体数据上进行大规模自监督预训练，生成无需大量微调即可通用的冻结表示。


<details>
  <summary>Details</summary>
Motivation: 现有医学领域基础模型通常需要大量微调或依赖资源密集的解码器，且许多编码器的预训练目标偏向特定任务。因此需要一种强大、任务无关的基础模型，只需特征提取而无需大量微调。

Method: 对Vision Transformers和DINOv2进行简单而有效的适配，使其适用于体数据，包括改进patch嵌入、位置编码和体数据增强，使架构具有深度感知能力，同时在10.5万个CT体数据上进行大规模3D预训练。

Result: 大规模3D预训练产生了稳定、鲁棒的冻结表示，能够在下游任务中表现出强大的泛化能力，为医学影像研究建立了强大的低资源基线。

Conclusion: TAP-CT提供了一种有效的任务无关CT基础模型预训练方法，通过简单的架构适配和大规模数据训练，实现了无需大量微调即可通用的表示学习，并开源所有模型和代码以促进透明性和可复现性。

Abstract: Existing foundation models (FMs) in the medical domain often require extensive fine-tuning or rely on training resource-intensive decoders, while many existing encoders are pretrained with objectives biased toward specific tasks. This illustrates a need for a strong, task-agnostic foundation model that requires minimal fine-tuning beyond feature extraction. In this work, we introduce a suite of task-agnostic pretraining of CT foundation models (TAP-CT): a simple yet effective adaptation of Vision Transformers (ViTs) and DINOv2 for volumetric data, enabling scalable self-supervised pretraining directly on 3D CT volumes. Our approach incorporates targeted modifications to patch embeddings, positional encodings, and volumetric augmentations, making the architecture depth-aware while preserving the simplicity of the underlying architectures. We show that large-scale 3D pretraining on an extensive in-house CT dataset (105K volumes) yields stable, robust frozen representations that generalize strongly across downstream tasks. To promote transparency and reproducibility, and to establish a powerful, low-resource baseline for future research in medical imaging, we will release all pretrained models, experimental configurations, and downstream benchmark code at https://huggingface.co/fomofo/tap-ct-b-3d.

中文标题: TAP-CT：计算机断层扫描基础模型的三维任务无关预训练

中文摘要: 现有医学领域的基础模型通常需要大量微调或依赖训练资源密集的解码器，而许多现有编码器的预训练目标偏向特定任务。这表明需要一个强大、任务无关的基础模型，只需特征提取而无需大量微调。在这项工作中，我们引入了一套CT基础模型的任务无关预训练方法（TAP-CT）：对Vision Transformers和DINOv2进行简单而有效的适配，使其适用于体数据，能够直接在3D CT体数据上进行可扩展的自监督预训练。我们的方法包含针对patch嵌入、位置编码和体数据增强的针对性修改，使架构具有深度感知能力，同时保持底层架构的简单性。我们展示了在广泛内部CT数据集（10.5万个体数据）上进行的大规模3D预训练产生了稳定、鲁棒的冻结表示，能够在下游任务中表现出强大的泛化能力。为促进透明性和可复现性，并为医学影像未来研究建立强大的低资源基线，我们将在https://huggingface.co/fomofo/tap-ct-b-3d发布所有预训练模型、实验配置和下游基准测试代码。

</details>


### [82] [Quantum-Inspired Spectral Geometry for Neural Operator Equivalence and Structured Pruning](https://arxiv.org/abs/2512.00880)
*Haijian Shao,Wei Liu,Xing Deng*

Main category: cs.CV

TL;DR: 提出量子启发的几何框架，将神经算子表示为布洛赫超球面上的归一化奇异值谱，证明谱距离与功能等价性的严格关系，并基于此开发量子度量驱动的功能冗余图和一次性结构化剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 多模态智能在资源受限和异构硬件上的快速发展暴露出关键瓶颈：多模态特征异质性、动态场景中的实时需求以及硬件特定算子冗余。需要一种统一框架来处理跨模态和跨架构的算子替代性问题。

Method: 引入量子启发的几何框架，将每个算子表示为其在布洛赫超球面上的归一化奇异值谱。证明谱到功能等价定理，表明消失的Fubini-Study/Wasserstein-2距离意味着可证明的功能接近性。基于此度量提出量子度量驱动的功能冗余图和一次性结构化剪枝。

Result: 受控仿真验证了所提度量优于幅度和随机基线方法。大规模多模态transformer和异构硬件（华为昇腾、寒武纪MLU、昆仑芯）的广泛实验验证将在扩展期刊版本中展示。

Conclusion: 该工作建立了跨模态和跨架构算子可替代性的首个严格理论基础，为异构硬件上的高效多模态智能提供了量子启发的几何框架和结构化剪枝方法。

Abstract: The rapid growth of multimodal intelligence on resource-constrained and heterogeneous domestic hardware exposes critical bottlenecks: multimodal feature heterogeneity, real-time requirements in dynamic scenarios, and hardware-specific operator redundancy. This work introduces a quantum-inspired geometric framework for neural operators that represents each operator by its normalized singular value spectrum on the Bloch hypersphere. We prove a tight spectral-to-functional equivalence theorem showing that vanishing Fubini--Study/Wasserstein-2 distance implies provable functional closeness, establishing the first rigorous foundation for cross-modal and cross-architecture operator substitutability. Based on this metric, we propose Quantum Metric-Driven Functional Redundancy Graphs (QM-FRG) and one-shot structured pruning. Controlled simulation validates the superiority of the proposed metric over magnitude and random baselines. An extensive experimental validation on large-scale multimodal transformers and domestic heterogeneous hardware (Huawei Ascend, Cambricon MLU, Kunlunxin) hardware is deferred to an extended journal version currently in preparation.

中文标题: 量子启发的谱几何用于神经算子等价性和结构化剪枝

中文摘要: 多模态智能在资源受限和异构硬件上的快速发展暴露出关键瓶颈：多模态特征异质性、动态场景中的实时需求以及硬件特定算子冗余。本工作引入了一个量子启发的神经算子几何框架，将每个算子表示为其在布洛赫超球面上的归一化奇异值谱。我们证明了一个紧密的谱到功能等价定理，表明消失的Fubini-Study/Wasserstein-2距离意味着可证明的功能接近性，从而建立了跨模态和跨架构算子可替代性的首个严格基础。基于此度量，我们提出了量子度量驱动的功能冗余图和一次性结构化剪枝。受控仿真验证了所提度量优于幅度和随机基线方法。大规模多模态transformer和异构硬件（华为昇腾、寒武纪MLU、昆仑芯）的广泛实验验证将在扩展期刊版本中展示。

</details>


### [83] [Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints](https://arxiv.org/abs/2512.00882)
*Xisheng Feng*

Main category: cs.CV

TL;DR: 提出"看、背、答"框架，通过自生成知识提示增强视觉语言模型性能，解决专业领域中的推理驱动幻觉问题，在农业基准测试中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在专业领域（如精准农业）存在性能瓶颈，主要原因是"推理驱动幻觉"——语言先验覆盖了视觉感知。核心问题是"模态鸿沟"：视觉嵌入无法可靠激活模型中已编码的细粒度专家知识。

Method: 提出参数高效的"看、背、答"三阶段框架：1) "看"阶段生成客观视觉描述和候选集；2) "背"阶段使用轻量级1.7B路由器将视觉线索转化为针对性查询，触发候选特定的参数化知识；3) "答"阶段并行对齐描述与背诵知识，选择最一致的标签。

Result: 在AgroBench上取得最先进结果：杂草识别准确率比Qwen-VL提升23.6%，超越GPT-4o且无需外部搜索开销。模块化设计通过将被动感知转化为主动可控的知识检索来缓解幻觉问题。

Conclusion: 该框架通过自生成知识提示有效增强视觉语言模型性能，解决了专业领域的模态鸿沟问题，为参数高效的知识激活提供了新思路。

Abstract: Vision-Language Models (VLMs) exhibit significant performance plateaus in specialized domains like precision agriculture, primarily due to "Reasoning-Driven Hallucination" where linguistic priors override visual perception. A key bottleneck is the "Modality Gap": visual embeddings fail to reliably activate the fine-grained expert knowledge already encoded in model parameters. We propose "Look, Recite, Then Answer," a parameter-efficient framework that enhances VLMs via self-generated knowledge hints while keeping backbone models frozen. The framework decouples inference into three stages: (1) Look generates objective visual descriptions and candidate sets; (2) Recite employs a lightweight 1.7B router to transform visual cues into targeted queries that trigger candidate-specific parametric knowledge; (3) Answer performs parallel evidence alignment between descriptions and recited knowledge to select the most consistent label. On AgroBench, our method achieves state-of-the-art results, improving Weed Identification accuracy by 23.6% over Qwen-VL and surpassing GPT-4o without external search overhead. This modular design mitigates hallucinations by transforming passive perception into active, controllable knowledge retrieval

中文标题: 看、背、答：通过自生成知识提示增强视觉语言模型性能

中文摘要: 视觉语言模型在精准农业等专业领域表现出显著的性能瓶颈，主要源于"推理驱动幻觉"——语言先验覆盖了视觉感知。关键瓶颈是"模态鸿沟"：视觉嵌入无法可靠激活模型参数中已编码的细粒度专家知识。我们提出"看、背、答"参数高效框架，通过自生成知识提示增强视觉语言模型性能，同时保持骨干模型冻结。该框架将推理解耦为三个阶段：(1) "看"生成客观视觉描述和候选集；(2) "背"使用轻量级1.7B路由器将视觉线索转化为针对性查询，触发候选特定的参数化知识；(3) "答"并行对齐描述与背诵知识，选择最一致的标签。在AgroBench上，我们的方法取得最先进结果，杂草识别准确率比Qwen-VL提升23.6%，超越GPT-4o且无需外部搜索开销。这种模块化设计通过将被动感知转化为主动可控的知识检索来缓解幻觉问题。

</details>


### [84] [HanDyVQA: A Video QA Benchmark for Fine-Grained Hand-Object Interaction Dynamics](https://arxiv.org/abs/2512.00885)
*Masatoshi Tateno,Gido Kato,Hirokatsu Kataoka,Yoichi Sato,Takuma Yagi*

Main category: cs.CV

TL;DR: HanDyVQA是一个细粒度视频问答基准，专注于手-物体交互的动态变化，包含6种问题类型和11.1K个QA对，用于评估模型对手部操作和物体状态变化的时空推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有手-物体交互基准主要关注操作或结果效果的粗粒度层面，缺乏对交互动态的细粒度时空推理能力评估。需要一个新的基准来全面覆盖手-物体交互的操作和效果两个方面。

Method: 创建了包含6种互补问题类型的视频问答基准：动作、过程、物体、位置、状态变化和物体部件。收集了11.1K个多项选择QA对，识别操作风格、手部/物体运动和部件级状态变化。还包括10.3K个分割掩码用于物体和物体部件问题。

Result: 评估了最新的视频基础模型，发现即使表现最好的Gemini-2.5-Pro模型也只达到73%的平均准确率，远低于人类表现（97%）。分析显示在空间关系、运动和部件级几何理解方面仍存在挑战。将显式的手-物体交互线索整合到视觉特征中可以提升性能。

Conclusion: HanDyVQA基准揭示了当前模型在理解手-物体交互动态方面的局限性，为开发具有更深层次交互动态理解能力的未来模型提供了见解和方向。

Abstract: Hand-object interaction (HOI) inherently involves dynamics where human manipulations produce distinct spatio-temporal effects on objects. However, existing semantic HOI benchmarks focused either on manipulation or on the resulting effects at a coarse level, lacking fine-grained spatio-temporal reasoning to capture the underlying dynamics in HOI. We introduce HanDyVQA, a fine-grained video question-answering benchmark that comprehensively covers both the manipulation and effect aspects of HOI. HanDyVQA comprises six complementary question types (Action, Process, Objects, Location, State Change, and Object Parts), totalling 11.1K multiple-choice QA pairs. Collected QA pairs recognizing manipulation styles, hand/object motions, and part-level state changes. HanDyVQA also includes 10.3K segmentation masks for Objects and Object Parts questions, enabling the evaluation of object/part-level reasoning in video object segmentation. We evaluated recent video foundation models on our benchmark and found that even the best-performing model, Gemini-2.5-Pro, reached only 73% average accuracy, which is far from human performance (97%). Further analysis shows the remaining challenges in spatial relationship, motion, and part-level geometric understanding. We also found that integrating explicit HOI-related cues into visual features improves performance, offering insights for developing future models with a deeper understanding of HOI dynamics.

中文标题: HanDyVQA：用于细粒度手-物体交互动态的视频问答基准

中文摘要: 手-物体交互本质上涉及动态变化，其中人类操作会对物体产生独特的时空效应。然而，现有的语义手-物体交互基准要么关注操作，要么关注结果效果的粗粒度层面，缺乏捕捉手-物体交互中潜在动态的细粒度时空推理。我们引入了HanDyVQA，这是一个细粒度的视频问答基准，全面覆盖了手-物体交互的操作和效果两个方面。HanDyVQA包含六种互补的问题类型（动作、过程、物体、位置、状态变化和物体部件），总计11.1K个多项选择QA对。收集的QA对识别操作风格、手部/物体运动和部件级状态变化。HanDyVQA还包括10.3K个用于物体和物体部件问题的分割掩码，能够在视频物体分割中评估物体/部件级推理。我们在基准上评估了最近的视频基础模型，发现即使表现最好的模型Gemini-2.5-Pro也只达到了73%的平均准确率，远低于人类表现（97%）。进一步分析显示了在空间关系、运动和部件级几何理解方面仍存在的挑战。我们还发现，将显式的手-物体交互相关线索整合到视觉特征中可以改善性能，为开发具有更深层次手-物体交互动态理解能力的未来模型提供了见解。

</details>


### [85] [Multilingual Training-Free Remote Sensing Image Captioning](https://arxiv.org/abs/2512.00887)
*Carlos Rebelo,Gil Rocha,João Daniel Silva,Bruno Martins*

Main category: cs.CV

TL;DR: 首个无需训练的多语言遥感图像描述方法，通过检索增强提示和PageRank重排序，在10种语言上达到与监督英语系统相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像描述方法依赖大型标注数据集且主要关注英语，限制了全球适用性。需要开发无需训练、支持多语言的方法来提高包容性和可扩展性。

Method: 1) 使用领域适应的SigLIP2编码器检索相关描述和示例；2) 两种变体：图像盲设置（仅文本提示+多语言LLM）和图像感知设置（VLM联合处理图像和提示）；3) 引入基于PageRank的图重排序策略提高检索一致性。

Result: 在10种语言的4个基准数据集上：1) 与完全监督英语系统竞争；2) PageRank重排序提升性能高达35%；3) VLM生成视觉基础但词汇多样的描述，LLM获得更强的BLEU/CIDEr分数；4) 直接生成优于翻译策略。

Conclusion: 提出了首个系统评估的多语言无需训练遥感图像描述方法，通过检索增强提示和图重排序，实现了与监督系统相当的性能，推动了更具包容性和可扩展性的地球观测系统发展。

Abstract: Remote sensing image captioning has advanced rapidly through encoder--decoder models, although the reliance on large annotated datasets and the focus on English restricts global applicability. To address these limitations, we propose the first training-free multilingual approach, based on retrieval-augmented prompting. For a given aerial image, we employ a domain-adapted SigLIP2 encoder to retrieve related captions and few-shot examples from a datastore, which are then provided to a language model. We explore two variants: an image-blind setup, where a multilingual Large Language Model (LLM) generates the caption from textual prompts alone, and an image-aware setup, where a Vision--Language Model (VLM) jointly processes the prompt and the input image. To improve the coherence of the retrieved content, we introduce a graph-based re-ranking strategy using PageRank on a graph of images and captions. Experiments on four benchmark datasets across ten languages demonstrate that our approach is competitive with fully supervised English-only systems and generalizes to other languages. Results also highlight the importance of re-ranking with PageRank, yielding up to 35% improvements in performance metrics. Additionally, it was observed that while VLMs tend to generate visually grounded but lexically diverse captions, LLMs can achieve stronger BLEU and CIDEr scores. Lastly, directly generating captions in the target language consistently outperforms other translation-based strategies. Overall, our work delivers one of the first systematic evaluations of multilingual, training-free captioning for remote sensing imagery, advancing toward more inclusive and scalable multimodal Earth observation systems.

中文标题: 无需训练的多语言遥感图像描述生成

中文摘要: 遥感图像描述生成通过编码器-解码器模型快速发展，但对大型标注数据集的依赖和对英语的关注限制了其全球适用性。为解决这些限制，我们提出了首个基于检索增强提示的无需训练多语言方法。对于给定的航空图像，我们使用领域适应的SigLIP2编码器从数据存储中检索相关描述和少量示例，然后提供给语言模型。我们探索了两种变体：图像盲设置，其中多语言大型语言模型仅从文本提示生成描述；以及图像感知设置，其中视觉-语言模型联合处理提示和输入图像。为提高检索内容的一致性，我们引入了基于图的重排序策略，在图像和描述图上使用PageRank算法。在十个语言的四个基准数据集上的实验表明，我们的方法可与完全监督的仅英语系统竞争，并能推广到其他语言。结果还突出了PageRank重排序的重要性，性能指标提升高达35%。此外，观察到虽然视觉-语言模型倾向于生成视觉基础但词汇多样的描述，但大型语言模型可以获得更强的BLEU和CIDEr分数。最后，直接在目标语言中生成描述始终优于其他基于翻译的策略。总体而言，我们的工作提供了对遥感图像多语言、无需训练描述生成的首次系统评估之一，朝着更具包容性和可扩展性的多模态地球观测系统迈进。

</details>


### [86] [Accelerating Streaming Video Large Language Models via Hierarchical Token Compression](https://arxiv.org/abs/2512.00891)
*Yiyu Wang,Xuyang Liu,Xiyan Gui,Xinying Lin,Boxue Yang,Chenfei Liao,Tailai Chen,Linfeng Zhang*

Main category: cs.CV

TL;DR: STC是一种用于加速流式视频大语言模型的层次化令牌压缩框架，通过缓存相似帧特征和剪枝不重要的视觉令牌，在保持99%准确率的同时显著降低计算延迟。


<details>
  <summary>Details</summary>
Motivation: 流式视频大语言模型在处理连续视频流时面临计算成本高的问题，主要瓶颈在于视觉Transformer编码阶段对时间相似帧的冗余处理，以及LLM预填充阶段过长的令牌序列导致的延迟和内存开销。

Method: 提出STC框架，包含两个令牌级加速器：STC-Cacher通过缓存和重用时间相似帧的特征来减少ViT编码开销；STC-Pruner在视觉令牌进入LLM前压缩序列，基于空间和时间相关性保留最显著的令牌。

Result: 在4个基线流式VideoLLM和5个基准测试上的实验表明，STC优于其他压缩方法，在ReKV框架上保持99%准确率的同时，将ViT编码延迟降低24.5%，LLM预填充延迟降低45.3%。

Conclusion: STC是一个即插即用的层次化框架，能有效优化流式视频大语言模型的ViT编码和LLM预填充阶段，在保持高准确率的同时显著加速处理速度，适用于实时视频理解任务。

Abstract: Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \textbf{S}treaming \textbf{T}oken \textbf{C}ompression (\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \textbf{99\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \textbf{24.5\%} and \textbf{45.3\%}.

中文标题: 通过层次化令牌压缩加速流式视频大语言模型

中文摘要: 流式视频大语言模型在各种视频理解任务中表现出色，但由于处理连续视频流中密集视觉令牌的高计算成本，在实时部署中面临重大挑战。在流式视频场景中，主要瓶颈在于视觉Transformer编码阶段，其中对时间相似帧的冗余处理导致效率低下。此外，LLM预填充期间膨胀的令牌序列进一步加剧了延迟和内存开销。为解决这些挑战，我们提出流式令牌压缩（STC），这是一个即插即用的层次化框架，可无缝集成到现有的流式VideoLLM中，优化ViT编码和LLM预填充阶段以加速处理。STC引入了两个令牌级加速器：STC-Cacher通过缓存和重用时间相似帧的特征来减少ViT编码开销；STC-Pruner在视觉令牌进入LLM前压缩序列，基于空间和时间相关性保留最显著的令牌。在4个基线流式VideoLLM和5个基准测试上的广泛实验表明，STC优于其他压缩方法。值得注意的是，STC在ReKV框架上保持高达99%的准确率，同时将ViT编码延迟和LLM预填充延迟分别降低24.5%和45.3%。

</details>


### [87] [Hierarchical Semantic Alignment for Image Clustering](https://arxiv.org/abs/2512.00904)
*Xingyu Zhu,Beier Zhu,Yunfan Li,Junfeng Fang,Shuo Wang,Kesen Zhao,Hanwang Zhang*

Main category: cs.CV

TL;DR: CAE方法通过结合图像描述和名词概念构建层次化语义对齐，在无需训练的情况下显著提升图像聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用名词作为外部语义知识，但忽略了名词固有的歧义性，这会扭曲语义表示并降低聚类质量。需要一种能处理语义模糊性的方法。

Method: 提出CAE方法：1) 从WordNet选择相关名词和从描述数据集选择描述，构建与图像特征对齐的语义空间；2) 通过最优传输将图像特征与选定的名词和描述对齐，获得更具区分性的语义空间；3) 结合增强的语义特征和图像特征进行聚类。

Result: 在8个数据集上的广泛实验证明了方法的有效性，在ImageNet-1K数据集上，准确率比最先进的无训练方法提高了4.2%，调整兰德指数提高了2.9%。

Conclusion: CAE方法通过层次化语义对齐有效解决了名词歧义性问题，在无训练的情况下显著提升了图像聚类性能，为利用外部语义知识提供了新思路。

Abstract: Image clustering is a classic problem in computer vision, which categorizes images into different groups. Recent studies utilize nouns as external semantic knowledge to improve clus- tering performance. However, these methods often overlook the inherent ambiguity of nouns, which can distort semantic representations and degrade clustering quality. To address this issue, we propose a hierarChical semAntic alignmEnt method for image clustering, dubbed CAE, which improves cluster- ing performance in a training-free manner. In our approach, we incorporate two complementary types of textual seman- tics: caption-level descriptions, which convey fine-grained attributes of image content, and noun-level concepts, which represent high-level object categories. We first select relevant nouns from WordNet and descriptions from caption datasets to construct a semantic space aligned with image features. Then, we align image features with selected nouns and captions via optimal transport to obtain a more discriminative semantic space. Finally, we combine the enhanced semantic and image features to perform clustering. Extensive experiments across 8 datasets demonstrate the effectiveness of our method, notably surpassing the state-of-the-art training-free approach with a 4.2% improvement in accuracy and a 2.9% improvement in adjusted rand index (ARI) on the ImageNet-1K dataset.

中文标题: 层次化语义对齐的图像聚类方法

中文摘要: 图像聚类是计算机视觉中的经典问题，旨在将图像分类到不同的组中。最近的研究利用名词作为外部语义知识来提高聚类性能。然而，这些方法往往忽略了名词固有的歧义性，这会扭曲语义表示并降低聚类质量。为了解决这个问题，我们提出了一种用于图像聚类的层次化语义对齐方法，称为CAE，该方法以无需训练的方式提高聚类性能。在我们的方法中，我们结合了两种互补类型的文本语义：描述图像内容细粒度属性的描述级描述，以及表示高级对象类别的名词级概念。我们首先从WordNet选择相关名词，从描述数据集选择描述，构建与图像特征对齐的语义空间。然后，我们通过最优传输将图像特征与选定的名词和描述对齐，以获得更具区分性的语义空间。最后，我们结合增强的语义特征和图像特征进行聚类。在8个数据集上的广泛实验证明了我们方法的有效性，在ImageNet-1K数据集上，准确率比最先进的无训练方法提高了4.2%，调整兰德指数提高了2.9%。

</details>


### [88] [Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation](https://arxiv.org/abs/2512.00944)
*An Yang,Chenyu Liu,Jun Du,Jianqing Gao,Jia Pan,Jinshui Hu,Baocai Yin,Bing Yin,Cong Liu*

Main category: cs.CV

TL;DR: 提出了一种用于3D高斯分割的紧凑渐进表示方法，通过二进制编码减少内存开销，采用渐进训练策略提升细粒度分割能力


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯分割方法依赖高维类别特征导致内存开销大，细粒度分割面临标签空间拥挤和缺乏稳定多粒度控制机制的问题

Method: 1) 粗到细的二进制编码方案，通过二进制到十进制映射将特征压缩为单个整数；2) 渐进训练策略将全景分割分解为独立子任务；3) 在分割训练中微调不透明度以解决光度渲染与语义分割的不兼容性

Result: 在多个基准测试中达到最先进的分割性能，同时显著减少内存消耗并加速推理

Conclusion: 提出的Binary-Gaussian方法通过紧凑的二进制表示和渐进训练策略，有效解决了3D高斯分割中的内存开销和细粒度分割挑战

Abstract: 3D Gaussian Splatting (3D-GS) has emerged as an efficient 3D representation and a promising foundation for semantic tasks like segmentation. However, existing 3D-GS-based segmentation methods typically rely on high-dimensional category features, which introduce substantial memory overhead. Moreover, fine-grained segmentation remains challenging due to label space congestion and the lack of stable multi-granularity control mechanisms. To address these limitations, we propose a coarse-to-fine binary encoding scheme for per-Gaussian category representation, which compresses each feature into a single integer via the binary-to-decimal mapping, drastically reducing memory usage. We further design a progressive training strategy that decomposes panoptic segmentation into a series of independent sub-tasks, reducing inter-class conflicts and thereby enhancing fine-grained segmentation capability. Additionally, we fine-tune opacity during segmentation training to address the incompatibility between photometric rendering and semantic segmentation, which often leads to foreground-background confusion. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art segmentation performance while significantly reducing memory consumption and accelerating inference.

中文标题: Binary-Gaussian：用于3D高斯分割的紧凑渐进表示

中文摘要: 3D高斯泼溅（3D-GS）已成为一种高效的3D表示方法，并成为分割等语义任务的有前景基础。然而，现有的基于3D-GS的分割方法通常依赖高维类别特征，这带来了巨大的内存开销。此外，由于标签空间拥挤和缺乏稳定的多粒度控制机制，细粒度分割仍然具有挑战性。为了解决这些限制，我们提出了一种用于每个高斯类别表示的粗到细二进制编码方案，通过二进制到十进制映射将每个特征压缩为单个整数，从而大幅减少内存使用。我们进一步设计了一种渐进训练策略，将全景分割分解为一系列独立的子任务，减少类间冲突，从而增强细粒度分割能力。此外，我们在分割训练期间微调不透明度，以解决光度渲染与语义分割之间的不兼容性，这种不兼容性通常会导致前景-背景混淆。在多个基准测试上的广泛实验表明，我们的方法在显著减少内存消耗和加速推理的同时，实现了最先进的分割性能。

</details>


### [89] [Adaptive Evidential Learning for Temporal-Semantic Robustness in Moment Retrieval](https://arxiv.org/abs/2512.00953)
*Haojian Huang,Kaijing Ma,Jin Chen,Haodong Chen,Zhou Wu,Xianghao Zang,Han Fang,Chao Ban,Hao Sun,Mulin Chen,Zhongjiang He*

Main category: cs.CV

TL;DR: 本文提出DEMR框架，通过反射翻转融合和查询重构解决时刻检索中的模态不平衡和不确定性估计偏差问题，引入几何正则器实现与困难时刻的自适应对齐，显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 传统时刻检索方法使用预训练模型，难以处理细粒度信息和确定性推理，导致与复杂或模糊时刻对齐困难。深度证据回归（DER）构建的基线存在模态不平衡处理不足和不确定性正则器结构问题，使得高不确定性被错误分配给准确样本而非挑战性样本，缺乏复杂视频场景的适应性。

Method: 提出DEMR框架：1）反射翻转融合（RFF）块实现跨模态对齐；2）查询重构任务增强文本敏感性；3）几何正则器优化不确定性预测，实现与困难时刻的自适应对齐。

Result: 在标准数据集和去偏数据集ActivityNet-CD、Charades-CD上的实验表明，DEMR在有效性、鲁棒性和可解释性方面均有显著提升，优于现有方法。

Conclusion: DEMR框架通过解决模态不平衡和不确定性估计偏差问题，实现了时刻检索中时序语义的鲁棒性，为复杂视频场景下的时刻检索提供了有效解决方案。

Abstract: In the domain of moment retrieval, accurately identifying temporal segments within videos based on natural language queries remains challenging. Traditional methods often employ pre-trained models that struggle with fine-grained information and deterministic reasoning, leading to difficulties in aligning with complex or ambiguous moments. To overcome these limitations, we explore Deep Evidential Regression (DER) to construct a vanilla Evidential baseline. However, this approach encounters two major issues: the inability to effectively handle modality imbalance and the structural differences in DER's heuristic uncertainty regularizer, which adversely affect uncertainty estimation. This misalignment results in high uncertainty being incorrectly associated with accurate samples rather than challenging ones. Our observations indicate that existing methods lack the adaptability required for complex video scenarios. In response, we propose Debiased Evidential Learning for Moment Retrieval (DEMR), a novel framework that incorporates a Reflective Flipped Fusion (RFF) block for cross-modal alignment and a query reconstruction task to enhance text sensitivity, thereby reducing bias in uncertainty estimation. Additionally, we introduce a Geom-regularizer to refine uncertainty predictions, enabling adaptive alignment with difficult moments and improving retrieval accuracy. Extensive testing on standard datasets and debiased datasets ActivityNet-CD and Charades-CD demonstrates significant enhancements in effectiveness, robustness, and interpretability, positioning our approach as a promising solution for temporal-semantic robustness in moment retrieval. The code is publicly available at https://github.com/KaijingOfficial/DEMR.

中文标题: 面向时刻检索的时序语义鲁棒性自适应证据学习

中文摘要: 在时刻检索领域，基于自然语言查询准确识别视频中的时间片段仍然具有挑战性。传统方法通常采用预训练模型，这些模型难以处理细粒度信息和确定性推理，导致与复杂或模糊时刻的对齐困难。为了克服这些限制，我们探索了深度证据回归（DER）来构建一个基础的证据基线。然而，这种方法面临两个主要问题：无法有效处理模态不平衡，以及DER启发式不确定性正则器的结构差异，这对不确定性估计产生了不利影响。这种错位导致高不确定性被错误地与准确样本而非挑战性样本相关联。我们的观察表明，现有方法缺乏复杂视频场景所需的适应性。为此，我们提出了面向时刻检索的去偏证据学习（DEMR），这是一个新颖的框架，包含用于跨模态对齐的反射翻转融合（RFF）块和用于增强文本敏感性的查询重构任务，从而减少不确定性估计中的偏差。此外，我们引入了几何正则器来优化不确定性预测，实现与困难时刻的自适应对齐并提高检索准确性。在标准数据集和去偏数据集ActivityNet-CD和Charades-CD上的广泛测试表明，该方法在有效性、鲁棒性和可解释性方面均有显著提升，使其成为时刻检索中时序语义鲁棒性的一个有前景的解决方案。代码已在https://github.com/KaijingOfficial/DEMR公开。

</details>


### [90] [Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction](https://arxiv.org/abs/2512.00960)
*Boran Wen,Ye Lu,Keyan Wan,Sirui Wang,Jiahong Zhou,Junxuan Liang,Xinpeng Liu,Bang Xiao,Dingbang Huang,Ruiyang Liu,Yong-Lu Li*

Main category: cs.CV

TL;DR: 提出了4DHOISolver优化框架和Open4DHOI数据集，用于从单目视频中高效重建4D人-物交互运动，通过人工标注接触点解决病态重建问题，并展示了在机器人模仿学习中的应用价值。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需要从多样化的大规模人-物交互数据中学习，而单目互联网视频提供了丰富的交互数据源。然而，从这些野外视频中准确提取4D交互数据面临病态重建、缺乏精确接触对应关系等挑战，现有方法难以实现高效可扩展的重建。

Method: 提出了4DHOISolver优化框架，利用稀疏的人工标注接触点来约束4D HOI重建问题，确保时空一致性和物理合理性。基于此框架构建了Open4DHOI大规模数据集，包含144种对象类型和103种动作。

Result: 成功构建了包含多样化交互的大规模4D HOI数据集，重建的运动具有高时空一致性和物理合理性。通过强化学习智能体能够有效模仿恢复的运动，验证了重建数据的实用性。基准测试显示现有3D基础模型在预测精确接触对应关系方面仍有不足。

Conclusion: 4DHOISolver框架和Open4DHOI数据集为解决单目视频中4D人-物交互重建问题提供了有效方案，强调了人工参与标注接触点在当前阶段的重要性，为机器人学习和计算机视觉社区提供了有价值的资源和开放挑战。

Abstract: Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at https://wenboran2002.github.io/open4dhoi/

中文标题: 高效可扩展的单目人-物交互运动重建

中文摘要: 通用机器人必须从多样化、大规模的人-物交互（HOI）中学习，才能在现实世界中稳健运行。单目互联网视频提供了一个几乎无限且易于获取的数据源，捕捉了无与伦比的人类活动、物体和环境多样性。然而，从这些野外视频中准确且可扩展地提取4D交互数据仍然是一个重大且未解决的挑战。因此，在这项工作中，我们引入了4DHOISolver，这是一个新颖且高效的优化框架，通过利用稀疏的人工参与接触点标注来约束病态的4D HOI重建问题，同时保持高度的时空一致性和物理合理性。利用该框架，我们引入了Open4DHOI，这是一个新的大规模4D HOI数据集，包含144种对象类型和103种动作的多样化目录。此外，我们通过使基于强化学习的智能体能够模仿恢复的运动来证明我们重建的有效性。然而，对现有3D基础模型的全面基准测试表明，自动预测精确的人-物接触对应关系仍然是一个未解决的问题，这强调了我们人工参与策略的即时必要性，同时向社区提出了一个开放挑战。数据和代码将在https://wenboran2002.github.io/open4dhoi/公开提供。

</details>


### [91] [PhotoFramer: Multi-modal Image Composition Instruction](https://arxiv.org/abs/2512.00993)
*Zhiyuan You,Ke Wang,He Zhang,Xin Cai,Jinjin Gu,Tianfan Xue,Chao Dong,Zhoutong Zhang*

Main category: cs.CV

TL;DR: PhotoFramer是一个多模态图像构图指导框架，能够为构图不佳的图像提供自然语言改进建议并生成构图良好的示例图像。


<details>
  <summary>Details</summary>
Motivation: 许多普通用户在拍照时难以获得良好构图，需要专业的构图指导。现有的构图辅助工具通常只提供裁剪建议或示例图像，缺乏具体的、可操作的指导说明。

Method: 1. 构建大规模数据集：将构图指导分为三个子任务层次（平移、放大、视角变换）；2. 平移和放大数据从现有裁剪数据集中采样；3. 视角变换数据通过两阶段流程获取：从多视角数据集中采样不同视角的配对，训练退化模型将良好构图照片转化为构图不佳的照片，然后将该模型应用于专业拍摄照片以合成训练配对；4. 微调能够联合处理文本和图像的模型。

Result: 文本指令能有效引导图像构图，将文本指导与示例图像结合比仅使用示例的基线方法有持续改进。PhotoFramer为日常用户提供了实用的构图辅助工具。

Conclusion: PhotoFramer通过结合文本指导和示例图像，为普通用户提供了可操作的构图改进建议，使专业摄影知识更加普及。该框架在构图指导方面表现出色，代码、模型权重和数据集已开源。

Abstract: Composition matters during the photo-taking process, yet many casual users struggle to frame well-composed images. To provide composition guidance, we introduce PhotoFramer, a multi-modal composition instruction framework. Given a poorly composed image, PhotoFramer first describes how to improve the composition in natural language and then generates a well-composed example image. To train such a model, we curate a large-scale dataset. Inspired by how humans take photos, we organize composition guidance into a hierarchy of sub-tasks: shift, zoom-in, and view-change tasks. Shift and zoom-in data are sampled from existing cropping datasets, while view-change data are obtained via a two-stage pipeline. First, we sample pairs with varying viewpoints from multi-view datasets, and train a degradation model to transform well-composed photos into poorly composed ones. Second, we apply this degradation model to expert-taken photos to synthesize poor images to form training pairs. Using this dataset, we finetune a model that jointly processes and generates both text and images, enabling actionable textual guidance with illustrative examples. Extensive experiments demonstrate that textual instructions effectively steer image composition, and coupling them with exemplars yields consistent improvements over exemplar-only baselines. PhotoFramer offers a practical step toward composition assistants that make expert photographic priors accessible to everyday users. Codes, model weights, and datasets have been released in https://zhiyuanyou.github.io/photoframer.

中文标题: PhotoFramer：多模态图像构图指导框架

中文摘要: 构图在拍照过程中很重要，但许多普通用户难以拍摄出构图良好的图像。为了提供构图指导，我们引入了PhotoFramer，一个多模态构图指导框架。给定一个构图不佳的图像，PhotoFramer首先用自然语言描述如何改进构图，然后生成一个构图良好的示例图像。为了训练这样的模型，我们策划了一个大规模数据集。受人类拍照方式的启发，我们将构图指导组织成子任务层次结构：平移、放大和视角变换任务。平移和放大数据从现有的裁剪数据集中采样，而视角变换数据通过两阶段流程获取。首先，我们从多视角数据集中采样具有不同视角的配对，并训练一个退化模型将构图良好的照片转化为构图不佳的照片。其次，我们将这个退化模型应用于专业拍摄的照片，合成出构图不佳的图像以形成训练配对。使用这个数据集，我们微调了一个能够联合处理和生成文本和图像的模型，实现了具有可操作性的文本指导和示例图像。大量实验表明，文本指令能有效引导图像构图，将它们与示例图像结合比仅使用示例的基线方法有持续改进。PhotoFramer为构图助手提供了一个实用的步骤，使日常用户能够获得专业的摄影先验知识。代码、模型权重和数据集已在https://zhiyuanyou.github.io/photoframer发布。

</details>


### [92] [S2AM3D: Scale-controllable Part Segmentation of 3D Point Cloud](https://arxiv.org/abs/2512.00995)
*Han Su,Tianyu Huang,Zichen Wan,Xiaohe Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: S2AM3D是一个用于3D点云分割的新方法，通过结合2D分割先验和3D一致性监督，解决了现有方法泛化性差和视图不一致的问题，并支持实时调整分割粒度。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云部件级分割面临两大挑战：原生3D模型因数据稀缺而缺乏泛化能力，而引入2D预训练知识又会导致不同视图间的分割结果不一致。需要一种既能利用2D先验知识又能保证3D一致性的方法。

Method: 提出S2AM3D方法，包含点一致性部件编码器和尺度感知提示解码器。编码器通过原生3D对比学习聚合多视图2D特征，生成全局一致的点特征；解码器通过连续尺度信号实时调整分割粒度。同时构建了大规模高质量部件级点云数据集用于训练。

Result: 在多个评估设置中取得领先性能，在处理复杂结构和尺寸变化显著的部件时表现出卓越的鲁棒性和可控性。构建的数据集包含超过10万个样本，为模型训练提供了充足的监督信号。

Conclusion: S2AM3D成功解决了3D点云部件级分割中的泛化性和一致性问题，通过结合2D先验和3D监督实现了高质量、可控制的分割效果，为3D计算机视觉领域提供了有效的解决方案。

Abstract: Part-level point cloud segmentation has recently attracted significant attention in 3D computer vision. Nevertheless, existing research is constrained by two major challenges: native 3D models lack generalization due to data scarcity, while introducing 2D pre-trained knowledge often leads to inconsistent segmentation results across different views. To address these challenges, we propose S2AM3D, which incorporates 2D segmentation priors with 3D consistent supervision. We design a point-consistent part encoder that aggregates multi-view 2D features through native 3D contrastive learning, producing globally consistent point features. A scale-aware prompt decoder is then proposed to enable real-time adjustment of segmentation granularity via continuous scale signals. Simultaneously, we introduce a large-scale, high-quality part-level point cloud dataset with more than 100k samples, providing ample supervision signals for model training. Extensive experiments demonstrate that S2AM3D achieves leading performance across multiple evaluation settings, exhibiting exceptional robustness and controllability when handling complex structures and parts with significant size variations.

中文标题: S2AM3D：可控制尺度的3D点云部件分割

中文摘要: 部件级点云分割最近在3D计算机视觉中引起了广泛关注。然而，现有研究面临两大挑战：原生3D模型因数据稀缺而缺乏泛化能力，而引入2D预训练知识往往导致不同视图间的分割结果不一致。为解决这些挑战，我们提出了S2AM3D，它将2D分割先验与3D一致性监督相结合。我们设计了一个点一致性部件编码器，通过原生3D对比学习聚合多视图2D特征，生成全局一致的点特征。然后提出了一个尺度感知提示解码器，通过连续尺度信号实现分割粒度的实时调整。同时，我们引入了一个大规模、高质量的部件级点云数据集，包含超过10万个样本，为模型训练提供了充足的监督信号。大量实验表明，S2AM3D在多个评估设置中取得了领先性能，在处理复杂结构和尺寸变化显著的部件时表现出卓越的鲁棒性和可控性。

</details>


### [93] [Provenance-Driven Reliable Semantic Medical Image Vector Reconstruction via Lightweight Blockchain-Verified Latent Fingerprints](https://arxiv.org/abs/2512.00999)
*Mohsin Rasheed,Abdullah Al-Mamun*

Main category: cs.CV

TL;DR: 提出结合语义感知重建和轻量级区块链溯源的医学图像修复框架，在保持解剖结构的同时确保数据可信度


<details>
  <summary>Details</summary>
Motivation: 医学图像在临床诊断中至关重要，但现实数据常受损坏、噪声和篡改影响，传统重建方法虽能恢复像素但可能损害解剖保真度，且缺乏可信的溯源机制，影响AI辅助诊断的可靠性

Method: 1) 语义感知重建框架：结合高级潜在嵌入与混合U-Net架构，保留临床相关结构；2) 轻量级区块链溯源层：采用无标度图设计，记录每个重建事件的可验证指纹，实现低开销的可追溯性

Result: 在多个数据集和损坏类型上的评估显示，相比现有方法，该方法在结构一致性、恢复准确性和溯源完整性方面均有显著提升，同时保持较低的计算开销

Conclusion: 通过整合语义引导重建与安全可追溯性，该框架推进了医学成像中可靠AI的发展，增强了诊断信心和监管合规性，为医疗环境提供了更可信的解决方案

Abstract: Medical imaging is essential for clinical diagnosis, yet real-world data frequently suffers from corruption, noise, and potential tampering, challenging the reliability of AI-assisted interpretation. Conventional reconstruction techniques prioritize pixel-level recovery and may produce visually plausible outputs while compromising anatomical fidelity, an issue that can directly impact clinical outcomes. We propose a semantic-aware medical image reconstruction framework that integrates high-level latent embeddings with a hybrid U-Net architecture to preserve clinically relevant structures during restoration. To ensure trust and accountability, we incorporate a lightweight blockchain-based provenance layer using scale-free graph design, enabling verifiable recording of each reconstruction event without imposing significant overhead. Extensive evaluation across multiple datasets and corruption types demonstrates improved structural consistency, restoration accuracy, and provenance integrity compared with existing approaches. By uniting semantic-guided reconstruction with secure traceability, our solution advances dependable AI for medical imaging, enhancing both diagnostic confidence and regulatory compliance in healthcare environments.

中文标题: 基于轻量级区块链验证潜在指纹的溯源驱动可靠语义医学图像向量重建

中文摘要: 医学成像对于临床诊断至关重要，然而现实世界数据经常遭受损坏、噪声和潜在篡改，这挑战了AI辅助解释的可靠性。传统的重建技术优先考虑像素级恢复，可能产生视觉上合理但损害解剖保真度的输出，这一问题可能直接影响临床结果。我们提出了一种语义感知的医学图像重建框架，将高级潜在嵌入与混合U-Net架构相结合，以在恢复过程中保留临床相关结构。为确保信任和可追溯性，我们采用基于无标度图设计的轻量级区块链溯源层，实现每个重建事件的可验证记录而不会带来显著开销。在多个数据集和损坏类型上的广泛评估表明，与现有方法相比，我们的方法在结构一致性、恢复准确性和溯源完整性方面都有所改进。通过将语义引导重建与安全可追溯性相结合，我们的解决方案推进了医学成像中可靠的AI，增强了医疗环境中的诊断信心和监管合规性。

</details>


### [94] [LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency](https://arxiv.org/abs/2512.01008)
*Zhongbin Guo,Jiahe Liu,Wenyu Gao,Yushan Li,Chengzhi Li,Ping Jian*

Main category: cs.CV

TL;DR: LISA-3D是一个两阶段框架，通过为LISA模型添加几何感知的LoRA层并重用冻结的SAM-3D，将语言-图像分割提升到3D，利用多视角一致性训练，无需3D-文本监督，在语言到3D任务上显著优于单视角基线。


<details>
  <summary>Details</summary>
Motivation: 文本驱动的3D重建需要能够理解开放词汇指令并在不同视角下保持一致的掩码生成器。现有方法在跨视角一致性方面存在不足，需要开发一个既理解语言指令又保持几何一致性的系统。

Method: 1. 两阶段框架：第一阶段为LISA模型添加几何感知的LoRA层，第二阶段重用冻结的SAM-3D重建器。2. 利用RGB-D序列和相机位姿构建可微分重投影损失，强制跨视角一致性。3. 将生成的掩码与RGB图像拼接形成RGBA提示输入SAM-3D。4. 仅训练1160万个参数，保持模块化和数据高效。

Result: 在ScanRefer和Nr3D数据集上，LISA-3D将语言到3D的准确率比单视角基线提高了高达+15.6个百分点。系统支持零样本部署到未见类别，能够生成高斯泼溅或纹理网格，无需重新训练SAM-3D。

Conclusion: LISA-3D提供了一个实用、模块化且数据高效的解决方案，通过多视角一致性将语言-图像分割提升到3D，显著提高了语言引导3D重建的准确性，支持零样本部署，为语言驱动的3D内容创建开辟了新途径。

Abstract: Text-driven 3D reconstruction demands a mask generator that simultaneously understands open-vocabulary instructions and remains consistent across viewpoints. We present LISA-3D, a two-stage framework that lifts language-image segmentation into 3D by retrofitting the instruction-following model LISA with geometry-aware Low-Rank Adaptation (LoRA) layers and reusing a frozen SAM-3D reconstructor. During training we exploit off-the-shelf RGB-D sequences and their camera poses to build a differentiable reprojection loss that enforces cross-view agreement without requiring any additional 3D-text supervision. The resulting masks are concatenated with RGB images to form RGBA prompts for SAM-3D, which outputs Gaussian splats or textured meshes without retraining. Across ScanRefer and Nr3D, LISA-3D improves language-to-3D accuracy by up to +15.6 points over single-view baselines while adapting only 11.6M parameters. The system is modular, data-efficient, and supports zero-shot deployment on unseen categories, providing a practical recipe for language-guided 3D content creation. Our code will be available at https://github.com/binisalegend/LISA-3D.

中文标题: LISA-3D：通过多视角一致性将语言-图像分割提升到3D

中文摘要: 文本驱动的3D重建需要一个同时理解开放词汇指令并在不同视角下保持一致的掩码生成器。我们提出了LISA-3D，这是一个两阶段框架，通过为指令跟随模型LISA配备几何感知的低秩适应层并重用冻结的SAM-3D重建器，将语言-图像分割提升到3D。在训练期间，我们利用现成的RGB-D序列及其相机位姿构建可微分重投影损失，以强制跨视角一致性，而无需任何额外的3D-文本监督。生成的掩码与RGB图像拼接形成RGBA提示，输入到SAM-3D中，无需重新训练即可输出高斯泼溅或纹理网格。在ScanRefer和Nr3D数据集上，LISA-3D将语言到3D的准确率比单视角基线提高了高达+15.6个百分点，同时仅适应了1160万个参数。该系统是模块化的、数据高效的，并支持在未见类别上进行零样本部署，为语言引导的3D内容创建提供了实用方案。我们的代码将在https://github.com/binisalegend/LISA-3D上提供。

</details>


### [95] [Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model](https://arxiv.org/abs/2512.01030)
*Jing He,Haodong Li,Mingzhi Sheng,Ying-Cong Chen*

Main category: cs.CV

TL;DR: Lotus-2是一个两阶段确定性框架，通过利用预训练扩散模型的世界先验，使用少量训练样本（仅59K）在单目深度估计和表面法线预测任务中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 从单张图像恢复像素级几何属性本质上是病态问题，因为存在外观模糊性和2D观测到3D结构的非单射映射。虽然判别式回归模型通过大规模监督取得了良好性能，但其成功受限于数据的规模、质量和多样性，以及有限的物理推理能力。扩散模型展现出强大的世界先验，编码了从海量图像-文本数据中学到的几何和语义信息，但直接重用其随机生成公式对于确定性几何推理是次优的：前者优化目标是多样化和高保真图像生成，而后者需要稳定和准确的预测。

Method: Lotus-2采用两阶段确定性框架：第一阶段，核心预测器使用单步确定性公式，包含干净数据目标和轻量级局部连续性模块（LCM），生成全局一致的结构而无网格伪影；第二阶段，细节锐化器在核心预测器定义的流形内执行约束多步校正流细化，通过无噪声确定性流匹配增强细粒度几何。

Result: 仅使用59K训练样本（不到现有大规模数据集的1%），Lotus-2在单目深度估计中建立了新的最先进结果，并在表面法线预测中取得了高度竞争力的性能。

Conclusion: 扩散模型可以作为确定性世界先验，实现超越传统判别式和生成式范式的高质量几何推理。Lotus-2提供了一个最优适应协议，充分利用预训练生成先验，实现稳定、准确和细粒度的几何密集预测。

Abstract: Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.

中文标题: Lotus-2：利用强大图像生成模型推进几何密集预测

中文摘要: 从单张图像恢复像素级几何属性本质上是病态的，因为存在外观模糊性和2D观测到3D结构的非单射映射。虽然判别式回归模型通过大规模监督取得了强大性能，但其成功受限于可用数据的规模、质量和多样性以及有限的物理推理。最近的扩散模型展现出强大的世界先验，编码了从海量图像-文本数据中学到的几何和语义，但直接重用其随机生成公式对于确定性几何推理是次优的：前者优化目标是多样化和高保真图像生成，而后者需要稳定和准确的预测。在这项工作中，我们提出了Lotus-2，一个用于稳定、准确和细粒度几何密集预测的两阶段确定性框架，旨在提供一个最优适应协议以充分利用预训练生成先验。具体来说，在第一阶段，核心预测器采用单步确定性公式，包含干净数据目标和轻量级局部连续性模块（LCM），生成全局一致的结构而无网格伪影。在第二阶段，细节锐化器在核心预测器定义的流形内执行约束多步校正流细化，通过无噪声确定性流匹配增强细粒度几何。仅使用59K训练样本，不到现有大规模数据集的1%，Lotus-2在单目深度估计中建立了新的最先进结果，并在表面法线预测中取得了高度竞争力的性能。这些结果表明扩散模型可以作为确定性世界先验，实现超越传统判别式和生成式范式的高质量几何推理。

</details>


### [96] [TRoVe: Discovering Error-Inducing Static Feature Biases in Temporal Vision-Language Models](https://arxiv.org/abs/2512.01048)
*Maya Varma,Jean-Benoit Delbrouck,Sophie Ostmeier,Akshay Chaudhari,Curtis Langlotz*

Main category: cs.CV

TL;DR: TRoVe是一种自动发现时序视觉语言模型中引发错误的静态特征偏差的方法，通过评估特征对分类错误的影响和模型对特征的依赖程度来识别偏差，相比基线方法提升28.6%。


<details>
  <summary>Details</summary>
Motivation: 时序视觉语言模型在处理视觉变化任务时可能过度依赖静态特征（如背景、物体特征）而非动态变化，这种静态特征偏差会导致系统性预测错误，需要在模型部署前识别和解决。

Method: TRoVe从标注的验证数据集中提取候选静态特征，通过两个标准评分：1）特征对分类错误的影响；2）模型在预测时对该特征的依赖程度。使用包含101个训练模型和真实标注的评估框架进行定量验证。

Result: TRoVe在识别静态特征偏差方面比最接近的基线方法提升28.6%，应用于7个现成VLMs和2个时序任务时发现了先前未知的偏差，证明偏差知识能提高测试性能。

Conclusion: TRoVe能有效发现时序视觉语言模型中的静态特征偏差，为模型部署前的偏差检测提供了自动化工具，有助于提高模型的鲁棒性和可靠性。

Abstract: Vision-language models (VLMs) have made great strides in addressing temporal understanding tasks, which involve characterizing visual changes across a sequence of images. However, recent works have suggested that when making predictions, VLMs may rely on static feature biases, such as background or object features, rather than dynamic visual changes. Static feature biases are a type of shortcut and can contribute to systematic prediction errors on downstream tasks; as a result, identifying and characterizing error-inducing static feature biases is critical prior to real-world model deployment. In this work, we introduce TRoVe, an automated approach for discovering error-inducing static feature biases learned by temporal VLMs. Given a trained VLM and an annotated validation dataset associated with a downstream classification task, TRoVe extracts candidate static features from the dataset and scores each feature by (i) the effect of the feature on classification errors as well as (ii) the extent to which the VLM relies on the feature when making predictions. In order to quantitatively evaluate TRoVe, we introduce an evaluation framework consisting of 101 trained temporal VLMs paired with ground-truth annotations for learned static feature biases. We use this framework to demonstrate that TRoVe can accurately identify error-inducing static feature biases in VLMs, achieving a 28.6% improvement over the closest baseline. Finally, we apply TRoVe to 7 off-the-shelf VLMs and 2 temporal understanding tasks, surfacing previously-unknown static feature biases and demonstrating that knowledge of learned biases can aid in improving model performance at test time. Our code is available at https://github.com/Stanford-AIMI/TRoVe.

中文标题: TRoVe：在时序视觉语言模型中探索引发错误的静态特征偏差

中文摘要: 视觉语言模型（VLMs）在处理时序理解任务方面取得了显著进展，这些任务涉及描述一系列图像中的视觉变化。然而，最近的研究表明，VLMs在做出预测时可能依赖于静态特征偏差，如背景或物体特征，而非动态的视觉变化。静态特征偏差是一种捷径，可能导致下游任务出现系统性预测错误；因此，在实际部署模型之前，识别和描述引发错误的静态特征偏差至关重要。在本工作中，我们提出了TRoVe，一种自动发现时序VLMs学习到的引发错误的静态特征偏差的方法。给定一个训练好的VLM和一个带有下游分类任务标注的验证数据集，TRoVe从数据集中提取候选静态特征，并根据以下两个标准对每个特征进行评分：（i）该特征对分类错误的影响，以及（ii）VLM在做出预测时对该特征的依赖程度。为了定量评估TRoVe，我们引入了一个评估框架，包含101个训练好的时序VLMs及其学习到的静态特征偏差的真实标注。我们使用该框架证明TRoVe能够准确识别VLMs中的引发错误的静态特征偏差，相比最接近的基线方法提升了28.6%。最后，我们将TRoVe应用于7个现成的VLMs和2个时序理解任务，发现了先前未知的静态特征偏差，并证明了解学习到的偏差知识有助于提高模型在测试时的性能。我们的代码可在https://github.com/Stanford-AIMI/TRoVe获取。

</details>


### [97] [Parameter Reduction Improves Vision Transformers: A Comparative Study of Sharing and Width Reduction](https://arxiv.org/abs/2512.01059)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.CV

TL;DR: 研究发现ViT-B/16在ImageNet-1K上存在过参数化问题，通过两种MLP参数缩减策略（权重共享和隐藏层减半）不仅减少了32.7%参数，还提升了准确率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 虽然通常认为增加视觉Transformer规模能提升性能，但实际发现模型准确性和训练行为并不总是随规模单调增长。本研究旨在探索ViT-B/16在ImageNet-1K上是否存在过参数化问题，以及通过参数缩减策略能否在减少参数的同时维持甚至提升性能。

Method: 针对ViT-B/16的MLP模块设计了两种参数缩减策略：1) GroupedMLP：在相邻transformer块之间共享MLP权重；2) ShallowMLP：将MLP隐藏维度减半。两种策略都减少了32.7%的基线参数，在ImageNet-1K上进行训练和评估。

Result: GroupedMLP达到81.47% top-1准确率（基线81.05%），保持相同计算成本；ShallowMLP达到81.25%准确率，推理吞吐量提升38%。两种模型都显著改善了训练稳定性，将峰值到最终准确率下降从基线的0.47%降低到0.03-0.06%。

Conclusion: ViT-B/16在ImageNet-1K上存在过参数化现象，MLP容量可以减少而不损害性能。参数共享和减少宽度等架构约束可作为有用的归纳偏置，参数分配方式对视觉Transformer设计至关重要。

Abstract: Although scaling laws and many empirical results suggest that increasing the size of Vision Transformers often improves performance, model accuracy and training behavior are not always monotonically increasing with scale. Focusing on ViT-B/16 trained on ImageNet-1K, we study two simple parameter-reduction strategies applied to the MLP blocks, each removing 32.7\% of the baseline parameters. Our \emph{GroupedMLP} variant shares MLP weights between adjacent transformer blocks and achieves 81.47\% top-1 accuracy while maintaining the baseline computational cost. Our \emph{ShallowMLP} variant halves the MLP hidden dimension and reaches 81.25\% top-1 accuracy with a 38\% increase in inference throughput. Both models outperform the 86.6M-parameter baseline (81.05\%) and exhibit substantially improved training stability, reducing peak-to-final accuracy degradation from 0.47\% to the range 0.03\% to 0.06\%. These results suggest that, for ViT-B/16 on ImageNet-1K with a standard training recipe, the model operates in an overparameterized regime in which MLP capacity can be reduced without harming performance and can even slightly improve it. More broadly, our findings suggest that architectural constraints such as parameter sharing and reduced width may act as useful inductive biases, and highlight the importance of how parameters are allocated when designing Vision Transformers. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/parameter-efficient-vit-mlps.

中文标题: 参数缩减提升视觉Transformer性能：共享与宽度缩减策略的对比研究

中文摘要: 尽管扩展定律和许多实证结果表明增加视觉Transformer的规模通常能提升性能，但模型准确性和训练行为并不总是随规模单调增长。我们以在ImageNet-1K上训练的ViT-B/16为研究对象，研究了应用于MLP模块的两种简单参数缩减策略，每种策略都移除了基线模型32.7%的参数。我们的GroupedMLP变体在相邻transformer块之间共享MLP权重，在保持基线计算成本的同时实现了81.47%的top-1准确率。我们的ShallowMLP变体将MLP隐藏维度减半，达到81.25%的top-1准确率，同时推理吞吐量提升了38%。两种模型都优于拥有86.6M参数的基线模型（81.05%），并显著改善了训练稳定性，将峰值到最终准确率的下降从0.47%降低到0.03%至0.06%的范围。这些结果表明，对于使用标准训练方法在ImageNet-1K上的ViT-B/16，模型处于过参数化状态，可以在不损害性能的情况下减少MLP容量，甚至能略微提升性能。更广泛地说，我们的发现表明参数共享和减少宽度等架构约束可以作为有用的归纳偏置，并强调了在设计视觉Transformer时参数分配方式的重要性。所有代码可在以下网址获取：https://github.com/AnanthaPadmanaban-KrishnaKumar/parameter-efficient-vit-mlps。

</details>


### [98] [Accelerating Inference of Masked Image Generators via Reinforcement Learning](https://arxiv.org/abs/2512.01094)
*Pranav Subbaraman,Shufan Li,Siyan Zhao,Aditya Grover*

Main category: cs.CV

TL;DR: 本文提出Speed-RL方法，使用强化学习加速掩码生成模型的推理过程，能在保持图像质量的同时实现3倍加速。


<details>
  <summary>Details</summary>
Motivation: 掩码生成模型（MGM）虽然能生成高质量图像，但需要大量采样步骤，导致推理速度缓慢。传统蒸馏方法将加速问题视为分布匹配问题，但本文认为这可以更好地表述为强化学习问题。

Method: 提出Speed-RL范式，将加速问题重新定义为强化学习问题。结合质量奖励和速度奖励，使用强化学习微调基础模型，优化目标是生成高质量图像的同时减少步骤数。

Result: 实验表明，该方法能够将基础模型加速3倍，同时保持可比的图像质量。

Conclusion: Speed-RL为加速预训练掩码生成模型提供了一种新颖有效的强化学习范式，在保持生成质量的同时显著提升推理速度。

Abstract: Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images. However, they need many sampling steps to create high-quality generations, resulting in slow inference speed. In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps. Unlike conventional distillation methods which formulate the acceleration problem as a distribution matching problem, where a few-step student model is trained to match the distribution generated by a many-step teacher model, we consider this problem as a reinforcement learning problem. Since the goal of acceleration is to generate high quality images in fewer steps, we can combine a quality reward with a speed reward and finetune the base model using reinforcement learning with the combined reward as the optimization target. Through extensive experiments, we show that the proposed method was able to accelerate the base model by a factor of 3x while maintaining comparable image quality.

中文标题: 通过强化学习加速掩码图像生成器的推理

中文摘要: 掩码生成模型（MGM）在生成高保真图像方面表现出强大的能力。然而，它们需要许多采样步骤来创建高质量生成，导致推理速度缓慢。在这项工作中，我们提出了Speed-RL，一种新颖的范式，用于加速预训练的MGM以在更少的步骤中生成高质量图像。与传统的蒸馏方法不同，后者将加速问题表述为分布匹配问题，其中训练一个少步学生模型来匹配多步教师模型生成的分布，我们将此问题视为强化学习问题。由于加速的目标是在更少的步骤中生成高质量图像，我们可以将质量奖励与速度奖励相结合，并使用强化学习微调基础模型，以组合奖励作为优化目标。通过大量实验，我们表明所提出的方法能够将基础模型加速3倍，同时保持可比的图像质量。

</details>


### [99] [CycliST: A Video Language Model Benchmark for Reasoning on Cyclical State Transitions](https://arxiv.org/abs/2512.01095)
*Simon Kohaut,Daniel Ochs,Shun Zhang,Benedict Flade,Julian Eggert,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.CV

TL;DR: CycliST是一个新的视频语言模型基准测试，专注于评估模型在循环状态转换上的文本推理能力。该基准使用合成的结构化视频序列测试模型对周期性模式的时空认知，发现当前最先进的模型在检测和利用循环模式方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前视频语言模型在理解现实世界中的周期性过程（如物体运动、视觉属性变化）方面存在局限性。需要专门的基准来评估模型对循环状态转换的推理能力，以推动视觉推理模型的发展。

Method: 创建CycliST基准数据集，包含合成的结构化视频序列，展示物体运动和视觉属性的周期性模式。采用分层评估系统，通过增加循环物体数量、场景杂乱度和光照条件来逐步提高难度。对开源和专有的最先进视频语言模型进行广泛实验。

Result: 实验显示当前视频语言模型在泛化到循环动态（如线性和轨道运动）以及时间依赖的视觉属性变化（如颜色和尺度）方面存在局限性。模型无法可靠检测和利用循环模式，缺乏时间理解概念，不能从场景中提取定量洞察。没有单一模型在所有任务上表现一致优秀。

Conclusion: CycliST基准揭示了当前视频语言模型在理解周期性模式方面的显著技术差距。该基准为开发超越当前技术水平、能更好理解周期模式的视觉推理模型提供了有针对性的挑战和全面的评估框架。

Abstract: We present CycliST, a novel benchmark dataset designed to evaluate Video Language Models (VLM) on their ability for textual reasoning over cyclical state transitions. CycliST captures fundamental aspects of real-world processes by generating synthetic, richly structured video sequences featuring periodic patterns in object motion and visual attributes. CycliST employs a tiered evaluation system that progressively increases difficulty through variations in the number of cyclic objects, scene clutter, and lighting conditions, challenging state-of-the-art models on their spatio-temporal cognition. We conduct extensive experiments with current state-of-the-art VLMs, both open-source and proprietary, and reveal their limitations in generalizing to cyclical dynamics such as linear and orbital motion, as well as time-dependent changes in visual attributes like color and scale. Our results demonstrate that present-day VLMs struggle to reliably detect and exploit cyclic patterns, lack a notion of temporal understanding, and are unable to extract quantitative insights from scenes, such as the number of objects in motion, highlighting a significant technical gap that needs to be addressed. More specifically, we find no single model consistently leads in performance: neither size nor architecture correlates strongly with outcomes, and no model succeeds equally well across all tasks. By providing a targeted challenge and a comprehensive evaluation framework, CycliST paves the way for visual reasoning models that surpass the state-of-the-art in understanding periodic patterns.

中文标题: CycliST：用于循环状态转换推理的视频语言模型基准测试

中文摘要: 我们提出了CycliST，这是一个新颖的基准数据集，旨在评估视频语言模型在循环状态转换上的文本推理能力。CycliST通过生成合成的、结构丰富的视频序列来捕捉现实世界过程的基本方面，这些序列展示了物体运动和视觉属性的周期性模式。CycliST采用分层评估系统，通过变化循环物体数量、场景杂乱度和光照条件来逐步增加难度，挑战最先进模型的时空认知能力。我们对当前最先进的开源和专有视频语言模型进行了广泛实验，揭示了它们在泛化到循环动态（如线性和轨道运动）以及时间依赖的视觉属性变化（如颜色和尺度）方面的局限性。我们的结果表明，当前的视频语言模型难以可靠地检测和利用循环模式，缺乏时间理解概念，并且无法从场景中提取定量洞察（如运动物体数量），这突显了需要解决的重要技术差距。具体而言，我们发现没有单一模型在性能上始终领先：模型大小或架构与结果没有强相关性，也没有模型在所有任务上同样成功。通过提供有针对性的挑战和全面的评估框架，CycliST为超越当前技术水平、能更好理解周期性模式的视觉推理模型铺平了道路。

</details>


### [100] [Structural Prognostic Event Modeling for Multimodal Cancer Survival Analysis](https://arxiv.org/abs/2512.01116)
*Yilan Zhang,Li Nanbo,Changchun Yang,Jürgen Schmidhuber,Xin Gao*

Main category: cs.CV

TL;DR: SlotSPE：一种基于槽位的结构预后事件建模框架，通过槽注意力将多模态癌症数据压缩为紧凑、互异的槽表示，有效捕获稀疏但关键的预后事件，提升生存预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态癌症生存分析方法难以高效建模模态内和模态间交互，主要挑战在于从高维复杂输入中捕获少量但关键的预后事件，这些事件表现为高层次结构信号（如空间组织学模式或通路共激活），具有稀疏性、患者特异性和无标注特性。

Method: 提出SlotSPE框架，受因子编码原理启发，使用槽注意力将每位患者的多模态输入压缩为紧凑、模态特定的互异槽集合。这些槽表示作为预后事件的编码，支持复杂模态内和模态间交互的高效建模，并能无缝整合增强预后相关性的生物学先验知识。

Result: 在十个癌症基准测试中，SlotSPE在8/10队列中优于现有方法，整体性能提升2.9%。在基因组数据缺失情况下保持稳健，并通过结构化事件分解显著提升可解释性。

Conclusion: SlotSPE框架通过结构预后事件建模有效解决了多模态癌症生存分析中的关键挑战，在预测性能、稳健性和可解释性方面均表现出色，为癌症预后提供了新的分析范式。

Abstract: The integration of histology images and gene profiles has shown great promise for improving survival prediction in cancer. However, current approaches often struggle to model intra- and inter-modal interactions efficiently and effectively due to the high dimensionality and complexity of the inputs. A major challenge is capturing critical prognostic events that, though few, underlie the complexity of the observed inputs and largely determine patient outcomes. These events, manifested as high-level structural signals such as spatial histologic patterns or pathway co-activations, are typically sparse, patient-specific, and unannotated, making them inherently difficult to uncover. To address this, we propose SlotSPE, a slot-based framework for structural prognostic event modeling. Specifically, inspired by the principle of factorial coding, we compress each patient's multimodal inputs into compact, modality-specific sets of mutually distinctive slots using slot attention. By leveraging these slot representations as encodings for prognostic events, our framework enables both efficient and effective modeling of complex intra- and inter-modal interactions, while also facilitating seamless incorporation of biological priors that enhance prognostic relevance. Extensive experiments on ten cancer benchmarks show that SlotSPE outperforms existing methods in 8 out of 10 cohorts, achieving an overall improvement of 2.9%. It remains robust under missing genomic data and delivers markedly improved interpretability through structured event decomposition.

中文标题: 多模态癌症生存分析的结构预后事件建模

中文摘要: 组织学图像和基因谱的整合在改善癌症生存预测方面显示出巨大潜力。然而，由于输入的高维性和复杂性，当前方法往往难以高效有效地建模模态内和模态间交互。一个主要挑战是捕获关键的预后事件，这些事件虽然数量少，但构成了观察输入复杂性的基础，并在很大程度上决定了患者结局。这些事件表现为高层次结构信号（如空间组织学模式或通路共激活），通常具有稀疏性、患者特异性和无标注特性，使其难以被发现。为此，我们提出了SlotSPE，一种基于槽位的结构预后事件建模框架。具体来说，受因子编码原理启发，我们使用槽注意力将每位患者的多模态输入压缩为紧凑、模态特定的互异槽集合。通过将这些槽表示作为预后事件的编码，我们的框架能够高效有效地建模复杂的模态内和模态间交互，同时促进无缝整合增强预后相关性的生物学先验知识。在十个癌症基准测试上的广泛实验表明，SlotSPE在8/10队列中优于现有方法，实现了2.9%的整体改进。它在基因组数据缺失情况下保持稳健，并通过结构化事件分解显著提升了可解释性。

</details>


### [101] [SocialFusion: Addressing Social Degradation in Pre-trained Vision-Language Models](https://arxiv.org/abs/2512.01148)
*Hamza Tahboub,Weiyan Shi,Gang Hua,Huaizu Jiang*

Main category: cs.CV

TL;DR: SocialFusion是一个解决预训练视觉语言模型中"社交退化"问题的框架，通过冻结视觉编码器与语言模型的最小连接，在五个社交任务上实现正向迁移，性能媲美任务专用模型。


<details>
  <summary>Details</summary>
Motivation: 当前强大的预训练视觉语言模型在统一学习多个社交感知任务时表现不佳，存在负迁移问题。研究发现这是由于"社交退化"现象导致的——通用视觉语言预训练过程损害了视觉编码器表示细微社交信息的能力。

Method: 提出SocialFusion框架，通过线性表示探测和梯度冲突分析两个视角研究社交退化问题。框架学习冻结视觉编码器与语言模型之间的最小连接，避免预训练过程中的社交信息损失。

Result: SocialFusion在所有五个社交任务上都表现出正向迁移，利用任务间的协同效应提升整体性能，在各种基准测试中达到与任务专用最先进模型相当的性能。

Conclusion: 当前VLM预训练策略可能对获取通用社交能力有害，需要更多社交感知的训练范式。SocialFusion为解决这一问题提供了有效方案。

Abstract: Understanding social interactions from visual cues is a fundamental challenge for a socially competent AI. While powerful pre-trained vision-language models (VLMs) have shown remarkable general capabilities, they surprisingly struggle to unify and learn multiple social perception tasks simultaneously, often exhibiting negative transfer. We identify that this negative transfer stems from a critical issue we term "social degradation," whereby the general visual-linguistic pre-training process of VLMs impairs the visual encoder's ability to represent nuanced social information. We investigate this behavior further under two lenses: decodability through linear representation probing and compatibility through gradient conflict analysis, revealing that both play a role in the degradation, especially the former, which is significantly compromised in the VLM pre-training process. To address these issues, we propose SocialFusion, a unified framework that learns a minimal connection between a frozen visual encoder and a language model. Compared with existing VLMs, it exhibits positive transfer across all five social tasks, leveraging synergies between them to enhance overall performance and achieves comparable performance to task-specific state-of-the-art models on various benchmarks. Our findings suggest that current VLM pre-training strategies may be detrimental to acquiring general social competence and highlight the need for more socially-aware training paradigms.

中文标题: SocialFusion：解决预训练视觉语言模型中的社交退化问题

中文摘要: 从视觉线索理解社交互动是社交能力AI的基本挑战。虽然强大的预训练视觉语言模型显示出卓越的通用能力，但它们出人意料地难以同时统一和学习多个社交感知任务，经常表现出负迁移。我们发现这种负迁移源于一个关键问题，我们称之为"社交退化"，即VLM的通用视觉语言预训练过程损害了视觉编码器表示细微社交信息的能力。我们通过线性表示探测的可解码性和梯度冲突分析的兼容性两个视角进一步研究这一行为，揭示两者都在退化中起作用，尤其是前者，在VLM预训练过程中显著受损。为解决这些问题，我们提出SocialFusion，一个学习冻结视觉编码器与语言模型之间最小连接的统一框架。与现有VLM相比，它在所有五个社交任务上都表现出正向迁移，利用任务间的协同效应提升整体性能，并在各种基准测试中达到与任务专用最先进模型相当的性能。我们的发现表明，当前VLM预训练策略可能对获取通用社交能力有害，并强调需要更多社交感知的训练范式。

</details>


### [102] [DPAC: Distribution-Preserving Adversarial Control for Diffusion Sampling](https://arxiv.org/abs/2512.01153)
*Han-Jin Lee,Han-Ju Lee,Jin-Seong Kim,Seok-Hwan Choi*

Main category: cs.CV

TL;DR: DPAC是一种新的扩散模型对抗控制方法，通过将对抗梯度投影到分数函数的切空间来保持样本分布，在实现目标分类的同时最小化分布漂移，显著提升样本质量。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性引导的扩散采样方法虽然能成功实现目标分类，但会导致样本质量下降，因为对抗控制轨迹与原始生成轨迹之间的偏差会累积。需要一种既能实现对抗目标又能保持原始分布特性的方法。

Method: 1. 将扩散采样过程形式化为随机最优控制问题，使用路径KL散度衡量分布漂移
2. 推导出最优控制条件：对抗梯度应投影到分数函数的切空间
3. 提出DPAC算法，在离散求解器中实现二阶精度的分布保持

Result: 1. 理论证明：切向投影能抵消Wasserstein距离的O(Δt)误差项，实现O(Δt^2)质量差距
2. 实验验证：在ImageNet-100上，DPAC在相同攻击成功率下获得更低的FID和路径KL
3. 方法对分数估计误差具有二阶鲁棒性

Conclusion: DPAC通过将对抗控制限制在生成分布的切空间，在保持对抗有效性的同时最小化分布漂移，为高质量对抗性扩散采样提供了理论保证和实用方法。

Abstract: Adversarially guided diffusion sampling often achieves the target class, but sample quality degrades as deviations between the adversarially controlled and nominal trajectories accumulate. We formalize this degradation as a path-space Kullback-Leibler divergence(path-KL) between controlled and nominal (uncontrolled) diffusion processes, thereby showing via Girsanov's theorem that it exactly equals the control energy. Building on this stochastic optimal control (SOC) view, we theoretically establish that minimizing this path-KL simultaneously tightens upper bounds on both the 2-Wasserstein distance and Fréchet Inception Distance (FID), revealing a principled connection between adversarial control energy and perceptual fidelity. From a variational perspective, we derive a first-order optimality condition for the control: among all directions that yield the same classification gain, the component tangent to iso-(log-)density surfaces (i.e., orthogonal to the score) minimizes path-KL, whereas the normal component directly increases distributional drift. This leads to DPAC (Distribution-Preserving Adversarial Control), a diffusion guidance rule that projects adversarial gradients onto the tangent space defined by the generative score geometry. We further show that in discrete solvers, the tangent projection cancels the O(Δt) leading error term in the Wasserstein distance, achieving an O(Δt^2) quality gap; moreover, it remains second-order robust to score or metric approximation. Empirical studies on ImageNet-100 validate the theoretical predictions, confirming that DPAC achieves lower FID and estimated path-KL at matched attack success rates.

中文标题: DPAC：用于扩散采样的分布保持对抗控制

中文摘要: 对抗性引导的扩散采样通常能够达到目标类别，但随着对抗控制轨迹与名义轨迹之间偏差的累积，样本质量会下降。我们将这种退化形式化为受控扩散过程与名义（未受控）扩散过程之间的路径空间Kullback-Leibler散度（路径-KL），从而通过Girsanov定理证明它恰好等于控制能量。基于这种随机最优控制（SOC）视角，我们从理论上证明最小化该路径-KL同时收紧2-Wasserstein距离和Fréchet Inception距离（FID）的上界，揭示了对抗控制能量与感知保真度之间的原则性联系。从变分角度来看，我们推导出控制的一阶最优性条件：在所有产生相同分类增益的方向中，与等（对数）密度曲面相切的分量（即与分数正交）最小化路径-KL，而法向分量直接增加分布漂移。这导致了DPAC（分布保持对抗控制），一种将对抗梯度投影到由生成分数几何定义的切空间的扩散引导规则。我们进一步证明，在离散求解器中，切向投影抵消了Wasserstein距离中的O(Δt)主导误差项，实现了O(Δt^2)的质量差距；此外，它对分数或度量近似保持二阶鲁棒性。在ImageNet-100上的实证研究验证了理论预测，确认DPAC在匹配的攻击成功率下实现了更低的FID和估计路径-KL。

</details>


### [103] [VSRD++: Autolabeling for 3D Object Detection via Instance-Aware Volumetric Silhouette Rendering](https://arxiv.org/abs/2512.01178)
*Zihua Liu,Hiroki Sakuma,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: VSRD++是一种弱监督单目3D目标检测框架，通过实例感知体积轮廓渲染和多视角自动标注生成3D伪标签，无需3D标注，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D目标检测方法严重依赖大量3D标注，这些标注获取成本高且劳动密集。作者希望开发一种弱监督方法，仅使用2D标注就能实现准确的3D检测，降低标注成本。

Method: 采用两阶段框架：1）多视角3D自动标注阶段，使用实例感知体积轮廓渲染将物体表面表示为SDFs，通过SDF分解为立方体SDF和RDF来优化3D边界框，引入速度属性和置信度处理动态物体；2）单目3D检测器训练阶段，使用生成的伪标签训练检测器。

Result: 在KITTI-360数据集上的实验表明，VSRD++在静态和动态场景中都显著优于现有的弱监督单目3D目标检测方法，证明了其有效性。

Conclusion: VSRD++成功实现了无需3D标注的弱监督单目3D目标检测，通过创新的实例感知体积渲染和动态物体建模方法，为3D场景理解提供了更高效的解决方案。

Abstract: Monocular 3D object detection is a fundamental yet challenging task in 3D scene understanding. Existing approaches heavily depend on supervised learning with extensive 3D annotations, which are often acquired from LiDAR point clouds through labor-intensive labeling processes. To tackle this problem, we propose VSRD++, a novel weakly supervised framework for monocular 3D object detection that eliminates the reliance on 3D annotations and leverages neural-field-based volumetric rendering with weak 2D supervision. VSRD++ consists of a two-stage pipeline: multi-view 3D autolabeling and subsequent monocular 3D detector training. In the multi-view autolabeling stage, object surfaces are represented as signed distance fields (SDFs) and rendered as instance masks via the proposed instance-aware volumetric silhouette rendering. To optimize 3D bounding boxes, we decompose each instance's SDF into a cuboid SDF and a residual distance field (RDF) that captures deviations from the cuboid. To address the geometry inconsistency commonly observed in volume rendering methods applied to dynamic objects, we model the dynamic objects by including velocity into bounding box attributes as well as assigning confidence to each pseudo-label. Moreover, we also employ a 3D attribute initialization module to initialize the dynamic bounding box parameters. In the monocular 3D object detection phase, the optimized 3D bounding boxes serve as pseudo labels for training monocular 3D object detectors. Extensive experiments on the KITTI-360 dataset demonstrate that VSRD++ significantly outperforms existing weakly supervised approaches for monocular 3D object detection on both static and dynamic scenes. Code is available at https://github.com/Magicboomliu/VSRD_plus_plus

中文标题: VSRD++：基于实例感知体积轮廓渲染的3D目标检测自动标注方法

中文摘要: 单目3D目标检测是3D场景理解中的基础但具有挑战性的任务。现有方法严重依赖于具有大量3D标注的监督学习，这些标注通常通过劳动密集型的标注过程从LiDAR点云获取。为了解决这个问题，我们提出了VSRD++，一种新颖的弱监督单目3D目标检测框架，消除了对3D标注的依赖，并利用基于神经场的体积渲染与弱2D监督。VSRD++包含一个两阶段流程：多视角3D自动标注和随后的单目3D检测器训练。在多视角自动标注阶段，物体表面被表示为有符号距离场（SDFs），并通过提出的实例感知体积轮廓渲染渲染为实例掩码。为了优化3D边界框，我们将每个实例的SDF分解为立方体SDF和捕获与立方体偏差的残差距离场（RDF）。为了解决在应用于动态物体的体积渲染方法中常见的几何不一致性问题，我们通过将速度纳入边界框属性以及为每个伪标签分配置信度来建模动态物体。此外，我们还采用3D属性初始化模块来初始化动态边界框参数。在单目3D目标检测阶段，优化的3D边界框作为伪标签用于训练单目3D目标检测器。在KITTI-360数据集上的大量实验表明，VSRD++在静态和动态场景中都显著优于现有的弱监督单目3D目标检测方法。代码可在https://github.com/Magicboomliu/VSRD_plus_plus获取。

</details>


### [104] [TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image](https://arxiv.org/abs/2512.01204)
*Ziqian Wang,Yonghao He,Licheng Yang,Wei Zou,Hongxuan Ma,Liu Liu,Wei Sui,Yuxin Guo,Hu Su*

Main category: cs.CV

TL;DR: TabletopGen是一个无需训练的自动框架，能从文本或单张图像生成实例级交互式3D桌面场景，通过实例分割、3D重建和空间对齐技术解决现有方法在桌面场景高密度布局和复杂空间关系方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本或图像的3D场景生成方法主要关注大规模场景，难以捕捉桌面场景特有的高密度布局和复杂空间关系。桌面场景生成对于具身AI（特别是机器人操作策略学习和数据合成）至关重要，需要高保真、物理交互的3D模拟场景。

Method: TabletopGen采用训练免费的全自动框架：1）输入参考图像（可由文本到图像模型生成）；2）实例分割和补全获取每个实例图像；3）将每个实例重建为3D模型并进行规范坐标对齐；4）通过创新的姿态和尺度对齐方法（包括可微分旋转优化器和俯视图空间对齐机制）进行姿态和尺度估计；5）组装成无碰撞、模拟就绪的桌面场景。

Result: 大量实验和用户研究表明，TabletopGen在视觉保真度、布局准确性和物理合理性方面达到最先进性能，显著超越现有方法，能够生成具有丰富风格和空间多样性的逼真桌面场景。

Conclusion: TabletopGen成功解决了桌面场景生成的挑战，通过创新的两阶段空间推理方法实现了从2D参考到准确3D重建的转换，为具身AI应用提供了高质量的交互式3D桌面场景生成能力。

Abstract: Generating high-fidelity, physically interactive 3D simulated tabletop scenes is essential for embodied AI--especially for robotic manipulation policy learning and data synthesis. However, current text- or image-driven 3D scene generation methods mainly focus on large-scale scenes, struggling to capture the high-density layouts and complex spatial relations that characterize tabletop scenes. To address these challenges, we propose TabletopGen, a training-free, fully automatic framework that generates diverse, instance-level interactive 3D tabletop scenes. TabletopGen accepts a reference image as input, which can be synthesized by a text-to-image model to enhance scene diversity. We then perform instance segmentation and completion on the reference to obtain per-instance images. Each instance is reconstructed into a 3D model followed by canonical coordinate alignment. The aligned 3D models then undergo pose and scale estimation before being assembled into a collision-free, simulation-ready tabletop scene. A key component of our framework is a novel pose and scale alignment approach that decouples the complex spatial reasoning into two stages: a Differentiable Rotation Optimizer for precise rotation recovery and a Top-view Spatial Alignment mechanism for robust translation and scale estimation, enabling accurate 3D reconstruction from 2D reference. Extensive experiments and user studies show that TabletopGen achieves state-of-the-art performance, markedly surpassing existing methods in visual fidelity, layout accuracy, and physical plausibility, capable of generating realistic tabletop scenes with rich stylistic and spatial diversity. Our code will be publicly available.

中文标题: TabletopGen：从文本或单张图像生成实例级交互式3D桌面场景

中文摘要: 生成高保真、物理交互的3D模拟桌面场景对于具身AI至关重要——特别是对于机器人操作策略学习和数据合成。然而，当前基于文本或图像的3D场景生成方法主要关注大规模场景，难以捕捉桌面场景特有的高密度布局和复杂空间关系。为解决这些挑战，我们提出了TabletopGen，一个无需训练、全自动的框架，能够生成多样化、实例级的交互式3D桌面场景。TabletopGen接受参考图像作为输入，该图像可由文本到图像模型合成以增强场景多样性。然后我们对参考图像进行实例分割和补全以获得每个实例的图像。每个实例被重建为3D模型，随后进行规范坐标对齐。对齐后的3D模型经过姿态和尺度估计，然后被组装成无碰撞、模拟就绪的桌面场景。我们框架的一个关键组件是一种新颖的姿态和尺度对齐方法，将复杂的空间推理解耦为两个阶段：用于精确旋转恢复的可微分旋转优化器和用于鲁棒平移和尺度估计的俯视图空间对齐机制，从而实现从2D参考到准确3D重建的转换。大量实验和用户研究表明，TabletopGen达到了最先进的性能，在视觉保真度、布局准确性和物理合理性方面显著超越现有方法，能够生成具有丰富风格和空间多样性的逼真桌面场景。我们的代码将公开提供。

</details>


### [105] [Closing the Approximation Gap of Partial AUC Optimization: A Tale of Two Formulations](https://arxiv.org/abs/2512.01213)
*Yangbangyan Jiang,Qianqian Xu,Huiyang Shao,Zhiyong Yang,Shilong Bao,Xiaochun Cao,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了两种实例级极小极大重构方法，用于缩小部分AUC优化的近似差距，具有线性计算复杂度和理论保证。


<details>
  <summary>Details</summary>
Motivation: 部分AUC是现实场景中重要的评估指标，但现有优化方法存在不可控的近似误差和可扩展性限制，需要缩小近似差距。

Method: 提出两种实例级极小极大重构：1）渐近消失差距的公式；2）无偏但变量更多的公式。通过建立等价实例级问题、阈值学习和平滑技术实现。

Result: 算法具有线性计算复杂度、$O(ε^{-1/3})$收敛速度，提供了紧致泛化界$\tilde{O}(α^{-1}\n_+^{-1} + β^{-1}\n_-^{-1})$，实验验证了方法优势。

Conclusion: 成功缩小了PAUC优化的近似差距，提出的两种公式在理论和实验上均表现出色，为实际应用提供了有效解决方案。

Abstract: As a variant of the Area Under the ROC Curve (AUC), the partial AUC (PAUC) focuses on a specific range of false positive rate (FPR) and/or true positive rate (TPR) in the ROC curve. It is a pivotal evaluation metric in real-world scenarios with both class imbalance and decision constraints. However, selecting instances within these constrained intervals during its calculation is NP-hard, and thus typically requires approximation techniques for practical resolution. Despite the progress made in PAUC optimization over the last few years, most existing methods still suffer from uncontrollable approximation errors or a limited scalability when optimizing the approximate PAUC objectives. In this paper, we close the approximation gap of PAUC optimization by presenting two simple instance-wise minimax reformulations: one with an asymptotically vanishing gap, the other with the unbiasedness at the cost of more variables. Our key idea is to first establish an equivalent instance-wise problem to lower the time complexity, simplify the complicated sample selection procedure by threshold learning, and then apply different smoothing techniques. Equipped with an efficient solver, the resulting algorithms enjoy a linear per-iteration computational complexity w.r.t. the sample size and a convergence rate of $O(ε^{-1/3})$ for typical one-way and two-way PAUCs. Moreover, we provide a tight generalization bound of our minimax reformulations. The result explicitly demonstrates the impact of the TPR/FPR constraints $α$/$β$ on the generalization and exhibits a sharp order of $\tilde{O}(α^{-1}\n_+^{-1} + β^{-1}\n_-^{-1})$. Finally, extensive experiments on several benchmark datasets validate the strength of our proposed methods.

中文标题: 缩小部分AUC优化的近似差距：两种公式的故事

中文摘要: 作为ROC曲线下面积（AUC）的一种变体，部分AUC（PAUC）关注ROC曲线中假正率（FPR）和/或真正率（TPR）的特定范围。在具有类别不平衡和决策约束的现实场景中，这是一个关键评估指标。然而，在其计算过程中在这些约束区间内选择实例是NP难的，因此通常需要近似技术来解决实际问题。尽管过去几年在PAUC优化方面取得了进展，但大多数现有方法在优化近似PAUC目标时仍然存在不可控的近似误差或有限的可扩展性。在本文中，我们通过提出两种简单的实例级极小极大重构来缩小PAUC优化的近似差距：一种具有渐近消失的差距，另一种以更多变量为代价实现无偏性。我们的核心思想是首先建立一个等价的实例级问题以降低时间复杂度，通过阈值学习简化复杂的样本选择过程，然后应用不同的平滑技术。配备高效求解器后，所得算法对于典型单向和双向PAUC具有与样本大小成线性的每次迭代计算复杂度以及$O(ε^{-1/3})$的收敛速度。此外，我们提供了极小极大重构的紧致泛化界。结果明确展示了TPR/FPR约束$α$/$β$对泛化的影响，并呈现出$\tilde{O}(α^{-1}\n_+^{-1} + β^{-1}\n_-^{-1})$的尖锐阶数。最后，在多个基准数据集上的广泛实验验证了我们提出方法的优势。

</details>


### [106] [M4-BLIP: Advancing Multi-Modal Media Manipulation Detection through Face-Enhanced Local Analysis](https://arxiv.org/abs/2512.01214)
*Hang Wu,Ke Sun,Jiayi Ji,Xiaoshuai Sun,Rongrong Ji*

Main category: cs.CV

TL;DR: M4-BLIP是一个多模态媒体篡改检测框架，通过BLIP-2模型提取局部特征，并结合面部区域先验知识，通过专门的对齐融合模块整合局部和全局特征，显著提升检测准确性，同时结合大语言模型增强结果可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态媒体篡改检测方法往往忽视局部信息的重要性，而篡改通常发生在特定区域（特别是面部区域），这限制了检测的准确性和可靠性。为了解决这一问题，需要开发能够有效利用局部特征（尤其是面部信息）的检测框架。

Method: 提出M4-BLIP框架：1) 使用BLIP-2模型作为特征提取基础，利用其优秀的局部特征提取能力；2) 引入面部局部信息作为先验知识；3) 设计专门的对齐和融合模块，精心整合局部和全局特征；4) 与大语言模型集成，增强检测结果的可解释性。

Result: 通过大量定量和可视化实验验证，M4-BLIP框架在检测准确性方面显著优于当前最先进的竞争对手，证明了局部特征（特别是面部信息）在多模态媒体篡改检测中的重要性。

Conclusion: M4-BLIP通过有效整合局部面部信息和全局特征，显著提升了多模态媒体篡改检测的性能，同时通过大语言模型集成增强了结果的可解释性，为解决数字媒体可信度问题提供了有效方案。

Abstract: In the contemporary digital landscape, multi-modal media manipulation has emerged as a significant societal threat, impacting the reliability and integrity of information dissemination. Current detection methodologies in this domain often overlook the crucial aspect of localized information, despite the fact that manipulations frequently occur in specific areas, particularly in facial regions. In response to this critical observation, we propose the M4-BLIP framework. This innovative framework utilizes the BLIP-2 model, renowned for its ability to extract local features, as the cornerstone for feature extraction. Complementing this, we incorporate local facial information as prior knowledge. A specially designed alignment and fusion module within M4-BLIP meticulously integrates these local and global features, creating a harmonious blend that enhances detection accuracy. Furthermore, our approach seamlessly integrates with Large Language Models (LLM), significantly improving the interpretability of the detection outcomes. Extensive quantitative and visualization experiments validate the effectiveness of our framework against the state-of-the-art competitors.

中文标题: M4-BLIP：通过面部增强的局部分析推进多模态媒体篡改检测

中文摘要: 在当代数字环境中，多模态媒体篡改已成为重大的社会威胁，影响了信息传播的可靠性和完整性。该领域当前的检测方法往往忽视了局部信息这一关键方面，尽管篡改经常发生在特定区域，尤其是面部区域。针对这一关键观察，我们提出了M4-BLIP框架。这一创新框架利用BLIP-2模型（以其提取局部特征的能力而闻名）作为特征提取的基础。此外，我们引入局部面部信息作为先验知识。M4-BLIP中专门设计的对齐和融合模块精心整合了这些局部和全局特征，创建了一个和谐的融合，提高了检测准确性。此外，我们的方法与大语言模型无缝集成，显著提高了检测结果的可解释性。大量的定量和可视化实验验证了我们的框架相对于最先进竞争对手的有效性。

</details>


### [107] [S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance](https://arxiv.org/abs/2512.01223)
*Beining Xu,Siting Zhu,Zhao Jin,Junxian Li,Hesheng Wang*

Main category: cs.CV

TL;DR: S$^2$-MLLM通过隐式空间推理增强多模态大语言模型在3D视觉定位中的空间理解能力，避免低效的点云重建，实现高效准确的3D物体定位。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs主要处理2D视觉输入，难以理解3D场景的空间结构。现有方法依赖点云重建和视角依赖渲染，效率低下且空间推理能力有限。需要一种更高效的方法来增强MLLMs在3D视觉定位中的空间推理能力。

Method: 提出S$^2$-MLLM框架：1）采用空间指导策略，利用前馈3D重建的结构感知能力进行隐式空间推理；2）设计结构增强模块（SE），包含视图内和视图间注意力机制来捕获空间依赖关系；3）整合多级位置编码，关联视觉表示与空间位置信息。

Result: 在ScanRefer、Nr3D和Sr3D三个基准数据集上进行了广泛实验，S$^2$-MLLM在性能、泛化能力和效率方面均优于现有方法，实现了显著的性能提升。

Conclusion: S$^2$-MLLM通过隐式空间推理有效提升了MLLMs在3D视觉定位中的空间理解能力，避免了低效的点云重建过程，在多个数据集上实现了优越的性能表现，为3D视觉定位任务提供了高效解决方案。

Abstract: 3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.

中文标题: S$^2$-MLLM：通过结构指导提升多模态大语言模型在3D视觉定位中的空间推理能力

中文摘要: 3D视觉定位（3DVG）专注于基于自然语言描述在3D场景中定位物体，是具身AI和机器人的基础任务。多模态大语言模型（MLLMs）的最新进展推动了将其扩展到3DVG的研究。然而，MLLMs主要处理2D视觉输入，仅从这些有限视角难以理解场景的3D空间结构。现有方法主要利用重建点云的可视点依赖渲染为MLLMs在3DVG任务中提供明确的结构指导，导致效率低下和空间推理能力有限。为解决这一问题，我们提出了S$^2$-MLLM，一个通过隐式空间推理增强MLLMs空间推理能力的高效框架。我们引入了一种空间指导策略，利用前馈3D重建的结构感知能力。通过在训练期间获取3D结构理解，我们的模型可以在不依赖低效点云重建的情况下隐式推理3D场景。此外，我们提出了一个结构增强模块（SE），该模块首先采用视图内和视图间注意力机制来捕获视图内的依赖关系和视图间的对应关系。该模块进一步整合多级位置编码，将视觉表示与空间位置和视点信息关联，实现更准确的结构理解。大量实验表明，S$^2$-MLLM统一了卓越的性能、泛化能力和效率，在ScanRefer、Nr3D和Sr3D数据集上相比现有方法实现了显著性能提升。代码将在接受后提供。

</details>


### [108] [PSR: Scaling Multi-Subject Personalized Image Generation with Pairwise Subject-Consistency Rewards](https://arxiv.org/abs/2512.01236)
*Shulei Wang,Longhui Wei,Xin He,Jianbo Ouyang,Hui Lu,Zhou Zhao,Qi Tian*

Main category: cs.CV

TL;DR: PSR提出了一种可扩展的多主体个性化图像生成方法，通过构建高质量多主体数据集和设计成对主体一致性奖励，解决了现有模型在多主体场景下主体一致性和文本控制性下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有单主体个性化生成模型效果显著，但扩展到多主体时性能下降，特别是在保持主体一致性和遵循文本提示方面。这主要是由于缺乏高质量多主体数据集和精细的后训练策略。

Method: 1) 提出可扩展的多主体数据生成管道，利用强大的单主体生成模型构建多样化的高质量多主体训练数据；2) 设计成对主体一致性奖励和通用奖励，通过强化学习阶段提升主体一致性和文本可控性；3) 引入包含7个子集的新基准，从三个维度全面评估多主体个性化性能。

Result: 大量实验证明该方法在多主体个性化图像生成方面的有效性，显著提升了主体一致性和文本控制能力。

Conclusion: PSR通过可扩展的数据生成管道和精心设计的奖励机制，成功解决了多主体个性化图像生成的挑战，为这一领域提供了有效的解决方案。

Abstract: Personalized generation models for a single subject have demonstrated remarkable effectiveness, highlighting their significant potential. However, when extended to multiple subjects, existing models often exhibit degraded performance, particularly in maintaining subject consistency and adhering to textual prompts. We attribute these limitations to the absence of high-quality multi-subject datasets and refined post-training strategies. To address these challenges, we propose a scalable multi-subject data generation pipeline that leverages powerful single-subject generation models to construct diverse and high-quality multi-subject training data. Through this dataset, we first enable single-subject personalization models to acquire knowledge of synthesizing multi-image and multi-subject scenarios. Furthermore, to enhance both subject consistency and text controllability, we design a set of Pairwise Subject-Consistency Rewards and general-purpose rewards, which are incorporated into a refined reinforcement learning stage. To comprehensively evaluate multi-subject personalization, we introduce a new benchmark that assesses model performance using seven subsets across three dimensions. Extensive experiments demonstrate the effectiveness of our approach in advancing multi-subject personalized image generation. Github Link: https://github.com/wang-shulei/PSR

中文标题: PSR：通过成对主体一致性奖励扩展多主体个性化图像生成

中文摘要: 单主体个性化生成模型已展现出显著效果，突显了其巨大潜力。然而，当扩展到多主体时，现有模型往往表现出性能下降，特别是在保持主体一致性和遵循文本提示方面。我们将这些限制归因于缺乏高质量的多主体数据集和精细的后训练策略。为应对这些挑战，我们提出了一种可扩展的多主体数据生成管道，利用强大的单主体生成模型构建多样化和高质量的多主体训练数据。通过这个数据集，我们首先使单主体个性化模型获得合成多图像和多主体场景的知识。此外，为了增强主体一致性和文本可控性，我们设计了一套成对主体一致性奖励和通用奖励，并将其纳入精细的强化学习阶段。为了全面评估多主体个性化，我们引入了一个新的基准，使用七个子集在三个维度上评估模型性能。大量实验证明了我们方法在推进多主体个性化图像生成方面的有效性。

</details>


### [109] [TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition](https://arxiv.org/abs/2512.01248)
*Junyuan Zhang,Bin Wang,Qintong Zhang,Fan Wu,Zichen Wen,Jialin Lu,Junjie Shan,Ziqi Zhao,Shuya Yang,Ziling Wang,Ziyang Miao,Huaping Zhong,Yuhang Zang,Xiaoyi Dong,Ka-Ho Chow,Conghui He*

Main category: cs.CV

TL;DR: TRivia是一种自监督微调方法，让视觉语言模型能从无标注表格图像中学习表格识别，无需人工标注，通过问答奖励机制实现闭环学习，提出的TRivia-3B模型在多个基准测试中超越现有系统。


<details>
  <summary>Details</summary>
Motivation: 表格识别传统依赖监督学习，需要大量标注数据，成本高昂。专有模型性能虽好但闭源，开源模型因资源限制和隐私法规要求而性能落后。需要一种无需标注数据的自监督方法来弥合这一差距。

Method: 基于组相对策略优化的自监督微调方法，通过注意力引导模块为表格图像生成多样化问题，利用问答正确性作为奖励信号，形成闭环学习过程，自动识别最有学习价值的无标注样本。

Result: 提出的TRivia-3B模型在三个流行基准测试中超越了现有系统，包括Gemini 2.5 Pro和MinerU2.5等，成为开源、紧凑且最先进的表格识别模型。

Conclusion: TRivia成功实现了无需标注数据的表格识别学习，通过自监督微调方法显著提升了开源模型的性能，为资源受限和隐私敏感场景提供了可行的解决方案。

Abstract: Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: https://github.com/opendatalab/TRivia

中文标题: TRivia：基于自监督微调的视觉语言模型表格识别方法

中文摘要: 表格识别（TR）旨在将表格图像转换为半结构化表示，如HTML或Markdown。作为文档解析的核心组件，TR长期以来依赖于监督学习，最近的研究主要集中在使用标注数据微调视觉语言模型（VLMs）。虽然VLMs将TR提升到了新的水平，但要进一步推动性能需要大规模标注数据，这些数据获取成本高昂。因此，尽管专有模型不断突破性能边界，但开源模型由于资源有限，并且在实践中由于隐私法规通常是唯一可行的选择，仍然远远落后。为了弥合这一差距，我们引入了TRivia，一种自监督微调方法，使预训练的VLMs能够直接从无标注的表格图像中学习TR。基于组相对策略优化构建，TRivia自动识别最能促进学习的无标注样本，并通过基于问答的奖励机制消除了对人工标注的需求。注意力引导模块为每个表格图像生成多样化问题，而解释识别结果并正确回答问题的能力为优化TR模型提供了反馈。这种闭环过程使TR模型能够自主学习识别、结构化和推理表格，无需标注数据。利用这一流程，我们提出了TRivia-3B，一个开源的、紧凑的、最先进的TR模型，在三个流行基准测试中超越了现有系统（例如Gemini 2.5 Pro、MinerU2.5）。模型和代码发布于：https://github.com/opendatalab/TRivia

</details>


### [110] [ViscNet: Vision-Based In-line Viscometry for Fluid Mixing Process](https://arxiv.org/abs/2512.01268)
*Jongwon Sohn,Juhyeon Moon,Hyunjoon Jung,Jaewook Nam*

Main category: cs.CV

TL;DR: ViscNet是一种基于计算机视觉的粘度测量系统，通过分析混合过程中自由表面变形引起的光学畸变来推断流体粘度，无需接触流体，适用于自动化实验室操作。


<details>
  <summary>Details</summary>
Motivation: 传统粘度计具有侵入性且需要在受控实验室环境中使用，与实际工艺条件差异较大。需要一种非侵入式、适用于实际工艺条件的粘度测量方法，以支持过程监控和自动化实验室操作。

Method: 开发基于计算机视觉的粘度计，利用混合驱动的连续变形自由表面引起的光折射效应。当光线通过变形表面时，固定背景图案会发生光学畸变，系统通过分析这些畸变模式来推断粘度。采用多图案策略增强视觉线索，并集成不确定性量化以确保传感器可靠性。

Result: 在不同光照条件下，系统在log m2 s^-1单位上达到0.113的平均绝对误差，粘度等级预测准确率高达81%。对于粘度值接近的类别，性能有所下降，但多图案策略通过提供更丰富的视觉线索提高了鲁棒性。

Conclusion: ViscNet提供了一种实用的、非接触式的粘度测量替代方案，适用于自动化操作，能够提供带有置信度估计的粘度预测，为过程监控和自主实验室操作提供了可行的解决方案。

Abstract: Viscosity measurement is essential for process monitoring and autonomous laboratory operation, yet conventional viscometers remain invasive and require controlled laboratory environments that differ substantially from real process conditions. We present a computer-vision-based viscometer that infers viscosity by exploiting how a fixed background pattern becomes optically distorted as light refracts through the mixing-driven, continuously deforming free surface. Under diverse lighting conditions, the system achieves a mean absolute error of 0.113 in log m2 s^-1 units for regression and reaches up to 81% accuracy in viscosity-class prediction. Although performance declines for classes with closely clustered viscosity values, a multi-pattern strategy improves robustness by providing enriched visual cues. To ensure sensor reliability, we incorporate uncertainty quantification, enabling viscosity predictions with confidence estimates. This stand-off viscometer offers a practical, automation-ready alternative to existing viscometry methods.

中文标题: ViscNet：基于视觉的在线粘度测量用于流体混合过程

中文摘要: 粘度测量对于过程监控和自主实验室操作至关重要，然而传统粘度计仍然具有侵入性，并且需要与实际工艺条件差异较大的受控实验室环境。我们提出了一种基于计算机视觉的粘度计，通过利用固定背景图案在光线通过混合驱动的连续变形自由表面时发生的光学畸变来推断粘度。在不同光照条件下，系统在log m2 s^-1单位上达到0.113的平均绝对误差，粘度等级预测准确率高达81%。尽管对于粘度值接近的类别性能有所下降，但多图案策略通过提供更丰富的视觉线索提高了鲁棒性。为确保传感器可靠性，我们集成了不确定性量化，能够提供带有置信度估计的粘度预测。这种非接触式粘度计为现有粘度测量方法提供了一种实用的、适用于自动化的替代方案。

</details>


### [111] [Supervised Contrastive Machine Unlearning of Background Bias in Sonar Image Classification with Fine-Grained Explainable AI](https://arxiv.org/abs/2512.01291)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: 提出结合目标对比遗忘和可解释AI的框架，消除声纳图像分类中的海底背景偏差，提高模型泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有声纳图像AI模型过度依赖海底特征，导致泛化能力差，需要消除背景偏差并提高模型可解释性。

Method: 1. 目标对比遗忘模块扩展三元组损失减少背景偏差；2. 遗忘解释声纳框架提供遗忘内容可视化并适配LIME解释器。

Result: 在真实和合成声纳数据集上验证，显著提高了遗忘效果、模型鲁棒性和可解释性。

Conclusion: 提出的框架有效消除声纳图像分类中的背景偏差，提高泛化能力和模型可解释性，为声纳图像分析提供新方法。

Abstract: Acoustic sonar image analysis plays a critical role in object detection and classification, with applications in both civilian and defense domains. Despite the availability of real and synthetic datasets, existing AI models that achieve high accuracy often over-rely on seafloor features, leading to poor generalization. To mitigate this issue, we propose a novel framework that integrates two key modules: (i) a Targeted Contrastive Unlearning (TCU) module, which extends the traditional triplet loss to reduce seafloor-induced background bias and improve generalization, and (ii) the Unlearn to Explain Sonar Framework (UESF), which provides visual insights into what the model has deliberately forgotten while adapting the LIME explainer to generate more faithful and localized attributions for unlearning evaluation. Extensive experiments across both real and synthetic sonar datasets validate our approach, demonstrating significant improvements in unlearning effectiveness, model robustness, and interpretability.

中文标题: 基于监督对比机器遗忘与细粒度可解释AI的声纳图像分类背景偏差消除

中文摘要: 声纳图像分析在物体检测和分类中起着关键作用，在民用和国防领域都有重要应用。尽管存在真实和合成数据集，现有实现高精度的AI模型往往过度依赖海底特征，导致泛化能力差。为解决这一问题，我们提出了一个新颖的框架，集成了两个关键模块：(i) 目标对比遗忘模块，扩展了传统的三元组损失以减少海底引起的背景偏差并提高泛化能力；(ii) 遗忘解释声纳框架，提供模型故意遗忘内容的可视化洞察，同时适配LIME解释器为遗忘评估生成更忠实和局部化的归因。在真实和合成声纳数据集上的广泛实验验证了我们的方法，证明了在遗忘效果、模型鲁棒性和可解释性方面的显著改进。

</details>


### [112] [Diffusion Model in Latent Space for Medical Image Segmentation Task](https://arxiv.org/abs/2512.01292)
*Huynh Trinh Ngoc,Toan Nguyen Hai,Ba Luong Son,Long Tran Quoc*

Main category: cs.CV

TL;DR: MedSegLatDiff是一种结合VAE和潜在扩散模型的高效医学图像分割方法，在低维潜在空间操作以减少计算负担，使用加权交叉熵保留微小结构，在多个数据集上取得优异性能并生成多样分割结果。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像分割方法只能生成单一分割掩码，无法捕捉临床实践中固有的不确定性，而现有生成模型虽然能产生多个合理分割，但计算成本过高，限制了临床部署。

Method: 提出MedSegLatDiff框架：1）使用VAE将输入压缩到低维潜在空间，减少噪声和加速训练；2）在潜在空间进行扩散过程；3）在VAE掩码重建路径中用加权交叉熵替代MSE损失，以更好地保留微小结构。

Result: 在ISIC-2018（皮肤病变）、CVC-Clinic（息肉）和LIDC-IDRI（肺结节）三个数据集上评估，实现了最先进或极具竞争力的Dice和IoU分数，同时能够生成多样化的分割假设和置信度图。

Conclusion: MedSegLatDiff在保持高效计算的同时，提供了比确定性基线更好的可解释性和可靠性，通过生成多个合理分割和置信度图，更适合临床部署，为医学图像分割提供了新的解决方案。

Abstract: Medical image segmentation is crucial for clinical diagnosis and treatment planning. Traditional methods typically produce a single segmentation mask, failing to capture inherent uncertainty. Recent generative models enable the creation of multiple plausible masks per image, mimicking the collaborative interpretation of several clinicians. However, these approaches remain computationally heavy. We propose MedSegLatDiff, a diffusion based framework that combines a variational autoencoder (VAE) with a latent diffusion model for efficient medical image segmentation. The VAE compresses the input into a low dimensional latent space, reducing noise and accelerating training, while the diffusion process operates directly in this compact representation. We further replace the conventional MSE loss with weighted cross entropy in the VAE mask reconstruction path to better preserve tiny structures such as small nodules. MedSegLatDiff is evaluated on ISIC-2018 (skin lesions), CVC-Clinic (polyps), and LIDC-IDRI (lung nodules). It achieves state of the art or highly competitive Dice and IoU scores while simultaneously generating diverse segmentation hypotheses and confidence maps. This provides enhanced interpretability and reliability compared to deterministic baselines, making the model particularly suitable for clinical deployment.

中文标题: 基于潜在空间扩散模型的医学图像分割方法

中文摘要: 医学图像分割对于临床诊断和治疗规划至关重要。传统方法通常生成单一分割掩码，无法捕捉固有的不确定性。最近的生成模型能够为每张图像创建多个合理的掩码，模拟多位临床医生的协作解释。然而，这些方法计算量仍然很大。我们提出了MedSegLatDiff，这是一个基于扩散的框架，将变分自编码器（VAE）与潜在扩散模型相结合，用于高效的医学图像分割。VAE将输入压缩到低维潜在空间，减少噪声并加速训练，而扩散过程直接在这个紧凑表示中操作。我们进一步在VAE掩码重建路径中用加权交叉熵替代传统的MSE损失，以更好地保留微小结构（如小结节）。MedSegLatDiff在ISIC-2018（皮肤病变）、CVC-Clinic（息肉）和LIDC-IDRI（肺结节）数据集上进行了评估。它实现了最先进或极具竞争力的Dice和IoU分数，同时生成多样化的分割假设和置信度图。与确定性基线相比，这提供了增强的可解释性和可靠性，使该模型特别适合临床部署。

</details>


### [113] [EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly](https://arxiv.org/abs/2512.01296)
*Xiaokun Pan,Zhenzhe Li,Zhichao Ye,Hongjia Zhai,Guofeng Zhang*

Main category: cs.CV

TL;DR: EGG-Fusion是一个实时三维重建系统，通过几何感知高斯曲面元映射和信息滤波融合方法，在保持24 FPS实时处理的同时，相比现有方法精度提升20%以上。


<details>
  <summary>Details</summary>
Motivation: 当前基于可微分渲染的SLAM系统面临实时计算和传感器噪声敏感性的双重挑战，导致场景重建的几何保真度下降和实用性受限，需要一种既能保持实时性又能提高几何精度的解决方案。

Method: 提出EGG-Fusion系统，包含：1）鲁棒的稀疏到密集相机跟踪；2）几何感知高斯曲面元映射模块；3）基于信息滤波的融合方法，显式考虑传感器噪声；4）可微分高斯曲面元映射实现多视角一致表面建模和高效参数优化。

Result: 在Replica和ScanNet++基准数据集上实现0.6厘米的表面重建误差，相比最先进的高斯溅射方法精度提升超过20%，同时保持24 FPS的实时处理能力。

Conclusion: EGG-Fusion成功解决了可微分渲染方法在实时性和几何精度方面的挑战，成为最准确的可微分渲染实时重建系统之一，在保持实时处理的同时显著提高了表面重建精度。

Abstract: Real-time 3D reconstruction is a fundamental task in computer graphics. Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality. To address these challenges, we propose a novel real-time system EGG-Fusion, featuring robust sparse-to-dense camera tracking and a geometry-aware Gaussian surfel mapping module, introducing an information filter-based fusion method that explicitly accounts for sensor noise to achieve high-precision surface reconstruction. The proposed differentiable Gaussian surfel mapping effectively models multi-view consistent surfaces while enabling efficient parameter optimization. Extensive experimental results demonstrate that the proposed system achieves a surface reconstruction error of 0.6\textit{cm} on standardized benchmark datasets including Replica and ScanNet++, representing over 20\% improvement in accuracy compared to state-of-the-art (SOTA) GS-based methods. Notably, the system maintains real-time processing capabilities at 24 FPS, establishing it as one of the most accurate differentiable-rendering-based real-time reconstruction systems. Project Page: https://zju3dv.github.io/eggfusion/

中文标题: EGG-Fusion：基于几何感知高斯曲面元的实时高效三维重建

中文摘要: 实时三维重建是计算机图形学中的基础任务。最近，基于可微分渲染的SLAM系统显示出巨大潜力，通过可学习的场景表示（如神经辐射场和三维高斯溅射）实现照片级真实感场景渲染。当前的可微分渲染方法面临实时计算和传感器噪声敏感性的双重挑战，导致场景重建的几何保真度下降和实用性受限。为解决这些挑战，我们提出了新颖的实时系统EGG-Fusion，具有鲁棒的稀疏到密集相机跟踪和几何感知高斯曲面元映射模块，引入基于信息滤波的融合方法，显式考虑传感器噪声以实现高精度表面重建。所提出的可微分高斯曲面元映射有效建模多视角一致表面，同时实现高效参数优化。大量实验结果表明，所提系统在包括Replica和ScanNet++在内的标准化基准数据集上实现了0.6厘米的表面重建误差，相比最先进的高斯溅射方法精度提升超过20%。值得注意的是，系统在24 FPS下保持实时处理能力，使其成为最准确的可微分渲染实时重建系统之一。

</details>


### [114] [DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy](https://arxiv.org/abs/2512.01302)
*Jaewoo Song,Jooyoung Choi,Kanghyun Baek,Sangyub Lee,Daemin Park,Sungroh Yoon*

Main category: cs.CV

TL;DR: DCText是一种无需训练的视觉文本生成方法，采用分治策略解决长文本/多文本生成问题，通过文本分割、区域分配、顺序注意力掩码和局部噪声初始化，在保持图像质量的同时显著提升文本准确性和生成效率。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在长文本或多个文本生成时存在全局注意力稀释问题，导致文本渲染不准确。需要一种方法能够可靠处理复杂文本提示，同时保持图像质量和生成效率。

Method: 1. 分治策略：将长文本/多文本提示分解为短文本片段，分配到指定图像区域；2. 顺序注意力掩码：文本聚焦掩码（专注局部文本渲染）和上下文扩展掩码（保持整体连贯性）；3. 局部噪声初始化：提高文本准确性和区域对齐；4. 基于多模态扩散变换器的短文本生成能力。

Result: 在单句和多句基准测试中，DCText实现了最佳的文本准确性，同时不损害图像质量，并且具有最低的生成延迟。相比现有方法，在复杂文本场景下表现显著提升。

Conclusion: DCText通过分治策略和计划注意力掩码机制，有效解决了视觉文本生成中的长文本/多文本问题，在准确性、图像质量和效率方面均取得优异表现，为复杂文本渲染提供了实用解决方案。

Abstract: Despite recent text-to-image models achieving highfidelity text rendering, they still struggle with long or multiple texts due to diluted global attention. We propose DCText, a training-free visual text generation method that adopts a divide-and-conquer strategy, leveraging the reliable short-text generation of Multi-Modal Diffusion Transformers. Our method first decomposes a prompt by extracting and dividing the target text, then assigns each to a designated region. To accurately render each segment within their regions while preserving overall image coherence, we introduce two attention masks - Text-Focus and Context-Expansion - applied sequentially during denoising. Additionally, Localized Noise Initialization further improves text accuracy and region alignment without increasing computational cost. Extensive experiments on single- and multisentence benchmarks show that DCText achieves the best text accuracy without compromising image quality while also delivering the lowest generation latency.

中文标题: DCText：基于分治策略的视觉文本生成中的计划注意力掩码

中文摘要: 尽管最近的文本到图像模型实现了高保真度的文本渲染，但由于全局注意力被稀释，它们仍然难以处理长文本或多个文本。我们提出了DCText，一种无需训练的视觉文本生成方法，采用分治策略，利用多模态扩散变换器可靠的短文本生成能力。我们的方法首先通过提取和分割目标文本来分解提示，然后将每个部分分配到指定区域。为了在保持整体图像连贯性的同时准确渲染每个区域内的文本片段，我们引入了两种注意力掩码——文本聚焦和上下文扩展——在去噪过程中顺序应用。此外，局部噪声初始化进一步提高了文本准确性和区域对齐，而无需增加计算成本。在单句和多句基准测试上的广泛实验表明，DCText在不影响图像质量的情况下实现了最佳的文本准确性，同时还提供了最低的生成延迟。

</details>


### [115] [Gaussian Swaying: Surface-Based Framework for Aerodynamic Simulation with 3D Gaussians](https://arxiv.org/abs/2512.01306)
*Hongru Yan,Xiang Zhang,Zeyuan Chen,Fangyin Wei,Zhuowen Tu*

Main category: cs.CV

TL;DR: 提出Gaussian Swaying框架，使用3D高斯表示表面进行空气动力学模拟，统一模拟与渲染，实现高效精细的交互


<details>
  <summary>Details</summary>
Motivation: 自然场景中如树枝摇曳、旗帜飘动等空气动力学效应对于视觉和图形学的真实感至关重要，现有方法如基于网格的方法需要昂贵的网格化，基于粒子的方法依赖离散位置数据，都需要更高效精细的解决方案

Method: 使用3D高斯连续建模表面，提出高斯补丁统一模拟与渲染，支持动力学力计算同时提供法线用于轻量级着色

Result: 在合成和真实数据集上的综合实验表明，Gaussian Swaying在多个指标上达到最先进的性能和效率

Conclusion: Gaussian Swaying为真实空气动力学场景模拟提供了可扩展的方法，通过统一的3D高斯表示实现了高效精细的模拟与渲染

Abstract: Branches swaying in the breeze, flags rippling in the wind, and boats rocking on the water all show how aerodynamics shape natural motion -- an effect crucial for realism in vision and graphics. In this paper, we present Gaussian Swaying, a surface-based framework for aerodynamic simulation using 3D Gaussians. Unlike mesh-based methods that require costly meshing, or particle-based approaches that rely on discrete positional data, Gaussian Swaying models surfaces continuously with 3D Gaussians, enabling efficient and fine-grained aerodynamic interaction. Our framework unifies simulation and rendering on the same representation: Gaussian patches, which support force computation for dynamics while simultaneously providing normals for lightweight shading. Comprehensive experiments on both synthetic and real-world datasets across multiple metrics demonstrate that Gaussian Swaying achieves state-of-the-art performance and efficiency, offering a scalable approach for realistic aerodynamic scene simulation.

中文标题: 高斯摇摆：基于表面的3D高斯空气动力学模拟框架

中文摘要: 树枝在微风中摇曳、旗帜在风中飘动、船只在水中摇晃，这些都展示了空气动力学如何塑造自然运动——这种效应对视觉和图形学的真实感至关重要。在本文中，我们提出了Gaussian Swaying，一个基于表面的使用3D高斯进行空气动力学模拟的框架。与需要昂贵网格化的基于网格方法，或依赖离散位置数据的基于粒子方法不同，Gaussian Swaying使用3D高斯连续建模表面，实现高效和精细的空气动力学交互。我们的框架在相同表示上统一了模拟和渲染：高斯补丁既支持动力学的力计算，同时提供法线用于轻量级着色。在合成和真实数据集上的综合实验表明，Gaussian Swaying在多个指标上达到了最先进的性能和效率，为真实空气动力学场景模拟提供了可扩展的方法。

</details>


### [116] [Lost in Distortion: Uncovering the Domain Gap Between Computer Vision and Brain Imaging - A Study on Pretraining for Age Prediction](https://arxiv.org/abs/2512.01310)
*Yanteng Zhang,Songheng Li,Zeyu Shen,Qizhen Lan,Lipei Zhang,Yang Liu,Vince Calhoun*

Main category: cs.CV

TL;DR: 该研究探讨了脑成像数据质量对预训练的影响，发现不同质量水平的数据在脑龄预测任务上表现差异显著，揭示了计算机视觉实践与临床神经影像标准之间的差距。


<details>
  <summary>Details</summary>
Motivation: 大规模脑成像数据集为开发领域基础模型提供了机会，但与计算机视觉中的自然图像不同，神经影像数据在质量上存在高度异质性，从结构良好的扫描到严重失真或不完整的脑体积都有。这引发了一个基本问题：噪声或低质量扫描是否能对预训练做出有意义的贡献，还是反而会阻碍模型学习？

Method: 研究系统地探索了数据质量水平在预训练中的作用及其对下游任务的影响。具体而言，在不同质量水平的数据集上进行预训练，并在外部队列上进行脑龄预测的微调。

Result: 结果显示不同质量水平之间存在显著的性能差异，揭示了机会和局限性。高质量数据通常带来更好的下游任务性能，但某些情况下中等质量数据也可能提供有用的学习信号。

Conclusion: 研究强调了计算机视觉实践与临床神经影像标准之间的差距，强调需要领域感知的数据管理来确保可信且可泛化的领域特定基础模型。

Abstract: Large-scale brain imaging datasets provide unprecedented opportunities for developing domain foundation models through pretraining. However, unlike natural image datasets in computer vision, these neuroimaging data often exhibit high heterogeneity in quality, ranging from well-structured scans to severely distorted or incomplete brain volumes. This raises a fundamental question: can noise or low-quality scans contribute meaningfully to pretraining, or do they instead hinder model learning? In this study, we systematically explore the role of data quality level in pretraining and its impact on downstream tasks. Specifically, we perform pretraining on datasets with different quality levels and perform fine-tuning for brain age prediction on external cohorts. Our results show significant performance differences across quality levels, revealing both opportunities and limitations. We further discuss the gap between computer vision practices and clinical neuroimaging standards, emphasizing the necessity of domain-aware curation to ensure trusted and generalizable domain-specific foundation models.

中文标题: 失真中的迷失：揭示计算机视觉与脑成像之间的领域差距——关于年龄预测预训练的研究

中文摘要: 大规模脑成像数据集通过预训练为开发领域基础模型提供了前所未有的机会。然而，与计算机视觉中的自然图像数据集不同，这些神经影像数据通常在质量上表现出高度异质性，从结构良好的扫描到严重失真或不完整的脑体积都有。这引发了一个基本问题：噪声或低质量扫描是否能对预训练做出有意义的贡献，还是反而会阻碍模型学习？在本研究中，我们系统地探索了数据质量水平在预训练中的作用及其对下游任务的影响。具体而言，我们在不同质量水平的数据集上进行预训练，并在外部队列上进行脑龄预测的微调。我们的结果显示不同质量水平之间存在显著的性能差异，揭示了机会和局限性。我们进一步讨论了计算机视觉实践与临床神经影像标准之间的差距，强调需要领域感知的数据管理来确保可信且可泛化的领域特定基础模型。

</details>


### [117] [IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval](https://arxiv.org/abs/2512.01312)
*Ning Han,Yawen Zeng,Shaohua Long,Chengqing Li,Sijie Yang,Dun Tan,Jianfeng Dong,Jingjing Chen*

Main category: cs.CV

TL;DR: IVCR-200K是一个大规模多轮对话基准数据集，用于交互式视频语料库检索任务，支持视频检索和时刻检索，包含20万条高质量双语对话数据。


<details>
  <summary>Details</summary>
Motivation: 传统视频检索系统缺乏与用户的"交互"能力，单向检索模式无法满足至少80.8%用户的个性化和动态需求，需要更真实的交互式检索系统。

Method: 提出交互式视频语料库检索(IVCR)任务，构建IVCR-200K数据集（高质量、双语、多轮对话、抽象语义），并提出基于多模态大语言模型(MLLMs)的框架支持多种交互模式。

Result: 广泛实验证明了数据集和框架的有效性，能够实现多轮、对话式、真实的用户与检索系统交互。

Conclusion: IVCR任务和IVCR-200K数据集为交互式视频检索研究提供了重要基准，基于MLLMs的框架能够提供更可解释的解决方案。

Abstract: In recent years, significant developments have been made in both video retrieval and video moment retrieval tasks, which respectively retrieve complete videos or moments for a given text query. These advancements have greatly improved user satisfaction during the search process. However, previous work has failed to establish meaningful "interaction" between the retrieval system and the user, and its one-way retrieval paradigm can no longer fully meet the personalization and dynamic needs of at least 80.8\% of users. In this paper, we introduce the Interactive Video Corpus Retrieval (IVCR) task, a more realistic setting that enables multi-turn, conversational, and realistic interactions between the user and the retrieval system. To facilitate research on this challenging task, we introduce IVCR-200K, a high-quality, bilingual, multi-turn, conversational, and abstract semantic dataset that supports video retrieval and even moment retrieval. Furthermore, we propose a comprehensive framework based on multi-modal large language models (MLLMs) to help users interact in several modes with more explainable solutions. The extensive experiments demonstrate the effectiveness of our dataset and framework.

中文标题: IVCR-200K：用于交互式视频语料库检索的大规模多轮对话基准

中文摘要: 近年来，视频检索和视频时刻检索任务都取得了显著进展，分别能够为给定文本查询检索完整视频或时刻。这些进展极大地提高了用户在搜索过程中的满意度。然而，先前的工作未能建立检索系统与用户之间有意义的"交互"，其单向检索范式已无法完全满足至少80.8%用户的个性化和动态需求。在本文中，我们引入了交互式视频语料库检索(IVCR)任务，这是一个更真实的设置，使用户与检索系统之间能够进行多轮、对话式和真实的交互。为促进这一具有挑战性任务的研究，我们引入了IVCR-200K，这是一个高质量、双语、多轮、对话式和抽象语义的数据集，支持视频检索甚至时刻检索。此外，我们提出了一个基于多模态大语言模型(MLLMs)的综合框架，帮助用户以多种模式进行交互，并提供更可解释的解决方案。广泛的实验证明了我们数据集和框架的有效性。

</details>


### [118] [TokenPure: Watermark Removal through Tokenized Appearance and Structural Guidance](https://arxiv.org/abs/2512.01314)
*Pei Yang,Yepeng Liu,Kelly Peng,Yuan Gao,Yiren Song*

Main category: cs.CV

TL;DR: TokenPure是一个基于扩散变换器的水印去除框架，通过将水印图像分解为视觉标记和结构标记，实现高效水印去除同时保持内容一致性。


<details>
  <summary>Details</summary>
Motivation: 在数字经济时代，数字水印对于AI生成内容和其他虚拟资产的所有权证明至关重要。设计能够抵御各种攻击和处理操作的鲁棒水印变得尤为重要，因此需要开发有效的水印去除方法。

Method: TokenPure采用基于扩散变换器的框架，将水印去除任务重新定义为条件生成问题。通过将水印图像分解为两个互补的标记集：用于纹理的视觉标记和用于几何的结构标记，这些标记共同调节扩散过程，绕过初始携带水印的噪声。

Result: 综合实验表明，TokenPure在水印去除和重建保真度方面达到了最先进的性能，在感知质量和一致性方面显著优于现有基线方法。

Conclusion: TokenPure通过基于标记的条件重建解决了彻底破坏水印与保持内容一致性之间的权衡问题，能够合成具有细粒度一致性和结构完整性的无水印图像。

Abstract: In the digital economy era, digital watermarking serves as a critical basis for ownership proof of massive replicable content, including AI-generated and other virtual assets. Designing robust watermarks capable of withstanding various attacks and processing operations is even more paramount. We introduce TokenPure, a novel Diffusion Transformer-based framework designed for effective and consistent watermark removal. TokenPure solves the trade-off between thorough watermark destruction and content consistency by leveraging token-based conditional reconstruction. It reframes the task as conditional generation, entirely bypassing the initial watermark-carrying noise. We achieve this by decomposing the watermarked image into two complementary token sets: visual tokens for texture and structural tokens for geometry. These tokens jointly condition the diffusion process, enabling the framework to synthesize watermark-free images with fine-grained consistency and structural integrity. Comprehensive experiments show that TokenPure achieves state-of-the-art watermark removal and reconstruction fidelity, substantially outperforming existing baselines in both perceptual quality and consistency.

中文标题: TokenPure：通过标记化外观和结构引导的水印去除

中文摘要: 在数字经济时代，数字水印作为海量可复制内容（包括AI生成和其他虚拟资产）所有权证明的关键基础。设计能够抵御各种攻击和处理操作的鲁棒水印变得尤为重要。我们提出了TokenPure，一种基于扩散变换器的新型框架，旨在实现有效且一致的水印去除。TokenPure通过利用基于标记的条件重建，解决了彻底破坏水印与内容一致性之间的权衡问题。它将任务重新定义为条件生成，完全绕过了初始携带水印的噪声。我们通过将水印图像分解为两个互补的标记集来实现这一点：用于纹理的视觉标记和用于几何的结构标记。这些标记共同调节扩散过程，使框架能够合成具有细粒度一致性和结构完整性的无水印图像。综合实验表明，TokenPure在水印去除和重建保真度方面达到了最先进的性能，在感知质量和一致性方面显著优于现有基线方法。

</details>


### [119] [FOD-S2R: A FOD Dataset for Sim2Real Transfer Learning based Object Detection](https://arxiv.org/abs/2512.01315)
*Ashish Vashist,Qiranul Saadiyean,Suresh Sundaram,Chandra Sekhar Seelamantula*

Main category: cs.CV

TL;DR: 提出了首个专门针对飞机燃油箱内异物检测的合成-真实数据集FOD-S2R，通过系统实验证明合成数据能有效提升真实环境下的检测性能，缩小仿真到真实差距。


<details>
  <summary>Details</summary>
Motivation: 飞机燃油箱内的异物碎片（FOD）存在严重安全隐患，但现有数据集主要针对开放环境，缺乏专门针对燃油箱等封闭复杂环境的数据集，限制了自动化检测系统的开发。

Method: 构建了包含真实图像（3,114张）和合成图像（3,137张）的FOD-S2R数据集，使用虚幻引擎生成合成数据，涵盖多种视场角、距离、光照等条件变化，并基于该数据集对多种先进物体检测模型进行基准测试。

Result: 实验证明引入合成数据能显著提高FOD检测精度和模型对真实世界条件的泛化能力，有效缩小仿真到真实差距，为自动化检测系统开发提供了可靠基础。

Conclusion: FOD-S2R是首个专门针对燃油箱封闭环境的合成-真实数据集，证明了合成数据在提升真实世界FOD检测性能方面的有效性，为航空维护自动化检测系统的发展提供了重要支持。

Abstract: Foreign Object Debris (FOD) within aircraft fuel tanks presents critical safety hazards including fuel contamination, system malfunctions, and increased maintenance costs. Despite the severity of these risks, there is a notable lack of dedicated datasets for the complex, enclosed environments found inside fuel tanks. To bridge this gap, we present a novel dataset, FOD-S2R, composed of real and synthetic images of the FOD within a simulated aircraft fuel tank. Unlike existing datasets that focus on external or open-air environments, our dataset is the first to systematically evaluate the effectiveness of synthetic data in enhancing the real-world FOD detection performance in confined, closed structures. The real-world subset consists of 3,114 high-resolution HD images captured in a controlled fuel tank replica, while the synthetic subset includes 3,137 images generated using Unreal Engine. The dataset is composed of various Field of views (FOV), object distances, lighting conditions, color, and object size. Prior research has demonstrated that synthetic data can reduce reliance on extensive real-world annotations and improve the generalizability of vision models. Thus, we benchmark several state-of-the-art object detection models and demonstrate that introducing synthetic data improves the detection accuracy and generalization to real-world conditions. These experiments demonstrate the effectiveness of synthetic data in enhancing the model performance and narrowing the Sim2Real gap, providing a valuable foundation for developing automated FOD detection systems for aviation maintenance.

中文标题: FOD-S2R：基于仿真到真实迁移学习的异物检测数据集

中文摘要: 飞机燃油箱内的异物碎片（FOD）会带来严重的安全隐患，包括燃油污染、系统故障和维护成本增加。尽管这些风险很严重，但目前缺乏专门针对燃油箱内复杂封闭环境的数据集。为了填补这一空白，我们提出了一个新颖的数据集FOD-S2R，包含在模拟飞机燃油箱内FOD的真实和合成图像。与现有专注于外部或开放环境的数据集不同，我们的数据集首次系统评估了合成数据在增强封闭结构内真实世界FOD检测性能方面的有效性。真实世界子集包含在受控燃油箱复制品中捕获的3,114张高分辨率HD图像，而合成子集包含使用虚幻引擎生成的3,137张图像。该数据集包含各种视场角、物体距离、光照条件、颜色和物体大小。先前研究表明，合成数据可以减少对大量真实世界标注的依赖，并提高视觉模型的泛化能力。因此，我们对几种最先进的物体检测模型进行了基准测试，并证明引入合成数据可以提高检测精度和对真实世界条件的泛化能力。这些实验证明了合成数据在增强模型性能和缩小仿真到真实差距方面的有效性，为开发航空维护自动化FOD检测系统提供了宝贵基础。

</details>


### [120] [Rethinking Intracranial Aneurysm Vessel Segmentation: A Perspective from Computational Fluid Dynamics Applications](https://arxiv.org/abs/2512.01319)
*Feiyang Xiao,Yichi Zhang,Xigui Li,Yuanye Zhou,Chen Jiang,Xin Guo,Limei Han,Yuxin Li,Fengping Zhu,Yuan Cheng*

Main category: cs.CV

TL;DR: 该研究提出了首个专注于CFD应用的颅内动脉瘤血管分割数据集（IAVS），包含多中心3D MRA图像和血流动力学分析结果，建立了标准化CFD适用性评估系统，并开发了两阶段分割框架作为基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前颅内动脉瘤血管分割方法主要关注图像层面的评估指标，忽视了分割结果在后续计算流体动力学（CFD）应用中的实际有效性。现有数据集缺乏拓扑完整性和CFD适用性考虑，限制了分割方法在临床血流动力学分析中的应用价值。

Method: 1）构建IAVS数据集：包含641个3D MRA图像和587个标注，是多中心收集，包含血流动力学分析结果；2）建立两阶段评估基准：第一阶段为动脉瘤全局定位，第二阶段为IA-Vessel细粒度分割；3）开发两阶段分割框架作为基线方法；4）建立标准化CFD适用性评估系统，实现分割掩码到CFD模型的自动转换。

Result: 1）创建了首个专注于CFD应用的颅内动脉瘤血管分割数据集IAVS；2）建立了包含两个阶段的评估基准；3）开发了可作为即用型方法和强基线的两阶段分割框架；4）建立了标准化的CFD适用性评估系统，能够自动评估分割结果在CFD应用中的适用性。

Conclusion: 该研究通过构建IAVS数据集和标准化CFD适用性评估系统，填补了颅内动脉瘤血管分割领域在CFD应用评估方面的空白，为开发临床相关分割技术提供了重要资源和评估框架，促进了分割方法在血流动力学分析中的实际应用。

Abstract: The precise segmentation of intracranial aneurysms and their parent vessels (IA-Vessel) is a critical step for hemodynamic analyses, which mainly depends on computational fluid dynamics (CFD). However, current segmentation methods predominantly focus on image-based evaluation metrics, often neglecting their practical effectiveness in subsequent CFD applications. To address this deficiency, we present the Intracranial Aneurysm Vessel Segmentation (IAVS) dataset, the first comprehensive, multi-center collection comprising 641 3D MRA images with 587 annotations of aneurysms and IA-Vessels. In addition to image-mask pairs, IAVS dataset includes detailed hemodynamic analysis outcomes, addressing the limitations of existing datasets that neglect topological integrity and CFD applicability. To facilitate the development and evaluation of clinically relevant techniques, we construct two evaluation benchmarks including global localization of aneurysms (Stage I) and fine-grained segmentation of IA-Vessel (Stage II) and develop a simple and effective two-stage framework, which can be used as a out-of-the-box method and strong baseline. For comprehensive evaluation of applicability of segmentation results, we establish a standardized CFD applicability evaluation system that enables the automated and consistent conversion of segmentation masks into CFD models, offering an applicability-focused assessment of segmentation outcomes. The dataset, code, and model will be public available at https://github.com/AbsoluteResonance/IAVS.

中文标题: 重新思考颅内动脉瘤血管分割：从计算流体动力学应用的角度

中文摘要: 颅内动脉瘤及其母血管（IA-Vessel）的精确分割是血流动力学分析的关键步骤，主要依赖于计算流体动力学（CFD）。然而，当前的分割方法主要关注基于图像的评估指标，往往忽视了其在后续CFD应用中的实际有效性。为了解决这一不足，我们提出了颅内动脉瘤血管分割（IAVS）数据集，这是首个全面的多中心收集，包含641个3D MRA图像和587个动脉瘤及IA-Vessel的标注。除了图像-掩码对之外，IAVS数据集还包括详细的血流动力学分析结果，解决了现有数据集忽视拓扑完整性和CFD适用性的局限性。为了促进临床相关技术的开发和评估，我们构建了两个评估基准，包括动脉瘤的全局定位（第一阶段）和IA-Vessel的细粒度分割（第二阶段），并开发了一个简单有效的两阶段框架，可作为即用型方法和强大的基线。为了全面评估分割结果的适用性，我们建立了一个标准化的CFD适用性评估系统，能够将分割掩码自动且一致地转换为CFD模型，提供以适用性为重点的分割结果评估。数据集、代码和模型将在https://github.com/AbsoluteResonance/IAVS公开提供。

</details>


### [121] [Optimizing Stroke Risk Prediction: A Machine Learning Pipeline Combining ROS-Balanced Ensembles and XAI](https://arxiv.org/abs/2512.01333)
*A S M Ahsanul Sarkar Akib,Raduana Khawla,Abdul Hasib*

Main category: cs.CV

TL;DR: 本文开发了一个结合ROS平衡集成和XAI的机器学习管道，用于卒中风险预测，在卒中预测数据集上达到99.09%的准确率，并通过可解释性分析识别出年龄、高血压和血糖水平三个关键临床变量。


<details>
  <summary>Details</summary>
Motivation: 卒中是全球主要的死亡和永久性损伤原因，早期风险评估对于及时干预和有效预防策略至关重要。现有方法在准确性和可解释性方面存在不足，需要开发既准确又透明的预测模型。

Method: 采用全面的机器学习框架，包括数据预处理、特征工程，使用随机过采样（ROS）解决类别不平衡问题。评估了10种不同机器学习模型，采用5折交叉验证，最终优化了集成模型（随机森林+ExtraTrees+XGBoost），并使用LIME进行可解释性分析。

Result: 优化的集成模型在卒中预测数据集（SPD）上表现出色，达到99.09%的准确率。通过LIME可解释性分析识别出三个关键临床变量：年龄、高血压和血糖水平，提高了模型的透明度和临床适用性。

Conclusion: 研究表明，将集成学习与可解释AI（XAI）相结合，可以提供高度准确且可解释的卒中风险评估。该框架通过早期预测、数据驱动的预防和个性化临床决策，有潜力改变卒中预测和心血管风险管理。

Abstract: Stroke is a major cause of death and permanent impairment, making it a major worldwide health concern. For prompt intervention and successful preventative tactics, early risk assessment is essential. To address this challenge, we used ensemble modeling and explainable AI (XAI) techniques to create an interpretable machine learning framework for stroke risk prediction. A thorough evaluation of 10 different machine learning models using 5-fold cross-validation across several datasets was part of our all-inclusive strategy, which also included feature engineering and data pretreatment (using Random Over-Sampling (ROS) to solve class imbalance). Our optimized ensemble model (Random Forest + ExtraTrees + XGBoost) performed exceptionally well, obtaining a strong 99.09% accuracy on the Stroke Prediction Dataset (SPD). We improved the model's transparency and clinical applicability by identifying three important clinical variables using LIME-based interpretability analysis: age, hypertension, and glucose levels. Through early prediction, this study highlights how combining ensemble learning with explainable AI (XAI) can deliver highly accurate and interpretable stroke risk assessment. By enabling data-driven prevention and personalized clinical decisions, our framework has the potential to transform stroke prediction and cardiovascular risk management.

中文标题: 优化卒中风险预测：结合ROS平衡集成和XAI的机器学习管道

中文摘要: 卒中是导致死亡和永久性损伤的主要原因，是全球重大的健康问题。为了及时干预和采取有效的预防策略，早期风险评估至关重要。为应对这一挑战，我们使用集成建模和可解释AI（XAI）技术，创建了一个可解释的机器学习框架用于卒中风险预测。我们的全面策略包括特征工程和数据预处理（使用随机过采样（ROS）解决类别不平衡问题），并对10种不同机器学习模型在多个数据集上进行了5折交叉验证的全面评估。我们优化的集成模型（随机森林+ExtraTrees+XGBoost）表现优异，在卒中预测数据集（SPD）上获得了99.09%的准确率。通过基于LIME的可解释性分析，我们识别出三个重要的临床变量：年龄、高血压和血糖水平，提高了模型的透明度和临床适用性。本研究强调了通过早期预测，将集成学习与可解释AI（XAI）相结合，可以提供高度准确且可解释的卒中风险评估。我们的框架通过实现数据驱动的预防和个性化临床决策，有潜力改变卒中预测和心血管风险管理。

</details>


### [122] [EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans](https://arxiv.org/abs/2512.01340)
*Yingjie Zhou,Xilei Zhu,Siyu Ren,Ziyi Zhao,Ziwen Wang,Farong Wen,Yu Zhou,Jiezhang Cao,Xiongkuo Min,Fengjiao Chen,Xiaoyu Li,Xuezhi Cao,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: 该论文提出了EvalTalker框架，用于评估多主体说话人像生成的质量，并构建了首个大规模Multi-Talker质量评估数据集THQA-MT。


<details>
  <summary>Details</summary>
Motivation: 当前Multi-Talker技术存在明显的质量下降问题，导致用户体验不佳，需要建立有效的质量评估方法来推动高质量Multi-Talker技术的发展。

Method: 1. 构建THQA-MT数据集：包含5,492个Multi-Talker生成的说话人像，来自15个代表性模型和400个真实肖像；2. 通过主观实验分析感知差异和12种失真类型；3. 提出EvalTalker框架：整合全局质量、人类特征、身份一致性感知，并集成Qwen-Sync进行多模态同步感知。

Result: EvalTalker在实验中表现出与主观评分的高度相关性，为Multi-Talker质量评估提供了可靠工具。

Conclusion: EvalTalker框架为高质量Multi-Talker生成和评估研究奠定了坚实基础，有助于推动多主体说话人像技术的发展。

Abstract: Speech-driven Talking Human (TH) generation, commonly known as "Talker," currently faces limitations in multi-subject driving capabilities. Extending this paradigm to "Multi-Talker," capable of animating multiple subjects simultaneously, introduces richer interactivity and stronger immersion in audiovisual communication. However, current Multi-Talkers still exhibit noticeable quality degradation caused by technical limitations, resulting in suboptimal user experiences. To address this challenge, we construct THQA-MT, the first large-scale Multi-Talker-generated Talking Human Quality Assessment dataset, consisting of 5,492 Multi-Talker-generated THs (MTHs) from 15 representative Multi-Talkers using 400 real portraits collected online. Through subjective experiments, we analyze perceptual discrepancies among different Multi-Talkers and identify 12 common types of distortion. Furthermore, we introduce EvalTalker, a novel TH quality assessment framework. This framework possesses the ability to perceive global quality, human characteristics, and identity consistency, while integrating Qwen-Sync to perceive multimodal synchrony. Experimental results demonstrate that EvalTalker achieves superior correlation with subjective scores, providing a robust foundation for future research on high-quality Multi-Talker generation and evaluation.

中文标题: EvalTalker：学习评估真实肖像驱动的多主体说话人像

中文摘要: 语音驱动的说话人像生成，通常称为"Talker"，目前在多主体驱动能力方面存在局限性。将此范式扩展到"Multi-Talker"，能够同时动画化多个主体，为视听通信引入了更丰富的交互性和更强的沉浸感。然而，当前的Multi-Talker仍然表现出由技术限制引起的明显质量下降，导致用户体验不佳。为应对这一挑战，我们构建了THQA-MT，这是第一个大规模Multi-Talker生成的说话人像质量评估数据集，包含来自15个代表性Multi-Talker使用在线收集的400个真实肖像生成的5,492个Multi-Talker生成的说话人像。通过主观实验，我们分析了不同Multi-Talker之间的感知差异，并识别了12种常见的失真类型。此外，我们引入了EvalTalker，一种新颖的说话人像质量评估框架。该框架具备感知全局质量、人类特征和身份一致性的能力，同时集成Qwen-Sync来感知多模态同步性。实验结果表明，EvalTalker实现了与主观评分的优越相关性，为未来高质量Multi-Talker生成和评估研究提供了坚实基础。

</details>


### [123] [InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision](https://arxiv.org/abs/2512.01342)
*Chenting Wang,Yuhan Zhu,Yicheng Xu,Jiange Yang,Ziang Yan,Yali Wang,Yi Wang,Limin Wang*

Main category: cs.CV

TL;DR: InternVideo-Next提出了一种无需视频-文本监督的通用视频基础模型，通过两阶段预训练方案解决传统掩码视频建模中的架构问题，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 大规模视频-文本预训练依赖噪声大、语义覆盖有限的合成字幕，常忽略隐含的世界知识（如物体运动、3D几何和物理线索）。而掩码视频建模直接利用时空结构但在通用任务上落后于文本监督方法。研究发现这种差距源于被忽视的架构问题：像素级重建难以收敛且其低级要求常与语义冲突，而潜在预测常导致捷径学习。

Method: 提出Encoder-Predictor-Decoder（EPD）框架，将传统编码器-解码器设计解耦，其中预测器作为潜在世界模型。采用两阶段预训练方案：第一阶段使用条件扩散解码器并注入可靠的图像级语义先验，增强语义和收敛性；第二阶段通过预测冻结的第一阶段目标来学习世界知识，缓解捷径学习。

Result: 在公开、未标记的视频数据上训练，InternVideo-Next在多个基准测试中实现了最先进的结果，包括视频理解、动作识别等任务，为通用视频表示学习提供了可扩展的路径。

Conclusion: InternVideo-Next通过创新的两阶段预训练框架，成功解决了传统掩码视频建模中的架构限制，在不依赖视频-文本监督的情况下实现了强大的通用视频表示学习能力，为视频基础模型的发展提供了新方向。

Abstract: Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning.

中文标题: InternVideo-Next：迈向无需视频-文本监督的通用视频基础模型

中文摘要: 大规模视频-文本预训练实现了强大的性能，但依赖于噪声大、语义覆盖有限的合成字幕，常常忽略了隐含的世界知识，如物体运动、3D几何和物理线索。相比之下，掩码视频建模直接利用时空结构，但在通用任务上落后于文本监督方法。我们发现这种差距源于被忽视的架构问题：像素级重建难以收敛且其低级要求常与语义冲突，而潜在预测常鼓励捷径学习。为解决这些问题，我们将传统的编码器-解码器设计解耦为编码器-预测器-解码器（EPD）框架，其中预测器作为潜在世界模型，并提出InternVideo-Next，这是一个两阶段预训练方案，为该世界模型构建语义一致且细节保留的潜在空间。首先，像素MVM中的传统线性解码器强制预测器输出潜在向量线性投影到像素空间，导致与语义抽象的冲突。我们的第一阶段提出条件扩散解码器并注入可靠的图像级语义先验，以增强语义和收敛性，从而弥合像素级保真度与高级语义抽象。第二阶段通过预测冻结的第一阶段目标在该空间内进一步学习世界知识，缓解捷径学习。在公开、未标记的视频上训练，InternVideo-Next在多个基准测试中实现了最先进的结果，并为通用视频表示学习提供了可扩展的路径。

</details>


### [124] [Handwritten Text Recognition for Low Resource Languages](https://arxiv.org/abs/2512.01348)
*Sayantan Dey,Alireza Alaei,Partha Pratim Roy*

Main category: cs.CV

TL;DR: 本文提出了BharatOCR，一种用于低资源语言（印地语、乌尔都语）的段落级手写文本识别系统，采用ViT-Transformer解码器-语言模型架构，在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如印地语、乌尔都语）的段落级手写文本识别面临挑战，因为这些语言缺乏全面的语言资源，需要开发鲁棒的OCR系统来处理这些复杂脚本。

Method: 提出BharatOCR系统，采用ViT-Transformer解码器-语言模型架构：1) Vision Transformer提取视觉特征；2) Transformer解码器生成文本序列；3) 预训练RoBERTa语言模型优化输出；4) 使用DeiT进行掩码图像建模；5) 隐式行分割逐行处理段落图像。

Result: 在多个数据集上取得最先进性能：NUST-UHWR（96.24%字符识别率）、PUCIT-OUHL（92.05%）、Parimal-Urdu（94.80%）、印地语数据集（80.64%），优于现有乌尔都语文本识别方法。

Conclusion: BharatOCR系统成功解决了低资源语言段落级手写文本识别的挑战，通过结合视觉特征提取、序列生成和语言模型优化，在印地语和乌尔都语识别任务中取得了卓越性能。

Abstract: Despite considerable progress in handwritten text recognition, paragraph-level handwritten text recognition, especially in low-resource languages, such as Hindi, Urdu and similar scripts, remains a challenging problem. These languages, often lacking comprehensive linguistic resources, require special attention to develop robust systems for accurate optical character recognition (OCR). This paper introduces BharatOCR, a novel segmentation-free paragraph-level handwritten Hindi and Urdu text recognition. We propose a ViT-Transformer Decoder-LM architecture for handwritten text recognition, where a Vision Transformer (ViT) extracts visual features, a Transformer decoder generates text sequences, and a pre-trained language model (LM) refines the output to improve accuracy, fluency, and coherence. Our model utilizes a Data-efficient Image Transformer (DeiT) model proposed for masked image modeling in this research work. In addition, we adopt a RoBERTa architecture optimized for masked language modeling (MLM) to enhance the linguistic comprehension and generative capabilities of the proposed model. The transformer decoder generates text sequences from visual embeddings. This model is designed to iteratively process a paragraph image line by line, called implicit line segmentation. The proposed model was evaluated using our custom dataset ('Parimal Urdu') and ('Parimal Hindi'), introduced in this research work, as well as two public datasets. The proposed model achieved benchmark results in the NUST-UHWR, PUCIT-OUHL, and Parimal-Urdu datasets, achieving character recognition rates of 96.24%, 92.05%, and 94.80%, respectively. The model also provided benchmark results using the Hindi dataset achieving a character recognition rate of 80.64%. The results obtained from our proposed model indicated that it outperformed several state-of-the-art Urdu text recognition methods.

中文标题: 面向低资源语言的笔迹文本识别

中文摘要: 尽管笔迹文本识别取得了显著进展，但段落级别的笔迹文本识别，特别是在印地语、乌尔都语等低资源语言中，仍然是一个具有挑战性的问题。这些语言通常缺乏全面的语言资源，需要特别关注以开发用于准确光学字符识别（OCR）的鲁棒系统。本文介绍了BharatOCR，一种新颖的无分割段落级别手写印地语和乌尔都语文本识别方法。我们提出了一种ViT-Transformer解码器-语言模型架构用于笔迹文本识别，其中视觉变换器（ViT）提取视觉特征，变换器解码器生成文本序列，预训练语言模型（LM）优化输出以提高准确性、流畅性和连贯性。我们的模型利用了本研究中提出的用于掩码图像建模的数据高效图像变换器（DeiT）模型。此外，我们采用针对掩码语言建模（MLM）优化的RoBERTa架构，以增强所提出模型的语言理解和生成能力。变换器解码器从视觉嵌入中生成文本序列。该模型设计为逐行迭代处理段落图像，称为隐式行分割。所提出的模型使用我们自定义的数据集（'Parimal Urdu'和'Parimal Hindi'）以及两个公共数据集进行评估。所提出的模型在NUST-UHWR、PUCIT-OUHL和Parimal-Urdu数据集中取得了基准结果，分别实现了96.24%、92.05%和94.80%的字符识别率。该模型还使用印地语数据集实现了80.64%的字符识别率。从我们提出的模型获得的结果表明，它优于几种最先进的乌尔都语文本识别方法。

</details>


### [125] [OpenBox: Annotate Any Bounding Boxes in 3D](https://arxiv.org/abs/2512.01352)
*In-Jae Lee,Mungyeom Kim,Kwonyoung Ryu,Pierre Musacchio,Jaesik Park*

Main category: cs.CV

TL;DR: OpenBox是一个两阶段自动3D边界框标注系统，利用2D视觉基础模型，通过跨模态实例对齐和自适应边界框生成，无需自训练即可为自动驾驶场景中的物体提供高质量3D标注。


<details>
  <summary>Details</summary>
Motivation: 现有无监督和开放词汇3D物体检测方法存在三个主要问题：1）统一标注3D边界框，忽略了物体的物理状态；2）需要多次自训练迭代进行标注优化，导致计算开销大；3）标注质量不理想。这些限制了自动驾驶场景中降低标注成本和识别未见物体的能力。

Method: OpenBox采用两阶段流水线：第一阶段通过跨模态实例对齐，将2D视觉基础模型处理的图像实例线索与对应的3D点云关联；第二阶段根据刚性和运动状态对实例进行分类，然后使用类别特定的尺寸统计生成自适应边界框。

Result: 在Waymo Open Dataset、Lyft Level 5 Perception dataset和nuScenes数据集上的实验表明，OpenBox在准确性和效率方面均优于基线方法，能够产生高质量的3D边界框标注。

Conclusion: OpenBox通过利用2D视觉基础模型和两阶段标注流程，成功解决了现有3D标注方法的局限性，实现了无需自训练的高质量3D边界框自动标注，为自动驾驶中的物体检测提供了更高效、更准确的解决方案。

Abstract: Unsupervised and open-vocabulary 3D object detection has recently gained attention, particularly in autonomous driving, where reducing annotation costs and recognizing unseen objects are critical for both safety and scalability. However, most existing approaches uniformly annotate 3D bounding boxes, ignore objects' physical states, and require multiple self-training iterations for annotation refinement, resulting in suboptimal quality and substantial computational overhead. To address these challenges, we propose OpenBox, a two-stage automatic annotation pipeline that leverages a 2D vision foundation model. In the first stage, OpenBox associates instance-level cues from 2D images processed by a vision foundation model with the corresponding 3D point clouds via cross-modal instance alignment. In the second stage, it categorizes instances by rigidity and motion state, then generates adaptive bounding boxes with class-specific size statistics. As a result, OpenBox produces high-quality 3D bounding box annotations without requiring self-training. Experiments on the Waymo Open Dataset, the Lyft Level 5 Perception dataset, and the nuScenes dataset demonstrate improved accuracy and efficiency over baselines.

中文标题: OpenBox：在3D中标注任意边界框

中文摘要: 无监督和开放词汇3D物体检测最近受到关注，特别是在自动驾驶领域，降低标注成本和识别未见物体对于安全性和可扩展性都至关重要。然而，大多数现有方法统一标注3D边界框，忽略物体的物理状态，并且需要多次自训练迭代进行标注优化，导致质量不理想和大量计算开销。为了解决这些挑战，我们提出了OpenBox，一个两阶段自动标注流水线，利用2D视觉基础模型。在第一阶段，OpenBox通过跨模态实例对齐，将2D视觉基础模型处理的图像中的实例级线索与对应的3D点云关联。在第二阶段，它根据刚性和运动状态对实例进行分类，然后使用类别特定的尺寸统计生成自适应边界框。因此，OpenBox无需自训练即可产生高质量的3D边界框标注。在Waymo Open Dataset、Lyft Level 5 Perception数据集和nuScenes数据集上的实验证明了其相对于基线方法的改进准确性和效率。

</details>


### [126] [SRAM: Shape-Realism Alignment Metric for No Reference 3D Shape Evaluation](https://arxiv.org/abs/2512.01373)
*Sheng Liu,Tianyu Luan,Phani Nuney,Xuelu Feng,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出SRAM度量，利用LLM作为桥梁评估3D形状真实感，无需地面实况参考，通过网格编码和专用解码器实现，在新数据集上验证效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着3D生成技术的广泛应用，对逼真3D形状的需求增加，但传统评估方法依赖地面实况，而实际应用中形状的真实感评估往往没有参考标准，需要新的无参考评估方法。

Method: 1. 使用网格编码方法将3D形状转换为语言标记空间；2. 利用大型语言模型作为形状信息与真实感评估的桥梁；3. 设计专用真实感解码器对齐LLM输出与人类感知；4. 构建RealismGrading数据集，包含16种算法在多个对象上生成的形状。

Result: 1. 提出的SRAM度量与人类感知相关性良好；2. 在性能上优于现有评估方法；3. 通过k折交叉验证证明具有良好的泛化能力；4. RealismGrading数据集更具代表性。

Conclusion: SRAM是一种有效的无参考3D形状真实感评估度量，通过LLM桥梁和专用解码器设计，能够准确反映人类对形状真实感的感知，在实用性和泛化性方面表现优异。

Abstract: 3D generation and reconstruction techniques have been widely used in computer games, film, and other content creation areas. As the application grows, there is a growing demand for 3D shapes that look truly realistic. Traditional evaluation methods rely on a ground truth to measure mesh fidelity. However, in many practical cases, a shape's realism does not depend on having a ground truth reference. In this work, we propose a Shape-Realism Alignment Metric that leverages a large language model (LLM) as a bridge between mesh shape information and realism evaluation. To achieve this, we adopt a mesh encoding approach that converts 3D shapes into the language token space. A dedicated realism decoder is designed to align the language model's output with human perception of realism. Additionally, we introduce a new dataset, RealismGrading, which provides human-annotated realism scores without the need for ground truth shapes. Our dataset includes shapes generated by 16 different algorithms on over a dozen objects, making it more representative of practical 3D shape distributions. We validate our metric's performance and generalizability through k-fold cross-validation across different objects. Experimental results show that our metric correlates well with human perceptions and outperforms existing methods, and has good generalizability.

中文标题: SRAM：用于无参考3D形状评估的形状真实感对齐度量

中文摘要: 3D生成和重建技术已广泛应用于电脑游戏、电影和其他内容创作领域。随着应用的增长，对看起来真正逼真的3D形状的需求也在增加。传统的评估方法依赖于地面实况来测量网格保真度。然而，在许多实际情况下，形状的真实感并不依赖于有地面实况参考。在这项工作中，我们提出了一种形状真实感对齐度量，利用大型语言模型作为网格形状信息和真实感评估之间的桥梁。为实现这一目标，我们采用了一种网格编码方法，将3D形状转换为语言标记空间。设计了一个专用的真实感解码器，使语言模型的输出与人类对真实感的感知保持一致。此外，我们引入了一个新的数据集RealismGrading，该数据集提供了人类标注的真实感分数，无需地面实况形状。我们的数据集包含16种不同算法在十多个对象上生成的形状，使其更能代表实际的3D形状分布。我们通过对不同对象的k折交叉验证来验证我们度量的性能和泛化能力。实验结果表明，我们的度量与人类感知相关性良好，优于现有方法，并具有良好的泛化能力。

</details>


### [127] [Reversible Inversion for Training-Free Exemplar-guided Image Editing](https://arxiv.org/abs/2512.01382)
*Yuke Li,Lianli Gao,Ji Zhang,Pengpeng Zeng,Lichuan Xiang,Hongkai Wen,Heng Tao Shen,Jingkuan Song*

Main category: cs.CV

TL;DR: 提出ReInversion方法，通过两阶段可逆反演实现免训练的范例引导图像编辑，无需大规模预训练，计算效率高。


<details>
  <summary>Details</summary>
Motivation: 现有范例引导图像编辑方法需要大规模预训练，计算成本高；标准反演技术虽然免训练但效果不佳，需要更优的免训练解决方案。

Method: 提出ReInversion方法：1）两阶段可逆反演过程，先以源图像为条件，再以参考图像为条件；2）掩码引导选择性去噪策略，限制编辑区域，保持背景一致性。

Result: 方法在定性和定量评估中均达到最先进的EIE性能，同时计算开销最低，实现了高效高质量的范例引导图像编辑。

Conclusion: ReInversion为范例引导图像编辑提供了一种有效且高效的免训练解决方案，克服了现有方法计算成本高和标准反演效果差的问题。

Abstract: Exemplar-guided Image Editing (EIE) aims to modify a source image according to a visual reference. Existing approaches often require large-scale pre-training to learn relationships between the source and reference images, incurring high computational costs. As a training-free alternative, inversion techniques can be used to map the source image into a latent space for manipulation. However, our empirical study reveals that standard inversion is sub-optimal for EIE, leading to poor quality and inefficiency. To tackle this challenge, we introduce \textbf{Reversible Inversion ({ReInversion})} for effective and efficient EIE. Specifically, ReInversion operates as a two-stage denoising process, which is first conditioned on the source image and subsequently on the reference. Besides, we introduce a Mask-Guided Selective Denoising (MSD) strategy to constrain edits to target regions, preserving the structural consistency of the background. Both qualitative and quantitative comparisons demonstrate that our ReInversion method achieves state-of-the-art EIE performance with the lowest computational overhead.

中文标题: 可逆反演：用于免训练的范例引导图像编辑

中文摘要: 范例引导图像编辑（EIE）旨在根据视觉参考修改源图像。现有方法通常需要大规模预训练来学习源图像和参考图像之间的关系，计算成本高昂。作为一种免训练的替代方案，反演技术可用于将源图像映射到潜在空间进行操作。然而，我们的实证研究表明，标准反演对于EIE来说是次优的，导致质量差和效率低。为了应对这一挑战，我们引入了\textbf{可逆反演（ReInversion）}来实现有效且高效的EIE。具体来说，ReInversion作为一个两阶段去噪过程运行，首先以源图像为条件，随后以参考图像为条件。此外，我们引入了掩码引导选择性去噪（MSD）策略，将编辑限制在目标区域，保持背景的结构一致性。定性和定量比较均表明，我们的ReInversion方法以最低的计算开销实现了最先进的EIE性能。

</details>


### [128] [Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries](https://arxiv.org/abs/2512.01419)
*Tushar Pranav,Eshan Pandey,Austria Lyka Diane Bala,Aman Chadha,Indriyati Atmosukarto,Donny Soh Cheng Lock*

Main category: cs.CV

TL;DR: RICE-VL是一个评估视觉语言模型在东盟11国文化理解能力的新基准，包含超过28,000个人工标注的视觉问答样本和1,000个图像-边界框对，揭示了现有模型在低资源国家和抽象文化领域的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型存在西方中心偏见，在东南亚等文化多元地区的表现受限，需要专门的文化理解评估基准来促进包容性模型开发。

Method: 创建RICE-VL基准，包含三种格式的视觉问答任务（对错判断、填空、开放式）和视觉定位任务；提出SEA-LAVE评估指标，从文本准确性、文化对齐和国家识别三个维度评估；对6个开源和闭源模型进行全面评估。

Result: 评估显示现有模型在低资源国家（如老挝、缅甸）和抽象文化领域（如宗教、传统）表现显著较差；视觉定位任务中模型在复杂场景中定位文化元素的能力有限；揭示了模型文化理解能力的系统性差距。

Conclusion: RICE-VL暴露了视觉语言模型在文化理解方面的局限性，强调了开发更具包容性的模型以更好服务全球多元文化群体的必要性。

Abstract: Vision-Language Models (VLMs) excel in multimodal tasks but often exhibit Western-centric biases, limiting their effectiveness in culturally diverse regions like Southeast Asia (SEA). To address this, we introduce RICE-VL, a novel benchmark evaluating VLM cultural understanding across 11 ASEAN countries. RICE-VL includes over 28,000 human-curated Visual Question Answering (VQA) samples -- covering True or False, Fill-in-the-Blank, and open-ended formats -- and 1,000 image-bounding box pairs for Visual Grounding, annotated by culturally informed experts across 14 sub-ground categories. We propose SEA-LAVE, an extension of the LAVE metric, assessing textual accuracy, cultural alignment, and country identification. Evaluations of six open- and closed-source VLMs reveal significant performance gaps in low-resource countries and abstract cultural domains. The Visual Grounding task tests models' ability to localize culturally significant elements in complex scenes, probing spatial and contextual accuracy. RICE-VL exposes limitations in VLMs' cultural comprehension and highlights the need for inclusive model development to better serve diverse global populations.

中文标题: Rice-VL：评估视觉语言模型在东盟国家的文化理解能力

中文摘要: 视觉语言模型在多模态任务中表现出色，但通常存在西方中心偏见，限制了其在东南亚等文化多元地区的有效性。为此，我们引入了RICE-VL，这是一个评估视觉语言模型在11个东盟国家文化理解能力的新基准。RICE-VL包含超过28,000个人工标注的视觉问答样本——涵盖对错判断、填空和开放式格式——以及1,000个图像-边界框对用于视觉定位，由跨14个子类别的文化专家标注。我们提出了SEA-LAVE，这是LAVE指标的扩展，评估文本准确性、文化对齐和国家识别。对6个开源和闭源模型的评估揭示了在低资源国家和抽象文化领域的显著性能差距。视觉定位任务测试模型在复杂场景中定位文化重要元素的能力，探究空间和上下文准确性。RICE-VL暴露了视觉语言模型在文化理解方面的局限性，并强调了包容性模型开发的必要性，以更好地服务全球多元文化群体。

</details>


### [129] [MDiff4STR: Mask Diffusion Model for Scene Text Recognition](https://arxiv.org/abs/2512.01422)
*Yongkun Du,Miaomiao Zhao,Songlin Fan,Zhineng Chen,Caiyan Jia,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: MDiff4STR首次将掩码扩散模型应用于场景文本识别，通过六种噪声策略和令牌替换噪声机制解决了训练-推理噪声差距和过度自信预测问题，在保持高效推理的同时超越了现有最先进方法的准确性。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型在视觉语言任务中展现出效率和准确性的良好平衡，但尚未应用于场景文本识别。原始掩码扩散模型在STR任务中虽然提高了效率，但准确性落后于自回归模型，因此需要专门改进以充分发挥其在STR任务中的潜力。

Method: 提出MDiff4STR掩码扩散模型，包含两个核心改进：1）开发六种噪声策略以缩小训练与推理之间的噪声差距；2）提出令牌替换噪声机制，通过非掩码噪声类型促使模型修正过度自信的错误预测。

Result: 在标准和挑战性STR基准测试中，MDiff4STR在多种场景（不规则、艺术、遮挡、中文文本）下均优于现有流行模型，超越了最先进自回归模型的准确性，同时仅需三个去噪步骤实现快速推理。

Conclusion: MDiff4STR成功将掩码扩散模型引入场景文本识别任务，通过针对性改进策略解决了关键挑战，在保持高效推理的同时实现了卓越的识别准确性，为STR领域提供了新的有效解决方案。

Abstract: Mask Diffusion Models (MDMs) have recently emerged as a promising alternative to auto-regressive models (ARMs) for vision-language tasks, owing to their flexible balance of efficiency and accuracy. In this paper, for the first time, we introduce MDMs into the Scene Text Recognition (STR) task. We show that vanilla MDM lags behind ARMs in terms of accuracy, although it improves recognition efficiency. To bridge this gap, we propose MDiff4STR, a Mask Diffusion model enhanced with two key improvement strategies tailored for STR. Specifically, we identify two key challenges in applying MDMs to STR: noising gap between training and inference, and overconfident predictions during inference. Both significantly hinder the performance of MDMs. To mitigate the first issue, we develop six noising strategies that better align training with inference behavior. For the second, we propose a token-replacement noise mechanism that provides a non-mask noise type, encouraging the model to reconsider and revise overly confident but incorrect predictions. We conduct extensive evaluations of MDiff4STR on both standard and challenging STR benchmarks, covering diverse scenarios including irregular, artistic, occluded, and Chinese text, as well as whether the use of pretraining. Across these settings, MDiff4STR consistently outperforms popular STR models, surpassing state-of-the-art ARMs in accuracy, while maintaining fast inference with only three denoising steps. Code: https://github.com/Topdu/OpenOCR.

中文标题: MDiff4STR：用于场景文本识别的掩码扩散模型

中文摘要: 掩码扩散模型（MDMs）最近作为自回归模型（ARMs）的有前景替代方案在视觉语言任务中崭露头角，因其在效率和准确性之间的灵活平衡。在本文中，我们首次将MDMs引入场景文本识别（STR）任务。我们发现，尽管原始MDM提高了识别效率，但在准确性方面仍落后于ARMs。为弥补这一差距，我们提出了MDiff4STR，这是一个针对STR任务量身定制的、采用两种关键改进策略增强的掩码扩散模型。具体而言，我们确定了将MDMs应用于STR的两个关键挑战：训练与推理之间的噪声差距，以及推理过程中的过度自信预测。这两者都显著阻碍了MDMs的性能。为缓解第一个问题，我们开发了六种噪声策略，以更好地使训练与推理行为对齐。对于第二个问题，我们提出了一种令牌替换噪声机制，提供了一种非掩码噪声类型，鼓励模型重新考虑并修正过度自信但不正确的预测。我们在标准和具有挑战性的STR基准上对MDiff4STR进行了广泛评估，涵盖了包括不规则、艺术、遮挡和中文文本在内的多样化场景，以及是否使用预训练。在这些设置中，MDiff4STR始终优于流行的STR模型，在准确性方面超越了最先进的ARMs，同时仅需三个去噪步骤即可保持快速推理。代码：https://github.com/Topdu/OpenOCR。

</details>


### [130] [\textit{ViRectify}: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models](https://arxiv.org/abs/2512.01424)
*Xusen Hei,Jiali Chen,Jinyu Yang,Mengchen Zhao,Yi Cai*

Main category: cs.CV

TL;DR: ViRectify是一个针对多模态大语言模型视频推理纠错能力的挑战性基准，包含30K+实例，涵盖动态感知、科学推理和具身决策领域，通过轨迹证据驱动框架提升纠错性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在复杂视频推理中经常出错，但现有基准缺乏对其纠错能力的系统评估，需要建立一个专门的测试平台来揭示模型弱点并推动性能改进。

Method: 1. 通过AI辅助标注流程和人工验证构建包含30K+实例的数据集；2. 提出轨迹证据驱动的纠错框架，包括逐步错误轨迹建模和基于视觉证据的纠错奖励建模；3. 要求模型执行逐步错误识别并生成基于关键视频证据的推理依据。

Result: 1. 在16个先进MLLMs上的评估显示ViRectify具有挑战性，GPT-5仅达到31.94%纠错准确率；2. 提出的框架使Qwen2.5-VL-7B持续优于72B变体；3. 分析揭示了模型间纠错的系统性不对称性。

Conclusion: ViRectify为全面评估多模态大语言模型在视频推理纠错方面的能力提供了新方向，数据集可作为反思学习的宝贵资源，提出的框架能有效提升模型纠错性能。

Abstract: As multimodal large language models (MLLMs) frequently exhibit errors in complex video reasoning scenarios, correcting these errors is critical for uncovering their weaknesses and improving performance. However, existing benchmarks lack systematic evaluation of MLLMs' ability to identify and correct these video reasoning errors. To bridge this gap, we propose \textit{ViRectify}, a comprehensive benchmark to evaluate their fine-grained correction capability. Through an AI-assisted annotation pipeline with human verification, we construct a dataset of over 30\textit{K} instances spanning dynamic perception, scientific reasoning, and embodied decision-making domains. In \textit{ViRectify}, we challenge MLLMs to perform step-wise error identification and generate rationales with key video evidence grounding. In addition, we further propose the trajectory evidence-driven correction framework, comprising step-wise error trajectory and reward modeling on visual evidence-grounded correction. It encourages the model to explicitly concentrate on error propagation and key timestamps for correction. Extensive evaluation across 16 advanced MLLMs demonstrates that our \textit{ViRectify} serves as a challenging testbed, where GPT-5 achieves only 31.94\% correction accuracy. Our framework enables a Qwen2.5-VL-7B to consistently outperform the variants of 72B on \textit{ViRectify}, showing the effectiveness of our approach. Further analysis uncovers systematic asymmetries in error correction across models, and our dataset is also a valuable data resource to perform reflection learning. We believe \textit{ViRectify} provides a new direction for comprehensively evaluating the advanced MLLMs in video reasoning.

中文标题: ViRectify：面向多模态大语言模型的视频推理纠错挑战基准

中文摘要: 随着多模态大语言模型（MLLMs）在复杂视频推理场景中频繁出现错误，纠正这些错误对于揭示其弱点并提升性能至关重要。然而，现有基准缺乏对MLLMs识别和纠正视频推理错误的系统评估。为填补这一空白，我们提出了ViRectify，一个全面的基准来评估其细粒度纠错能力。通过AI辅助标注流程与人工验证，我们构建了一个包含超过30K实例的数据集，涵盖动态感知、科学推理和具身决策领域。在ViRectify中，我们挑战MLLMs执行逐步错误识别，并生成基于关键视频证据的推理依据。此外，我们进一步提出了轨迹证据驱动的纠错框架，包括逐步错误轨迹建模和基于视觉证据的纠错奖励建模。该框架鼓励模型明确关注错误传播和纠错的关键时间戳。对16个先进MLLMs的广泛评估表明，我们的ViRectify是一个具有挑战性的测试平台，其中GPT-5仅达到31.94%的纠错准确率。我们的框架使Qwen2.5-VL-7B能够持续优于72B变体在ViRectify上的表现，显示了方法的有效性。进一步分析揭示了模型间纠错的系统性不对称性，我们的数据集也是进行反思学习的宝贵数据资源。我们相信ViRectify为全面评估先进MLLMs在视频推理方面的能力提供了新方向。

</details>


### [131] [ResDiT: Evoking the Intrinsic Resolution Scalability in Diffusion Transformers](https://arxiv.org/abs/2512.01426)
*Yiyang Ma,Feng Zhou,Xuedan Yin,Pu Cao,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: ResDiT是一种无需训练的方法，通过校正位置嵌入和局部增强机制，使预训练的扩散变换器能够生成高分辨率图像，避免布局崩溃和纹理退化。


<details>
  <summary>Details</summary>
Motivation: 现有预训练扩散变换器（DiTs）在扩展到高分辨率图像合成时面临空间布局崩溃和纹理保真度下降的问题。先前方法需要复杂的多阶段流程，作者希望探索DiTs的内在生成机制，开发更高效、无需训练的高分辨率扩展方法。

Method: 1. 识别位置嵌入（PEs）是控制空间布局的核心因素，提出PE缩放技术校正分辨率变化时的位置编码错误；2. 开发基于基础分辨率局部注意力的局部增强机制；3. 设计补丁级融合模块聚合全局和局部线索；4. 采用高斯加权拼接策略消除网格伪影。

Result: ResDiT能够一致地生成高保真度、高分辨率的图像，避免了布局崩溃和纹理退化问题。该方法与下游任务（包括空间控制生成）无缝集成，在综合评估中表现出色。

Conclusion: ResDiT通过揭示和利用扩散变换器的内在生成机制，提供了一种无需训练的高分辨率扩展方法，有效解决了预训练DiTs在高分辨率合成中的布局和保真度问题，具有实用性和可扩展性。

Abstract: Leveraging pre-trained Diffusion Transformers (DiTs) for high-resolution (HR) image synthesis often leads to spatial layout collapse and degraded texture fidelity. Prior work mitigates these issues with complex pipelines that first perform a base-resolution (i.e., training-resolution) denoising process to guide HR generation. We instead explore the intrinsic generative mechanisms of DiTs and propose ResDiT, a training-free method that scales resolution efficiently. We identify the core factor governing spatial layout, position embeddings (PEs), and show that the original PEs encode incorrect positional information when extrapolated to HR, which triggers layout collapse. To address this, we introduce a PE scaling technique that rectifies positional encoding under resolution changes. To further remedy low-fidelity details, we develop a local-enhancement mechanism grounded in base-resolution local attention. We design a patch-level fusion module that aggregates global and local cues, together with a Gaussian-weighted splicing strategy that eliminates grid artifacts. Comprehensive evaluations demonstrate that ResDiT consistently delivers high-fidelity, high-resolution image synthesis and integrates seamlessly with downstream tasks, including spatially controlled generation.

中文标题: ResDiT：激发扩散变换器的内在分辨率可扩展性

中文摘要: 利用预训练的扩散变换器（DiTs）进行高分辨率（HR）图像合成通常会导致空间布局崩溃和纹理保真度下降。先前的工作通过复杂的流程来缓解这些问题，首先执行基础分辨率（即训练分辨率）去噪过程来指导高分辨率生成。我们转而探索DiTs的内在生成机制，并提出ResDiT，一种无需训练即可高效扩展分辨率的方法。我们识别了控制空间布局的核心因素——位置嵌入（PEs），并表明原始PEs在扩展到高分辨率时编码了错误的位置信息，这触发了布局崩溃。为了解决这个问题，我们引入了PE缩放技术，在分辨率变化下校正位置编码。为了进一步修复低保真度的细节，我们开发了基于基础分辨率局部注意力的局部增强机制。我们设计了一个聚合全局和局部线索的补丁级融合模块，以及一个消除网格伪影的高斯加权拼接策略。综合评估表明，ResDiT始终提供高保真度、高分辨率的图像合成，并与下游任务（包括空间控制生成）无缝集成。

</details>


### [132] [Language-Guided Open-World Anomaly Segmentation](https://arxiv.org/abs/2512.01427)
*Klara Reichard,Nikolas Brasch,Nassir Navab,Federico Tombari*

Main category: cs.CV

TL;DR: Clipomaly是首个基于CLIP的开放世界异常分割方法，无需异常训练数据，能在推理时动态扩展词汇表，为未知物体分配语义标签，在自动驾驶异常分割中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有开放世界和异常分割方法存在两个主要问题：1）无法为未知区域分配有意义的语义标签；2）难以区分和学习未知类别的表示。开放词汇分割方法虽然能泛化到新类别，但需要固定词汇表，不适用于未知类别不受约束的异常分割场景。

Method: 提出Clipomaly方法，基于CLIP的共享图像-文本嵌入空间，采用零样本方法无需异常特定训练数据。模型在推理时动态扩展词汇表而无需重新训练，利用语言引导为未知物体分配人类可解释的名称。

Result: Clipomaly在已建立的异常分割基准测试中实现了最先进的性能，能够稳健地检测和命名超出常见类别定义（如Cityscapes）的异常，同时提供了实际部署所需的可解释性和灵活性。

Conclusion: Clipomaly成功解决了开放世界异常分割中的关键挑战，通过语言引导实现了对未知物体的语义标注，为自动驾驶系统的实际部署提供了更可靠和可解释的解决方案。

Abstract: Open-world and anomaly segmentation methods seek to enable autonomous driving systems to detect and segment both known and unknown objects in real-world scenes. However, existing methods do not assign semantically meaningful labels to unknown regions, and distinguishing and learning representations for unknown classes remains difficult. While open-vocabulary segmentation methods show promise in generalizing to novel classes, they require a fixed inference vocabulary and thus cannot be directly applied to anomaly segmentation where unknown classes are unconstrained. We propose Clipomaly, the first CLIP-based open-world and anomaly segmentation method for autonomous driving. Our zero-shot approach requires no anomaly-specific training data and leverages CLIP's shared image-text embedding space to both segment unknown objects and assign human-interpretable names to them. Unlike open-vocabulary methods, our model dynamically extends its vocabulary at inference time without retraining, enabling robust detection and naming of anomalies beyond common class definitions such as those in Cityscapes. Clipomaly achieves state-of-the-art performance on established anomaly segmentation benchmarks while providing interpretability and flexibility essential for practical deployment.

中文标题: 语言引导的开放世界异常分割

中文摘要: 开放世界和异常分割方法旨在使自动驾驶系统能够在真实场景中检测和分割已知和未知物体。然而，现有方法无法为未知区域分配有语义意义的标签，并且区分和学习未知类别的表示仍然困难。虽然开放词汇分割方法在泛化到新类别方面显示出潜力，但它们需要固定的推理词汇表，因此无法直接应用于未知类别不受约束的异常分割。我们提出了Clipomaly，这是首个基于CLIP的自动驾驶开放世界和异常分割方法。我们的零样本方法不需要异常特定的训练数据，并利用CLIP共享的图像-文本嵌入空间来分割未知物体并为它们分配人类可解释的名称。与开放词汇方法不同，我们的模型在推理时动态扩展其词汇表而无需重新训练，能够稳健地检测和命名超出常见类别定义（如Cityscapes中的定义）的异常。Clipomaly在已建立的异常分割基准测试中实现了最先进的性能，同时提供了实际部署所需的可解释性和灵活性。

</details>


### [133] [FastAnimate: Towards Learnable Template Construction and Pose Deformation for Fast 3D Human Avatar Animation](https://arxiv.org/abs/2512.01444)
*Jian Shu,Nanjie Yao,Gangjian Zhang,Junlong Ren,Yu Feng,Hao Wang*

Main category: cs.CV

TL;DR: FastAnimate提出了一种统一的学习框架，通过U-Net架构快速生成人体模板，并采用数据驱动的细化技术增强结构完整性，在效率和动画质量之间取得平衡，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体化身动画方法通常分为两个阶段：规范模板构建和目标姿态变形。当前模板构建方法需要大量骨骼绑定，且对特定姿态会产生伪影。同时，目标姿态变形受到线性混合蒙皮（LBS）引起的结构扭曲影响，显著降低了动画的真实感。

Method: 提出统一学习框架分两阶段解决问题：1）使用U-Net架构在前馈过程中解耦纹理和姿态信息，实现快速人体模板生成；2）提出数据驱动的细化技术，通过增强结构完整性来改进目标姿态变形。

Result: 实验表明，该模型在不同姿态下提供一致性能，在效率和质量之间达到最佳平衡，超越了最先进的方法。

Conclusion: FastAnimate通过统一的学习框架解决了3D人体化身动画中的模板构建和姿态变形问题，实现了快速、高质量的动画生成，为相关领域提供了有效的解决方案。

Abstract: 3D human avatar animation aims at transforming a human avatar from an arbitrary initial pose to a specified target pose using deformation algorithms. Existing approaches typically divide this task into two stages: canonical template construction and target pose deformation. However, current template construction methods demand extensive skeletal rigging and often produce artifacts for specific poses. Moreover, target pose deformation suffers from structural distortions caused by Linear Blend Skinning (LBS), which significantly undermines animation realism. To address these problems, we propose a unified learning-based framework to address both challenges in two phases. For the former phase, to overcome the inefficiencies and artifacts during template construction, we leverage a U-Net architecture that decouples texture and pose information in a feed-forward process, enabling fast generation of a human template. For the latter phase, we propose a data-driven refinement technique that enhances structural integrity. Extensive experiments show that our model delivers consistent performance across diverse poses with an optimal balance between efficiency and quality,surpassing state-of-the-art (SOTA) methods.

中文标题: FastAnimate：面向快速3D人体化身动画的可学习模板构建与姿态变形

中文摘要: 3D人体化身动画旨在使用变形算法将人体化身从任意初始姿态转换为指定目标姿态。现有方法通常将此任务分为两个阶段：规范模板构建和目标姿态变形。然而，当前的模板构建方法需要大量骨骼绑定，并且通常会对特定姿态产生伪影。此外，目标姿态变形受到线性混合蒙皮（LBS）引起的结构扭曲影响，这显著削弱了动画的真实感。为了解决这些问题，我们提出了一个统一的学习框架，分两个阶段应对这两个挑战。对于前一阶段，为了克服模板构建过程中的低效率和伪影问题，我们利用U-Net架构在前馈过程中解耦纹理和姿态信息，实现快速生成人体模板。对于后一阶段，我们提出了一种数据驱动的细化技术来增强结构完整性。大量实验表明，我们的模型在不同姿态下提供一致性能，在效率和质量之间达到最佳平衡，超越了最先进的方法。

</details>


### [134] [ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling](https://arxiv.org/abs/2512.01481)
*Qisen Wang,Yifan Zhao,Peisen Shen,Jialu Li,Jia Li*

Main category: cs.CV

TL;DR: ChronosObserver是一种无需训练的方法，通过世界状态超空间和超空间引导采样，实现3D一致、时间同步的多视角视频生成，解决了现有方法在泛化和可扩展性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有相机控制视频生成模型难以直接生成3D一致且时间同步的多视角视频，这是构建4D世界的关键能力。现有方法依赖数据增强或测试时优化，但存在泛化能力有限和可扩展性问题。

Method: 提出无需训练的ChronosObserver方法，包含两个核心组件：1) 世界状态超空间：表示4D世界场景的时空约束；2) 超空间引导采样：利用超空间同步多视角的扩散采样轨迹。

Result: 实验证明该方法能够生成高保真、3D一致的时间同步多视角视频，无需对扩散模型进行额外训练或微调。

Conclusion: ChronosObserver通过创新的超空间表示和采样同步机制，有效解决了4D世界场景中多视角视频生成的挑战，为无需训练的3D一致视频生成提供了新思路。

Abstract: Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.

中文标题: ChronosObserver：通过超空间扩散采样驯服4D世界

中文摘要: 尽管主流的相机控制视频生成模型能够产生电影般的效果，但将其直接提升到生成3D一致且高保真的时间同步多视角视频仍然具有挑战性，而这正是驯服4D世界的关键能力。一些工作依赖于数据增强或测试时优化，但这些策略受到有限模型泛化能力和可扩展性问题的限制。为此，我们提出了ChronosObserver，这是一种无需训练的方法，包括用于表示4D世界场景时空约束的世界状态超空间，以及使用超空间同步多视角扩散采样轨迹的超空间引导采样。实验结果表明，我们的方法无需对扩散模型进行训练或微调，即可实现高保真和3D一致的时间同步多视角视频生成。

</details>


### [135] [A variational method for curve extraction with curvature-dependent energies](https://arxiv.org/abs/2512.01494)
*Majid Arthaud,Antonin Chambolle,Vincent Duval*

Main category: cs.CV

TL;DR: 提出一种基于变分方法的曲线提取技术，通过离散化能量和Smirnov分解定理，设计双层最小化方法从图像中自动提取曲线和1D结构，并扩展到曲率相关能量。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够自动从图像中提取曲线和1D结构的变分方法，特别是在处理曲率相关能量时，传统方法存在局限性。

Method: 基于能量离散化和Smirnov向量场分解定理，设计双层最小化方法；通过将曲线提升到位置-方向空间，使用子黎曼或Finsler度量处理曲率相关能量。

Result: 开发了一种主要无监督的曲线提取方法，能够处理曲率相关能量，通过空间提升技术有效提取复杂曲线结构。

Conclusion: 该变分方法为曲线提取提供了有效的框架，特别是通过空间提升技术处理曲率相关能量，在图像分析和计算机视觉中具有应用潜力。

Abstract: We introduce a variational approach for extracting curves between a list of possible endpoints, based on the discretization of an energy and Smirnov's decomposition theorem for vector fields. It is used to design a bi-level minimization approach to automatically extract curves and 1D structures from an image, which is mostly unsupervised. We extend then the method to curvature-dependent energies, using a now classical lifting of the curves in the space of positions and orientations equipped with an appropriate sub-Riemanian or Finslerian metric.

中文标题: 基于曲率相关能量的曲线提取变分方法

中文摘要: 我们提出了一种基于能量离散化和Smirnov向量场分解定理的变分方法，用于在可能端点列表之间提取曲线。该方法用于设计双层最小化方法，以主要无监督的方式从图像中自动提取曲线和1D结构。然后，我们通过将曲线提升到配备适当子黎曼或Finsler度量的位置-方向空间，将方法扩展到曲率相关能量。

</details>


### [136] [QuantumCanvas: A Multimodal Benchmark for Visual Learning of Atomic Interactions](https://arxiv.org/abs/2512.01519)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.CV

TL;DR: QuantumCanvas是一个多模态基准数据集，专注于原子对之间的量子相互作用，包含2,850个元素对，每个都有18种电子、热力学和几何特性，以及十通道图像表示。该数据集通过视觉和数值模态的结合，为学习可转移的量子相互作用提供了原则性基础。


<details>
  <summary>Details</summary>
Motivation: 当前分子和材料机器学习模型缺乏物理可转移性，它们主要拟合整个分子或晶体的相关性，而不是学习原子对之间的量子相互作用。然而，键合、电荷重新分布、轨道杂化和电子耦合都源于这些定义多体系统中局部量子场的二体相互作用。

Method: 创建了包含2,850个元素对的大规模多模态基准数据集，每个元素对标注了18种电子、热力学和几何特性，并配以十通道图像表示，这些图像源自l-和m-分辨的轨道密度、角场变换、共占据图和电荷密度投影。这些基于物理的图像编码了空间、角度和静电对称性，无需显式坐标。

Result: 在18个目标上对8种架构进行基准测试：GATv2在能隙上获得0.201 eV的MAE；EGNN在HOMO和LUMO上分别获得0.265 eV和0.274 eV的MAE；DimeNet在总能量上获得2.27 eV MAE，在排斥能量上获得0.132 eV MAE；多模态融合模型在Mermin自由能上获得2.15 eV MAE。在QuantumCanvas上预训练进一步提高了在QM9、MD17和CrysMTM等更大数据集上的收敛稳定性和泛化能力。

Conclusion: 通过将轨道物理与基于视觉的表征学习统一起来，QuantumCanvas为通过耦合视觉和数值模态学习可转移的量子相互作用提供了原则性和可解释的基础。该数据集和模型实现已开源。

Abstract: Despite rapid advances in molecular and materials machine learning, most models still lack physical transferability: they fit correlations across whole molecules or crystals rather than learning the quantum interactions between atomic pairs. Yet bonding, charge redistribution, orbital hybridization, and electronic coupling all emerge from these two-body interactions that define local quantum fields in many-body systems. We introduce QuantumCanvas, a large-scale multimodal benchmark that treats two-body quantum systems as foundational units of matter. The dataset spans 2,850 element-element pairs, each annotated with 18 electronic, thermodynamic, and geometric properties and paired with ten-channel image representations derived from l- and m-resolved orbital densities, angular field transforms, co-occupancy maps, and charge-density projections. These physically grounded images encode spatial, angular, and electrostatic symmetries without explicit coordinates, providing an interpretable visual modality for quantum learning. Benchmarking eight architectures across 18 targets, we report mean absolute errors of 0.201 eV on energy gap using GATv2, 0.265 eV on HOMO and 0.274 eV on LUMO using EGNN. For energy-related quantities, DimeNet attains 2.27 eV total-energy MAE and 0.132 eV repulsive-energy MAE, while a multimodal fusion model achieves a 2.15 eV Mermin free-energy MAE. Pretraining on QuantumCanvas further improves convergence stability and generalization when fine-tuned on larger datasets such as QM9, MD17, and CrysMTM. By unifying orbital physics with vision-based representation learning, QuantumCanvas provides a principled and interpretable basis for learning transferable quantum interactions through coupled visual and numerical modalities. Dataset and model implementations are available at https://github.com/KurbanIntelligenceLab/QuantumCanvas.

中文标题: QuantumCanvas：用于原子相互作用视觉学习的多模态基准

中文摘要: 尽管分子和材料机器学习发展迅速，但大多数模型仍缺乏物理可转移性：它们拟合整个分子或晶体的相关性，而不是学习原子对之间的量子相互作用。然而，键合、电荷重新分布、轨道杂化和电子耦合都源于这些定义多体系统中局部量子场的二体相互作用。我们引入了QuantumCanvas，这是一个大规模多模态基准，将二体量子系统视为物质的基本单元。该数据集涵盖2,850个元素对，每个都标注了18种电子、热力学和几何特性，并配以十通道图像表示，这些图像源自l-和m-分辨的轨道密度、角场变换、共占据图和电荷密度投影。这些基于物理的图像编码了空间、角度和静电对称性，无需显式坐标，为量子学习提供了可解释的视觉模态。在18个目标上对8种架构进行基准测试，我们报告了使用GATv2在能隙上的平均绝对误差为0.201 eV，使用EGNN在HOMO和LUMO上的平均绝对误差分别为0.265 eV和0.274 eV。对于能量相关量，DimeNet在总能量上获得2.27 eV MAE，在排斥能量上获得0.132 eV MAE，而多模态融合模型在Mermin自由能上获得2.15 eV MAE。在QuantumCanvas上预训练进一步提高了在QM9、MD17和CrysMTM等更大数据集上进行微调时的收敛稳定性和泛化能力。通过将轨道物理与基于视觉的表征学习统一起来，QuantumCanvas为通过耦合视觉和数值模态学习可转移的量子相互作用提供了原则性和可解释的基础。数据集和模型实现可在https://github.com/KurbanIntelligenceLab/QuantumCanvas获取。

</details>


### [137] [Diffusion Fuzzy System: Fuzzy Rule Guided Latent Multi-Path Diffusion Modeling](https://arxiv.org/abs/2512.01533)
*Hailong Yang,Te Zhang,Kup-sze Choi,Zhaohong Deng*

Main category: cs.CV

TL;DR: 本文提出了扩散模糊系统（DFS），一种由模糊规则引导的潜在空间多路径扩散模型，解决了传统扩散模型在处理异质特征时的协调效率低和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在处理具有显著特征差异的图像集合时存在困难，难以捕捉复杂特征并产生冲突结果。现有多路径方法协调效率低下且计算成本高昂，需要更有效的解决方案。

Method: 1. 使用多个专门学习特定类别图像特征的扩散路径；2. 采用基于规则链的推理动态引导扩散过程；3. 引入基于模糊隶属度的潜在空间压缩机制降低计算成本。

Result: 在LSUN Bedroom、LSUN Church和MS COCO数据集上的实验表明：DFS实现了更稳定的训练和更快的收敛速度；在图像质量和文本-图像对齐方面超越基线模型；生成图像与目标参考的对比准确性更高。

Conclusion: DFS通过模糊规则引导的多路径扩散建模，有效解决了传统扩散模型在处理异质特征时的协调和计算效率问题，在多个指标上优于现有方法。

Abstract: Diffusion models have emerged as a leading technique for generating images due to their ability to create high-resolution and realistic images. Despite their strong performance, diffusion models still struggle in managing image collections with significant feature differences. They often fail to capture complex features and produce conflicting results. Research has attempted to address this issue by learning different regions of an image through multiple diffusion paths and then combining them. However, this approach leads to inefficient coordination among multiple paths and high computational costs. To tackle these issues, this paper presents a Diffusion Fuzzy System (DFS), a latent-space multi-path diffusion model guided by fuzzy rules. DFS offers several advantages. First, unlike traditional multi-path diffusion methods, DFS uses multiple diffusion paths, each dedicated to learning a specific class of image features. By assigning each path to a different feature type, DFS overcomes the limitations of multi-path models in capturing heterogeneous image features. Second, DFS employs rule-chain-based reasoning to dynamically steer the diffusion process and enable efficient coordination among multiple paths. Finally, DFS introduces a fuzzy membership-based latent-space compression mechanism to reduce the computational costs of multi-path diffusion effectively. We tested our method on three public datasets: LSUN Bedroom, LSUN Church, and MS COCO. The results show that DFS achieves more stable training and faster convergence than existing single-path and multi-path diffusion models. Additionally, DFS surpasses baseline models in both image quality and alignment between text and images, and also shows improved accuracy when comparing generated images to target references.

中文标题: 扩散模糊系统：模糊规则引导的潜在多路径扩散建模

中文摘要: 扩散模型因其能够生成高分辨率和逼真图像的能力，已成为图像生成领域的主导技术。尽管性能强大，扩散模型在处理具有显著特征差异的图像集合时仍然存在困难。它们往往难以捕捉复杂特征并产生冲突结果。已有研究尝试通过多个扩散路径学习图像的不同区域，然后将它们组合来解决这个问题。然而，这种方法导致多个路径之间协调效率低下且计算成本高昂。为了解决这些问题，本文提出了扩散模糊系统（DFS），一种由模糊规则引导的潜在空间多路径扩散模型。DFS具有多个优势。首先，与传统的多路径扩散方法不同，DFS使用多个扩散路径，每个路径专门学习特定类别的图像特征。通过为每个路径分配不同的特征类型，DFS克服了多路径模型在捕捉异质图像特征方面的局限性。其次，DFS采用基于规则链的推理来动态引导扩散过程，实现多个路径之间的高效协调。最后，DFS引入了基于模糊隶属度的潜在空间压缩机制，有效降低多路径扩散的计算成本。我们在三个公共数据集上测试了我们的方法：LSUN卧室、LSUN教堂和MS COCO。结果表明，与现有的单路径和多路径扩散模型相比，DFS实现了更稳定的训练和更快的收敛速度。此外，DFS在图像质量以及文本与图像的对齐方面都超越了基线模型，并且在比较生成图像与目标参考图像时也显示出更高的准确性。

</details>


### [138] [Deep Unsupervised Anomaly Detection in Brain Imaging: Large-Scale Benchmarking and Bias Analysis](https://arxiv.org/abs/2512.01534)
*Alexander Frotscher,Christian F. Baumgartner,Thomas Wolfers*

Main category: cs.CV

TL;DR: 该研究建立了首个大规模、多中心的脑影像深度无监督异常检测基准，评估了多种算法在脑部MRI异常检测中的性能，发现重建类方法（特别是扩散模型）在病灶分割上表现最佳，但所有算法都存在系统性偏差，如扫描仪效应、小病灶漏检等，表明当前方法主要受算法限制而非数据不足。


<details>
  <summary>Details</summary>
Motivation: 脑部磁共振成像的深度无监督异常检测无需病灶特异性标注即可识别病理异常，具有临床转化潜力。然而，碎片化的评估、异构的数据集和不一致的指标阻碍了临床转化进展。本研究旨在建立标准化的大规模基准，系统评估算法性能并识别系统性偏差。

Method: 研究构建了大规模多中心数据集：训练集包含来自健康个体的2,976个T1和2,972个T2加权扫描（6-89岁，6台扫描仪）；验证集92个扫描用于超参数调优和无偏阈值估计；测试集包含2,221个T1w和1,262个T2w扫描，涵盖健康数据集和多种临床队列。系统评估了不同算法在扫描仪、病灶类型/大小、人口统计学（年龄、性别）等因素下的鲁棒性。

Result: 所有算法的Dice分割性能在0.03-0.65之间，显示巨大变异性。重建类方法（特别是扩散模型）在病灶分割上表现最强，而基于特征的方法在分布偏移下表现出更好的鲁棒性。但大多数算法存在系统性偏差：扫描仪相关效应明显；小病灶和低对比度病灶更容易漏检；假阳性随年龄和性别变化。增加健康训练数据仅带来有限改善，表明当前方法主要受算法限制而非数据不足。

Conclusion: 该基准为未来研究建立了透明基础，并明确了临床转化的优先方向：图像原生预训练、原则性偏差度量、公平感知建模和鲁棒域适应。当前无监督异常检测框架主要受算法限制，而非数据可用性问题。

Abstract: Deep unsupervised anomaly detection in brain magnetic resonance imaging offers a promising route to identify pathological deviations without requiring lesion-specific annotations. Yet, fragmented evaluations, heterogeneous datasets, and inconsistent metrics have hindered progress toward clinical translation. Here, we present a large-scale, multi-center benchmark of deep unsupervised anomaly detection for brain imaging. The training cohort comprised 2,976 T1 and 2,972 T2-weighted scans from healthy individuals across six scanners, with ages ranging from 6 to 89 years. Validation used 92 scans to tune hyperparameters and estimate unbiased thresholds. Testing encompassed 2,221 T1w and 1,262 T2w scans spanning healthy datasets and diverse clinical cohorts. Across all algorithms, the Dice-based segmentation performance varied between 0.03 and 0.65, indicating substantial variability. To assess robustness, we systematically evaluated the impact of different scanners, lesion types and sizes, as well as demographics (age, sex). Reconstruction-based methods, particularly diffusion-inspired approaches, achieved the strongest lesion segmentation performance, while feature-based methods showed greater robustness under distributional shifts. However, systematic biases, such as scanner-related effects, were observed for the majority of algorithms, including that small and low-contrast lesions were missed more often, and that false positives varied with age and sex. Increasing healthy training data yields only modest gains, underscoring that current unsupervised anomaly detection frameworks are limited algorithmically rather than by data availability. Our benchmark establishes a transparent foundation for future research and highlights priorities for clinical translation, including image native pretraining, principled deviation measures, fairness-aware modeling, and robust domain adaptation.

中文标题: 脑影像深度无监督异常检测：大规模基准测试与偏差分析

中文摘要: 脑磁共振成像的深度无监督异常检测提供了一条无需病灶特异性标注即可识别病理偏差的有前景途径。然而，碎片化的评估、异构的数据集和不一致的指标阻碍了临床转化进展。在此，我们提出了一个用于脑影像深度无监督异常检测的大规模、多中心基准。训练队列包含来自健康个体的2,976个T1和2,972个T2加权扫描，涵盖六台扫描仪，年龄范围从6岁到89岁。验证使用92个扫描来调整超参数并估计无偏阈值。测试包含2,221个T1w和1,262个T2w扫描，涵盖健康数据集和多样化的临床队列。在所有算法中，基于Dice的分割性能在0.03到0.65之间变化，表明存在显著变异性。为了评估鲁棒性，我们系统评估了不同扫描仪、病灶类型和大小以及人口统计学（年龄、性别）的影响。基于重建的方法，特别是受扩散启发的算法，实现了最强的病灶分割性能，而基于特征的方法在分布偏移下表现出更好的鲁棒性。然而，大多数算法存在系统性偏差，例如扫描仪相关效应，包括小病灶和低对比度病灶更容易被漏检，以及假阳性随年龄和性别变化。增加健康训练数据仅带来适度改善，强调当前无监督异常检测框架在算法上受限而非数据可用性。我们的基准为未来研究建立了透明基础，并突出了临床转化的优先事项，包括图像原生预训练、原则性偏差度量、公平感知建模和鲁棒域适应。

</details>


### [139] [FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention](https://arxiv.org/abs/2512.01540)
*Zipeng Wang,Dan Xu*

Main category: cs.CV

TL;DR: FlashVGGT提出了一种高效的视觉几何变换器，通过描述符压缩注意力机制解决了VGGT中全局自注意力计算复杂度高的问题，将推理时间减少到VGGT的9.3%，并能扩展到3000张以上图像序列。


<details>
  <summary>Details</summary>
Motivation: 现有VGGT等3D重建方法使用全自注意力机制处理所有图像标记，导致二次复杂度问题，在处理长图像序列时计算开销巨大，可扩展性差。

Method: 提出描述符注意力机制：将每帧空间信息压缩为紧凑的描述符标记，然后通过图像标记与描述符之间的交叉注意力计算全局关系，大大减少计算量。同时采用分块递归机制重用先前分块的缓存描述符，支持在线长序列推理。

Result: FlashVGGT在重建精度上与VGGT相当，但在1000张图像时推理时间仅为VGGT的9.3%，并能高效扩展到超过3000张图像的长序列。

Conclusion: FlashVGGT通过描述符压缩注意力机制有效解决了视觉几何变换器的计算瓶颈，实现了高效可扩展的3D重建，为长序列多视图重建提供了实用解决方案。

Abstract: 3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.

中文标题: FlashVGGT：具有压缩描述符注意力的高效可扩展视觉几何变换器

中文摘要: 从多视图图像进行3D重建是计算机视觉的核心挑战。最近，前馈方法已成为传统逐场景优化技术的高效稳健替代方案。其中，最先进的模型如视觉几何接地变换器（VGGT）利用所有图像标记的完全自注意力来捕捉全局关系。然而，由于自注意力的二次复杂性和长图像序列中生成的大量标记，这种方法存在可扩展性差的问题。在这项工作中，我们引入了FlashVGGT，一种通过描述符注意力机制解决这一瓶颈的高效替代方案。FlashVGGT不是在所有标记上应用密集的全局注意力，而是将每帧的空间信息压缩为一组紧凑的描述符标记。然后，全局注意力计算为完整图像标记集与这个较小描述符集之间的交叉注意力，显著减少了计算开销。此外，描述符的紧凑性使得通过分块递归机制能够在线推理长序列，该机制重用先前分块的缓存描述符。实验结果表明，FlashVGGT实现了与VGGT竞争的重建精度，同时对于1000张图像，推理时间仅为VGGT的9.3%，并能高效扩展到超过3000张图像的序列。

</details>


### [140] [RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions](https://arxiv.org/abs/2512.01582)
*Junran Peng,Yiheng Huang,Silei Shen,Zeji Wei,Jingwei Yang,Baojie Wang,Yonghao He,Chuanchen Luo,Man Zhang,Xucheng Yin,Wei Sui*

Main category: cs.CV

TL;DR: RoleMotion是一个大规模人体运动数据集，专注于场景特定的角色扮演运动合成，包含25个经典场景、110个功能角色、500多种行为、10296个高质量人体和手部运动序列，以及27831个细粒度文本描述，支持文本驱动的全身运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有文本运动数据集存在以下问题：1) 数据是分散构建的，运动数据是非功能性的且孤立，难以覆盖各种场景中的社交活动；2) 运动数据质量不一致；3) 文本标注缺乏细粒度细节。因此需要构建一个专注于场景和角色的大规模高质量数据集。

Method: 1) 精心设计和收集数据集，重点关注场景和角色；2) 构建比现有方法更强的评估器，并证明其可靠性；3) 在数据集上评估各种文本到运动方法；4) 探索身体和手部运动生成的相互作用。

Result: 1) 创建了包含25个场景、110个角色、500+行为、10296个高质量运动序列和27831个细粒度文本描述的数据集；2) 构建了可靠的评估器；3) 实验结果表明数据集在文本驱动全身生成方面具有高质量和功能性。

Conclusion: RoleMotion是一个高质量、功能性强的大规模人体运动数据集，专注于场景特定的角色扮演运动合成，通过细粒度文本描述支持文本驱动的全身运动生成，为相关研究提供了有价值的资源。

Abstract: In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these datasets. In contrast, RoleMotion is meticulously designed and collected with a particular focus on scenes and roles. The dataset features 25 classic scenes, 110 functional roles, over 500 behaviors, and 10296 high-quality human motion sequences of body and hands, annotated with 27831 fine-grained text descriptions. We build an evaluator stronger than existing counterparts, prove its reliability, and evaluate various text-to-motion methods on our dataset. Finally, we explore the interplay of motion generation of body and hands. Experimental results demonstrate the high-quality and functionality of our dataset on text-driven whole-body generation.

中文标题: RoleMotion：面向具有细粒度描述的鲁棒场景特定角色扮演运动合成的大规模数据集

中文摘要: 本文介绍了RoleMotion，这是一个大规模的人体运动数据集，包含了丰富的角色扮演和功能性运动数据，专门适应各种特定场景。现有的文本数据集主要是分散构建的，作为各种子集的混合体，其数据是非功能性的且孤立的，难以协同工作以覆盖各种场景中的社交活动。此外，这些数据集中运动数据的质量不一致，文本标注缺乏细粒度细节。相比之下，RoleMotion是精心设计和收集的，特别关注场景和角色。该数据集包含25个经典场景、110个功能角色、超过500种行为以及10296个高质量的人体和手部运动序列，标注了27831个细粒度文本描述。我们构建了一个比现有对应物更强的评估器，证明了其可靠性，并在我们的数据集上评估了各种文本到运动方法。最后，我们探索了身体和手部运动生成的相互作用。实验结果表明我们的数据集在文本驱动全身生成方面具有高质量和功能性。

</details>


### [141] [Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation](https://arxiv.org/abs/2512.01589)
*Thao Thi Phuong Dao,Tan-Cong Nguyen,Trong-Le Do,Truong Hoang Viet,Nguyen Chi Thanh,Huynh Nguyen Thuan,Do Vo Cong Nguyen,Minh-Khoi Pham,Mai-Khiem Tran,Viet-Tham Huynh,Trong-Thuan Nguyen,Trung-Nghia Le,Vo Thanh Toan,Tam V. Nguyen,Minh-Triet Tran,Thanh Dinh Le*

Main category: cs.CV

TL;DR: 本研究提出了AbscessHeNe数据集，包含4,926张头颈部脓肿CT切片，用于开发语义分割模型和未来基于内容的检索系统。最佳模型Dice系数为0.39，显示任务挑战性。


<details>
  <summary>Details</summary>
Motivation: 头颈部脓肿是可能危及生命的急性感染，需要及时诊断和治疗。准确检测和勾画这些病变在影像学上对临床决策至关重要，但目前缺乏专门针对此任务的标注数据集。

Method: 创建了AbscessHeNe数据集，包含4,926张对比增强CT切片，每张都有像素级脓肿标注和临床元数据。评估了CNN、Transformer和Mamba等多种最先进的分割架构作为性能基准。

Result: 最佳模型性能：Dice相似系数0.39，交并比0.27，归一化表面距离0.67。这些相对较低的指标表明头颈部脓肿分割具有挑战性，需要进一步研究改进。

Conclusion: AbscessHeNe数据集为头颈部脓肿分割研究提供了重要资源，并为未来基于内容的索引和检索系统奠定了基础。当前分割性能有限，需要更先进的方法来改善临床实用性。

Abstract: Abscesses in the head and neck represent an acute infectious process that can potentially lead to sepsis or mortality if not diagnosed and managed promptly. Accurate detection and delineation of these lesions on imaging are essential for diagnosis, treatment planning, and surgical intervention. In this study, we introduce AbscessHeNe, a curated and comprehensively annotated dataset comprising 4,926 contrast-enhanced CT slices with clinically confirmed head and neck abscesses. The dataset is designed to facilitate the development of robust semantic segmentation models that can accurately delineate abscess boundaries and evaluate deep neck space involvement, thereby supporting informed clinical decision-making. To establish performance baselines, we evaluate several state-of-the-art segmentation architectures, including CNN, Transformer, and Mamba-based models. The highest-performing model achieved a Dice Similarity Coefficient of 0.39, Intersection-over-Union of 0.27, and Normalized Surface Distance of 0.67, indicating the challenges of this task and the need for further research. Beyond segmentation, AbscessHeNe is structured for future applications in content-based multimedia indexing and case-based retrieval. Each CT scan is linked with pixel-level annotations and clinical metadata, providing a foundation for building intelligent retrieval systems and supporting knowledge-driven clinical workflows. The dataset will be made publicly available at https://github.com/drthaodao3101/AbscessHeNe.git.

中文标题: 基于脓肿分割的头颈部CT内容索引与检索研究

中文摘要: 头颈部脓肿是一种急性感染过程，如不及时诊断和处理可能导致败血症或死亡。在影像学上准确检测和勾画这些病变对于诊断、治疗规划和手术干预至关重要。本研究介绍了AbscessHeNe，这是一个经过精心整理和全面标注的数据集，包含4,926张临床确诊的头颈部脓肿对比增强CT切片。该数据集旨在促进开发能够准确勾画脓肿边界并评估深部颈部间隙受累情况的鲁棒语义分割模型，从而支持临床决策。为建立性能基准，我们评估了几种最先进的分割架构，包括CNN、Transformer和Mamba模型。性能最佳的模型实现了0.39的Dice相似系数、0.27的交并比和0.67的归一化表面距离，表明该任务的挑战性以及进一步研究的必要性。除了分割之外，AbscessHeNe还针对未来基于内容的多媒体索引和基于案例的检索应用进行了结构化设计。每个CT扫描都链接了像素级标注和临床元数据，为构建智能检索系统和支持知识驱动的临床工作流程奠定了基础。该数据集将在https://github.com/drthaodao3101/AbscessHeNe.git公开提供。

</details>


### [142] [Depth Matching Method Based on ShapeDTW for Oil-Based Mud Imager](https://arxiv.org/abs/2512.01611)
*Fengfeng Li,Zhou Feng,Hongliang Wu,Hao Zhang,Han Tian,Peng Liu,Lixin Yuan*

Main category: cs.CV

TL;DR: 基于ShapeDTW的油基泥浆成像仪深度匹配方法，通过提取局部形状特征构建形态敏感距离矩阵，使用HOG1D和原始信号组合特征，实现复杂纹理、深度偏移或局部缩放图像的精确对齐。


<details>
  <summary>Details</summary>
Motivation: 油基泥浆微电阻率成像仪采用上下垫片交错设计，即使在速度校正后，垫片图像之间仍存在深度不对齐问题，需要更精确的深度匹配方法。

Method: 提出基于形状动态时间规整（ShapeDTW）算法的深度匹配方法，提取局部形状特征构建形态敏感距离矩阵，采用一维梯度方向直方图（HOG1D）和原始信号的组合特征作为形状描述符。

Result: 现场测试表明，该方法能够对具有复杂纹理、深度偏移或局部缩放的图像实现精确对齐，并为特征扩展提供了灵活框架。

Conclusion: 基于ShapeDTW的深度匹配方法有效解决了油基泥浆成像仪的深度对齐问题，具有较好的适应性和可扩展性。

Abstract: In well logging operations using the oil-based mud (OBM) microresistivity imager, which employs an interleaved design with upper and lower pad sets, depth misalignment issues persist between the pad images even after velocity correction. This paper presents a depth matching method for borehole images based on the Shape Dynamic Time Warping (ShapeDTW) algorithm. The method extracts local shape features to construct a morphologically sensitive distance matrix, better preserving structural similarity between sequences during alignment. We implement this by employing a combined feature set of the one-dimensional Histogram of Oriented Gradients (HOG1D) and the original signal as the shape descriptor. Field test examples demonstrate that our method achieves precise alignment for images with complex textures, depth shifts, or local scaling. Furthermore, it provides a flexible framework for feature extension, allowing the integration of other descriptors tailored to specific geological features.

中文标题: 基于ShapeDTW的油基泥浆成像仪深度匹配方法

中文摘要: 在使用油基泥浆（OBM）微电阻率成像仪的测井作业中，由于采用上下垫片交错设计，即使在速度校正后，垫片图像之间仍存在深度不对齐问题。本文提出了一种基于形状动态时间规整（ShapeDTW）算法的井眼图像深度匹配方法。该方法提取局部形状特征构建形态敏感距离矩阵，在序列对齐过程中更好地保持结构相似性。我们通过采用一维梯度方向直方图（HOG1D）和原始信号的组合特征作为形状描述符来实现这一目标。现场测试实例表明，我们的方法能够对具有复杂纹理、深度偏移或局部缩放的图像实现精确对齐。此外，它提供了一个灵活的特征扩展框架，允许集成针对特定地质特征定制的其他描述符。

</details>


### [143] [Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2512.01636)
*Xin Wang,Haipeng Zhang,Mang Li,Zhaohui Xia,Yueguo Chen,Yu Zhang,Chunyu Wei*

Main category: cs.CV

TL;DR: Fusion-Diff是一个用于零样本组合图像检索的生成式编辑框架，通过在联合视觉-语言空间中进行多模态融合特征编辑，显著缩小模态差距，仅需20万合成样本就能实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 监督式组合图像检索方法需要昂贵的三元组标注，这促使了零样本解决方案的发展。现有基于文本或扩散的方法难以有效弥合视觉-语言模态差距，因此需要一种更有效的多模态对齐方法。

Method: 提出Fusion-Diff框架：1）在联合视觉-语言空间中进行多模态融合特征编辑策略，显著缩小模态差距；2）引入轻量级Control-Adapter，仅需在20万合成样本上进行微调就能实现高性能。

Result: 在标准CIR基准测试（CIRR、FashionIQ和CIRCO）上，Fusion-Diff显著优于先前的零样本方法，展示了卓越的性能表现。

Conclusion: Fusion-Diff通过联合视觉-语言空间中的生成式编辑，有效解决了零样本组合图像检索中的模态对齐问题，同时保持了高数据效率，为多模态检索提供了新思路。

Abstract: Composed Image Retrieval (CIR) enables fine-grained visual search by combining a reference image with a textual modification. While supervised CIR methods achieve high accuracy, their reliance on costly triplet annotations motivates zero-shot solutions. The core challenge in zero-shot CIR (ZS-CIR) stems from a fundamental dilemma: existing text-centric or diffusion-based approaches struggle to effectively bridge the vision-language modality gap. To address this, we propose Fusion-Diff, a novel generative editing framework with high effectiveness and data efficiency designed for multimodal alignment. First, it introduces a multimodal fusion feature editing strategy within a joint vision-language (VL) space, substantially narrowing the modality gap. Second, to maximize data efficiency, the framework incorporates a lightweight Control-Adapter, enabling state-of-the-art performance through fine-tuning on only a limited-scale synthetic dataset of 200K samples. Extensive experiments on standard CIR benchmarks (CIRR, FashionIQ, and CIRCO) demonstrate that Fusion-Diff significantly outperforms prior zero-shot approaches. We further enhance the interpretability of our model by visualizing the fused multimodal representations.

中文标题: 联合视觉-语言空间中的生成式编辑用于零样本组合图像检索

中文摘要: 组合图像检索通过结合参考图像和文本修改实现细粒度视觉搜索。虽然监督式CIR方法能达到高精度，但对昂贵三元组标注的依赖促使了零样本解决方案的发展。零样本CIR的核心挑战源于一个基本困境：现有的基于文本或扩散的方法难以有效弥合视觉-语言模态差距。为此，我们提出了Fusion-Diff，一个具有高有效性和数据效率的新型生成式编辑框架，专为多模态对齐设计。首先，它在联合视觉-语言空间中引入了多模态融合特征编辑策略，显著缩小了模态差距。其次，为了最大化数据效率，该框架包含一个轻量级Control-Adapter，仅需在20万合成样本上进行微调就能实现最先进的性能。在标准CIR基准测试（CIRR、FashionIQ和CIRCO）上的大量实验表明，Fusion-Diff显著优于先前的零样本方法。我们通过可视化融合的多模态表示进一步增强了模型的可解释性。

</details>


### [144] [ViT$^3$: Unlocking Test-Time Training in Vision](https://arxiv.org/abs/2512.01643)
*Dongchen Han,Yining Li,Tianyu Li,Zixuan Cao,Ziming Wang,Jun Song,Yu Cheng,Bo Zheng,Gao Huang*

Main category: cs.CV

TL;DR: ViT$^3$是一种纯测试时训练（TTT）架构，通过将注意力重新表述为在线学习问题，实现了线性计算复杂度和可并行计算，在各种视觉任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 测试时训练（TTT）为高效序列建模提供了新方向，但视觉领域的TTT设计缺乏系统研究和实践指导，需要建立设计原则来构建有效的视觉TTT模型。

Method: 通过系统的实证研究分析TTT设计选择，提炼出六个实用见解作为设计原则，基于这些原则开发了ViT$^3$模型——一个纯TTT架构，将注意力操作重新表述为在线学习问题，在测试时从键值对构建内部模型。

Result: ViT$^3$在图像分类、图像生成、对象检测和语义分割等多样视觉任务上，始终匹配或优于Mamba和线性注意力变体等先进线性复杂度模型，有效缩小了与高度优化的视觉Transformer之间的性能差距。

Conclusion: 该研究为视觉TTT模型建立了系统的设计原则，提出的ViT$^3$基线模型展示了TTT在视觉任务中的潜力，为未来高效视觉序列建模研究提供了重要参考。

Abstract: Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling. TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time. This reformulation opens a rich and flexible design space while achieving linear computational complexity. However, crafting a powerful visual TTT design remains challenging: fundamental choices for the inner module and inner training lack comprehensive understanding and practical guidelines. To bridge this critical gap, in this paper, we present a systematic empirical study of TTT designs for visual sequence modeling. From a series of experiments and analyses, we distill six practical insights that establish design principles for effective visual TTT and illuminate paths for future improvement. These findings culminate in the Vision Test-Time Training (ViT$^3$) model, a pure TTT architecture that achieves linear complexity and parallelizable computation. We evaluate ViT$^3$ across diverse visual tasks, including image classification, image generation, object detection, and semantic segmentation. Results show that ViT$^3$ consistently matches or outperforms advanced linear-complexity models (e.g., Mamba and linear attention variants) and effectively narrows the gap to highly optimized vision Transformers. We hope this study and the ViT$^3$ baseline can facilitate future work on visual TTT models. Code is available at https://github.com/LeapLabTHU/ViTTT.

中文标题: ViT$^3$：解锁视觉中的测试时训练

中文摘要: 测试时训练（TTT）最近已成为高效序列建模的一个有前景的方向。TTT将注意力操作重新表述为一个在线学习问题，在测试时从键值对构建一个紧凑的内部模型。这种重新表述开启了一个丰富而灵活的设计空间，同时实现了线性计算复杂度。然而，构建一个强大的视觉TTT设计仍然具有挑战性：对于内部模块和内部训练的基本选择缺乏全面的理解和实践指导。为了弥合这一关键差距，本文对视觉序列建模的TTT设计进行了系统的实证研究。通过一系列实验和分析，我们提炼出六个实用见解，为有效的视觉TTT建立了设计原则，并为未来的改进指明了方向。这些发现最终形成了视觉测试时训练（ViT$^3$）模型，这是一个纯TTT架构，实现了线性复杂度和可并行计算。我们在各种视觉任务上评估ViT$^3$，包括图像分类、图像生成、对象检测和语义分割。结果表明，ViT$^3$始终匹配或优于先进的线性复杂度模型（例如Mamba和线性注意力变体），并有效缩小了与高度优化的视觉Transformer之间的差距。我们希望这项研究和ViT$^3$基线能够促进未来视觉TTT模型的工作。代码可在https://github.com/LeapLabTHU/ViTTT获取。

</details>


### [145] [Bridging the Scale Gap: Balanced Tiny and General Object Detection in Remote Sensing Imagery](https://arxiv.org/abs/2512.01665)
*Zhicheng Zhao,Yin Huang,Lingma Sun,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 提出了ScaleBridge-Det框架，通过尺度自适应专家路由和密度引导查询分配，解决了遥感影像中微小目标与大型目标共存时的尺度不平衡检测问题。


<details>
  <summary>Details</summary>
Motivation: 遥感影像中微小目标检测面临的主要挑战是：在密集微小目标与大型目标共存的场景中，现有方法难以实现跨尺度的平衡检测性能。虽然大型基础模型在通用视觉任务上表现出色，但由于遥感影像的极端尺度变化和密度分布特性，它们在微小目标检测中的应用尚未被探索。

Method: 提出了ScaleBridge-Det框架，包含两个核心模块：1）路由增强混合注意力（REM）模块，通过自适应路由动态选择和融合尺度特定的专家特征，生成互补的多尺度表示；2）密度引导动态查询（DGQ）模块，预测目标密度以自适应调整查询位置和数量，实现高效资源分配。

Result: 在AI-TOD-V2和DTOD基准数据集上实现了最先进的性能，同时在VisDrone跨域数据集上表现出优越的鲁棒性。该框架能够同时优化密集微小目标和通用目标的检测性能，无需权衡。

Conclusion: ScaleBridge-Det是首个专为遥感影像微小目标检测设计的大型检测框架，通过创新的尺度自适应专家路由和密度引导查询分配机制，成功弥合了不同尺度目标之间的检测性能鸿沟，为遥感影像目标检测提供了新的解决方案。

Abstract: Tiny object detection in remote sensing imagery has attracted significant research interest in recent years. Despite recent progress, achieving balanced detection performance across diverse object scales remains a formidable challenge, particularly in scenarios where dense tiny objects and large objects coexist. Although large foundation models have revolutionized general vision tasks, their application to tiny object detection remains unexplored due to the extreme scale variation and density distribution inherent to remote sensing imagery. To bridge this scale gap, we propose ScaleBridge-Det, to the best of our knowledge, the first large detection framework designed for tiny objects, which could achieve balanced performance across diverse scales through scale-adaptive expert routing and density-guided query allocation. Specifically, we introduce a Routing-Enhanced Mixture Attention (REM) module that dynamically selects and fuses scale-specific expert features via adaptive routing to address the tendency of standard MoE models to favor dominant scales. REM generates complementary and discriminative multi-scale representations suitable for both tiny and large objects. Furthermore, we present a Density-Guided Dynamic Query (DGQ) module that predicts object density to adaptively adjust query positions and numbers, enabling efficient resource allocation for objects of varying scales. The proposed framework allows ScaleBridge-Det to simultaneously optimize performance for both dense tiny and general objects without trade-offs. Extensive experiments on benchmark and cross-domain datasets demonstrate that ScaleBridge-Det achieves state-of-the-art performance on AI-TOD-V2 and DTOD, while exhibiting superior cross-domain robustness on VisDrone.

中文标题: 弥合尺度鸿沟：遥感影像中平衡的微小与通用目标检测

中文摘要: 近年来，遥感影像中的微小目标检测引起了广泛的研究兴趣。尽管已有进展，但在密集微小目标与大型目标共存的场景中，实现跨不同目标尺度的平衡检测性能仍然是一个严峻挑战。虽然大型基础模型已经革新了通用视觉任务，但由于遥感影像固有的极端尺度变化和密度分布特性，它们在微小目标检测中的应用尚未被探索。为了弥合这一尺度鸿沟，我们提出了ScaleBridge-Det，据我们所知，这是首个专为微小目标设计的大型检测框架，通过尺度自适应专家路由和密度引导查询分配，能够实现跨不同尺度的平衡性能。具体而言，我们引入了路由增强混合注意力（REM）模块，通过自适应路由动态选择和融合尺度特定的专家特征，以解决标准MoE模型倾向于主导尺度的问题。REM生成互补且具有区分性的多尺度表示，适用于微小和大型目标。此外，我们提出了密度引导动态查询（DGQ）模块，预测目标密度以自适应调整查询位置和数量，实现对不同尺度目标的高效资源分配。所提出的框架使ScaleBridge-Det能够同时优化密集微小目标和通用目标的性能，无需权衡。在基准和跨域数据集上的大量实验表明，ScaleBridge-Det在AI-TOD-V2和DTOD上实现了最先进的性能，同时在VisDrone上表现出优越的跨域鲁棒性。

</details>


### [146] [GRASP: Guided Residual Adapters with Sample-wise Partitioning](https://arxiv.org/abs/2512.01675)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: GRASP提出了一种用于长尾文本到图像生成的引导残差适配器方法，通过样本划分和特定簇的适配器微调来解决梯度冲突问题，在医学影像和ImageNet-LT数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型在长尾分布场景（如医学影像中的罕见病理）中表现不佳，存在模式坍塌问题，导致尾部类别输出质量差、多样性不足，这限制了合成数据增强在罕见条件下的应用。研究发现频繁头部类别和罕见尾部类别之间的梯度冲突是主要原因。

Method: GRASP使用外部先验知识将样本静态划分为最小化组内梯度冲突的簇，然后在预训练模型的transformer前馈层中注入簇特定的残差适配器进行微调，绕过学习到的门控机制以确保稳定性和效率。

Result: 在长尾MIMIC-CXR-LT数据集上，GRASP在FID和多样性指标上表现优异，特别是对于罕见类别，优于基线方法如普通微调和混合专家变体。在NIH-CXR-LT的下游分类任务中，尾部标签的分类性能显著提升。在ImageNet-LT上的泛化实验证实了方法的广泛适用性。

Conclusion: GRASP是一种轻量级、可扩展的方法，能够有效解决长尾文本到图像生成中的梯度冲突问题，提高罕见类别的生成质量和多样性，可直接集成到现有扩散管道中。

Abstract: Recent advances in text-to-image diffusion models enable high-fidelity generation across diverse prompts. However, these models falter in long-tail settings, such as medical imaging, where rare pathologies comprise a small fraction of data. This results in mode collapse: tail-class outputs lack quality and diversity, undermining the goal of synthetic data augmentation for underrepresented conditions. We pinpoint gradient conflicts between frequent head and rare tail classes as the primary culprit, a factor unaddressed by existing sampling or conditioning methods that mainly steer inference without altering the learned distribution. To resolve this, we propose GRASP: Guided Residual Adapters with Sample-wise Partitioning. GRASP uses external priors to statically partition samples into clusters that minimize intra-group gradient clashes. It then fine-tunes pre-trained models by injecting cluster-specific residual adapters into transformer feedforward layers, bypassing learned gating for stability and efficiency. On the long-tail MIMIC-CXR-LT dataset, GRASP yields superior FID and diversity metrics, especially for rare classes, outperforming baselines like vanilla fine-tuning and Mixture of Experts variants. Downstream classification on NIH-CXR-LT improves considerably for tail labels. Generalization to ImageNet-LT confirms broad applicability. Our method is lightweight, scalable, and readily integrates with diffusion pipelines.

中文标题: GRASP：基于样本划分的引导残差适配器

中文摘要: 文本到图像扩散模型的最新进展使得能够根据多样化的提示生成高保真图像。然而，这些模型在长尾设置下表现不佳，例如医学影像领域，其中罕见病理仅占数据的一小部分。这导致模式坍塌：尾部类别的输出缺乏质量和多样性，从而破坏了为代表性不足条件进行合成数据增强的目标。我们确定频繁头部类别和罕见尾部类别之间的梯度冲突是主要原因，这是现有采样或条件方法未解决的问题，这些方法主要指导推理而不改变学习到的分布。为了解决这个问题，我们提出了GRASP：基于样本划分的引导残差适配器。GRASP使用外部先验知识将样本静态划分为最小化组内梯度冲突的簇。然后，它通过在transformer前馈层中注入簇特定的残差适配器来微调预训练模型，绕过学习到的门控机制以确保稳定性和效率。在长尾MIMIC-CXR-LT数据集上，GRASP在FID和多样性指标上表现优异，特别是对于罕见类别，优于基线方法如普通微调和混合专家变体。在NIH-CXR-LT上的下游分类任务中，尾部标签的分类性能显著提升。在ImageNet-LT上的泛化实验证实了广泛的适用性。我们的方法是轻量级、可扩展的，并且可以轻松集成到扩散管道中。

</details>


### [147] [Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation](https://arxiv.org/abs/2512.01677)
*Haodong Yan,Hang Yu,Zhide Zhong,Weilin Yuan,Xin Gong,Zehang Luo,Chengxi Heyu,Junfeng Li,Wenxuan Song,Shunbo Zhou,Haoang Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于结构和接触感知表示的开世界手物交互视频生成方法，通过2D表示同时保证可扩展性和交互保真度，解决了现有方法在2D和3D表示之间的困境。


<details>
  <summary>Details</summary>
Motivation: 生成真实的手物交互视频面临物理约束建模的挑战，现有方法使用HOI表示作为辅助生成目标，但在2D和3D表示之间存在可扩展性和交互保真度无法兼顾的困境。

Method: 提出结构和接触感知表示，捕捉手物接触、遮挡和整体结构上下文，无需3D标注；采用共享与专业化策略的联合生成范式，同时生成交互导向表示和视频。

Result: 在两个真实世界数据集上超越现有方法，生成物理真实且时间连贯的HOI视频；在挑战性开世界场景中表现出强大的泛化能力。

Conclusion: 提出的结构和接触感知表示以及联合生成范式能够有效学习细粒度交互物理，实现可扩展的开世界手物交互视频生成。

Abstract: Generating realistic hand-object interactions (HOI) videos is a significant challenge due to the difficulty of modeling physical constraints (e.g., contact and occlusion between hands and manipulated objects). Current methods utilize HOI representation as an auxiliary generative objective to guide video synthesis. However, there is a dilemma between 2D and 3D representations that cannot simultaneously guarantee scalability and interaction fidelity. To address this limitation, we propose a structure and contact-aware representation that captures hand-object contact, hand-object occlusion, and holistic structure context without 3D annotations. This interaction-oriented and scalable supervision signal enables the model to learn fine-grained interaction physics and generalize to open-world scenarios. To fully exploit the proposed representation, we introduce a joint-generation paradigm with a share-and-specialization strategy that generates interaction-oriented representations and videos. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on two real-world datasets in generating physics-realistic and temporally coherent HOI videos. Furthermore, our approach exhibits strong generalization to challenging open-world scenarios, highlighting the benefit of our scalable design. Our project page is https://hgzn258.github.io/SCAR/.

中文标题: 基于结构和接触感知表示的开世界手物交互视频生成

中文摘要: 生成真实的手物交互视频是一个重大挑战，因为难以建模物理约束（例如手和操纵物体之间的接触和遮挡）。现有方法利用HOI表示作为辅助生成目标来指导视频合成。然而，2D和3D表示之间存在无法同时保证可扩展性和交互保真度的困境。为了解决这一限制，我们提出了一种结构和接触感知表示，无需3D标注即可捕捉手物接触、手物遮挡和整体结构上下文。这种交互导向且可扩展的监督信号使模型能够学习细粒度交互物理并泛化到开世界场景。为了充分利用所提出的表示，我们引入了具有共享与专业化策略的联合生成范式，同时生成交互导向表示和视频。大量实验表明，我们的方法在两个真实世界数据集上超越了最先进的方法，生成物理真实且时间连贯的HOI视频。此外，我们的方法在挑战性开世界场景中表现出强大的泛化能力，突出了我们可扩展设计的优势。

</details>


### [148] [Cross-Domain Validation of a Resection-Trained Self-Supervised Model on Multicentre Mesothelioma Biopsies](https://arxiv.org/abs/2512.01681)
*Farzaneh Seyedshahi,Francesca Damiola,Sylvie Lantuejoul,Ke Yuan,John Le Quesne*

Main category: cs.CV

TL;DR: 在间皮瘤研究中，大多数计算病理学模型基于大块切除组织训练，限制了其在常见小活检样本中的应用。本研究证明，在切除组织上训练的自监督编码器可应用于活检材料，捕获有意义的形态学模式，用于预测患者生存和肿瘤亚型分类。


<details>
  <summary>Details</summary>
Motivation: 间皮瘤的准确亚型分类和预后预测对于指导治疗和患者护理至关重要。然而，大多数计算病理学模型都是在大型切除组织图像上训练的，这限制了它们在现实世界中的应用，因为小活检样本更为常见。需要开发能够适用于活检材料的AI模型。

Method: 采用自监督编码器在间皮瘤切除组织上进行预训练，然后将该模型应用于多中心活检材料。通过捕获活检样本中的形态学模式，模型能够进行患者生存预测和肿瘤亚型分类，实现了从切除组织到活检样本的跨域验证。

Result: 研究结果表明，在切除组织上训练的自监督编码器能够成功应用于活检材料，捕获有意义的形态学特征。该模型在多中心活检数据集上表现出良好的性能，能够准确预测患者生存并分类肿瘤亚型，验证了该方法的跨域有效性。

Conclusion: 本研究证明了基于切除组织训练的自监督模型可以成功应用于活检样本，捕获有意义的形态学模式用于预后预测和亚型分类。这种方法展示了AI驱动工具在支持间皮瘤诊断和治疗规划方面的潜力，特别是在活检样本占主导的临床实践中。

Abstract: Accurate subtype classification and outcome prediction in mesothelioma are essential for guiding therapy and patient care. Most computational pathology models are trained on large tissue images from resection specimens, limiting their use in real-world settings where small biopsies are common. We show that a self-supervised encoder trained on resection tissue can be applied to biopsy material, capturing meaningful morphological patterns. Using these patterns, the model can predict patient survival and classify tumor subtypes. This approach demonstrates the potential of AI-driven tools to support diagnosis and treatment planning in mesothelioma.

中文标题: 基于切除组织训练的自监督模型在多中心间皮瘤活检中的跨域验证

中文摘要: 间皮瘤的准确亚型分类和预后预测对于指导治疗和患者护理至关重要。大多数计算病理学模型都是在大型切除组织图像上训练的，这限制了它们在现实世界中的应用，因为小活检样本更为常见。我们证明，在切除组织上训练的自监督编码器可以应用于活检材料，捕获有意义的形态学模式。利用这些模式，模型能够预测患者生存并分类肿瘤亚型。这种方法展示了AI驱动工具在支持间皮瘤诊断和治疗规划方面的潜力。

</details>


### [149] [DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models](https://arxiv.org/abs/2512.01686)
*Patrick Kwon,Chen Chen*

Main category: cs.CV

TL;DR: DreamingComics是一个基于视频扩散Transformer的故事可视化框架，通过区域感知位置编码和布局控制，显著提升了角色一致性和风格相似性。


<details>
  <summary>Details</summary>
Motivation: 当前故事可视化方法仅通过文本定位角色，难以保持艺术一致性，需要更有效的布局控制和角色一致性方法。

Method: 基于预训练视频扩散Transformer模型，提出RegionalRoPE区域感知位置编码方案，使用掩码条件损失约束视觉特征，集成基于LLM的布局生成器。

Result: 相比先前方法，角色一致性提升29.2%，风格相似性提升36.2%，同时展现出高空间准确性。

Conclusion: DreamingComics通过布局感知的故事可视化框架，有效解决了角色一致性和艺术风格保持的问题，为漫画风格故事生成提供了新方法。

Abstract: Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at https://yj7082126.github.io/dreamingcomics/

中文标题: DreamingComics：基于视频模型的主体与布局定制生成的故事可视化流水线

中文摘要: 当前的故事可视化方法倾向于仅通过文本定位主体，并且在保持艺术一致性方面面临挑战。为解决这些限制，我们引入了DreamingComics，一个布局感知的故事可视化框架。我们基于预训练的视频扩散Transformer模型，利用其时空先验来增强身份和风格一致性。对于基于布局的位置控制，我们提出了RegionalRoPE，一种区域感知的位置编码方案，根据目标布局重新索引嵌入。此外，我们引入了掩码条件损失，进一步将每个主体的视觉特征约束到其指定区域。为了从自然语言脚本推断布局，我们集成了基于LLM的布局生成器，该生成器经过训练以产生漫画风格的布局，从而实现灵活可控的布局条件。我们对我们的方法进行了全面评估，结果显示与先前方法相比，角色一致性提高了29.2%，风格相似性提高了36.2%，同时显示出高空间准确性。我们的项目页面可在https://yj7082126.github.io/dreamingcomics/获取。

</details>


### [150] [SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation](https://arxiv.org/abs/2512.01701)
*Xiuli Bi,Die Xiao,Junchao Fan,Bin Xiao*

Main category: cs.CV

TL;DR: SSR方法通过语义层面的跨模态原型对齐和空间层面的超像素引导校正，解决了CLIP-based弱监督分割中的过度激活问题，在PASCAL VOC和MS COCO上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的弱监督语义分割方法存在两个主要问题：1）在非目标前景区域的过度激活（类间重叠）；2）在背景区域的过度激活。这些问题限制了分割性能的进一步提升。

Method: 提出SSR方法，包含两个核心组件：1）跨模态原型对齐（CMPA）：通过对比学习机制强制图像和文本特征空间对齐，减少类间重叠；2）超像素引导校正（SGC）：利用超像素空间先验过滤非目标区域干扰，校正背景过度激活。

Result: 在PASCAL VOC数据集上达到79.5% mIoU，在MS COCO数据集上达到50.6% mIoU，超越了所有单阶段方法和更复杂的多阶段方法，实现了新的SOTA性能。

Conclusion: SSR方法通过语义和空间两个层面的校正，有效解决了CLIP-based弱监督分割中的过度激活问题，证明了跨模态对齐和空间先验在提升分割性能方面的重要作用。

Abstract: In recent years, Contrastive Language-Image Pretraining (CLIP) has been widely applied to Weakly Supervised Semantic Segmentation (WSSS) tasks due to its powerful cross-modal semantic understanding capabilities. This paper proposes a novel Semantic and Spatial Rectification (SSR) method to address the limitations of existing CLIP-based weakly supervised semantic segmentation approaches: over-activation in non-target foreground regions and background areas. Specifically, at the semantic level, the Cross-Modal Prototype Alignment (CMPA) establishes a contrastive learning mechanism to enforce feature space alignment across modalities, reducing inter-class overlap while enhancing semantic correlations, to rectify over-activation in non-target foreground regions effectively; at the spatial level, the Superpixel-Guided Correction (SGC) leverages superpixel-based spatial priors to precisely filter out interference from non-target regions during affinity propagation, significantly rectifying background over-activation. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that our method outperforms all single-stage approaches, as well as more complex multi-stage approaches, achieving mIoU scores of 79.5% and 50.6%, respectively.

中文标题: SSR：基于CLIP的弱监督分割的语义与空间校正方法

中文摘要: 近年来，对比语言-图像预训练（CLIP）凭借其强大的跨模态语义理解能力，被广泛应用于弱监督语义分割（WSSS）任务。本文提出了一种新颖的语义与空间校正（SSR）方法，以解决现有基于CLIP的弱监督语义分割方法的局限性：在非目标前景区域和背景区域的过度激活问题。具体而言，在语义层面，跨模态原型对齐（CMPA）建立对比学习机制，强制跨模态特征空间对齐，减少类间重叠同时增强语义相关性，有效校正非目标前景区域的过度激活；在空间层面，超像素引导校正（SGC）利用基于超像素的空间先验，在亲和力传播过程中精确过滤非目标区域的干扰，显著校正背景过度激活。在PASCAL VOC和MS COCO数据集上的大量实验表明，我们的方法优于所有单阶段方法以及更复杂的多阶段方法，分别实现了79.5%和50.6%的mIoU分数。

</details>


### [151] [FreqEdit: Preserving High-Frequency Features for Robust Multi-Turn Image Editing](https://arxiv.org/abs/2512.01755)
*Yucheng Liao,Jiajun Liang,Kaiqian Cui,Baoquan Zhao,Haoran Xie,Wei Liu,Qing Li,Xudong Mao*

Main category: cs.CV

TL;DR: FreqEdit是一个无需训练的框架，通过高频特征注入、自适应注入策略和路径补偿机制，解决多轮图像编辑中的高频信息丢失问题，实现10+轮稳定编辑。


<details>
  <summary>Details</summary>
Motivation: 基于指令的图像编辑在单次编辑中表现良好，但在多轮编辑中会出现严重的质量退化。通过系统分析发现，高频信息的渐进性丢失是导致质量退化的主要原因。

Method: FreqEdit包含三个协同组件：1）从参考速度场注入高频特征以保留细粒度细节；2）自适应注入策略，通过空间调制注入强度实现精确的区域控制；3）路径补偿机制，定期重新校准编辑轨迹以防止过度约束。

Result: 在广泛实验中，FreqEdit在身份保持和指令遵循方面均优于七个最先进的基线方法，能够实现10+轮连续迭代的稳定编辑。

Conclusion: FreqEdit通过解决多轮编辑中的高频信息丢失问题，为基于指令的图像编辑提供了稳定可靠的解决方案，在保持图像质量和遵循编辑指令之间取得了良好平衡。

Abstract: Instruction-based image editing through natural language has emerged as a powerful paradigm for intuitive visual manipulation. While recent models achieve impressive results on single edits, they suffer from severe quality degradation under multi-turn editing. Through systematic analysis, we identify progressive loss of high-frequency information as the primary cause of this quality degradation. We present FreqEdit, a training-free framework that enables stable editing across 10+ consecutive iterations. Our approach comprises three synergistic components: (1) high-frequency feature injection from reference velocity fields to preserve fine-grained details, (2) an adaptive injection strategy that spatially modulates injection strength for precise region-specific control, and (3) a path compensation mechanism that periodically recalibrates the editing trajectory to prevent over-constraint. Extensive experiments demonstrate that FreqEdit achieves superior performance in both identity preservation and instruction following compared to seven state-of-the-art baselines.

中文标题: FreqEdit：为鲁棒多轮图像编辑保留高频特征

中文摘要: 基于自然语言指令的图像编辑已成为直观视觉操作的强大范式。虽然最近模型在单次编辑中取得了令人印象深刻的结果，但在多轮编辑下会出现严重的质量退化。通过系统分析，我们确定高频信息的渐进性丢失是这种质量退化的主要原因。我们提出了FreqEdit，这是一个无需训练的框架，能够在10+次连续迭代中实现稳定编辑。我们的方法包含三个协同组件：（1）从参考速度场注入高频特征以保留细粒度细节，（2）自适应注入策略，通过空间调制注入强度实现精确的区域特定控制，以及（3）路径补偿机制，定期重新校准编辑轨迹以防止过度约束。大量实验表明，与七个最先进的基线方法相比，FreqEdit在身份保持和指令遵循方面均实现了卓越性能。

</details>


### [152] [VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis](https://arxiv.org/abs/2512.01769)
*Hafsa Billah*

Main category: cs.CV

TL;DR: VideoScoop是一个通用的视频情境分析框架，使用扩展关系模型和图模型表示视频内容，支持连续查询处理和复杂情境检测，实现领域无关的视频分析。


<details>
  <summary>Details</summary>
Motivation: 当前视频情境分析要么依赖人工（易出错且劳动密集），要么使用特定领域的定制算法（缺乏通用性）。需要一种通用的、领域无关的视频分析框架来自动识别复杂情境。

Method: 1. 使用先进技术一次性提取视频内容；2. 用扩展关系模型（R++）和图模型表示内容；3. R++支持连续查询处理；4. 图模型检测复杂情境；5. 参数化模板实现领域无关性。

Result: 在辅助生活、公民监控和一般监控三个领域进行了广泛实验，使用不同长度视频数据集验证了方法的准确性、效率和鲁棒性。

Conclusion: VideoScoop提供了一个通用的视频情境分析框架，克服了现有方法的局限性，支持领域无关的复杂情境检测，具有实际应用价值。

Abstract: Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain.
  This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.

中文标题: VideoScoop：一个非传统的领域无关视频分析框架

中文摘要: 自动理解视频内容对于公民监控（CM）、一般监控（SL）、辅助生活（AL）等多个应用领域至关重要。数十年的图像和视频分析（IVA）研究已经推进了内容提取任务（如对象识别和跟踪）。然而，识别有意义的活动或情境（例如两个物体靠近）仍然很困难，仅靠内容提取无法实现。目前，视频情境分析（VSA）要么通过人工参与的方式进行，这种方式容易出错且劳动密集；要么通过为特定视频类型或情境设计的定制算法实现。这些算法不是通用的，需要为每个新情境或来自新领域的视频开发新的算法/软件。

本报告提出了一个通用的VSA框架，克服了上述限制。视频内容使用最先进的视频内容提取技术一次性提取。它们使用两种替代模型表示——扩展关系模型（R++）和图模型。当使用R++表示时，提取的内容可以作为数据流，通过提出的视频分析连续查询语言实现连续查询处理。图模型通过支持检测难以或无法使用关系模型单独检测的情境来补充这一点。现有的图算法和新开发的算法支持各种情境检测。为了支持领域无关性，跨领域的原始情境变体被识别并表示为参数化模板。在三个领域（AL、CM和SL）的几个有趣情境上进行了广泛的实验，使用来自这些领域的不同长度视频数据集评估了所提出方法的准确性、效率和鲁棒性。

</details>


### [153] [Robust Rigid and Non-Rigid Medical Image Registration Using Learnable Edge Kernels](https://arxiv.org/abs/2512.01771)
*Ahsan Raza Siyal,Markus Haltmeier,Ruth Steiger,Malik Galijasevic,Elke Ruth Gizewski,Astrid Ellen Grams*

Main category: cs.CV

TL;DR: 提出了一种将可学习边缘核与基于学习的刚性和非刚性配准技术相结合的方法，通过自适应边缘检测增强医学图像配准性能


<details>
  <summary>Details</summary>
Motivation: 医学图像配准在临床和研究应用中至关重要，但传统方法在处理对比度差异、空间扭曲和模态特定变化方面存在困难。需要一种能够更好地提取结构特征的方法来改进多模态图像对齐。

Method: 将可学习边缘核集成到基于学习的刚性和非刚性配准技术中。从预定义的边缘检测核开始，添加随机噪声扰动，在训练过程中学习提取针对任务优化的边缘特征。设计了刚性配准的四个变体模型和非刚性配准的四个变体模型。

Result: 在医学大学提供的数据集上评估了三种设置：无颅骨去除的刚性配准、有颅骨去除的刚性配准以及非刚性配准。还在两个公开可用数据集上进行了评估。在所有实验中，该方法始终优于最先进的技术。

Conclusion: 提出的方法通过自适应边缘检测增强了配准过程，能够捕获医学成像中关键的结构特征，在改善多模态图像对齐和解剖结构分析方面显示出潜力。

Abstract: Medical image registration is crucial for various clinical and research applications including disease diagnosis or treatment planning which require alignment of images from different modalities, time points, or subjects. Traditional registration techniques often struggle with challenges such as contrast differences, spatial distortions, and modality-specific variations. To address these limitations, we propose a method that integrates learnable edge kernels with learning-based rigid and non-rigid registration techniques. Unlike conventional layers that learn all features without specific bias, our approach begins with a predefined edge detection kernel, which is then perturbed with random noise. These kernels are learned during training to extract optimal edge features tailored to the task. This adaptive edge detection enhances the registration process by capturing diverse structural features critical in medical imaging. To provide clearer insight into the contribution of each component in our design, we introduce four variant models for rigid registration and four variant models for non-rigid registration. We evaluated our approach using a dataset provided by the Medical University across three setups: rigid registration without skull removal, with skull removal, and non-rigid registration. Additionally, we assessed performance on two publicly available datasets. Across all experiments, our method consistently outperformed state-of-the-art techniques, demonstrating its potential to improve multi-modal image alignment and anatomical structure analysis.

中文标题: 使用可学习边缘核的鲁棒刚性和非刚性医学图像配准

中文摘要: 医学图像配准对于各种临床和研究应用至关重要，包括疾病诊断或治疗计划，这些应用需要对齐来自不同模态、时间点或受试者的图像。传统配准技术常常难以应对对比度差异、空间扭曲和模态特定变化等挑战。为了解决这些限制，我们提出了一种将可学习边缘核与基于学习的刚性和非刚性配准技术相结合的方法。与学习所有特征而无特定偏见的传统层不同，我们的方法从预定义的边缘检测核开始，然后添加随机噪声扰动。这些核在训练过程中学习提取针对任务优化的边缘特征。这种自适应边缘检测通过捕获医学成像中关键的各种结构特征来增强配准过程。为了更清楚地了解我们设计中每个组件的贡献，我们为刚性配准引入了四个变体模型，为非刚性配准引入了四个变体模型。我们使用医学大学提供的数据集在三种设置下评估了我们的方法：无颅骨去除的刚性配准、有颅骨去除的刚性配准以及非刚性配准。此外，我们还评估了在两个公开可用数据集上的性能。在所有实验中，我们的方法始终优于最先进的技术，证明了其在改善多模态图像对齐和解剖结构分析方面的潜力。

</details>


### [154] [Learned Image Compression for Earth Observation: Implications for Downstream Segmentation Tasks](https://arxiv.org/abs/2512.01788)
*Christian Mollière,Iker Cumplido,Marco Zeulner,Lukas Liesenhoff,Matthias Schubert,Julia Gottfriedsen*

Main category: cs.CV

TL;DR: 本文评估了学习型压缩算法在地球观测图像压缩中的应用，与传统JPEG 2000相比，学习型压缩在大规模多通道光学图像上表现更优，但在小规模单通道热红外数据上传统方法仍有竞争力。联合端到端优化并未带来性能提升。


<details>
  <summary>Details</summary>
Motivation: 卫星地球观测数据快速增长带来了传输和存储挑战，需要评估任务特定的学习型压缩算法是否能减少数据量同时保留关键信息，特别是对下游分割任务的影响。

Method: 比较传统压缩（JPEG 2000）与学习型压缩（离散混合高斯似然）在三个地球观测分割任务上的表现：火灾、云层和建筑物检测。评估重建质量（PSNR）和分割精度，并测试联合端到端优化的效果。

Result: 学习型压缩在大规模多通道光学图像上显著优于JPEG 2000，在重建质量和分割精度方面都有更好表现。但在小规模单通道热红外数据集上，传统编码器仍具竞争力。联合端到端优化并未改善性能。

Conclusion: 学习型压缩在地球观测应用中具有潜力，特别是在大规模多通道光学图像上。然而，对于数据有限的小规模单通道数据集，传统方法仍可考虑。联合优化策略在当前架构下未显示出优势。

Abstract: The rapid growth of data from satellite-based Earth observation (EO) systems poses significant challenges in data transmission and storage. We evaluate the potential of task-specific learned compression algorithms in this context to reduce data volumes while retaining crucial information. In detail, we compare traditional compression (JPEG 2000) versus a learned compression approach (Discretized Mixed Gaussian Likelihood) on three EO segmentation tasks: Fire, cloud, and building detection. Learned compression notably outperforms JPEG 2000 for large-scale, multi-channel optical imagery in both reconstruction quality (PSNR) and segmentation accuracy. However, traditional codecs remain competitive on smaller, single-channel thermal infrared datasets due to limited data and architectural constraints. Additionally, joint end-to-end optimization of compression and segmentation models does not improve performance over standalone optimization.

中文标题: 地球观测的学习型图像压缩：对下游分割任务的影响

中文摘要: 卫星地球观测系统的数据快速增长给数据传输和存储带来了重大挑战。我们评估了任务特定学习型压缩算法在此背景下减少数据量同时保留关键信息的潜力。具体而言，我们在三个地球观测分割任务上比较了传统压缩（JPEG 2000）与学习型压缩方法（离散混合高斯似然）：火灾、云层和建筑物检测。学习型压缩在大规模多通道光学图像上显著优于JPEG 2000，无论是在重建质量（PSNR）还是分割精度方面。然而，由于数据有限和架构限制，传统编码器在小规模单通道热红外数据集上仍具竞争力。此外，压缩和分割模型的联合端到端优化并未比独立优化带来性能提升。

</details>


### [155] [SAM3-UNet: Simplified Adaptation of Segment Anything Model 3](https://arxiv.org/abs/2512.01789)
*Xinyu Xiong,Zihuang Wu,Lei Lu,Yufa Xia*

Main category: cs.CV

TL;DR: SAM3-UNet是Segment Anything Model 3的简化变体，通过添加适配器和轻量级U-Net解码器，以低成本将SAM3适配到下游分割任务，在多个任务上表现优于SAM2-UNet和其他SOTA方法，训练时GPU内存需求低于6GB。


<details>
  <summary>Details</summary>
Motivation: Segment Anything Model 3 (SAM3)作为强大的基础分割模型，直接应用于下游任务成本较高。需要一种低成本的适配方法，使SAM3能够高效地迁移到各种下游分割任务中。

Method: SAM3-UNet包含三个组件：1) SAM3图像编码器，2) 参数高效微调的简单适配器，3) 轻量级U-Net风格解码器。这种设计实现了对SAM3的低成本适配。

Result: 在镜像检测和显著目标检测等多个任务上的初步实验表明，SAM3-UNet优于之前的SAM2-UNet和其他最先进方法，同时在批量大小为12的训练中GPU内存需求低于6GB。

Conclusion: SAM3-UNet提供了一种简单有效的方案，以低成本将强大的SAM3基础模型适配到下游分割任务，在性能和效率之间取得了良好平衡。

Abstract: In this paper, we introduce SAM3-UNet, a simplified variant of Segment Anything Model 3 (SAM3), designed to adapt SAM3 for downstream tasks at a low cost. Our SAM3-UNet consists of three components: a SAM3 image encoder, a simple adapter for parameter-efficient fine-tuning, and a lightweight U-Net-style decoder. Preliminary experiments on multiple tasks, such as mirror detection and salient object detection, demonstrate that the proposed SAM3-UNet outperforms the prior SAM2-UNet and other state-of-the-art methods, while requiring less than 6 GB of GPU memory during training with a batch size of 12. The code is publicly available at https://github.com/WZH0120/SAM3-UNet.

中文标题: SAM3-UNet：Segment Anything Model 3的简化适配

中文摘要: 本文介绍了SAM3-UNet，这是Segment Anything Model 3 (SAM3)的简化变体，旨在以低成本将SAM3适配到下游任务。我们的SAM3-UNet包含三个组件：SAM3图像编码器、用于参数高效微调的简单适配器，以及轻量级U-Net风格解码器。在镜像检测和显著目标检测等多个任务上的初步实验表明，所提出的SAM3-UNet优于之前的SAM2-UNet和其他最先进方法，同时在批量大小为12的训练中GPU内存需求低于6GB。代码已公开在https://github.com/WZH0120/SAM3-UNet。

</details>


### [156] [Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos](https://arxiv.org/abs/2512.01803)
*Xavier Thomas,Youngsun Lim,Ananya Srinivasan,Audrey Zheng,Deepti Ghadiyaram*

Main category: cs.CV

TL;DR: 提出了一种新的视频生成评估指标，通过融合骨骼几何和外观特征学习真实人类动作分布，用于评估生成视频中动作的合理性和质量，相比现有方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型发展迅速，但缺乏能够评估复杂人类动作视觉和时间正确性的稳健指标。现有方法存在外观偏见、缺乏时间理解能力，难以识别生成视频中的复杂运动动态和解剖不合理性。

Method: 1. 融合外观无关的人体骨骼几何特征与基于外观的特征，捕捉真实世界运动的细微差别、约束和时间平滑性；2. 构建学习到的真实世界动作分布作为参考；3. 通过测量生成视频表示与真实动作分布之间的距离来量化动作质量。

Result: 1. 在新开发的多层面基准上，相比现有最先进方法实现了超过68%的显著改进；2. 在已建立的外部基准上表现具有竞争力；3. 与人类感知有更强的相关性。

Conclusion: 该研究提出了一种新颖的视频生成评估指标，能够有效评估生成视频中人类动作的质量和合理性，揭示了当前视频生成模型的局限性，并为该领域的高级研究建立了新标准。

Abstract: Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.

中文标题: 生成式动作评估：合成视频中人体运动的评估

中文摘要: 尽管视频生成模型发展迅速，但评估复杂人类动作的视觉和时间正确性的稳健指标仍然难以捉摸。关键问题在于，现有的纯视觉编码器和多模态大语言模型（MLLMs）存在强烈的外观偏见，缺乏时间理解能力，因此在生成的视频中难以辨别复杂的运动动态和解剖学不合理性。我们通过引入一种从真实世界人类动作学习到的潜在空间衍生出的新颖评估指标来解决这一差距。我们的方法首先通过融合外观无关的人体骨骼几何特征与基于外观的特征，捕捉真实世界运动的细微差别、约束和时间平滑性。我们假设这种组合特征空间提供了动作合理性的稳健表示。给定一个生成的视频，我们的指标通过测量其底层表示与这个学习的真实世界动作分布之间的距离来量化其动作质量。为了严格验证，我们开发了一个新的多层面基准，专门设计用于探究人类动作保真度的时间挑战性方面。通过大量实验，我们表明我们的指标在我们的基准上比现有最先进方法实现了超过68%的显著改进，在已建立的外部基准上表现具有竞争力，并且与人类感知有更强的相关性。我们的深入分析揭示了当前视频生成模型的关键局限性，并为视频生成的高级研究建立了新标准。

</details>


### [157] [Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights](https://arxiv.org/abs/2512.01816)
*Juanxi Tian,Siyuan Li,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.CV

TL;DR: Envision是一个用于评估多模态模型在因果事件序列生成能力的基准测试，发现现有模型在动态世界过程建模方面存在显著不足，特别是时空一致性方面。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型过度依赖静态单图像生成任务，导致模型只能进行静态模式匹配而无法建模动态世界过程，限制了世界知识的内化和动态场景生成能力。

Method: 提出Envision基准，包含1000个四阶段因果事件提示，覆盖六个领域；开发Envision-Score综合评估指标，整合一致性、物理性和美学维度；评估15个模型（10个专门T2I模型，5个统一模型）。

Result: 专门T2I模型擅长美学渲染但缺乏世界知识；统一模型在因果叙事连贯性上优于专门模型，但仍次于闭源模型；所有模型在时空一致性方面都存在显著挑战。

Conclusion: 专注于静态单图像生成阻碍了多模态模型的动态世界建模能力，需要开发能够处理因果事件序列的新基准和方法来促进真正的世界知识内化。

Abstract: Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.

中文标题: Envision：用于因果世界过程洞察的统一理解与生成基准测试

中文摘要: 当前的多模态模型旨在通过统一理解和生成来超越单模态表示的限制，通常使用文本到图像（T2I）任务来校准语义一致性。然而，它们在训练和评估中对静态、单图像生成的依赖导致了静态模式匹配和语义融合的过拟合，同时从根本上阻碍了它们建模随时间展开的动态过程的能力。为了解决这些限制，我们提出了Envision——一个用于链式文本到多图像生成的因果事件进展基准。基于世界知识并以时空因果关系为结构，它重新组织了现有的评估维度，并包含了跨越六个科学和人文学科领域的1000个四阶段提示。为了将评估从单图像过渡到序列帧，并评估模型是否真正内化了世界知识同时遵守因果时间约束，我们引入了Envision-Score，这是一个整合了多维一致性、物理性和美学的整体度量标准。对15个模型（10个专门的T2I模型，5个统一模型）的综合评估发现：专门的T2I模型在美学渲染方面表现出熟练度，但缺乏内在的世界知识。统一的多模态模型弥补了这一差距，在因果叙事连贯性方面持续优于专门的对应模型。然而，即使是这些统一架构仍然次于闭源模型，并且难以克服时空一致性的核心挑战。这表明，对因果孤立的单图像的关注阻碍了多帧推理和生成，促进了静态模式匹配而非动态世界建模——最终限制了世界知识的内化和生成。

</details>


### [158] [Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling](https://arxiv.org/abs/2512.01821)
*Meng Cao,Haokun Lin,Haoyuan Li,Haoran Tang,Rongtao Xu,Dong An,Xue Liu,Ian Reid,Xiaodan Liang*

Main category: cs.CV

TL;DR: MILO提出了一种通过隐式空间世界建模来增强多模态大语言模型空间推理能力的新范式，结合视觉生成器提供几何感知反馈，并引入相对位置编码方案，在多个基准测试中显著提升了3D空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型的空间推理能力不足，主要依赖文本描述调整，存在"视觉文盲"问题——仅通过文本符号学习空间概念，缺乏与视觉表现的联系。需要模拟人类空间想象能力，将符号推理与感知经验相结合。

Method: 提出MILO隐式空间世界建模范式：1) 集成视觉生成器提供几何感知反馈；2) 提出RePE相对位置编码方案，捕捉相对相机位姿变换；3) 构建GeoGen大规模几何感知生成数据集，包含约2,241个视频和67,827个观察-动作-结果三元组。

Result: 实验表明，该方法在多个基线和基准测试中显著增强了空间推理能力，相比绝对坐标系方案表现更优，提供了更全面的3D空间理解。

Conclusion: MILO通过隐式空间世界建模成功模拟了人类空间想象能力，将符号推理与视觉感知相结合，有效解决了多模态大语言模型的视觉文盲问题，为空间推理能力的发展提供了新方向。

Abstract: Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.

中文标题: 通过想象看世界：通过隐式空间世界建模学习场景几何

中文摘要: 空间推理——理解和解释世界三维结构的能力——是多模态大语言模型中关键但尚未充分发展的能力。当前方法主要依赖口头描述调整，存在视觉文盲问题，即仅通过文本符号学习空间概念，缺乏与其视觉表现的联系。为弥合这一差距，本文引入MILO，一种模拟人类空间想象的隐式空间世界建模范式。MILO集成视觉生成器以提供几何感知反馈，从而将MLLM的符号推理隐式地锚定在感知经验中。作为该范式的补充，我们提出RePE（相对位置编码），一种捕捉相对相机位姿变换的新型编码方案，相比绝对坐标系提供更优性能。为支持训练，我们构建了GeoGen，一个大规模几何感知生成数据集，包含约2,241个视频和67,827个观察-动作-结果三元组。实验表明，我们的方法在多个基线和基准测试中显著增强了空间推理能力，提供了更全面的3D空间理解。

</details>


### [159] [CauSight: Learning to Supersense for Visual Causal Discovery](https://arxiv.org/abs/2512.01827)
*Yize Zhang,Meiqi Chen,Sirui Chen,Bo Peng,Yanxi Zhang,Tianyu Li,Chaochao Lu*

Main category: cs.CV

TL;DR: CauSight是一个用于视觉因果发现的新型视觉语言模型，通过构建大规模VCG-32K数据集和因果感知推理，在视觉因果发现任务上显著超越GPT-4.1


<details>
  <summary>Details</summary>
Motivation: 人类能够理解视觉场景中的因果关系，而不仅仅是感知实体存在。为了让AI系统具备这种因果思维能力，需要开发能够从视觉场景中推断因果关系的模型。

Method: 1) 构建VCG-32K数据集（32,000+带实体级因果图标注的图像）；2) 开发CauSight模型，采用因果感知推理；3) 训练方法包括：VCG-32K数据整理、因果思维树(ToCT)合成推理轨迹、带因果奖励的强化学习优化推理策略。

Result: CauSight在视觉因果发现任务上显著超越GPT-4.1，实现了三倍以上的性能提升（21%绝对增益）。

Conclusion: CauSight通过大规模数据集和因果感知推理框架，成功实现了视觉因果发现，为AI系统赋予因果思维能力提供了有效途径。

Abstract: Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), a large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, a novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with a designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over a threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight.

中文标题: CauSight：学习超感知以进行视觉因果发现

中文摘要: 因果思维使人类能够理解所见事物的原因，而不仅仅是看到什么。为了在现代AI系统中复制这种能力，我们引入了视觉因果发现任务。该任务要求模型推断不同场景中视觉实体之间的因果关系，而不仅仅是感知它们的存在。为此，我们首先构建了视觉因果图数据集（VCG-32K），这是一个包含超过32,000张带有实体级因果图标注图像的大规模集合，并进一步开发了CauSight，一个通过因果感知推理执行视觉因果发现的新型视觉语言模型。我们的训练方法整合了三个组成部分：（1）从VCG-32K整理训练数据，（2）因果思维树（ToCT）用于合成推理轨迹，（3）带有设计的因果奖励的强化学习来优化推理策略。实验表明，CauSight在视觉因果发现上优于GPT-4.1，实现了超过三倍的性能提升（21%绝对增益）。我们的代码、模型和数据集已在项目页面完全开源：https://github.com/OpenCausaLab/CauSight。

</details>


### [160] [OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic](https://arxiv.org/abs/2512.01830)
*Songyan Zhang,Wenhui Huang,Zhan Chen,Chua Jiahao Collister,Qihang Huang,Chen Lv*

Main category: cs.CV

TL;DR: OpenREAD是一个基于视觉语言模型的自动驾驶框架，通过端到端强化微调实现从高级推理到低级规划的完整优化，利用大语言模型作为评判者量化开放式问题的推理质量，显著提升自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶的两阶段微调方法存在局限性：监督微调限制了推理的泛化能力，而强化微调难以应用于场景理解这类开放式问题，因为相应的奖励难以量化。这制约了知识驱动自动驾驶范式的进一步发展。

Method: 1. 在开源驾驶知识数据集上构建大规模思维链标注；2. 使用Qwen3大语言模型作为强化微调中的评判者，量化开放式问题的推理质量；3. 实现从高级推理到低级轨迹规划的端到端强化微调。

Result: 实验证实，联合端到端强化微调在上游和下游任务中都带来了显著改进，使OpenREAD在推理和规划基准测试中实现了最先进的性能。

Conclusion: OpenREAD框架通过将大语言模型作为评判者引入强化微调过程，成功解决了自动驾驶中开放式推理的奖励量化问题，实现了从高级推理到低级规划的端到端优化，显著提升了自动驾驶系统的整体性能。

Abstract: Recently, two-stage fine-tuning strategies, e.g., acquiring essential driving knowledge through supervised fine-tuning (SFT) and further enhancing decision-making and planning via reinforcement fine-tuning (RFT), have shown strong potential in advancing the knowledge-driven autonomous driving (AD) paradigm. However, the learning nature of SFT still limits the generalization of reasoning, thereby constraining the full potential of driving performance. Meanwhile, current RFT approaches are primarily applied to downstream tasks, since scene understanding is an open-ended problem where corresponding rewards are difficult to quantify. To address these limitations, we propose OpenREAD, an OPEN-ended REasoning reinforced vision-language model (VLM)-based autonomous driving (AD) framework that enables end-to-end RFT across the full spectrum from high-level reasoning to low-level trajectory planning. Specifically, we begin by constructing large-scale Chain-of-Thought (CoT) annotations on open-source driving-related knowledge datasets, and employ the powerful Qwen3 large language model (LLM) as the critic in RFT to quantify reasoning quality for open-ended questions during reward modeling. Extensive experiments confirm that joint end-to-end RFT yields substantial improvements in both upstream and downstream tasks, enabling OpenREAD to achieve state-of-the-art performance on reasoning and planning benchmarks.

中文标题: OpenREAD：基于LLM作为评判者的端到端自动驾驶强化开放式推理

中文摘要: 最近，两阶段微调策略（例如，通过监督微调获取必要的驾驶知识，并通过强化微调进一步增强决策和规划）在推进知识驱动的自动驾驶范式方面显示出强大的潜力。然而，监督微调的学习性质仍然限制了推理的泛化能力，从而制约了驾驶性能的充分发挥。同时，当前的强化微调方法主要应用于下游任务，因为场景理解是一个开放式问题，相应的奖励难以量化。为了解决这些限制，我们提出了OpenREAD，一个基于视觉语言模型的强化开放式推理自动驾驶框架，能够实现从高级推理到低级轨迹规划的端到端强化微调。具体而言，我们首先在开源驾驶相关知识数据集上构建大规模思维链标注，并利用强大的Qwen3大语言模型作为强化微调中的评判者，在奖励建模过程中量化开放式问题的推理质量。大量实验证实，联合端到端强化微调在上游和下游任务中都带来了显著改进，使OpenREAD在推理和规划基准测试中实现了最先进的性能。

</details>


### [161] [PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models](https://arxiv.org/abs/2512.01843)
*Zeqing Wang,Keze Wang,Lei Zhang*

Main category: cs.CV

TL;DR: PhyDetEx是一个用于检测和解释文本到视频(T2V)模型生成内容物理合理性的系统，通过构建物理不合理性检测数据集和微调视觉语言模型，能够识别视频中的物理不合理事件并提供违反物理原理的文本解释。


<details>
  <summary>Details</summary>
Motivation: 尽管T2V模型在视频质量、长度和指令跟随能力方面取得了显著进展，但这些模型是否真正理解物理规律并生成物理合理的视频仍是一个未解决的问题。现有的视觉语言模型作为通用评估器难以有效识别生成视频中的物理不合理内容。

Method: 1) 构建PID数据集：包含500个手动标注视频的测试集和2,588个配对视频的训练集，每个不合理视频通过精心改写真实世界视频的标题来诱导T2V模型生成物理不合理内容；2) 采用轻量级微调方法，使视觉语言模型不仅能检测物理不合理事件，还能生成关于违反物理原理的文本解释。

Result: 1) 开发了PhyDetEx系统，能够有效检测和解释T2V模型生成视频的物理合理性；2) 对一系列最先进的T2V模型进行基准测试，发现尽管近期T2V模型在生成物理合理内容方面取得了显著进展，但理解和遵守物理规律仍然是一个挑战，特别是对于开源模型。

Conclusion: T2V模型在物理合理性方面仍有改进空间，PhyDetEx提供了一个有效的评估框架和工具，有助于推动T2V模型更好地理解和遵守物理规律。数据集、训练代码和检查点已开源。

Abstract: Driven by the growing capacity and training scale, Text-to-Video (T2V) generation models have recently achieved substantial progress in video quality, length, and instruction-following capability. However, whether these models can understand physics and generate physically plausible videos remains a question. While Vision-Language Models (VLMs) have been widely used as general-purpose evaluators in various applications, they struggle to identify the physically impossible content from generated videos. To investigate this issue, we construct a \textbf{PID} (\textbf{P}hysical \textbf{I}mplausibility \textbf{D}etection) dataset, which consists of a \textit{test split} of 500 manually annotated videos and a \textit{train split} of 2,588 paired videos, where each implausible video is generated by carefully rewriting the caption of its corresponding real-world video to induce T2V models producing physically implausible content. With the constructed dataset, we introduce a lightweight fine-tuning approach, enabling VLMs to not only detect physically implausible events but also generate textual explanations on the violated physical principles. Taking the fine-tuned VLM as a physical plausibility detector and explainer, namely \textbf{PhyDetEx}, we benchmark a series of state-of-the-art T2V models to assess their adherence to physical laws. Our findings show that although recent T2V models have made notable progress toward generating physically plausible content, understanding and adhering to physical laws remains a challenging issue, especially for open-source models. Our dataset, training code, and checkpoints are available at \href{https://github.com/Zeqing-Wang/PhyDetEx}{https://github.com/Zeqing-Wang/PhyDetEx}.

中文标题: PhyDetEx：检测和解释T2V模型的物理合理性

中文摘要: 随着模型容量和训练规模的不断增长，文本到视频(T2V)生成模型最近在视频质量、长度和指令跟随能力方面取得了实质性进展。然而，这些模型是否能够理解物理并生成物理合理的视频仍然是一个问题。虽然视觉语言模型(VLMs)已在各种应用中被广泛用作通用评估器，但它们难以从生成的视频中识别物理上不可能的内容。为了研究这个问题，我们构建了一个PID（物理不合理性检测）数据集，该数据集包含500个手动标注视频的测试集和2,588个配对视频的训练集，其中每个不合理视频都是通过精心改写其对应真实世界视频的标题来诱导T2V模型产生物理不合理内容而生成的。利用构建的数据集，我们引入了一种轻量级微调方法，使VLMs不仅能够检测物理不合理事件，还能生成关于违反物理原理的文本解释。将微调后的VLM作为物理合理性检测器和解释器，即PhyDetEx，我们对一系列最先进的T2V模型进行基准测试，以评估它们对物理定律的遵守程度。我们的研究结果表明，尽管最近的T2V模型在生成物理合理内容方面取得了显著进展，但理解和遵守物理规律仍然是一个具有挑战性的问题，特别是对于开源模型。我们的数据集、训练代码和检查点可在https://github.com/Zeqing-Wang/PhyDetEx获取。

</details>


### [162] [COACH: Collaborative Agents for Contextual Highlighting - A Multi-Agent Framework for Sports Video Analysis](https://arxiv.org/abs/2512.01853)
*Tsz-To Wong,Ching-Chun Huang,Hong-Han Shuai*

Main category: cs.CV

TL;DR: COACH是一个用于体育视频分析的多智能体框架，通过可配置的智能体协作解决现有端到端模型在时间层次理解、泛化能力、开发成本和可解释性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端体育视频分析模型难以处理从微观动作到宏观策略的时间层次结构，存在泛化能力差、新任务开发成本高、可解释性不足等问题，需要更灵活、可扩展的解决方案。

Method: 提出可重构的多智能体系统框架，每个智能体作为专门的"认知工具"负责特定分析方面，通过智能体的迭代调用和灵活组合构建自适应分析管道，支持从短期分析推理到长期生成摘要的多种任务。

Result: 在羽毛球分析的两个代表性任务中展示了框架的适应性，能够桥接细粒度事件检测和全局语义组织，验证了框架在跨任务体育视频智能分析中的有效性。

Conclusion: COACH框架代表了向灵活、可扩展、可解释的体育视频智能系统的范式转变，为健壮的跨任务分析提供了新的解决方案。

Abstract: Intelligent sports video analysis demands a comprehensive understanding of temporal context, from micro-level actions to macro-level game strategies. Existing end-to-end models often struggle with this temporal hierarchy, offering solutions that lack generalization, incur high development costs for new tasks, and suffer from poor interpretability. To overcome these limitations, we propose a reconfigurable Multi-Agent System (MAS) as a foundational framework for sports video understanding. In our system, each agent functions as a distinct "cognitive tool" specializing in a specific aspect of analysis. The system's architecture is not confined to a single temporal dimension or task. By leveraging iterative invocation and flexible composition of these agents, our framework can construct adaptive pipelines for both short-term analytic reasoning (e.g., Rally QA) and long-term generative summarization (e.g., match summaries). We demonstrate the adaptability of this framework using two representative tasks in badminton analysis, showcasing its ability to bridge fine-grained event detection and global semantic organization. This work presents a paradigm shift towards a flexible, scalable, and interpretable system for robust, cross-task sports video intelligence.The project homepage is available at https://aiden1020.github.io/COACH-project-page

中文标题: COACH：用于上下文高亮的协作智能体——一个用于体育视频分析的多智能体框架

中文摘要: 智能体育视频分析需要对时间上下文有全面的理解，从微观层面的动作到宏观层面的比赛策略。现有的端到端模型通常难以处理这种时间层次结构，提供的解决方案缺乏泛化能力，新任务开发成本高，且可解释性差。为了克服这些限制，我们提出了一个可重构的多智能体系统作为体育视频理解的基础框架。在我们的系统中，每个智能体作为一个专门的"认知工具"，专注于分析的特定方面。系统架构不局限于单一时间维度或任务。通过利用这些智能体的迭代调用和灵活组合，我们的框架可以构建自适应管道，用于短期分析推理（如回合问答）和长期生成摘要（如比赛总结）。我们使用羽毛球分析中的两个代表性任务展示了该框架的适应性，展示了其桥接细粒度事件检测和全局语义组织的能力。这项工作呈现了向灵活、可扩展和可解释的健壮跨任务体育视频智能系统的范式转变。项目主页可在 https://aiden1020.github.io/COACH-project-page 获取。

</details>


### [163] [KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM](https://arxiv.org/abs/2512.01889)
*Zaid Nasser,Mikhail Iumanov,Tianhao Li,Maxim Popov,Jaafar Mahmoud,Malik Mohrat,Ilya Obrubov,Ekaterina Derevyanka,Ivan Sosin,Sergey Kolyubin*

Main category: cs.CV

TL;DR: KM-ViPE是一个实时开放词汇SLAM框架，仅使用未标定的单目相机在动态环境中运行，通过紧密耦合DINO视觉特征与几何约束实现同时定位和开放词汇语义建图。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM系统通常需要深度传感器、离线标定，或者缺乏对动态场景的鲁棒性，限制了在自主机器人、AR/VR等实际应用中的实用性。KM-ViPE旨在解决这些问题，直接从原始RGB流中工作，适用于第一人称视角应用和互联网规模视频数据训练。

Method: 系统采用紧密耦合的视觉-语言-几何融合方法，使用基于高级特征的自适应鲁棒核处理移动物体和可移动静态物体。通过将几何特征和深度视觉特征与语言嵌入对齐，实现在线同时定位和开放词汇语义建图。

Result: KM-ViPE在性能上与最先进方法竞争，同时具备在线操作、无需深度数据和里程计估计、对动态场景鲁棒等独特优势。系统受益于互联网规模训练，适用于自主机器人和AR/VR应用。

Conclusion: KM-ViPE通过结合在线操作、未标定单目输入和动态场景鲁棒处理，推进了具身AI的实用空间智能能力，为自主机器人和AR/VR应用提供了良好解决方案。

Abstract: We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views). The system performs simultaneous online localization and open-vocabulary semantic mapping by fusing geometric and deep visual features aligned with language embeddings. Our results are competitive with state-of-the-art approaches, while existing solutions either operate offline, need depth data and/or odometry estimation, or lack dynamic scene robustness. KM-ViPE benefits from internet-scale training and uniquely combines online operation, uncalibrated monocular input, and robust handling of dynamic scenes, which makes it a good fit for autonomous robotics and AR/VR applications and advances practical spatial intelligence capabilities for embodied AI.

中文标题: KM-ViPE：用于开放词汇语义SLAM的在线紧密耦合视觉-语言-几何融合

中文摘要: 我们提出了KM-ViPE（知识映射视频姿态引擎），这是一个用于动态环境中未标定单目相机的实时开放词汇SLAM框架。与需要深度传感器和离线标定的系统不同，KM-ViPE直接在原始RGB流上运行，使其成为第一人称视角应用和从互联网规模视频数据中收集训练数据的理想选择。KM-ViPE通过基于高级特征的自适应鲁棒核将DINO视觉特征与几何约束紧密耦合，能够处理移动物体和可移动静态物体（例如在第一人称视角中移动的家具）。该系统通过融合与语言嵌入对齐的几何特征和深度视觉特征，实现在线同时定位和开放词汇语义建图。我们的结果与最先进方法具有竞争力，而现有解决方案要么离线运行，需要深度数据和/或里程计估计，要么缺乏动态场景鲁棒性。KM-ViPE受益于互联网规模训练，并独特地结合了在线操作、未标定单目输入和对动态场景的鲁棒处理，这使其非常适合自主机器人和AR/VR应用，并推进了具身AI的实用空间智能能力。

</details>


### [164] [StyleYourSmile: Cross-Domain Face Retargeting Without Paired Multi-Style Data](https://arxiv.org/abs/2512.01895)
*Avirup Dey,Vinay Namboodiri*

Main category: cs.CV

TL;DR: StyleYourSmile是一种无需配对多风格数据的跨域人脸重定向方法，通过双编码器框架分离身份特征和风格特征，结合扩散模型实现不同视觉域之间的面部表情重定向。


<details>
  <summary>Details</summary>
Motivation: 现有跨域人脸重定向方法通常需要真实人脸数据训练，但存在以下问题：1) 难以泛化到不同域；2) 需要测试时优化；3) 需要精心策划的多风格配对数据集来获取域不变的身份表示。这些限制使得跨域人脸重定向在实际应用中面临挑战。

Method: 提出StyleYourSmile方法，包含三个核心组件：1) 高效数据增强策略，无需精心策划的多风格配对数据；2) 双编码器框架，分别提取域不变的身份特征和域特定的风格变化；3) 基于扩散模型的生成框架，利用解耦的控制信号实现跨域面部表情重定向。

Result: 大量实验表明，StyleYourSmile在广泛的视觉域中实现了卓越的身份保持和重定向保真度，相比现有方法在跨域泛化能力、身份保持质量和重定向效果方面都有显著提升。

Conclusion: StyleYourSmile成功解决了跨域人脸重定向中需要配对多风格数据的限制，通过解耦身份和风格特征，实现了高效、高质量的跨域面部表情重定向，为实际应用提供了更实用的解决方案。

Abstract: Cross-domain face retargeting requires disentangled control over identity, expressions, and domain-specific stylistic attributes. Existing methods, typically trained on real-world faces, either fail to generalize across domains, need test-time optimizations, or require fine-tuning with carefully curated multi-style datasets to achieve domain-invariant identity representations. In this work, we introduce \textit{StyleYourSmile}, a novel one-shot cross-domain face retargeting method that eliminates the need for curated multi-style paired data. We propose an efficient data augmentation strategy alongside a dual-encoder framework, for extracting domain-invariant identity cues and capturing domain-specific stylistic variations. Leveraging these disentangled control signals, we condition a diffusion model to retarget facial expressions across domains. Extensive experiments demonstrate that \textit{StyleYourSmile} achieves superior identity preservation and retargeting fidelity across a wide range of visual domains.

中文标题: StyleYourSmile：无需配对多风格数据的跨域人脸重定向

中文摘要: 跨域人脸重定向需要对身份、表情和域特定风格属性进行解耦控制。现有方法通常在真实人脸数据上训练，要么难以跨域泛化，要么需要测试时优化，或者需要精心策划的多风格数据集进行微调以获得域不变的身份表示。本文提出StyleYourSmile，一种新颖的单次跨域人脸重定向方法，消除了对精心策划的多风格配对数据的需求。我们提出了一种高效的数据增强策略和双编码器框架，用于提取域不变的身份线索和捕获域特定的风格变化。利用这些解耦的控制信号，我们条件化一个扩散模型来实现跨域的面部表情重定向。大量实验表明，StyleYourSmile在广泛的视觉域中实现了卓越的身份保持和重定向保真度。

</details>


### [165] [SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception](https://arxiv.org/abs/2512.01908)
*Gurmeher Khurana,Lan Wei,Dandan Zhang*

Main category: cs.CV

TL;DR: SARL是一个空间感知的自监督学习框架，通过三个地图级目标（SAL、PPDA、RAM）增强BYOL架构，保持跨视图的注意力焦点、部件组成和几何关系一致性，在融合视觉-触觉数据的机器人感知任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 接触丰富的机器人操作需要编码局部几何的表征。视觉提供全局上下文但缺乏纹理、硬度等属性的直接测量，而触觉能提供这些线索。现有的自监督学习框架通常将特征图压缩为全局向量，丢弃了空间结构，与操作任务的需求不匹配。

Method: 提出SARL框架，在BYOL架构基础上增加三个地图级目标：显著性对齐（SAL）保持注意力焦点一致性，补丁原型分布对齐（PPDA）保持部件组成一致性，区域亲和匹配（RAM）保持几何关系一致性。这些损失作用于中间特征图，补充全局目标。

Result: 在六个下游任务中，SARL持续优于九个自监督学习基线。在几何敏感的边姿态回归任务中，SARL达到0.3955 MAE，比次优方法（0.5682 MAE）相对提升30%，接近监督学习的上限。

Conclusion: 对于融合的视觉-触觉数据，最有效的信号是结构化的空间等变性，其中特征随对象几何可预测地变化，这能够实现更强大的机器人感知能力。

Abstract: Contact-rich robotic manipulation requires representations that encode local geometry. Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues. Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information. Most self-supervised learning (SSL) frameworks, however, compress feature maps into a global vector, discarding spatial structure and misaligning with the needs of manipulation. To address this, we propose SARL, a spatially-aware SSL framework that augments the Bootstrap Your Own Latent (BYOL) architecture with three map-level objectives, including Saliency Alignment (SAL), Patch-Prototype Distribution Alignment (PPDA), and Region Affinity Matching (RAM), to keep attentional focus, part composition, and geometric relations consistent across views. These losses act on intermediate feature maps, complementing the global objective. SARL consistently outperforms nine SSL baselines across six downstream tasks with fused visual-tactile data. On the geometry-sensitive edge-pose regression task, SARL achieves a Mean Absolute Error (MAE) of 0.3955, a 30% relative improvement over the next-best SSL method (0.5682 MAE) and approaching the supervised upper bound. These findings indicate that, for fused visual-tactile data, the most effective signal is structured spatial equivariance, in which features vary predictably with object geometry, which enables more capable robotic perception.

中文标题: SARL：用于视觉-触觉感知的空间感知自监督表征学习

中文摘要: 接触丰富的机器人操作需要编码局部几何的表征。视觉提供全局上下文但缺乏纹理、硬度等属性的直接测量，而触觉能提供这些线索。现代视觉-触觉传感器在单个融合图像中捕获两种模态，产生本质上对齐的输入，非常适合需要视觉和触觉信息的操作任务。然而，大多数自监督学习（SSL）框架将特征图压缩为全局向量，丢弃了空间结构，与操作需求不匹配。为了解决这个问题，我们提出了SARL，一个空间感知的SSL框架，通过三个地图级目标增强Bootstrap Your Own Latent（BYOL）架构，包括显著性对齐（SAL）、补丁原型分布对齐（PPDA）和区域亲和匹配（RAM），以保持跨视图的注意力焦点、部件组成和几何关系一致性。这些损失作用于中间特征图，补充全局目标。SARL在六个下游任务中持续优于九个SSL基线，使用融合的视觉-触觉数据。在几何敏感的边姿态回归任务中，SARL实现了0.3955的平均绝对误差（MAE），比次优SSL方法（0.5682 MAE）相对提升了30%，并接近监督学习的上限。这些发现表明，对于融合的视觉-触觉数据，最有效的信号是结构化的空间等变性，其中特征随对象几何可预测地变化，这能够实现更强大的机器人感知能力。

</details>


### [166] [Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding](https://arxiv.org/abs/2512.01922)
*Zahra Mahdavi,Zahra Khodakaramimaghsoud,Hooman Khaloo,Sina Bakhshandeh Taleshani,Erfan Hashemi,Javad Mirzapour Kaleybar,Omid Nejati Manzari*

Main category: cs.CV

TL;DR: Med-VCD是一种用于医疗大型视觉语言模型的视觉对比解码方法，通过动态令牌稀疏化策略减少幻觉输出，提高事实准确性，同时避免二次解码带来的时间开销。


<details>
  <summary>Details</summary>
Motivation: 医疗大型视觉语言模型在医疗应用中存在幻觉问题，生成看似合理但实际错误的输出。现有方法大多依赖二次解码或回滚程序，显著降低推理速度，且通常是领域特定的，可能引入模态间或生成内容与真实内容之间的不对齐。

Method: Med-VCD采用稀疏视觉对比解码方法，通过新颖的令牌稀疏化策略动态选择视觉信息丰富的令牌，在保持关键视觉上下文的同时减少冗余，平衡效率与可靠性，无需二次解码的时间开销。

Result: 在涵盖眼科、放射学和病理学的8个医疗数据集评估中，Med-VCD将事实准确性平均提高了13%，幻觉准确性提高了6%，相对于基线医疗LVLMs有显著改进。

Conclusion: Med-VCD有效缓解了医疗大型视觉语言模型的幻觉问题，通过视觉对比解码和令牌稀疏化策略，在保持推理效率的同时提高了输出的可靠性和准确性。

Abstract: Large vision-language models (LVLMs) are now central to healthcare applications such as medical visual question answering and imaging report generation. Yet, these models remain vulnerable to hallucination outputs that appear plausible but are in fact incorrect. In the natural image domain, several decoding strategies have been proposed to mitigate hallucinations by reinforcing visual evidence, but most rely on secondary decoding or rollback procedures that substantially slow inference. Moreover, existing solutions are often domain-specific and may introduce misalignment between modalities or between generated and ground-truth content. We introduce Med-VCD, a sparse visual-contrastive decoding method that mitigates hallucinations in medical LVLMs without the time overhead of secondary decoding. Med-VCD incorporates a novel token-sparsification strategy that selects visually informed tokens on the fly, trimming redundancy while retaining critical visual context and thus balancing efficiency with reliability. Evaluations on eight medical datasets, spanning ophthalmology, radiology, and pathology tasks in visual question answering, report generation, and dedicated hallucination benchmarks, show that Med-VCD raises factual accuracy by an average of 13\% and improves hallucination accuracy by 6\% relative to baseline medical LVLMs.

中文标题: Med-VCD：通过视觉对比解码缓解医疗大型视觉语言模型的幻觉问题

中文摘要: 大型视觉语言模型（LVLMs）现在是医疗应用（如医学视觉问答和影像报告生成）的核心。然而，这些模型仍然容易产生看似合理但实际上错误的幻觉输出。在自然图像领域，已经提出了几种解码策略来通过强化视觉证据缓解幻觉，但大多数依赖于二次解码或回滚程序，显著降低了推理速度。此外，现有解决方案通常是领域特定的，可能引入模态之间或生成内容与真实内容之间的不对齐。我们提出了Med-VCD，一种稀疏视觉对比解码方法，可以在没有二次解码时间开销的情况下缓解医疗LVLMs的幻觉问题。Med-VCD采用新颖的令牌稀疏化策略，动态选择视觉信息丰富的令牌，在保留关键视觉上下文的同时减少冗余，从而平衡效率与可靠性。在涵盖眼科、放射学和病理学任务的8个医疗数据集评估中，包括视觉问答、报告生成和专门的幻觉基准测试，Med-VCD将事实准确性平均提高了13%，相对于基线医疗LVLMs将幻觉准确性提高了6%。

</details>


### [167] [Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory](https://arxiv.org/abs/2512.01934)
*Chenyi Wang,Yanmao Man,Raymond Muller,Ming Li,Z. Berkay Celik,Ryan Gerdes,Jonathan Petit*

Main category: cs.CV

TL;DR: AdvTraj是首个针对检测跟踪式多目标追踪的在线物理ID操纵攻击，通过对抗性轨迹将攻击者ID转移到目标对象，成功率达100%，对SOTA算法具有高迁移性。


<details>
  <summary>Details</summary>
Motivation: 多目标追踪在监控和自动驾驶等应用中至关重要，但现有攻击要么针对单个对象追踪器，要么通过攻击数字域中的目标检测模块来操纵ID，这些攻击模型特定、不鲁棒且仅影响离线数据集。需要研究针对追踪关联阶段的物理攻击。

Method: 提出AdvTraj攻击方法，通过生成对抗性轨迹来操纵ID分配，而不攻击目标检测模块。在CARLA模拟器中实现，针对SORT算法进行白盒攻击，并测试对其他SOTA MOT算法的迁移性。分析轨迹模式并提出两种可由人类步行者/驾驶员执行的通用对抗机动。

Result: 在CARLA模拟中，AdvTraj对各种场景下的白盒攻击（针对SORT）达到100%成功率，对SOTA MOT算法具有高达93%的攻击迁移成功率。识别出对抗轨迹的特定模式，并提出了两种实用的对抗机动。

Conclusion: 该研究揭示了SOTA MOT系统在对象关联阶段未被充分探索的弱点，为增强此类系统的鲁棒性提供了见解。AdvTraj是首个在线物理ID操纵攻击，对现实世界MOT系统构成实际威胁。

Abstract: Multi-Object Tracking (MOT) is a critical task in computer vision, with applications ranging from surveillance systems to autonomous driving. However, threats to MOT algorithms have yet been widely studied. In particular, incorrect association between the tracked objects and their assigned IDs can lead to severe consequences, such as wrong trajectory predictions. Previous attacks against MOT either focused on hijacking the trackers of individual objects, or manipulating the tracker IDs in MOT by attacking the integrated object detection (OD) module in the digital domain, which are model-specific, non-robust, and only able to affect specific samples in offline datasets. In this paper, we present AdvTraj, the first online and physical ID-manipulation attack against tracking-by-detection MOT, in which an attacker uses adversarial trajectories to transfer its ID to a targeted object to confuse the tracking system, without attacking OD. Our simulation results in CARLA show that AdvTraj can fool ID assignments with 100% success rate in various scenarios for white-box attacks against SORT, which also have high attack transferability (up to 93% attack success rate) against state-of-the-art (SOTA) MOT algorithms due to their common design principles. We characterize the patterns of trajectories generated by AdvTraj and propose two universal adversarial maneuvers that can be performed by a human walker/driver in daily scenarios. Our work reveals under-explored weaknesses in the object association phase of SOTA MOT systems, and provides insights into enhancing the robustness of such systems.

中文标题: 通过对抗性轨迹对多目标追踪进行物理ID转移攻击

中文摘要: 多目标追踪是计算机视觉中的关键任务，应用范围从监控系统到自动驾驶。然而，针对MOT算法的威胁尚未得到广泛研究。特别是，追踪对象与其分配ID之间的错误关联可能导致严重后果，如错误的轨迹预测。先前针对MOT的攻击要么侧重于劫持单个对象的追踪器，要么通过攻击数字域中的集成目标检测模块来操纵追踪器ID，这些攻击模型特定、不鲁棒且仅能影响离线数据集中的特定样本。在本文中，我们提出了AdvTraj，这是首个针对检测跟踪式MOT的在线物理ID操纵攻击，其中攻击者使用对抗性轨迹将其ID转移到目标对象以混淆追踪系统，而不攻击目标检测模块。我们在CARLA中的模拟结果显示，AdvTraj可以在各种场景下以100%的成功率欺骗针对SORT的白盒攻击的ID分配，由于SOTA MOT算法的共同设计原则，这些攻击也具有很高的攻击迁移性（高达93%的攻击成功率）。我们描述了AdvTraj生成的轨迹模式，并提出了两种可由人类步行者/驾驶员在日常场景中执行的通用对抗机动。我们的工作揭示了SOTA MOT系统在对象关联阶段未被充分探索的弱点，并为增强此类系统的鲁棒性提供了见解。

</details>


### [168] [Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2512.01949)
*Zhongyu Yang,Dannong Xu,Wei Pang,Yingfang Yuan*

Main category: cs.CV

TL;DR: Script是一种无需重新训练的即插即用令牌剪枝方法，通过图结构剪枝去除视觉冗余，结合查询条件语义剪枝保留相关信息，显著提升多模态大语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在处理高分辨率图像和视频时面临视觉令牌数量激增的问题，导致内存消耗和推理延迟过高。现有令牌剪枝方法存在两个主要缺陷：一是忽略用户查询相关性，二是受限于注意力机制，导致适应性差且效果有限。

Method: Script采用双模块设计：1）图结构剪枝模块：通过图结构分析识别并去除视觉冗余令牌；2）查询条件语义剪枝模块：基于用户查询条件，保留与查询相关的语义信息。该方法无需重新训练，可即插即用于不同MLLMs。

Result: 在14个图像和视频理解基准测试中，Script相比现有剪枝方法在模型效率和预测准确性方面表现更优。在LLaVA-NeXT-7B模型上，实现高达6.8倍预填充加速和10倍FLOP减少，同时保持96.88%的原始性能。

Conclusion: Script通过结合图结构分析和查询条件语义剪枝，有效解决了多模态大语言模型的令牌冗余问题，在显著提升计算效率的同时保持了模型性能，为MLLMs的实际部署提供了实用解决方案。

Abstract: The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.

中文标题: Script：面向多模态大语言模型的图结构化和查询条件语义令牌剪枝方法

中文摘要: 多模态大语言模型（MLLMs）中视觉令牌的快速增长导致内存消耗和推理延迟过高，尤其是在处理高分辨率图像和视频时。令牌剪枝是一种通过去除冗余来缓解此问题的技术，但现有方法往往忽略与用户查询的相关性，或受限于注意力机制的局限性，降低了其适应性和有效性。为解决这些挑战，我们提出了Script，一种即插即用的剪枝方法，无需重新训练即可在不同MLLMs中通用。Script包含两个模块：一个图结构剪枝模块用于去除视觉冗余令牌，以及一个查询条件语义剪枝模块用于保留查询相关的视觉信息。两者共同提升了多模态任务的性能。在图像和视频理解任务的十四个基准测试上的实验表明，与现有剪枝方法相比，Script始终实现更高的模型效率和预测准确性。在LLaVA-NeXT-7B上，它实现了高达6.8倍的预填充加速和10倍的FLOP减少，同时保留了96.88%的原始性能。

</details>


### [169] [SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning](https://arxiv.org/abs/2512.01975)
*Xu Zhang,Jin Yuan,Hanwang Zhang,Guojin Zhong,Yongsheng Zang,Jiacheng Lin,Zhiyong Li*

Main category: cs.CV

TL;DR: SGDiff是一个基于场景图引导扩散模型的新方法，用于图像协同分割与描述任务，能够从简单边界框提示生成多样化的语义对齐描述-掩码对。


<details>
  <summary>Details</summary>
Motivation: 传统可控图像理解任务需要用户提供详细提示（如文本或边界框）来获得单一结果，存在提示输入成本高和输出信息有限的问题。本文提出SegCaptioning新任务，旨在从简单提示（如边界框）生成多样化的语义解释，让用户灵活选择结果。

Method: 1. 提出提示中心场景图适配器，将用户提示映射到场景图以捕捉意图；2. 采用扩散过程结合场景图引导双模态Transformer，预测相关的描述-掩码对；3. 设计多实体对比学习损失，显式对齐视觉和文本实体。

Result: 在两个数据集上的实验表明，SGDiff在SegCaptioning任务中表现优异，能够在最小提示输入下为描述和分割任务都产生有希望的结果。

Conclusion: SGDiff通过场景图引导扩散模型成功解决了图像协同分割与描述任务，实现了从简单提示生成多样化语义对齐结果的能力，为可控图像理解提供了更灵活高效的解决方案。

Abstract: Controllable image semantic understanding tasks, such as captioning or segmentation, necessitate users to input a prompt (e.g., text or bounding boxes) to predict a unique outcome, presenting challenges such as high-cost prompt input or limited information output. This paper introduces a new task ``Image Collaborative Segmentation and Captioning'' (SegCaptioning), which aims to translate a straightforward prompt, like a bounding box around an object, into diverse semantic interpretations represented by (caption, masks) pairs, allowing flexible result selection by users. This task poses significant challenges, including accurately capturing a user's intention from a minimal prompt while simultaneously predicting multiple semantically aligned caption words and masks. Technically, we propose a novel Scene Graph Guided Diffusion Model that leverages structured scene graph features for correlated mask-caption prediction. Initially, we introduce a Prompt-Centric Scene Graph Adaptor to map a user's prompt to a scene graph, effectively capturing his intention. Subsequently, we employ a diffusion process incorporating a Scene Graph Guided Bimodal Transformer to predict correlated caption-mask pairs by uncovering intricate correlations between them. To ensure accurate alignment, we design a Multi-Entities Contrastive Learning loss to explicitly align visual and textual entities by considering inter-modal similarity, resulting in well-aligned caption-mask pairs. Extensive experiments conducted on two datasets demonstrate that SGDiff achieves superior performance in SegCaptioning, yielding promising results for both captioning and segmentation tasks with minimal prompt input.

中文标题: SGDiff：基于场景图引导扩散模型的图像协同分割与描述任务

中文摘要: 可控的图像语义理解任务（如描述或分割）需要用户输入提示（如文本或边界框）来预测唯一结果，这带来了高成本提示输入或有限信息输出的挑战。本文引入了一个新任务"图像协同分割与描述"（SegCaptioning），旨在将简单的提示（如围绕对象的边界框）转换为由（描述、掩码）对表示的多样化语义解释，允许用户灵活选择结果。该任务面临重大挑战，包括从最小提示中准确捕捉用户意图，同时预测多个语义对齐的描述词和掩码。技术上，我们提出了一种新颖的场景图引导扩散模型，利用结构化场景图特征进行相关掩码-描述预测。首先，我们引入了一个提示中心场景图适配器，将用户提示映射到场景图，有效捕捉其意图。随后，我们采用包含场景图引导双模态Transformer的扩散过程，通过揭示它们之间复杂相关性来预测相关的描述-掩码对。为确保准确对齐，我们设计了多实体对比学习损失，通过考虑模态间相似性来显式对齐视觉和文本实体，从而获得良好对齐的描述-掩码对。在两个数据集上进行的大量实验表明，SGDiff在SegCaptioning中实现了卓越性能，在最小提示输入下为描述和分割任务都带来了有希望的结果。

</details>


### [170] [Artemis: Structured Visual Reasoning for Perception Policy Learning](https://arxiv.org/abs/2512.01988)
*Wei Tang,Yanpeng Sun,Shan Zhang,Xiaofan Li,Piotr Koniusz,Wei Li,Na Zhao,Zechao Li*

Main category: cs.CV

TL;DR: Artemis是一个感知策略学习框架，使用结构化（标签，边界框）对进行视觉推理，避免纯语言推理的歧义，在视觉任务上表现优异并具有良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉感知策略框架使用自然语言进行中间推理，但纯语言推理会降低感知任务性能。核心问题在于推理形式不匹配：视觉感知需要空间和对象中心的推理，而非非结构化的语言推理。

Method: 提出Artemis框架，采用基于提议的结构化推理，每个中间步骤表示为（标签，边界框）对，捕捉可验证的视觉状态。框架基于Qwen2.5-VL-3B构建，支持中间状态跟踪和提议质量直接监督。

Result: 在定位和检测任务上表现优异，在计数和几何感知任务上具有显著泛化能力。在通用MLLM基准测试中也取得有竞争力性能，验证了空间对齐推理的有效性。

Conclusion: 结构化视觉推理（而非纯语言推理）能显著提升感知策略学习性能。空间对齐的推理为可扩展和通用的感知策略提供了原则性途径。

Abstract: Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.

中文标题: Artemis：用于感知策略学习的结构化视觉推理

中文摘要: 最近用于视觉感知策略的强化学习框架开始融入以自然语言表达的中间推理链。经验观察表明，这种纯粹基于语言的中间推理通常会降低感知任务的性能。我们认为核心问题不在于推理本身，而在于推理的形式：虽然这些推理链在非结构化的语言空间中进行语义推理，但视觉感知需要在空间和以对象为中心的空间中进行推理。为此，我们引入了Artemis，这是一个感知策略学习框架，执行基于提议的结构化推理，其中每个中间步骤都表示为（标签，边界框）对，捕捉可验证的视觉状态。这种设计能够明确跟踪中间状态，直接监督提议质量，并避免基于语言的推理引入的歧义。Artemis基于Qwen2.5-VL-3B构建，在定位和检测任务上实现了强大的性能，并在计数和几何感知任务上表现出显著的泛化能力。这些不同设置中的一致改进证实了将推理与空间表示对齐能够增强感知策略学习。由于其增强的视觉推理能力，Artemis在通用MLLM基准测试中也取得了有竞争力的性能，说明基于空间的推理为可扩展和通用的感知策略提供了原则性的途径。

</details>


### [171] [PAI-Bench: A Comprehensive Benchmark For Physical AI](https://arxiv.org/abs/2512.01989)
*Fengzhe Zhou,Jiannan Huang,Jialuo Li,Deva Ramanan,Humphrey Shi*

Main category: cs.CV

TL;DR: PAI-Bench是一个全面的物理AI基准测试，包含2808个真实世界案例，评估视频生成、条件视频生成和视频理解能力，发现当前模型在物理一致性和预测推理方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 物理AI旨在开发能够感知和预测真实世界动态的模型，但目前尚不清楚当前的多模态大语言模型和视频生成模型在多大程度上支持这些能力，因此需要建立一个统一的评估基准。

Method: 提出了PAI-Bench基准测试，包含2808个真实世界案例，涵盖视频生成、条件视频生成和视频理解三个任务，使用任务对齐的指标来评估物理合理性和领域特定推理能力。

Result: 研究发现视频生成模型虽然视觉保真度高，但往往难以保持物理一致的动态；多模态大语言模型在预测和因果解释方面表现有限，表明当前系统仍处于处理物理AI需求的早期阶段。

Conclusion: PAI-Bench为评估物理AI建立了现实基础，并突出了未来系统必须解决的关键差距，包括物理一致性和预测推理能力的提升。

Abstract: Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.

中文标题: PAI-Bench：物理AI的综合基准测试

中文摘要: 物理AI旨在开发能够感知和预测真实世界动态的模型；然而，当前多模态大语言模型和视频生成模型在多大程度上支持这些能力尚不明确。我们引入了物理AI基准测试（PAI-Bench），这是一个统一且全面的基准，评估视频生成、条件视频生成和视频理解方面的感知和预测能力，包含2808个真实世界案例，采用任务对齐的指标设计，旨在捕捉物理合理性和领域特定推理。我们的研究对近期模型进行了系统评估，结果表明视频生成模型尽管视觉保真度高，但往往难以保持物理一致的动态，而多模态大语言模型在预测和因果解释方面表现有限。这些观察表明，当前系统在处理物理AI的感知和预测需求方面仍处于早期阶段。总之，PAI-Bench为评估物理AI建立了现实基础，并突出了未来系统必须解决的关键差距。

</details>


### [172] [Learning Visual Affordance from Audio](https://arxiv.org/abs/2512.02005)
*Lidong Lu,Guo Chen,Zhu Wei,Yicheng Liu,Tong Lu*

Main category: cs.CV

TL;DR: 提出音频-视觉可供性定位(AV-AG)新任务，通过动作声音分割物体交互区域，构建首个AV-AG数据集，并提出AVAGFormer模型实现跨模态融合，在零样本泛化上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖文本指令或演示视频，存在模糊性或遮挡问题，而音频提供实时、语义丰富且视觉独立的线索，能更直观地理解交互区域。

Method: 构建首个AV-AG数据集，包含动作声音、物体图像和像素级可供性标注；提出AVAGFormer模型，配备语义条件跨模态混合器和双头解码器，有效融合音频和视觉信号进行掩码预测。

Result: AVAGFormer在AV-AG任务上达到最先进性能，超越相关任务基线；分析显示AV-AG与AVS的区别、端到端建模的优势以及各组件贡献。

Conclusion: 音频为可供性定位提供独特优势，AVAGFormer模型有效实现音频-视觉融合，数据集和代码已开源，为零样本泛化研究提供新基准。

Abstract: We introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments object interaction regions from action sounds. Unlike existing approaches that rely on textual instructions or demonstration videos, which often limited by ambiguity or occlusion, audio provides real-time, semantically rich, and visually independent cues for affordance grounding, enabling more intuitive understanding of interaction regions. To support this task, we construct the first AV-AG dataset, comprising a large collection of action sounds, object images, and pixel-level affordance annotations. The dataset also includes an unseen subset to evaluate zero-shot generalization. Furthermore, we propose AVAGFormer, a model equipped with a semantic-conditioned cross-modal mixer and a dual-head decoder that effectively fuses audio and visual signals for mask prediction. Experiments show that AVAGFormer achieves state-of-the-art performance on AV-AG, surpassing baselines from related tasks. Comprehensive analyses highlight the distinctions between AV-AG and AVS, the benefits of end-to-end modeling, and the contribution of each component. Code and dataset have been released on https://jscslld.github.io/AVAGFormer/.

中文标题: 从音频学习视觉可供性

中文摘要: 我们提出了音频-视觉可供性定位(AV-AG)这一新任务，通过动作声音分割物体交互区域。与依赖文本指令或演示视频的现有方法不同，音频为可供性定位提供实时、语义丰富且视觉独立的线索，能够更直观地理解交互区域。为支持此任务，我们构建了首个AV-AG数据集，包含大量动作声音、物体图像和像素级可供性标注。该数据集还包含未见子集以评估零样本泛化能力。此外，我们提出了AVAGFormer模型，配备语义条件跨模态混合器和双头解码器，有效融合音频和视觉信号进行掩码预测。实验表明，AVAGFormer在AV-AG任务上达到最先进性能，超越相关任务基线。综合分析突出了AV-AG与AVS的区别、端到端建模的优势以及各组件贡献。代码和数据集已在https://jscslld.github.io/AVAGFormer/上发布。

</details>


### [173] [MV-TAP: Tracking Any Point in Multi-View Videos](https://arxiv.org/abs/2512.02006)
*Jahyeok Koo,Inès Hyeonsu Kim,Mungyeom Kim,Junghyun Park,Seohyun Park,Jaeyeong Kim,Jung Yi,Seokju Cho,Seungryong Kim*

Main category: cs.CV

TL;DR: MV-TAP是一种新颖的多视角视频点跟踪方法，利用跨视角注意力机制和相机几何信息来聚合时空信息，在动态场景中实现更完整可靠的点轨迹估计。


<details>
  <summary>Details</summary>
Motivation: 多视角相机系统能够对复杂真实世界场景进行丰富观测，理解多视角设置中的动态对象已成为各种应用的核心。现有方法在多视角视频中的点跟踪方面存在局限性，需要利用跨视角信息来提高跟踪的完整性和可靠性。

Method: MV-TAP利用相机几何和跨视角注意力机制来聚合跨视角的时空信息。该方法构建了专门用于多视角跟踪的大规模合成训练数据集和真实世界评估集，通过跨视图信息融合实现更准确的点轨迹估计。

Result: 大量实验表明，MV-TAP在具有挑战性的基准测试中优于现有的点跟踪方法，为多视角点跟踪研究建立了有效的基线。

Conclusion: MV-TAP通过利用跨视角信息，在多视角视频中实现了更完整可靠的点跟踪，为多视角动态场景理解提供了有效的解决方案。

Abstract: Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications. In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information. MV-TAP utilizes camera geometry and a cross-view attention mechanism to aggregate spatio-temporal information across views, enabling more complete and reliable trajectory estimation in multi-view videos. To support this task, we construct a large-scale synthetic training dataset and real-world evaluation sets tailored for multi-view tracking. Extensive experiments demonstrate that MV-TAP outperforms existing point-tracking methods on challenging benchmarks, establishing an effective baseline for advancing research in multi-view point tracking.

中文标题: MV-TAP：多视角视频中的任意点跟踪

中文摘要: 多视角相机系统能够对复杂真实世界场景进行丰富观测，理解多视角设置中的动态对象已成为各种应用的核心。在这项工作中，我们提出了MV-TAP，一种新颖的点跟踪器，通过利用跨视角信息来跟踪动态场景多视角视频中的点。MV-TAP利用相机几何和跨视角注意力机制来聚合跨视角的时空信息，在多视角视频中实现更完整可靠的轨迹估计。为了支持这项任务，我们构建了专门用于多视角跟踪的大规模合成训练数据集和真实世界评估集。大量实验表明，MV-TAP在具有挑战性的基准测试中优于现有的点跟踪方法，为推进多视角点跟踪研究建立了有效的基线。

</details>


### [174] [AirSim360: A Panoramic Simulation Platform within Drone View](https://arxiv.org/abs/2512.02009)
*Xian Ge,Yuling Pan,Yuhang Zhang,Xiang Li,Weijun Zhang,Dizhe Zhang,Zhaoliang Wan,Xin Lin,Xiangkai Zhang,Juntao Liang,Jason Li,Wenjie Jiang,Bo Du,Ming-Hsuan Yang,Lu Qi*

Main category: cs.CV

TL;DR: AirSim360是一个用于无人机视角的全景仿真平台，专注于360度全方位理解，通过渲染对齐的数据标注范式、交互式行人感知系统和自动轨迹生成来支持各种任务，并收集了超过6万个全景样本。


<details>
  <summary>Details</summary>
Motivation: 360度全方位理解领域越来越受到关注，但缺乏大规模多样化数据是主要限制。现有仿真器无法系统地对全方位设置下的4D真实世界进行建模。

Method: 1. 渲染对齐的数据和标注范式，支持像素级几何、语义和实体级理解；2. 交互式行人感知系统，用于建模人类行为；3. 自动轨迹生成范式，支持导航任务。

Result: 收集了超过6万个全景样本，并在各种任务上进行了广泛实验，证明了仿真器的有效性。平台包括工具包、插件和收集的数据集将公开提供。

Conclusion: AirSim360是首个系统地在全方位设置下建模4D真实世界的仿真平台，为空间智能研究提供了重要工具和数据资源。

Abstract: The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.

中文标题: AirSim360：无人机视角下的全景仿真平台

中文摘要: 360度全方位理解领域在推进空间智能方面受到越来越多的关注。然而，缺乏大规模和多样化的数据仍然是一个主要限制。在这项工作中，我们提出了AirSim360，这是一个用于空中视角全方位数据的仿真平台，能够通过无人机进行广泛场景采样。具体来说，AirSim360专注于三个关键方面：用于像素级几何、语义和实体级理解的渲染对齐数据和标注范式；用于建模人类行为的交互式行人感知系统；以及支持导航任务的自动轨迹生成范式。此外，我们收集了超过6万个全景样本，并在各种任务上进行了广泛实验，以证明我们仿真器的有效性。与现有仿真器不同，我们的工作是首个系统地在全方位设置下建模4D真实世界的平台。整个平台，包括工具包、插件和收集的数据集，将在https://insta360-research-team.github.io/AirSim360-website上公开提供。

</details>


### [175] [TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models](https://arxiv.org/abs/2512.02014)
*Zhiheng Liu,Weiming Ren,Haozhe Liu,Zijian Zhou,Shoufa Chen,Haonan Qiu,Xiaoke Huang,Zhaochong An,Fanny Yang,Aditya Patel,Viktar Atliha,Tony Ng,Xiao Han,Chuyan Zhu,Chenyang Zhang,Ding Liu,Juan-Manuel Perez-Rua,Sen He,Jürgen Schmidhuber,Wenhu Chen,Ping Luo,Wei Liu,Tao Xiang,Jonas Schult,Yuren Cong*

Main category: cs.CV

TL;DR: TUNA提出了一种原生统一多模态模型，通过级联VAE编码器和表示编码器构建统一的连续视觉表示空间，实现了图像和视频的理解与生成任务在单一框架内的端到端处理。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型（UMMs）通常使用解耦的表示方法，导致理解任务和生成任务之间存在表示格式不匹配的问题。TUNA旨在通过构建统一的视觉表示空间来解决这一问题，实现更高效的多模态处理。

Method: TUNA采用级联架构：首先使用VAE编码器处理视觉输入，然后通过表示编码器将其转换为统一的连续表示。这种统一表示空间允许模型同时处理图像和视频的理解与生成任务，支持端到端训练。

Result: 在多项多模态理解和生成基准测试中，TUNA在图像/视频理解、图像/视频生成以及图像编辑任务上均取得了最先进的结果。实验表明统一表示设计优于解耦表示方案，且更强的预训练表示编码器能带来更好的性能。

Conclusion: TUNA通过统一的视觉表示空间成功实现了原生统一多模态建模，解决了表示格式不匹配问题。统一表示设计不仅提高了性能，还允许理解和生成任务相互促进而非干扰，展示了该方法的有效性和可扩展性。

Abstract: Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.

中文标题: TUNA：驯服统一视觉表示用于原生统一多模态模型

中文摘要: 统一多模态模型（UMMs）旨在单一框架内同时执行多模态理解和生成任务。我们提出了TUNA，一种原生UMM，通过级联VAE编码器和表示编码器构建统一的连续视觉表示。这种统一表示空间允许对图像和视频进行端到端处理，同时支持理解和生成任务。与先前使用解耦表示的UMMs相比，TUNA的统一视觉空间避免了由单独编码器引入的表示格式不匹配问题，在理解和生成方面均优于解耦方案。此外，我们观察到更强的预训练表示编码器在所有多模态任务中都能带来更好的性能，凸显了表示编码器的重要性。最后，在这种统一设置下，同时在理解和生成数据上进行联合训练使得两个任务能够相互促进而非干扰。我们在多模态理解和生成基准测试上的大量实验表明，TUNA在图像和视频理解、图像和视频生成以及图像编辑方面均取得了最先进的结果，证明了其统一表示设计的有效性和可扩展性。

</details>


### [176] [Generative Video Motion Editing with 3D Point Tracks](https://arxiv.org/abs/2512.02015)
*Yao-Chih Lee,Zhoutong Zhang,Jiahui Huang,Jui-Hsien Wang,Joon-Young Lee,Jia-Bin Huang,Eli Shechtman,Zhengqi Li*

Main category: cs.CV

TL;DR: 该论文提出了一种基于3D点轨迹的视频运动编辑框架，能够联合编辑相机和物体运动，解决了现有方法在复杂物体运动编辑上的局限性。


<details>
  <summary>Details</summary>
Motivation: 相机和物体运动是视频叙事的关键，但精确编辑这些捕获的运动仍然是一个重大挑战，特别是在复杂物体运动下。现有的运动控制方法要么缺乏完整场景上下文，要么只能提供有限的精细运动控制。

Method: 提出了一个基于轨迹条件的视频到视频框架，通过将视频生成模型条件化在源视频和配对的3D点轨迹上，这些轨迹表示源和目标运动。3D点轨迹建立稀疏对应关系，将丰富上下文从源视频转移到新运动中，同时保持时空一致性。相比2D轨迹，3D轨迹提供显式深度线索，使模型能够解析深度顺序和处理遮挡。采用两阶段训练策略，先在合成数据上训练，再在真实数据上微调。

Result: 模型支持多样化的运动编辑，包括联合相机/物体操作、运动转移和非刚性变形，解锁了视频编辑的新创意潜力。3D轨迹相比2D轨迹能更好地处理深度顺序和遮挡问题。

Conclusion: 提出的基于3D点轨迹的视频运动编辑框架能够精确编辑相机和物体运动，解决了现有方法的局限性，为视频编辑提供了新的创意可能性。

Abstract: Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.

中文标题: 基于3D点轨迹的生成式视频运动编辑

中文摘要: 相机和物体运动是视频叙事的核心。然而，精确编辑这些捕获的运动仍然是一个重大挑战，特别是在复杂的物体运动下。当前的运动控制图像到视频方法通常缺乏完整场景上下文以实现一致的视频编辑，而视频到视频方法虽然提供视角变化或基本的物体平移，但对精细物体运动的控制有限。我们提出了一个基于轨迹条件的视频到视频框架，能够实现相机和物体运动的联合编辑。我们通过将视频生成模型条件化在源视频和配对的3D点轨迹上来实现这一点，这些轨迹表示源和目标运动。这些3D轨迹建立了稀疏对应关系，将丰富上下文从源视频转移到新运动中，同时保持时空一致性。关键的是，与2D轨迹相比，3D轨迹提供显式深度线索，使模型能够解析深度顺序并处理遮挡，从而实现精确的运动编辑。通过在合成数据和真实数据上进行两阶段训练，我们的模型支持多样化的运动编辑，包括联合相机/物体操作、运动转移和非刚性变形，解锁了视频编辑的新创意潜力。

</details>


### [177] [Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now](https://arxiv.org/abs/2512.02016)
*Varun Varma Thozhiyoor,Shivam Tripathi,Venkatesh Babu Radhakrishnan,Anand Bhattad*

Main category: cs.CV

TL;DR: 视频生成模型在表现物体下落时存在物理错误，物体下落加速度明显低于地球重力加速度，即使经过时间尺度调整也无法纠正。研究者通过无单位的双物体测试协议发现模型违反了伽利略等效原理。通过少量数据微调可以部分纠正这一物理缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成模型越来越多地被评估为潜在的世界模型，需要验证它们是否能够编码和理解物理定律。本研究旨在调查这些模型对基本物理定律——重力的表示能力，特别是它们是否能够正确模拟物体下落运动。

Method: 首先测试现有视频生成模型生成物体下落的效果，发现加速度明显偏慢。然后排除时间尺度等混淆因素，引入无单位的双物体测试协议，通过比较两个不同高度下落物体的时间平方比与高度比的关系来测试伽利略等效原理。最后通过轻量级低秩适配器在100个单球下落视频上进行微调来纠正物理错误。

Result: 现有视频生成模型生成的物体下落加速度仅为1.81 m/s²，远低于地球重力加速度9.8 m/s²。即使进行时间尺度调整，重力误差仍然存在。双物体测试显示模型违反了伽利略等效原理。通过微调，有效重力加速度提升到6.43 m/s²，达到地球重力的65%，并且能够零样本泛化到双球下落和斜面运动。

Conclusion: 当前视频生成模型在重力物理定律表示上存在系统性缺陷，物体下落速度明显偏慢。通过少量针对性数据微调可以部分纠正这些物理错误，表明特定物理定律可以通过最小化数据进行修正，为改进视频生成模型的物理理解提供了方向。

Abstract: Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio $t_1^2/t_2^2 = h_1/h_2$, a relationship independent of $g$, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises $g_{\mathrm{eff}}$ from $1.81\,\mathrm{m/s^2}$ to $6.43\,\mathrm{m/s^2}$ (reaching $65\%$ of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.

中文标题: 生成视频中的物体下落速度比看起来更慢：模型遭受亚地球重力且不了解伽利略原理...目前如此

中文摘要: 视频生成器越来越多地被评估为潜在的世界模型，这要求它们能够编码和理解物理定律。我们研究了它们对基本定律——重力的表示能力。开箱即用的视频生成器一致生成以有效较慢加速度下落的物体。然而，这些物理测试常常受到模糊度量尺度的干扰。我们首先研究观察到的物理错误是否是这些模糊性的产物（例如，错误的帧率假设）。我们发现即使时间尺度调整也无法纠正高方差的重力伪影。为了严格地将基础物理表示与这些混淆因素隔离，我们引入了一个无单位的双物体协议，测试时间比$t_1^2/t_2^2 = h_1/h_2$，这是一个独立于$g$、焦距和尺度的关系。这种相对测试揭示了伽利略等效原理的违反。然后我们证明这种物理差距可以通过针对性专业化部分缓解。一个在仅100个单球下落视频上微调的轻量级低秩适配器将$g_{\mathrm{eff}}$从$1.81\,\mathrm{m/s^2}$提高到$6.43\,\mathrm{m/s^2}$（达到地球重力的65%）。这个专业化适配器还能零样本泛化到双球下落和斜面运动，为特定物理定律可以通过最小数据修正提供了初步证据。

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [178] [Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative Data Analysis](https://arxiv.org/abs/2512.00046)
*Angelina Parfenova,Andreas Marfurt,Alexander Denzler,Juergen Pfeffer*

Main category: cs.CL

TL;DR: 研究发现人类专家在复杂文本标注上表现更好，而LLM在简单文本上更优，两者存在互补性偏差模式。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何利用LLM自动化定性数据分析中的归纳编码过程，比较人类专家与AI在数据驱动标签生成方面的表现差异。

Method: 采用六个开源LLM与人类专家进行对比实验，使用归纳编码方法（标签从数据中自然出现），让专家评估引文难度，并比较生成的标签与黄金标准的偏差。

Result: 发现人类在复杂句子标注上表现更好但简单句子较差，LLM则相反；人类标注虽有时偏离黄金标准但更受人类认可，而某些LLM虽更接近真实标签但评价较低。

Conclusion: 人类专家和LLM在定性数据分析中存在互补性优势和偏差模式，LLM自动化需要平衡技术准确性与人类可接受性，未来可探索人机协作的混合方法。

Abstract: This paper investigates the automation of qualitative data analysis, focusing on inductive coding using large language models (LLMs). Unlike traditional approaches that rely on deductive methods with predefined labels, this research investigates the inductive process where labels emerge from the data. The study evaluates the performance of six open-source LLMs compared to human experts. As part of the evaluation, experts rated the perceived difficulty of the quotes they coded. The results reveal a peculiar dichotomy: human coders consistently perform well when labeling complex sentences but struggle with simpler ones, while LLMs exhibit the opposite trend. Additionally, the study explores systematic deviations in both human and LLM generated labels by comparing them to the golden standard from the test set. While human annotations may sometimes differ from the golden standard, they are often rated more favorably by other humans. In contrast, some LLMs demonstrate closer alignment with the true labels but receive lower evaluations from experts.

中文标题: 文本标注的归纳编码：比较人类专家与LLM在定性数据分析中的表现

中文摘要: 本文研究了定性数据分析的自动化，重点关注使用大型语言模型（LLM）进行归纳编码。与依赖预定义标签的演绎方法的传统方法不同，本研究探讨了标签从数据中自然出现的归纳过程。该研究评估了六个开源LLM与人类专家的表现。作为评估的一部分，专家对他们编码的引文感知难度进行了评分。结果显示了一个奇特的两分法：人类编码者在标注复杂句子时表现一致良好，但在处理简单句子时却遇到困难，而LLM则表现出相反的趋势。此外，该研究通过比较人类和LLM生成的标签与测试集中的黄金标准，探讨了系统性的偏差。虽然人类标注有时可能与黄金标准不同，但它们通常受到其他人类更有利的评价。相比之下，一些LLM显示出与真实标签更接近的对齐，但从专家那里获得的评价较低。

</details>


### [179] [Emergent Convergence in Multi-Agent LLM Annotation](https://arxiv.org/abs/2512.00047)
*Angelina Parfenova,Alexander Denzler,Juergen Pfeffer*

Main category: cs.CL

TL;DR: 多智能体LLM在标注任务中通过多轮讨论会自发收敛，即使没有明确角色提示也能产生类似协商的行为，展现出语义压缩和协调策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地被部署在协作环境中，但人们对它们作为黑盒智能体时如何协调知之甚少。本研究旨在探索多智能体LLM在协作任务中的协调动态和涌现行为。

Method: 在归纳编码任务中模拟了7500次多智能体、多轮讨论，生成超过125000条话语。引入了过程级指标：代码稳定性、语义自一致性、词汇置信度以及情感和收敛度量。通过分析输出嵌入的几何演化来探测更深层次的对齐信号。

Result: 结果显示：1) LLM群体在词汇和语义上收敛；2) 发展出不对称的影响模式；3) 尽管没有明确的角色提示，仍表现出类似协商的行为；4) 输出嵌入的内在维度随着轮次减少，表明语义压缩。

Conclusion: 这项工作展示了黑盒交互分析如何能够浮现涌现的协调策略，为基于内部探针的可解释性方法提供了可扩展的补充。多智能体LLM系统能够自发协调，即使在没有明确协调机制的情况下也能发展出复杂的交互模式。

Abstract: Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7500 multi-agent, multi-round discussions in an inductive coding task, generating over 125000 utterances that capture both final annotations and their interactional histories. We introduce process-level metrics: code stability, semantic self-consistency, and lexical confidence alongside sentiment and convergence measures, to track coordination dynamics. To probe deeper alignment signals, we analyze the evolving geometry of output embeddings, showing that intrinsic dimensionality declines over rounds, suggesting semantic compression. The results reveal that LLM groups converge lexically and semantically, develop asymmetric influence patterns, and exhibit negotiation-like behaviors despite the absence of explicit role prompting. This work demonstrates how black-box interaction analysis can surface emergent coordination strategies, offering a scalable complement to internal probe-based interpretability methods.

中文标题: 多智能体LLM标注中的涌现收敛

中文摘要: 大型语言模型（LLMs）越来越多地被部署在协作环境中，但人们对它们作为黑盒智能体时如何协调知之甚少。我们在归纳编码任务中模拟了7500次多智能体、多轮讨论，生成了超过125000条话语，这些话语既捕捉了最终标注，也记录了交互历史。我们引入了过程级指标：代码稳定性、语义自一致性、词汇置信度以及情感和收敛度量，以跟踪协调动态。为了探测更深层次的对齐信号，我们分析了输出嵌入的演化几何，显示内在维度随着轮次减少，表明语义压缩。结果显示，LLM群体在词汇和语义上收敛，发展出不对称的影响模式，并且尽管没有明确的角色提示，仍表现出类似协商的行为。这项工作展示了黑盒交互分析如何能够浮现涌现的协调策略，为基于内部探针的可解释性方法提供了可扩展的补充。

</details>


### [180] [Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis](https://arxiv.org/abs/2512.00214)
*Matej Klemen,Tjaša Arčon,Luka Terčon,Marko Robnik-Šikonja,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: 本文提出了一个基于语料库的智能LLM框架，用于自动化多语言语法分析，通过整合自然语言任务解释、代码生成和数据驱动推理，在UD语料库上验证了其在词序特征分析方面的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前实证语法研究虽然越来越数据驱动，但对标注语料库的系统分析仍然需要大量方法和技术努力。研究者希望探索如何利用智能大语言模型来简化这一过程，实现可解释的、基于数据的语言问题自动化分析。

Method: 提出了一个基于语料库的语法分析智能框架，整合了自然语言任务解释、代码生成和数据驱动推理。作为概念验证，将该框架应用于通用依存（UD）语料库，测试了13个词序特征和超过170种语言，评估维度包括主导词序准确性、词序覆盖完整性和分布保真度。

Result: 评估结果表明，将LLM推理与结构化语言数据相结合是可行的。系统在三个互补维度上表现出色：主导词序准确性反映了系统概括能力，词序覆盖完整性反映了识别能力，分布保真度反映了量化能力。这为基于语料库的语法查询的可解释、可扩展自动化提供了初步验证。

Conclusion: 本文提出的智能LLM框架为基于语料库的语法分析自动化提供了可行路径，展示了将大语言模型推理与结构化语言数据相结合的巨大潜力，为可解释、可扩展的语法查询自动化迈出了重要一步。

Abstract: Empirical grammar research has become increasingly data-driven, but the systematic analysis of annotated corpora still requires substantial methodological and technical effort. We explore how agentic large language models (LLMs) can streamline this process by reasoning over annotated corpora and producing interpretable, data-grounded answers to linguistic questions. We introduce an agentic framework for corpus-grounded grammatical analysis that integrates concepts such as natural-language task interpretation, code generation, and data-driven reasoning. As a proof of concept, we apply it to Universal Dependencies (UD) corpora, testing it on multilingual grammatical tasks inspired by the World Atlas of Language Structures (WALS). The evaluation spans 13 word-order features and over 170 languages, assessing system performance across three complementary dimensions - dominant-order accuracy, order-coverage completeness, and distributional fidelity - which reflect how well the system generalizes, identifies, and quantifies word-order variations. The results demonstrate the feasibility of combining LLM reasoning with structured linguistic data, offering a first step toward interpretable, scalable automation of corpus-based grammatical inquiry.

中文标题: 面向多语言语法分析的基于语料库的智能大语言模型

中文摘要: 实证语法研究日益数据驱动，但对标注语料库的系统分析仍需要大量的方法和技术努力。我们探索了智能大语言模型（LLMs）如何通过推理标注语料库并产生可解释的、基于数据的语言问题答案来简化这一过程。我们引入了一个基于语料库的语法分析智能框架，该框架整合了自然语言任务解释、代码生成和数据驱动推理等概念。作为概念验证，我们将其应用于通用依存（UD）语料库，在受世界语言结构图谱（WALS）启发的多语言语法任务上进行测试。评估涵盖13个词序特征和超过170种语言，从三个互补维度评估系统性能：主导词序准确性、词序覆盖完整性和分布保真度，这些维度反映了系统在概括、识别和量化词序变化方面的表现。结果表明，将LLM推理与结构化语言数据相结合是可行的，为基于语料库的语法查询的可解释、可扩展自动化迈出了第一步。

</details>


### [181] [Minimal-Edit Instruction Tuning for Low-Resource Indic GEC](https://arxiv.org/abs/2512.00219)
*Akhil Rajeev P*

Main category: cs.CL

TL;DR: 该研究提出了一种无需数据增强的指令微调方法，用于低资源印度语言语法纠错。使用12B参数的GEMMA 3模型，通过4位精度和参数高效微调进行指令微调，结合确定性约束解码策略，在马来语和印地语上取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 印度语言的语法纠错面临三大挑战：有限的监督数据、多样的文字脚本和丰富的形态变化。传统基于数据增强的方法成本高且复杂，需要一种更高效、可复现的替代方案。

Method: 采用指令微调的大语言模型和保守解码策略。使用12B参数的GEMMA 3模型，在bnb 4位精度下进行参数高效微调，采用Alpaca风格格式化。解码过程采用确定性约束感知程序，结合轻量级规范化器，鼓励最小化且保持语义的编辑。推理时使用基于确定性错误分类器分类体系、标签分布和训练语料优先级排序合成的语言特定提示。

Result: 在官方未调优的GLEU评估中，系统在马来语上获得92.41分（总体第六），在印地语上获得81.44分（总体第三）。这些结果表明该方法为印度语言语法纠错提供了可复现且计算高效的替代方案。

Conclusion: 分类器引导的提示设计、基于适配器的指令微调和确定性解码为印度语言语法纠错提供了可复现且计算高效的替代方案，避免了复杂的数据增强流程。该方法也为未来在更强的形态句法约束和以人为中心的保守编辑评估方面提供了研究方向。

Abstract: Grammatical error correction for Indic languages faces limited supervision, diverse scripts, and rich morphology. We propose an augmentation-free setup that uses instruction-tuned large language models and conservative decoding. A 12B GEMMA 3 model is instruction-tuned in bnb 4-bit precision with parameter-efficient fine-tuning (PEFT) and Alpaca-style formatting. Decoding follows a deterministic, constraint-aware procedure with a lightweight normaliser that encourages minimal, meaning-preserving edits. We operationalise inference, subsequent to instruction fine-tuning (IFT), via a fixed, language-specific prompt directly synthesised from a deterministic error classifier's taxonomy, label distributions, and precedence ordering computed on the training corpus.
  Under the official untuned GLEU evaluation, the system scores 92.41 on Malayalam, sixth overall, and 81.44 on Hindi, third overall. These results indicate that classifier-informed prompt design, adapter-based instruction tuning, and deterministic decoding provide a reproducible and a computationally efficient alternative to augmentation-centred pipelines for Indic GEC. The approach also motivates future work on stronger morphosyntactic constraints and human-centred evaluation of conservative edits.

中文标题: 低资源印度语言语法纠错的最小编辑指令微调

中文摘要: 印度语言的语法纠错面临有限的监督、多样的文字脚本和丰富的形态变化。我们提出了一种无需数据增强的设置，使用指令微调的大语言模型和保守解码策略。一个12B参数的GEMMA 3模型在bnb 4位精度下通过参数高效微调和Alpaca风格格式化进行指令微调。解码遵循确定性约束感知程序，结合轻量级规范化器，鼓励最小化且保持语义的编辑。我们在指令微调后通过固定的语言特定提示进行推理操作，该提示直接从确定性错误分类器的分类体系、标签分布和在训练语料上计算的优先级排序合成。

在官方未调优的GLEU评估中，系统在马来语上获得92.41分（总体第六），在印地语上获得81.44分（总体第三）。这些结果表明，分类器引导的提示设计、基于适配器的指令微调和确定性解码为印度语言语法纠错提供了可复现且计算高效的替代方案，避免了以数据增强为中心的流程。该方法也为未来在更强的形态句法约束和以人为中心的保守编辑评估方面提供了研究动机。

</details>


### [182] [BHRAM-IL: A Benchmark for Hallucination Recognition and Assessment in Multiple Indian Languages](https://arxiv.org/abs/2512.01852)
*Hrishikesh Terdalkar,Kirtan Bhojani,Aryan Dongare,Omm Aditya Behera*

Main category: cs.CL

TL;DR: BHRAM-IL是一个用于评估多种印度语言中大型语言模型幻觉问题的基准测试，涵盖印地语、古吉拉特语、马拉地语、奥里亚语和英语，包含36,047个精心设计的问题，用于检测模型生成的看似合理但错误或误导性的输出。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多语言应用中部署日益广泛，但经常生成看似合理但实际错误或误导性的输出（即幻觉）。虽然英语中的幻觉检测已得到广泛研究，但资源不足的印度语言在这方面仍基本未被探索。因此需要建立一个专门针对印度语言的幻觉识别和评估基准。

Method: 创建了BHRAM-IL基准测试，涵盖印地语、古吉拉特语、马拉地语、奥里亚语和英语五种语言。数据集包含36,047个精心设计的问题，分为九个类别，涵盖事实性、数值、推理和语言任务。在10,265个问题的子集上评估了14个最先进的多语言LLM，使用特定类别指标（归一化到0-1范围）分析跨语言和事实性幻觉。

Result: 评估了14个多语言LLM，所有类别和模型的聚合得分为0.23，语言校正模糊得分为0.385。这证明了BHRAM-IL在幻觉评估方面的有效性。数据集和代码已在GitHub和HuggingFace上开源，支持未来多语言幻觉检测和缓解研究。

Conclusion: BHRAM-IL是首个针对多种印度语言的幻觉识别和评估基准，填补了资源不足语言在幻觉检测研究方面的空白。基准测试展示了在多语言环境中评估LLM幻觉的有效性，并为未来研究提供了宝贵资源。

Abstract: Large language models (LLMs) are increasingly deployed in multilingual applications but often generate plausible yet incorrect or misleading outputs, known as hallucinations. While hallucination detection has been studied extensively in English, under-resourced Indian languages remain largely unexplored. We present BHRAM-IL, a benchmark for hallucination recognition and assessment in multiple Indian languages, covering Hindi, Gujarati, Marathi, Odia, along with English. The benchmark comprises 36,047 curated questions across nine categories spanning factual, numerical, reasoning, and linguistic tasks. We evaluate 14 state-of-the-art multilingual LLMs on a benchmark subset of 10,265 questions, analyzing cross-lingual and factual hallucinations across languages, models, scales, categories, and domains using category-specific metrics normalized to (0,1) range. Aggregation over all categories and models yields a primary score of 0.23 and a language-corrected fuzzy score of 0.385, demonstrating the usefulness of BHRAM-IL for hallucination-focused evaluation. The dataset, and the code for generation and evaluation are available on GitHub (https://github.com/sambhashana/BHRAM-IL/) and HuggingFace (https://huggingface.co/datasets/sambhashana/BHRAM-IL/) to support future research in multilingual hallucination detection and mitigation.

中文标题: BHRAM-IL：多印度语言幻觉识别与评估基准

中文摘要: 大型语言模型越来越多地部署在多语言应用中，但经常生成看似合理但实际错误或误导性的输出，即幻觉。虽然幻觉检测在英语中已得到广泛研究，但资源不足的印度语言在这方面仍基本未被探索。我们提出了BHRAM-IL，一个用于多印度语言幻觉识别和评估的基准测试，涵盖印地语、古吉拉特语、马拉地语、奥里亚语以及英语。该基准包含36,047个精心设计的问题，涵盖九个类别，包括事实性、数值、推理和语言任务。我们在10,265个问题的基准子集上评估了14个最先进的多语言LLM，使用特定类别指标（归一化到0-1范围）分析跨语言和事实性幻觉，涵盖语言、模型、规模、类别和领域。所有类别和模型的聚合得分为0.23，语言校正模糊得分为0.385，证明了BHRAM-IL在幻觉评估方面的有效性。数据集以及生成和评估代码已在GitHub和HuggingFace上提供，以支持未来多语言幻觉检测和缓解研究。

</details>


### [183] [OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion](https://arxiv.org/abs/2512.00234)
*Sai Koneru,Matthias Huck,Jan Niehues*

Main category: cs.CL

TL;DR: OmniFusion是一个端到端多模态翻译系统，通过将多模态基础模型与翻译大语言模型融合，支持语音、图像和文本的多种翻译组合，在同步翻译中减少延迟并提高质量。


<details>
  <summary>Details</summary>
Motivation: 当前纯文本翻译大语言模型虽然进步显著，但只能用于级联语音翻译管道，导致额外延迟且无法利用多模态上下文（如图像）进行消歧。多模态基础模型具备跨模态能力但缺乏专业翻译性能。需要构建能同时处理多模态输入并实现高质量翻译的系统。

Method: 提出新颖的融合策略，将预训练多模态基础模型（Omni 2.5-7B）多个层的隐藏状态连接到翻译大语言模型（SeedX PPO-7B），实现联合端到端训练。支持语音到文本、语音和图像到文本、文本和图像到文本三种翻译模式。

Result: OmniFusion有效利用音频和视觉输入，在同步语音翻译中相比级联管道减少1秒延迟，同时提高了整体翻译质量。模型支持多种翻译组合，代码已开源。

Conclusion: 通过融合多模态基础模型与翻译大语言模型，OmniFusion实现了高效的多模态翻译系统，在延迟和翻译质量方面均优于传统级联方法，为同步多语言多模态翻译提供了有效解决方案。

Abstract: There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality\footnote{Code is available at https://github.com/saikoneru/OmniFusion}.

中文标题: OmniFusion：通过模块化融合实现同步多语言多模态翻译

中文摘要: 开源纯文本翻译大语言模型（LLMs）在语言覆盖范围和质量方面取得了显著进展。然而，这些模型只能用于语音翻译（ST）的级联管道，先进行自动语音识别再进行翻译。这引入了额外的延迟，在同步ST（SimulST）中尤为关键，并且阻止了模型利用多模态上下文（如图像）进行消歧。预训练的多模态基础模型（MMFMs）已经具备跨多个模态的强大感知和推理能力，但通常缺乏专用翻译LLMs的多语言覆盖范围和专业翻译性能。为了构建有效的多模态翻译系统，我们提出了一种将MMFMs与翻译LLMs融合的端到端方法。我们引入了一种新颖的融合策略，将预训练MMFM多个层的隐藏状态连接到翻译LLM，实现联合端到端训练。由此产生的模型OmniFusion基于Omni 2.5-7B作为MMFM和SeedX PPO-7B作为翻译LLM构建，可以执行语音到文本、语音和图像到文本以及文本和图像到文本的翻译。实验表明，OmniFusion有效利用了音频和视觉输入，与级联管道相比，在SimulST中实现了1秒的延迟减少，同时也提高了整体翻译质量。

</details>


### [184] [Lost without translation -- Can transformer (language models) understand mood states?](https://arxiv.org/abs/2512.00274)
*Prakrithi Shivaprakash,Diptadhi Mukherjee,Lekhansh Shukla,Animesh Mukherjee,Prabhat Chand,Pratima Murthy*

Main category: cs.CL

TL;DR: 研究表明，当前的语言模型无法直接从印度语言中有效理解情绪状态，需要通过翻译到英语或中文才能获得较好的聚类效果，这对印度精神健康应用构成根本障碍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在精神病学中显示出潜力，但主要是英语中心的。不同语言有其独特的痛苦表达方式，模型在其他语言中理解情绪状态的能力尚不清楚，特别是在印度语言中。

Method: 收集了11种印度语言中四种情绪状态（抑郁、正常情绪、欣快躁狂、烦躁躁狂）的247个独特短语。测试了七种实验条件，比较了：（a）原生和罗马化脚本的直接嵌入（使用多语言和印度特定模型）和（b）翻译成英语和中文的短语嵌入的k-means聚类性能。使用基于调整兰德指数、归一化互信息、同质性和完整性的综合评分来衡量性能。

Result: 印度语言的直接嵌入无法有效聚类情绪状态（综合评分=0.002）。所有基于翻译的方法都显示出显著改进。使用Gemini翻译的英语（综合=0.60）和人工翻译的英语（综合=0.61）嵌入gemini-001获得了较高性能。令人惊讶的是，人工翻译的英语进一步翻译成中文并使用中文模型嵌入，表现最佳（综合=0.67）。专门的印度模型（IndicBERT和Sarvam-M）表现不佳。

Conclusion: 当前模型无法直接从印度语言中有效表示情绪状态，这对它们在印度精神病学诊断或治疗应用构成了根本障碍。虽然高质量翻译可以弥合这一差距，但依赖专有模型或复杂翻译流程是不可持续的。模型必须首先建立对多样化本地语言的理解，才能在全球精神健康中有效应用。

Abstract: Background: Large Language Models show promise in psychiatry but are English-centric. Their ability to understand mood states in other languages is unclear, as different languages have their own idioms of distress. Aim: To quantify the ability of language models to faithfully represent phrases (idioms of distress) of four distinct mood states (depression, euthymia, euphoric mania, dysphoric mania) expressed in Indian languages. Methods: We collected 247 unique phrases for the four mood states across 11 Indic languages. We tested seven experimental conditions, comparing k-means clustering performance on: (a) direct embeddings of native and Romanised scripts (using multilingual and Indic-specific models) and (b) embeddings of phrases translated to English and Chinese. Performance was measured using a composite score based on Adjusted Rand Index, Normalised Mutual Information, Homogeneity and Completeness. Results: Direct embedding of Indic languages failed to cluster mood states (Composite Score = 0.002). All translation-based approaches showed significant improvement. High performance was achieved using Gemini-translated English (Composite=0.60) and human-translated English (Composite=0.61) embedded with gemini-001. Surprisingly, human-translated English, further translated into Chinese and embedded with a Chinese model, performed best (Composite = 0.67). Specialised Indic models (IndicBERT and Sarvam-M) performed poorly. Conclusion: Current models cannot meaningfully represent mood states directly from Indic languages, posing a fundamental barrier to their psychiatric application for diagnostic or therapeutic purposes in India. While high-quality translation bridges this gap, reliance on proprietary models or complex translation pipelines is unsustainable. Models must first be built to understand diverse local languages to be effective in global mental health.

中文标题: 迷失在翻译中——Transformer（语言模型）能理解情绪状态吗？

中文摘要: 背景：大型语言模型在精神病学中显示出潜力，但主要是英语中心的。它们在其他语言中理解情绪状态的能力尚不清楚，因为不同语言有其独特的痛苦表达方式。目的：量化语言模型在印度语言中忠实表示四种不同情绪状态（抑郁、正常情绪、欣快躁狂、烦躁躁狂）短语（痛苦表达方式）的能力。方法：我们收集了11种印度语言中四种情绪状态的247个独特短语。我们测试了七种实验条件，比较了：（a）原生和罗马化脚本的直接嵌入（使用多语言和印度特定模型）和（b）翻译成英语和中文的短语嵌入的k-means聚类性能。使用基于调整兰德指数、归一化互信息、同质性和完整性的综合评分来衡量性能。结果：印度语言的直接嵌入无法有效聚类情绪状态（综合评分=0.002）。所有基于翻译的方法都显示出显著改进。使用Gemini翻译的英语（综合=0.60）和人工翻译的英语（综合=0.61）嵌入gemini-001获得了较高性能。令人惊讶的是，人工翻译的英语进一步翻译成中文并使用中文模型嵌入，表现最佳（综合=0.67）。专门的印度模型（IndicBERT和Sarvam-M）表现不佳。结论：当前模型无法直接从印度语言中有效表示情绪状态，这对它们在印度精神病学诊断或治疗应用构成了根本障碍。虽然高质量翻译可以弥合这一差距，但依赖专有模型或复杂翻译流程是不可持续的。模型必须首先建立对多样化本地语言的理解，才能在全球精神健康中有效应用。

</details>


### [185] [EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education](https://arxiv.org/abs/2512.00290)
*Guoqing Ma,Jia Zhu,Hanghui Guo,Weijie Shi,Yue Cui,Jiawei Shen,Zilong Li,Yidan Liang*

Main category: cs.CL

TL;DR: EduEval是一个用于评估中文教育中大语言模型的分层认知基准，包含6个认知维度、24种任务类型、超过11,000道题目，评估显示模型在事实性任务上表现良好但在创造性任务上存在困难，开源模型在某些推理任务上超越专有模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在教育应用中具有巨大潜力，但未经审查的部署可能对教育标准构成风险，因此需要建立严格的评估框架来确保模型在教育环境中的可靠性和有效性。

Method: 提出了EduAbility分类法，统一布鲁姆分类法和韦伯知识深度理论，构建包含记忆、理解、应用、推理、创造和伦理六个认知维度的分层框架；整合真实考试题目、课堂对话、学生作文和专家设计的提示；构建包含24种任务类型、超过11,000道题目的数据集；在零样本和少样本设置下评估14个领先的大语言模型。

Result: 模型在事实性任务上表现良好，但在课堂对话分类方面存在困难，在创造性内容生成方面表现不一致；有趣的是，几个开源模型在复杂的教育推理任务上超越了专有系统；少样本提示在不同认知维度上显示出不同的效果。

Conclusion: EduEval为评估中文教育中的大语言模型提供了全面的分层基准，揭示了模型在不同认知维度上的表现差异，表明不同的教育目标需要定制化的方法，为开发专门针对中文教育任务优化的模型提供了有针对性的基准指标。

Abstract: Large language models (LLMs) demonstrate significant potential for educational applications. However, their unscrutinized deployment poses risks to educational standards, underscoring the need for rigorous evaluation. We introduce EduEval, a comprehensive hierarchical benchmark for evaluating LLMs in Chinese K-12 education. This benchmark makes three key contributions: (1) Cognitive Framework: We propose the EduAbility Taxonomy, which unifies Bloom's Taxonomy and Webb's Depth of Knowledge to organize tasks across six cognitive dimensions including Memorization, Understanding, Application, Reasoning, Creativity, and Ethics. (2) Authenticity: Our benchmark integrates real exam questions, classroom conversation, student essays, and expert-designed prompts to reflect genuine educational challenges; (3) Scale: EduEval comprises 24 distinct task types with over 11,000 questions spanning primary to high school levels. We evaluate 14 leading LLMs under both zero-shot and few-shot settings, revealing that while models perform well on factual tasks, they struggle with classroom dialogue classification and exhibit inconsistent results in creative content generation. Interestingly, several open source models outperform proprietary systems on complex educational reasoning. Few-shot prompting shows varying effectiveness across cognitive dimensions, suggesting that different educational objectives require tailored approaches. These findings provide targeted benchmarking metrics for developing LLMs specifically optimized for diverse Chinese educational tasks.

中文标题: EduEval：用于评估中文教育中大语言模型的分层认知基准

中文摘要: 大语言模型在教育应用中展现出巨大潜力，但其未经审查的部署对教育标准构成风险，凸显了严格评估的必要性。我们提出了EduEval，这是一个用于评估中文K-12教育中大语言模型的全面分层基准。该基准有三个关键贡献：(1)认知框架：我们提出了EduAbility分类法，统一了布鲁姆分类法和韦伯知识深度理论，将任务组织在记忆、理解、应用、推理、创造和伦理六个认知维度上。(2)真实性：我们的基准整合了真实考试题目、课堂对话、学生作文和专家设计的提示，以反映真实的教育挑战；(3)规模：EduEval包含24种不同的任务类型，超过11,000道题目，涵盖小学到高中各个年级。我们在零样本和少样本设置下评估了14个领先的大语言模型，结果显示虽然模型在事实性任务上表现良好，但在课堂对话分类方面存在困难，在创造性内容生成方面表现出不一致的结果。有趣的是，几个开源模型在复杂的教育推理任务上超越了专有系统。少样本提示在不同认知维度上显示出不同的效果，表明不同的教育目标需要定制化的方法。这些发现为开发专门针对多样化中文教育任务优化的大语言模型提供了有针对性的基准指标。

</details>


### [186] [Comparative Analysis of 47 Context-Based Question Answer Models Across 8 Diverse Datasets](https://arxiv.org/abs/2512.00323)
*Muhammad Muneeb,David B. Ascher,Ahsan Baidar Bakht*

Main category: cs.CL

TL;DR: 本研究对47个CBQA模型在8个数据集上进行基准测试，发现ahotrod/electra_large_discriminator_squad2_512表现最佳（平均43%准确率），模型性能受上下文长度、模型大小、答案长度和上下文复杂度影响。


<details>
  <summary>Details</summary>
Motivation: CBQA模型在实际应用中需要针对不同数据集进行微调，这增加了实施成本。本研究旨在识别在多样化数据集上无需额外微调就能表现良好的模型，为实际应用提供即用型解决方案。

Method: 使用来自Hugging Face的47个CBQA模型，在8个不同的问答数据集上进行基准测试，评估模型性能而不进行额外微调。使用遗传算法整合多个模型的响应以提高准确率。

Result: ahotrod/electra_large_discriminator_squad2_512在所有数据集上平均准确率为43%，在多个特定数据集上表现优异。模型性能与上下文长度、模型大小、答案长度和上下文复杂度相关。遗传算法能有效提升整体准确率。

Conclusion: 在SQuAD数据集上训练的模型在跨数据集评估中表现最佳，特别是ahotrod/electra_large_discriminator_squad2_512。模型选择应考虑上下文长度、答案长度和复杂度等因素。遗传算法集成方法能进一步提升性能。

Abstract: Context-based question answering (CBQA) models provide more accurate and relevant answers by considering the contextual information. They effectively extract specific information given a context, making them functional in various applications involving user support, information retrieval, and educational platforms. In this manuscript, we benchmarked the performance of 47 CBQA models from Hugging Face on eight different datasets. This study aims to identify the best-performing model across diverse datasets without additional fine-tuning. It is valuable for practical applications where the need to retrain models for specific datasets is minimized, streamlining the implementation of these models in various contexts. The best-performing models were trained on the SQuAD v2 or SQuAD v1 datasets. The best-performing model was ahotrod/electra_large_discriminator_squad2_512, which yielded 43\% accuracy across all datasets. We observed that the computation time of all models depends on the context length and the model size. The model's performance usually decreases with an increase in the answer length. Moreover, the model's performance depends on the context complexity. We also used the Genetic algorithm to improve the overall accuracy by integrating responses from other models. ahotrod/electra_large_discriminator_squad2_512 generated the best results for bioasq10b-factoid (65.92\%), biomedical\_cpgQA (96.45\%), QuAC (11.13\%), and Question Answer Dataset (41.6\%). Bert-large-uncased-whole-word-masking-finetuned-squad achieved an accuracy of 82\% on the IELTS dataset.

中文标题: 47种基于上下文的问答模型在8个多样化数据集上的比较分析

中文摘要: 基于上下文的问答（CBQA）模型通过考虑上下文信息提供更准确和相关的答案。它们能有效地在给定上下文中提取特定信息，使其在用户支持、信息检索和教育平台等各种应用中发挥作用。在本研究中，我们在八个不同的数据集上对来自Hugging Face的47个CBQA模型进行了基准测试。本研究旨在识别在多样化数据集上表现最佳的模型，无需额外微调。这对于实际应用非常有价值，因为可以减少为特定数据集重新训练模型的需求，从而简化这些模型在各种上下文中的实施。表现最佳的模型是在SQuAD v2或SQuAD v1数据集上训练的。表现最佳的模型是ahotrod/electra_large_discriminator_squad2_512，在所有数据集上获得了43%的准确率。我们观察到所有模型的计算时间取决于上下文长度和模型大小。模型的性能通常随着答案长度的增加而下降。此外，模型的性能取决于上下文的复杂性。我们还使用遗传算法通过整合其他模型的响应来提高整体准确率。ahotrod/electra_large_discriminator_squad2_512在bioasq10b-factoid（65.92%）、biomedical_cpgQA（96.45%）、QuAC（11.13%）和Question Answer Dataset（41.6%）上产生了最佳结果。Bert-large-uncased-whole-word-masking-finetuned-squad在IELTS数据集上达到了82%的准确率。

</details>


### [187] [MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification](https://arxiv.org/abs/2512.01443)
*Xabier de Zuazo,Ibon Saratxaga,Eva Navas*

Main category: cs.CL

TL;DR: MEGConformer使用Conformer架构解码原始MEG信号，针对语音检测和音素分类任务，通过MEG专用数据增强和实例级归一化等技术，在LibriBrain 2025比赛中取得了优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 针对脑磁图（MEG）信号解码中的两个基础任务——语音检测和音素分类，需要开发能够处理原始306通道MEG信号的鲁棒解码器，以在LibriBrain 2025比赛中超越基线性能。

Method: 采用紧凑型Conformer架构处理原始MEG信号，包含轻量级卷积投影层和任务特定头部。语音检测任务使用MEG导向的SpecAugment数据增强；音素分类使用逆平方根类别加权和动态分组加载器处理100样本平均示例；关键采用实例级归一化缓解分布偏移。

Result: 在官方标准轨道分割和使用F1-macro进行模型选择下，最佳系统在排行榜上取得了88.9%（语音检测）和65.8%（音素分类）的性能，超越了比赛基线并在两个任务中都进入了前10名。

Conclusion: MEGConformer证明了Conformer架构在处理原始MEG信号解码任务中的有效性，通过任务特定的优化策略和实例级归一化技术，能够实现鲁棒的语音和音素分类性能。

Abstract: We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments.

中文标题: MEGConformer：基于Conformer的MEG解码器用于鲁棒的语音和音素分类

中文摘要: 我们为LibriBrain 2025 PNPL比赛提出了基于Conformer的解码器，针对两个基础MEG任务：语音检测和音素分类。我们的方法将紧凑型Conformer适配到原始306通道MEG信号，包含轻量级卷积投影层和任务特定头部。对于语音检测，MEG导向的SpecAugment提供了首次MEG特定数据增强的探索。对于音素分类，我们使用逆平方根类别加权和动态分组加载器来处理100样本平均示例。此外，简单的实例级归一化对于缓解保留分割上的分布偏移至关重要。使用官方标准轨道分割和F1-macro进行模型选择，我们的最佳系统在排行榜上取得了88.9%（语音）和65.8%（音素）的性能，超越了比赛基线并在两个任务中都进入了前10名。更多实现细节、技术文档、源代码和检查点可在https://github.com/neural2speech/libribrain-experiments获取。

</details>


### [188] [Evidence-Guided Schema Normalization for Temporal Tabular Reasoning](https://arxiv.org/abs/2512.00329)
*Ashish Thanga,Vibhu Dixit,Abhilash Shankarampeta,Vivek Gupta*

Main category: cs.CL

TL;DR: 本文提出了一种基于证据的时序表格模式规范化方法，通过生成3NF模式、SQL查询和执行来提升时序推理能力，发现模式设计质量对问答精度的影响大于模型容量。


<details>
  <summary>Details</summary>
Motivation: 当前QA系统在处理不断演化的半结构化表格（如维基百科信息框）时面临时序推理挑战，需要更好的方法来处理表格数据的时序变化和结构复杂性。

Method: 采用三步法：1) 从维基百科信息框生成符合第三范式（3NF）的模式；2) 生成SQL查询；3) 执行查询。核心是建立三个基于证据的原则：保持上下文的规范化、减少歧义的语义命名、一致的时序锚定。

Result: 最佳配置（Gemini 2.5 Flash模式 + Gemini-2.0-Flash查询）达到80.39 EM，相比基线（68.89 EM）提升了16.8%。研究挑战了模型规模假设，发现模式设计质量对QA精度的影响大于模型容量。

Conclusion: 时序表格推理中，模式设计质量比模型容量更重要。基于证据的规范化原则（上下文保持、语义命名、时序锚定）能显著提升问答系统性能，为处理演化表格数据提供了有效方法。

Abstract: Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\% improvement over the baseline (68.89 EM).

中文标题: 基于证据的时序表格模式规范化推理

中文摘要: 在演化半结构化表格上进行时序推理对当前问答系统构成挑战。我们提出了一种基于SQL的方法，包括：(1) 从维基百科信息框生成3NF模式，(2) 生成SQL查询，(3) 执行查询。我们的核心发现挑战了模型规模假设：模式设计质量对问答精度的影响大于模型容量。我们建立了三个基于证据的原则：保持上下文的规范化、减少歧义的语义命名、一致的时序锚定。我们的最佳配置（Gemini 2.5 Flash模式 + Gemini-2.0-Flash查询）达到80.39 EM，相比基线（68.89 EM）提升了16.8%。

</details>


### [189] [IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages](https://arxiv.org/abs/2512.00333)
*Ayush Maheshwari,Kaushal Sharma,Vivek Patel,Aditya Maheshwari*

Main category: cs.CL

TL;DR: IndicParam是一个专门评估大语言模型在低资源印度语言上表现的人工标注基准，包含13,000多道选择题，覆盖11种语言，测试结果显示即使是表现最好的GPT-5平均准确率也只有45.0%


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在高资源多语言任务上表现出色，但低资源和极低资源的印度语言严重缺乏评估基准，需要建立专门的测试集来评估模型在这些语言上的真实能力

Method: 创建了包含13,000多道人工标注选择题的基准，覆盖11种印度语言（分为低资源和极低资源两类），将问题标注为知识导向型或纯语言型，评估多种问题格式，包括列表匹配、断言-推理对、序列排序等

Result: 评估了19个大语言模型（包括专有和开源模型），表现最好的GPT-5平均准确率仅为45.0%，DeepSeek-3.2为43.1%，Claude-4.5为42.7%，表明模型在低资源印度语言上表现显著不足

Conclusion: IndicParam揭示了跨语言迁移的局限性，为印度语言建立了一个具有挑战性的基准，有助于推动模型在低资源语言上的改进

Abstract: While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.

中文标题: IndicParam：评估大语言模型在低资源印度语言上的基准

中文摘要: 虽然大语言模型在高资源多语言任务上表现出色，但低资源和极低资源的印度语言仍然严重缺乏评估。我们提出了IndicParam，这是一个包含13,000多道人工标注选择题的基准，覆盖11种此类语言（尼泊尔语、古吉拉特语、马拉地语、奥里亚语为低资源语言；多格拉语、迈蒂利语、拉贾斯坦语、梵语、博多语、桑塔利语、孔卡尼语为极低资源语言），外加梵语-英语混合语集。我们评估了19个大语言模型，包括专有和开源模型，结果显示即使是表现最好的GPT-5平均准确率也只有45.0%，其次是DeepSeek-3.2（43.1%）和Claude-4.5（42.7%）。我们还将每个问题标注为知识导向型或纯语言型，以区分事实回忆和语法熟练度。此外，我们评估了大语言模型处理多样化问题格式的能力——如基于列表的匹配、断言-推理对和序列排序——以及传统的选择题。IndicParam提供了对跨语言迁移局限性的见解，并为印度语言建立了一个具有挑战性的基准。数据集可在https://huggingface.co/datasets/bharatgenai/IndicParam获取。运行基准的脚本可在https://github.com/ayushbits/IndicParam找到。

</details>


### [190] [CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA](https://arxiv.org/abs/2512.00360)
*Vsevolod Kovalev,Parteek Kumar*

Main category: cs.CL

TL;DR: 提出了CourseTimeQA基准数据集（52.3小时讲座视频，902个查询）和CrossFusion-RAG方法，在单GPU延迟约束下显著提升了时间戳问答性能，相比BLIP-2检索器nDCG@10提高0.10，MRR提高0.08，延迟仅1.55秒。


<details>
  <summary>Details</summary>
Motivation: 研究在单GPU延迟/内存预算约束下，对教育讲座视频进行时间戳问答的问题。现有方法要么性能不足，要么计算开销过大，需要一种既高效又准确的解决方案。

Method: 提出CrossFusion-RAG方法：使用冻结编码器保持效率，学习512->768视觉投影，采用浅层查询无关交叉注意力处理ASR和视频帧，加入时间一致性正则化器，配合小型交叉注意力重排序器，实现轻量级跨模态融合。

Result: 在CourseTimeQA基准上，CrossFusion-RAG相比BLIP-2检索器nDCG@10提高0.10，MRR提高0.08，在单个A100上实现约1.55秒中位端到端延迟。在ASR噪声下表现鲁棒，并提供了全面的诊断和训练细节。

Conclusion: CrossFusion-RAG在延迟约束下显著提升了讲座视频时间戳问答性能，为教育视频分析提供了高效实用的解决方案，同时建立了可重复比较的基准和评估框架。

Abstract: We study timestamped question answering over educational lecture videos under a single-GPU latency/memory budget. Given a natural-language query, the system retrieves relevant timestamped segments and synthesizes a grounded answer. We present CourseTimeQA (52.3 h, 902 queries across six courses) and a lightweight, latency-constrained cross-modal retriever (CrossFusion-RAG) that combines frozen encoders, a learned 512->768 vision projection, shallow query-agnostic cross-attention over ASR and frames with a temporal-consistency regularizer, and a small cross-attentive reranker. On CourseTimeQA, CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over a strong BLIP-2 retriever while achieving approximately 1.55 s median end-to-end latency on a single A100. Closest comparators (zero-shot CLIP multi-frame pooling; CLIP + cross-encoder reranker + MMR; learned late-fusion gating; text-only hybrid with cross-encoder reranking and its MMR variant; caption-augmented text retrieval; non-learned temporal smoothing) are evaluated under matched hardware and indexing. We report robustness across ASR noise (WER quartiles), diagnostics for temporal localization, and full training/tuning details to support reproducible comparison.

中文标题: CourseTimeQA：一个讲座视频基准和面向延迟约束的跨模态融合方法用于时间戳问答

中文摘要: 我们在单GPU延迟/内存预算下研究教育讲座视频的时间戳问答。给定自然语言查询，系统检索相关的时间戳片段并合成基于证据的答案。我们提出了CourseTimeQA（52.3小时，涵盖六门课程的902个查询）和一个轻量级、延迟约束的跨模态检索器（CrossFusion-RAG），它结合了冻结编码器、学习的512->768视觉投影、在ASR和帧上进行浅层查询无关交叉注意力（带有时间一致性正则化器）以及一个小型交叉注意力重排序器。在CourseTimeQA上，CrossFusion-RAG相比强大的BLIP-2检索器将nDCG@10提高了0.10，MRR提高了0.08，同时在单个A100上实现了约1.55秒的中位端到端延迟。我们评估了最接近的比较方法（零样本CLIP多帧池化；CLIP+交叉编码器重排序器+MMR；学习的后期融合门控；文本混合检索+交叉编码器重排序及其MMR变体；字幕增强文本检索；非学习的时间平滑），并在匹配的硬件和索引条件下进行了比较。我们报告了在ASR噪声（WER四分位数）下的鲁棒性、时间定位的诊断以及完整的训练/调优细节，以支持可重复的比较。

</details>


### [191] [Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing](https://arxiv.org/abs/2512.00390)
*Nuo Chen,Hanpei Fang,Jiqun Liu,Wilson Wei,Tetsuya Sakai,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 研究发现LLMs在相关性标注中存在启动效应，但通过注入特定人格特征（如高开放性、低神经质）可以显著降低这种偏见，且最佳人格特征因模型和任务而异。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs被用作可扩展的相关性标注工具，但它们容易受到启动效应的影响，即先前的判断会影响后续判断。心理学理论表明人格特质与这种偏见相关，但LLMs中的模拟人格是否具有类似效应尚不清楚，因此需要研究人格特征如何影响LLMs的启动效应。

Method: 使用多个LLMs在TREC 2021和2022深度学习赛道数据集上研究大五人格特征对相关性标注中启动效应的影响。通过人格注入技术，测试不同人格特征组合（如高开放性、低神经质等）如何改变LLMs对启动效应的敏感性。

Result: 研究发现某些人格特征（特别是高开放性和低神经质）能够持续降低LLMs对启动效应的敏感性。然而，缓解启动效应最有效的人格特征会因使用的LLM模型和具体任务类型而有所不同。

Conclusion: 人格提示是一种有效缓解LLMs在相关性标注中阈值启动效应的方法。该研究将心理学证据与LLM评估实践联系起来，为改善LLM-based评估的可靠性提供了新思路。

Abstract: Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.

中文标题: 通过人格注入缓解基于大语言模型的相关性判断中的阈值启动效应

中文摘要: 最近的研究探索了将大语言模型作为可扩展的相关性标注工具，但研究表明它们容易受到启动效应的影响，即先前的相关性判断会影响后续的判断。尽管心理学理论将人格特质与此类偏见联系起来，但尚不清楚大语言模型中的模拟人格是否表现出类似效应。我们研究了LLMs中的大五人格特征如何影响相关性标注中的启动效应，在TREC 2021和2022深度学习赛道数据集上使用多个LLMs进行了实验。我们的结果表明，某些人格特征，如高开放性和低神经质，能够持续降低对启动效应的敏感性。此外，缓解启动效应最有效的人格特征可能因模型和任务类型而异。基于这些发现，我们提出了人格提示作为一种缓解阈值启动效应的方法，将心理学证据与基于LLM的评估实践联系起来。

</details>


### [192] [A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method of Error Analysis for EFL Writing Instruction](https://arxiv.org/abs/2512.00392)
*Damian Heywood,Joseph Andrew Carrier,Kyu-Hong Hwang*

Main category: cs.CL

TL;DR: 开发了一个基于AI的英语写作错误分析系统，使用LLM模型和语言学理论分类错误，在测试中能识别多种错误类型但存在上下文理解限制


<details>
  <summary>Details</summary>
Motivation: 传统EFL写作教学中的错误分析通常依赖人工评估，效率低且一致性差。本研究旨在开发AI辅助系统，实现自动化、详细的错误识别与分类，为EFL教学提供更精准的反馈。

Method: 基于Corder、Richards和James的语言学理论构建详细错误分类法，使用Claude 3.5 Sonnet和DeepSeek R1等LLM模型，通过Python编码的API调用实现系统，在单词和句子层面分类拼写、语法和标点错误。使用"English as she is spoke"文本进行最终测试。

Result: AI系统成功识别了多样化的错误类型，但在上下文理解方面存在局限性。当遇到未编码的错误时，系统偶尔会生成新的错误类别。系统提供了超越传统评分标准的细粒度反馈。

Conclusion: AI在EFL教学中具有自动化详细错误分析的潜力，但需要进一步改进上下文准确性，并将分类法扩展到文体和语篇层面的错误。

Abstract: This study describes the development of an AI-assisted error analysis system designed to identify, categorize, and correct writing errors in English. Utilizing Large Language Models (LLMs) like Claude 3.5 Sonnet and DeepSeek R1, the system employs a detailed taxonomy grounded in linguistic theories from Corder (1967), Richards (1971), and James (1998). Errors are classified at both word and sentence levels, covering spelling, grammar, and punctuation. Implemented through Python-coded API calls, the system provides granular feedback beyond traditional rubric-based assessments. Initial testing on isolated errors refined the taxonomy, addressing challenges like overlapping categories. Final testing used "English as she is spoke" by Jose da Fonseca (1855), a text rich with authentic linguistic errors, to evaluate the system's capacity for handling complex, multi-layered analysis. The AI successfully identified diverse error types but showed limitations in contextual understanding and occasionally generated new error categories when encountering uncoded errors. This research demonstrates AI's potential to transform EFL instruction by automating detailed error analysis and feedback. While promising, further development is needed to improve contextual accuracy and expand the taxonomy to stylistic and discourse-level errors.

中文标题: 英语写作错误分类法：面向EFL写作教学的AI错误分析方法

中文摘要: 本研究描述了一个AI辅助错误分析系统的开发，该系统旨在识别、分类和纠正英语写作错误。利用Claude 3.5 Sonnet和DeepSeek R1等大型语言模型，该系统采用了基于Corder（1967）、Richards（1971）和James（1998）语言学理论的详细分类法。错误在单词和句子层面进行分类，涵盖拼写、语法和标点。通过Python编码的API调用实现，该系统提供了超越传统评分标准的细粒度反馈。对孤立错误的初步测试完善了分类法，解决了重叠类别等挑战。最终测试使用Jose da Fonseca（1855）的"English as she is spoke"文本，该文本富含真实的语言错误，以评估系统处理复杂、多层次分析的能力。AI成功识别了多样化的错误类型，但在上下文理解方面显示出局限性，并且在遇到未编码错误时偶尔会生成新的错误类别。这项研究展示了AI通过自动化详细错误分析和反馈来改变EFL教学的潜力。虽然前景广阔，但需要进一步开发以提高上下文准确性，并将分类法扩展到文体和语篇层面的错误。

</details>


### [193] [CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency](https://arxiv.org/abs/2512.00417)
*Jiacheng Guo,Suozhi Huang,Zixin Yao,Yifan Zhang,Yifu Lu,Jiashuo Liu,Zihao Li,Yanyan Deng,Qixin Xiao,Jia Tian,Kanghong Zhan,Tianyi Li,Xiaochen Liu,Jason Ge,Chaoyang He,Kaixuan Huang,Lin Yang,Wenhao Huang,Mengdi Wang*

Main category: cs.CL

TL;DR: CryptoBench是首个针对加密货币领域的动态基准测试，包含每月50个专家设计的问题，评估LLM智能体在检索和预测任务中的表现，发现了检索能力强但预测能力弱的失衡现象。


<details>
  <summary>Details</summary>
Motivation: 现有通用基准无法充分评估LLM智能体在加密货币领域的实际能力，因为该领域具有极端时间敏感性、对抗性信息环境和多样化专业数据源等独特挑战，需要专门的评估工具。

Method: 构建实时动态基准，每月包含50个由加密货币专家设计的问题，采用四象限分类系统（简单检索、复杂检索、简单预测、复杂预测），评估10个LLM模型在直接和智能体框架下的表现。

Result: 发现LLM智能体存在检索-预测不平衡现象：许多领先模型在数据检索方面表现出色，但在预测分析任务中表现明显较弱，表明智能体表面事实基础扎实但缺乏深度分析能力。

Conclusion: CryptoBench为加密货币领域的LLM智能体评估提供了有价值的动态基准，揭示了当前模型在检索和预测能力上的不平衡，为未来模型改进提供了重要方向。

Abstract: This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \emph{extreme time-sensitivity}, \emph{a highly adversarial information environment}, and the critical need to synthesize data from \emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.
  Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.

中文标题: CryptoBench：加密货币领域中LLM智能体专家级评估的动态基准

中文摘要: 本文介绍了CryptoBench，这是首个由专家策划的动态基准，旨在严格评估大型语言模型（LLM）智能体在独特要求高、节奏快的加密货币领域中的实际能力。与通用的搜索和预测智能体基准不同，专业的加密货币分析面临特定挑战：极端的时间敏感性、高度对抗性的信息环境，以及从多样化的专业来源（如链上情报平台和实时去中心化金融仪表板）综合数据的迫切需求。因此，CryptoBench作为LLM智能体评估的一个更具挑战性和价值的场景。为应对这些挑战，我们构建了一个实时的动态基准，每月包含50个问题，由加密货币原生专业人士精心设计，以反映实际分析师的工作流程。这些任务在一个四象限系统中被严格分类：简单检索、复杂检索、简单预测和复杂预测。这种细粒度的分类能够精确评估LLM智能体的基础数据收集能力以及其高级分析和预测技能。
我们对十个LLM的评估（包括直接评估和在智能体框架内的评估）揭示了性能层次并发现了一个失败模式。我们观察到检索-预测不平衡现象，即许多领先模型尽管在数据检索方面表现出色，但在需要预测分析的任务中表现出明显的弱点。这突显了一个问题倾向：智能体表面上看起来事实基础扎实，但缺乏综合信息的更深层次分析能力。

</details>


### [194] [SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling](https://arxiv.org/abs/2512.00466)
*Yang Xiao,Chunpu Xu,Ruifeng Yuan,Jiashuo Wang,Wenjie Li,Pengfei Liu*

Main category: cs.CL

TL;DR: SCALE提出了一种选择性资源分配框架，通过根据子问题难度动态分配计算资源，解决了数学推理中统一资源分配导致的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前测试时计算扩展方法对所有推理子问题采用统一资源分配，导致具有挑战性的子问题得不到足够关注，而常规操作却消耗不成比例的资源，造成性能瓶颈和资源浪费。

Method: SCALE框架包含四个阶段：1) 问题分解为顺序推理子问题；2) 评估每个子问题的难度；3) 根据难度分配处理模式（简单问题用System 1，复杂问题用System 2）；4) 顺序执行并传播上下文。

Result: 实验表明SCALE显著优于统一扩展基线，在AIME25上准确率提升13.75个百分点（从57.50%到71.25%），同时计算成本降低33%-53%。

Conclusion: SCALE通过选择性资源分配解决了当前测试时扩展方法的根本限制，在数学推理任务中实现了显著的性能提升和资源效率优化。

Abstract: Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose \textbf{SCALE} (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.

中文标题: SCALE：选择性资源分配用于克服数学测试时扩展中的性能瓶颈

中文摘要: 测试时计算扩展已成为增强大型语言模型数学推理能力的强大范式，通过在推理过程中分配额外的计算资源。然而，当前方法对所有推理子问题采用统一资源分配，造成了根本性瓶颈：具有挑战性的子问题得不到足够关注，而常规操作却消耗不成比例的资源。这种统一分配导致性能瓶颈，使得额外的计算资源产生递减回报。受双过程理论启发，我们提出了SCALE（选择性资源分配）框架，该框架根据子问题难度选择性分配计算资源。SCALE通过四个阶段运行：(1) 将问题分解为顺序推理子问题；(2) 评估每个子问题的难度，区分常规操作和计算挑战性子问题；(3) 选择性分配处理模式，简单子问题使用System 1，复杂子问题使用System 2；(4) 顺序执行并传播上下文。通过将资源集中在具有挑战性的子问题上，同时高效处理常规操作，SCALE实现了显著的性能提升和优越的资源利用率。大量实验表明，SCALE显著优于统一扩展基线，在AIME25上准确率提升高达13.75个百分点（从57.50%到71.25%），同时计算成本降低33%-53%，代表了测试时扩展领域的重大进展，解决了当前方法的根本限制。

</details>


### [195] [CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective Multimodal and Multilingual Learning](https://arxiv.org/abs/2512.00496)
*Diego A. B. Moreira,Alef I. Ferreira,Jhessica Silva,Gabriel O. dos Santos,Gustavo Bonil,João Gondim,Marina dos Santos,Helena Maia,Simone Hashiguti,Nádia da Silva,Carolina Scarton,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: CACARA是一种经济高效的多模态多语言学习框架，通过文本中心的涌现对齐学习，无需完全重新训练即可集成新模态，并能从单语言训练中解锁多语言能力。


<details>
  <summary>Details</summary>
Motivation: 现有大多数多模态模型需要跨多个模态进行资源密集型训练，扩展到新语言也需要类似的高成本策略。研究者希望开发一种更经济高效的方法，能够无缝集成新模态并实现多语言能力，同时避免完全重新训练的高昂计算成本。

Method: 提出CACARA架构，采用涌现对齐学习范式。通过文本中心的方法，仅在新加入的模态上对与英语对齐的数据进行微调，无需对文本编码器进行多语言预训练或调优。这种方法保留了先前学到的知识，训练成本与单语言模型相当。

Result: 模型在音频到文本检索的R@1指标上实现了高达14.24个百分点的改进，优于最先进的多模态模型。更重要的是，通过仅对英语数据进行微调，模型就获得了对100多种语言的支持，而无需明确的多语言训练。

Conclusion: CACARA证明了涌现对齐范式可以从单语言训练中解锁多语言能力，提供了一种经济高效的多模态多语言学习策略。这种方法避免了跨每个模态和语言重新训练的高昂计算成本，为资源受限环境下的多模态学习开辟了新途径。

Abstract: As deep learning models evolve, new applications and challenges are rapidly emerging. Tasks that once relied on a single modality, such as text, images, or audio, are now enriched by seamless interactions between multimodal data. These connections bridge information gaps: an image can visually materialize a text, while audio can add context to an image. Researchers have developed numerous multimodal models, but most rely on resource-intensive training across multiple modalities. Similarly, extending these models to new languages often follows the same resource-heavy training strategy. In this work, we propose a multimodal and multilingual architecture, CACARA, trained through emergent alignment learning, enabling the seamless integration of new modalities into an existing bimodal/multimodal model without requiring full retraining. This work breaks new ground by demonstrating that this emergent alignment paradigm can unlock multilingual capabilities from monolingual training. By fine-tuning the newly incorporated modality only on data aligned with the English language, our model develops support for over 100 languages without explicit multilingual pretraining or tuning of the text encoder. Such emergent multimodal and multilingual properties are gained efficiently, preserving previously learned knowledge at a training cost comparable to that of a monolingual model. Our strategy achieves up to a 14.24 percentage points improvement in R@1 audio-to-text retrieval, outperforming state-of-the-art multimodal models -- all without the heavy computational cost of retraining across every modality and language.

中文标题: CACARA：基于文本中心方法的跨模态对齐实现经济高效的多模态与多语言学习

中文摘要: 随着深度学习模型的发展，新的应用和挑战迅速涌现。曾经依赖单一模态（如文本、图像或音频）的任务现在通过多模态数据之间的无缝交互得到了丰富。这些连接弥合了信息鸿沟：图像可以可视化地呈现文本，而音频可以为图像添加上下文。研究人员已经开发了许多多模态模型，但大多数依赖于跨多个模态的资源密集型训练。同样，将这些模型扩展到新语言通常也遵循相同的资源密集型训练策略。在这项工作中，我们提出了一种多模态和多语言架构CACARA，通过涌现对齐学习进行训练，能够将新模态无缝集成到现有的双模态/多模态模型中，而无需完全重新训练。这项工作开辟了新天地，证明了这种涌现对齐范式可以从单语言训练中解锁多语言能力。通过仅在新加入的模态上对与英语对齐的数据进行微调，我们的模型就获得了对100多种语言的支持，而无需对文本编码器进行明确的多语言预训练或调优。这种涌现的多模态和多语言特性是高效获得的，保留了先前学到的知识，训练成本与单语言模型相当。我们的策略在音频到文本检索的R@1指标上实现了高达14.24个百分点的改进，优于最先进的多模态模型——所有这些都无需承担跨每个模态和语言重新训练的高昂计算成本。

</details>


### [196] [G-KV: Decoding-Time KV Cache Eviction with Global Attention](https://arxiv.org/abs/2512.00504)
*Mengqi Liao,Lu Wang,Chaoyun Zhang,Zekai Shen,Xiaowei Mao,Si Qin,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang,Huaiyu Wan*

Main category: cs.CL

TL;DR: G-KV是一种KV缓存驱逐方法，使用全局评分机制结合局部和历史注意力分数来更准确地评估token重要性，并通过强化学习和蒸馏等后训练技术优化模型在压缩KV缓存设置下的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理长序列时面临显著的计算和内存挑战。现有的KV缓存压缩方法主要关注提示压缩或基于局部注意力分数的token驱逐，忽略了token的长期重要性。

Method: 提出G-KV方法，采用全局评分机制结合局部和历史注意力分数来评估token重要性。同时引入后训练技术，包括强化学习和蒸馏，以优化模型在压缩KV缓存设置下的性能。

Result: 该方法显著提高了推理效率，代码已在GitHub上开源。

Conclusion: G-KV通过全局注意力评分机制有效解决了长序列处理中的计算和内存挑战，为KV缓存压缩提供了更准确的token重要性评估方法。

Abstract: Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.

中文标题: G-KV：基于全局注意力的解码时KV缓存驱逐方法

中文摘要: 最近的大型推理语言模型在复杂任务中表现出色，但由于长序列长度而面临显著的计算和内存挑战。KV缓存压缩已成为大幅提高推理效率的有效方法。然而，现有方法通常关注提示压缩或基于局部注意力分数的token驱逐，忽略了token的长期重要性。我们提出了G-KV，一种采用全局评分机制的KV缓存驱逐方法，结合局部和历史注意力分数来更准确地评估token重要性。此外，我们引入了后训练技术，包括强化学习和蒸馏，以优化模型在压缩KV缓存设置下的性能。本文代码可在以下地址获取：https://github.com/microsoft/G-KV。

</details>


### [197] [Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical Fidelity](https://arxiv.org/abs/2512.00552)
*Subramanyam Sahoo,Vinija Jain,Saanidhya Vats,Siddharth Mohapatra,Rui Min,Aman Chadha,Divya Chaudhary*

Main category: cs.CL

TL;DR: 该研究开发了一个诊断框架来区分真正的数学推理与模式匹配，发现小型模型虽然答案准确率高，但推理保真度差，主要依赖表面模式而非逻辑计算。


<details>
  <summary>Details</summary>
Motivation: 当前数学推理评估过于依赖答案准确性，可能掩盖模型真正的推理能力缺陷。需要更精细的诊断工具来区分表面模式匹配和真正的逻辑计算能力。

Method: 提出了一个包含四个维度的诊断框架：1) 前向-后向一致性，2) 传递性覆盖，3) 反事实敏感性，4) 扰动鲁棒性。在Qwen3-0.6B模型和MenatQA数据集上进行案例研究。

Result: Qwen3-0.6B模型虽然答案准确率达到70%以上，但在四个诊断维度表现不佳：后向一致性仅15%，传递性覆盖32.2%，对反事实和扰动敏感，表明推理保真度低。

Conclusion: 小型模型可能通过模式匹配伪装推理能力，传统准确性指标不足以评估真正的数学推理。提出的诊断框架可推广用于评估不同模型的推理保真度。

Abstract: Current evaluation of mathematical reasoning in language models relies primarily on answer accuracy, potentially masking fundamental failures in logical computation. We introduce a diagnostic framework that distinguishes genuine mathematical reasoning from superficial pattern matching through four complementary axes: forward-backward consistency, transitivity coverage, counterfactual sensitivity, and perturbation robustness. Through a case study applying this framework to Qwen3-0.6B on the MenatQA dataset, we reveal a striking disconnect between surface performance and reasoning fidelity. While the model achieves reasonable answer accuracy (70%+), it demonstrates poor backward consistency (15%), limited transitivity coverage (32.2%), and brittle sensitivity to perturbations. Our diagnostics expose reasoning failures invisible to traditional accuracy metrics, suggesting that this small model relies heavily on pattern matching rather than genuine logical computation. While our empirical findings are based on a single 600M-parameter model, the diagnostic framework itself is model-agnostic and generalizable. We release our evaluation protocols to enable the research community to assess reasoning fidelity across different model scales and architectures, moving beyond surface-level accuracy toward verifiable mathematical reasoning.

中文标题: 《你能抓住我吗：小型推理模型如何伪装具有数学保真度的推理》

中文摘要: 当前语言模型的数学推理评估主要依赖于答案准确性，这可能掩盖了逻辑计算中的基本失败。我们引入了一个诊断框架，通过四个互补的维度来区分真正的数学推理和表面的模式匹配：前向-后向一致性、传递性覆盖、反事实敏感性和扰动鲁棒性。通过对Qwen3-0.6B在MenatQA数据集上应用此框架的案例研究，我们揭示了表面性能与推理保真度之间的显著脱节。虽然该模型达到了合理的答案准确率（70%+），但它表现出较差的后向一致性（15%）、有限的传递性覆盖（32.2%）以及对扰动的脆弱敏感性。我们的诊断暴露了传统准确性指标无法发现的推理失败，表明这个小模型严重依赖模式匹配而非真正的逻辑计算。虽然我们的实证发现基于单个600M参数模型，但诊断框架本身是模型无关且可推广的。我们发布了评估协议，使研究社区能够评估不同模型规模和架构的推理保真度，超越表面准确性，迈向可验证的数学推理。

</details>


### [198] [Slovak Conceptual Dictionary](https://arxiv.org/abs/2512.00579)
*Miroslav Blšták*

Main category: cs.CL

TL;DR: 本文介绍了首个斯洛伐克语概念词典，旨在解决斯洛伐克语在自然语言处理中因缺乏机器可读语言数据资源而面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 斯洛伐克语作为资源有限的语言，缺乏足够大的机器可读语言数据源，导致许多需要自动化处理斯洛伐克文本的任务相比其他语言效果较差甚至无法解决。

Method: 开发了首个斯洛伐克语概念词典，作为该语言的第一种此类语言工具。

Result: 成功创建了斯洛伐克语概念词典，填补了该语言在词典工具方面的空白。

Conclusion: 该概念词典的引入为斯洛伐克语自然语言处理任务提供了重要的语言工具支持，有助于改善该语言在自动化文本处理方面的表现。

Abstract: When solving tasks in the field of natural language processing, we sometimes need dictionary tools, such as lexicons, word form dictionaries or knowledge bases. However, the availability of dictionary data is insufficient in many languages, especially in the case of low resourced languages. In this article, we introduce a new conceptual dictionary for the Slovak language as the first linguistic tool of this kind. Since Slovak language is a language with limited linguistic resources and there are currently not available any machine-readable linguistic data sources with a sufficiently large volume of data, many tasks which require automated processing of Slovak text achieve weaker results compared to other languages and are almost impossible to solve.

中文标题: 斯洛伐克语概念词典

中文摘要: 在解决自然语言处理领域的任务时，我们有时需要词典工具，如词汇表、词形词典或知识库。然而，许多语言的词典数据可用性不足，特别是在低资源语言的情况下。在本文中，我们介绍了首个斯洛伐克语概念词典，作为该语言的第一种此类语言工具。由于斯洛伐克语是一种语言资源有限的语言，目前没有可用的机器可读语言数据源具有足够大的数据量，许多需要自动化处理斯洛伐克文本的任务相比其他语言效果较差，几乎无法解决。

</details>


### [199] [Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2512.00590)
*Alla Chepurova,Aydar Bulatov,Yuri Kuratov,Mikhail Burtsev*

Main category: cs.CL

TL;DR: Wikontic是一个多阶段管道，利用大语言模型从开放域文本构建与Wikidata对齐、本体感知的知识图谱，通过提取带限定符的三元组、强制执行类型和关系约束以及实体规范化来生成紧凑、一致且连接良好的知识图谱。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的系统通常将知识图谱作为文本检索的辅助结构，而对其内在质量探索不足。知识图谱能为大语言模型提供结构化、可验证的基础，但现有方法未能充分利用这一潜力。

Method: 提出Wikontic多阶段管道：1) 从开放域文本中提取带限定符的候选三元组；2) 强制执行基于Wikidata的类型和关系约束；3) 通过实体规范化减少重复；4) 构建紧凑、本体一致且连接良好的知识图谱。

Result: 在MuSiQue上，正确答案实体出现在96%的生成三元组中；在HotpotQA上，仅使用三元组设置达到76.0 F1；在MuSiQue上达到59.8 F1，匹配或超越多个仍需文本上下文的检索增强生成基线；在MINE-1基准上达到86%的信息保留性能（SOTA）；构建时仅使用不到1,000个输出token，比AriGraph少3倍，比GraphRAG少20倍以上。

Conclusion: Wikontic提高了生成知识图谱的质量，并为在大语言模型中利用结构化知识提供了可扩展的解决方案，证明了高质量知识图谱构建的可行性和效率。

Abstract: Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.

中文标题: Wikontic：使用大语言模型构建与Wikidata对齐、本体感知的知识图谱

中文摘要: 知识图谱为大语言模型提供了结构化、可验证的基础，但当前基于大语言模型的系统通常将知识图谱作为文本检索的辅助结构，对其内在质量探索不足。在这项工作中，我们提出了Wikontic，一个多阶段管道，通过从开放域文本中提取带限定符的候选三元组、强制执行基于Wikidata的类型和关系约束以及规范化实体以减少重复，从而构建知识图谱。生成的知识图谱紧凑、本体一致且连接良好；在MuSiQue上，正确答案实体出现在96%的生成三元组中。在HotpotQA上，我们的仅三元组设置达到76.0 F1，在MuSiQue上达到59.8 F1，匹配或超越了多个仍需文本上下文的检索增强生成基线。此外，Wikontic在MINE-1基准上达到了最先进的信息保留性能（86%），优于先前的知识图谱构建方法。Wikontic在构建时也很高效：知识图谱构建使用不到1,000个输出token，比AriGraph少约3倍，比GraphRAG少20倍以上。所提出的管道提高了生成知识图谱的质量，并为在大语言模型中利用结构化知识提供了可扩展的解决方案。

</details>


### [200] [ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages](https://arxiv.org/abs/2512.01077)
*Neha Joshi,Pamir Gogoi,Aasim Mirza,Aayush Jansari,Aditya Yadavalli,Ayushi Pandey,Arunima Shukla,Deepthi Sudharsan,Kalika Bali,Vivek Seshadri*

Main category: cs.CL

TL;DR: ELR-1000是一个包含1,060个传统食谱的多模态数据集，涵盖10种印度东部濒危语言，通过移动界面从农村社区收集。研究发现大型语言模型在翻译这些低资源文化特定内容时存在困难，但提供针对性背景信息能显著改善翻译质量。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决濒危语言在语言技术中的代表性不足问题。印度东部偏远地区存在许多濒危的本土语言，这些语言承载着丰富的文化传统，特别是通过食物和烹饪实践体现。然而，由于缺乏数字资源和数据集，这些语言在自然语言处理研究中被忽视。研究团队希望通过收集和记录这些濒危语言中的传统食谱，创建一个文化基础的数据集，促进对这些语言的语言技术开发。

Method: 研究方法包括：1）设计适合数字素养较低贡献者的移动界面，用于从印度东部偏远农村社区收集传统食谱；2）收集1,060个涵盖10种濒危语言的多模态食谱数据，包括语言文本和文化背景信息；3）使用最先进的大型语言模型（LLMs）进行翻译实验；4）评估模型在翻译这些低资源文化特定内容时的性能；5）探索提供针对性背景信息（语言背景、翻译示例、文化保护指南）对翻译质量的改善效果。

Result: 研究结果显示：1）尽管大型语言模型具备强大的翻译能力，但在处理低资源、文化特定的濒危语言内容时表现不佳；2）提供有针对性的背景信息（包括语言背景、翻译示例和文化保护指南）能显著提高翻译质量；3）ELR-1000数据集成功捕捉了10种濒危语言中的1,060个传统食谱及其社会文化背景；4）该数据集揭示了当前语言技术在处理代表性不足语言和文化领域时的局限性。

Conclusion: 该研究得出结论：1）需要专门为代表性不足的濒危语言和特定文化领域开发基准测试；2）有针对性的背景信息对于提高语言模型在低资源语言上的表现至关重要；3）ELR-1000数据集为开发更公平、更具文化意识的语言技术提供了重要资源；4）社区参与和数据收集方法对于保护濒危语言和文化传统具有重要意义；5）该工作呼吁NLP社区关注濒危语言技术开发，促进语言技术的多样性和包容性。

Abstract: We present a culturally-grounded multimodal dataset of 1,060 traditional recipes crowdsourced from rural communities across remote regions of Eastern India, spanning 10 endangered languages. These recipes, rich in linguistic and cultural nuance, were collected using a mobile interface designed for contributors with low digital literacy. Endangered Language Recipes (ELR)-1000 -- captures not only culinary practices but also the socio-cultural context embedded in indigenous food traditions. We evaluate the performance of several state-of-the-art large language models (LLMs) on translating these recipes into English and find the following: despite the models' capabilities, they struggle with low-resource, culturally-specific language. However, we observe that providing targeted context -- including background information about the languages, translation examples, and guidelines for cultural preservation -- leads to significant improvements in translation quality. Our results underscore the need for benchmarks that cater to underrepresented languages and domains to advance equitable and culturally-aware language technologies. As part of this work, we release the ELR-1000 dataset to the NLP community, hoping it motivates the development of language technologies for endangered languages.

中文标题: ELR-1000：一个社区生成的低资源印度本土濒危语言数据集

中文摘要: 我们提出了一个基于文化的多模态数据集，包含从印度东部偏远地区农村社区众包的1,060个传统食谱，涵盖10种濒危语言。这些富含语言和文化细微差别的食谱是通过为数字素养较低的贡献者设计的移动界面收集的。濒危语言食谱（ELR）-1000不仅捕捉了烹饪实践，还包含了土著食物传统中嵌入的社会文化背景。我们评估了几种最先进的大型语言模型（LLM）将这些食谱翻译成英语的性能，发现：尽管模型具备能力，但它们在处理低资源、文化特定语言方面存在困难。然而，我们观察到提供有针对性的背景信息——包括语言背景信息、翻译示例和文化保护指南——能显著提高翻译质量。我们的结果强调了需要为代表性不足的语言和领域提供基准测试，以促进公平和具有文化意识的语言技术发展。作为这项工作的一部分，我们向NLP社区发布了ELR-1000数据集，希望它能激励开发针对濒危语言的语言技术。

</details>


### [201] [ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization](https://arxiv.org/abs/2512.00617)
*Omer Jauhar Khan*

Main category: cs.CL

TL;DR: ART框架通过锦标赛式ELO排名和多智能体推理系统优化LLM输出，采用可配置的锦标赛参数、动态智能体选择和多种共识融合策略，相比单模型方法在响应质量上提升8.4%，ELO评分收敛R²值超过0.96。


<details>
  <summary>Details</summary>
Motivation: 单个LLM模型的响应存在不一致性、幻觉问题，且在不同查询领域质量参差不齐，需要系统化的方法来优化LLM输出质量。

Method: 采用锦标赛式ELO排名机制和多智能体推理框架，让多个LLM智能体通过结构化锦标赛工作流程进行竞争、批评和协作，包含可配置的锦标赛参数、动态智能体选择和多种共识融合策略。

Result: 实验评估显示在响应准确性、连贯性和可靠性方面相比基线单模型方法有显著改进，整体质量指标提升8.4%，ELO评分收敛R²值超过0.96。

Conclusion: ART框架为需要高质量、经过验证的LLM响应的应用提供了可扩展、生产就绪的解决方案，通过多智能体协作显著提升了LLM输出质量。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.

中文标题: ART：自适应响应调优框架——基于多智能体锦标赛方法的LLM响应优化

中文摘要: 大型语言模型（LLM）在自然语言理解和生成方面展现出卓越能力。然而，单模型响应往往存在不一致性、幻觉问题，且在不同查询领域质量参差不齐。本文提出了ART（自适应响应调优）框架，这是一种新颖的框架，采用锦标赛式ELO排名和多智能体推理来系统优化LLM输出。通过让多个LLM智能体通过结构化锦标赛工作流程进行竞争、批评和协作，ART产生优于单个模型输出的共识响应。我们的框架引入了可配置的锦标赛参数、动态智能体选择和多种共识融合策略。实验评估显示，相比基线单模型方法，在响应准确性、连贯性和可靠性方面有显著改进。ART框架为需要高质量、经过验证的LLM响应的应用提供了可扩展、生产就绪的解决方案，在整体质量指标上实现了8.4%的改进，ELO评分收敛R²值超过0.96。

</details>


### [202] [Sycophancy Claims about Language Models: The Missing Human-in-the-Loop](https://arxiv.org/abs/2512.00656)
*Jan Batzner,Volker Stocker,Stefan Schmid,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 该论文指出当前关于LLM谄媚性的研究存在方法论问题，特别是缺乏对人类感知的评估，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 当前文献中越来越多地声称大型语言模型表现出谄媚性响应模式，但相关研究存在方法论缺陷，特别是缺乏对人类感知的评估，这促使作者对现有研究进行批判性分析。

Method: 作者通过文献回顾，分析了测量LLM谄媚性的方法论挑战，识别了五种核心操作化定义，并评估了当前研究的局限性。

Result: 研究发现当前关于LLM谄媚性的研究存在重大方法论问题，特别是缺乏对人类感知的评估，难以区分谄媚性响应与AI对齐中的其他相关概念。

Conclusion: 论文得出结论，未来关于LLM谄媚性的研究需要纳入人类感知评估，并提供了具体的改进建议，以更准确地理解和测量这一现象。

Abstract: Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.

中文标题: 关于语言模型的谄媚性主张：缺失的人类参与环节

中文摘要: 大型语言模型（LLMs）中的谄媚性响应模式在文献中越来越多地被提出。我们回顾了测量LLM谄媚性的方法论挑战，并确定了五种核心操作化定义。尽管谄媚性本质上是人类中心的，但当前研究并未评估人类感知。我们的分析强调了区分谄媚性响应与AI对齐中相关概念的困难，并为未来研究提供了可行的建议。

</details>


### [203] [Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs](https://arxiv.org/abs/2512.00663)
*Tanmay Agrawal*

Main category: cs.CL

TL;DR: 本文提出一个可视化知识图谱框架，用于检测LLM在结合专有知识时产生的幻觉，通过结构化可视化帮助用户识别不一致性并提供反馈。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在企业环境中常结合专有知识提供上下文响应，但由于上下文窗口限制和预训练数据与专有知识不一致，容易产生看似可信的幻觉，现有缓解策略要么依赖昂贵的人工标注，要么依赖二次模型验证，都无法提供确定性保证。

Method: 引入一个框架，将专有知识和模型生成内容组织成交互式可视化知识图谱，将模型断言与底层真实来源链接并显示置信度，提供可视化界面让用户诊断不一致性、识别弱推理链并提供纠正反馈。

Result: 该框架为用户提供了清晰的潜在幻觉区域视图，创建了结构化的人机协作反馈循环，能够增强模型可靠性并持续改进响应质量。

Conclusion: 可视化知识图谱框架为LLM幻觉检测提供了有效的解决方案，通过结构化可视化界面实现人机协作，能够系统性地识别和纠正模型幻觉，提高企业环境中LLM应用的可靠性。

Abstract: Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.

中文标题: 绘制真相：用于LLM自动幻觉检测的结构化可视化方法

中文摘要: 大型语言模型在理解和生成自然语言方面取得了快速进展。在企业环境中，它们经常与闭源领域知识结合，以提供更具上下文信息的响应。然而，操作约束如有限的上下文窗口以及预训练数据与提供知识之间的不一致性常常导致幻觉，其中一些幻觉看起来高度可信并逃脱常规人工审查。当前的缓解策略要么依赖于昂贵的大规模黄金标准问答整理，要么依赖于二次模型验证，两者都无法提供确定性保证。本文介绍了一个框架，将专有知识和模型生成内容组织成交互式可视化知识图谱。目标是通过将模型断言与底层真实来源链接并指示置信度，为最终用户提供清晰的潜在幻觉区域视图。通过这个可视化界面，用户可以诊断不一致性、识别弱推理链并提供纠正反馈。由此产生的人机协作工作流程创建了一个结构化反馈循环，可以增强模型可靠性并持续改进响应质量。

</details>


### [204] [A Comparison of Human and ChatGPT Classification Performance on Complex Social Media Data](https://arxiv.org/abs/2512.00673)
*Breanna E. Green,Ashley L. Shea,Pengfei Zhao,Drew B. Margolin*

Main category: cs.CL

TL;DR: 该研究比较了人类与ChatGPT在复杂社交媒体数据分类任务上的表现，发现GPT-4在处理细微语言时存在困难，建议谨慎使用


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能工具如ChatGPT在计算社会科学家中使用日益增多，但对其在复杂任务（如分类包含细微语言的数据集）中的性能了解仍有不足，需要评估其实际表现

Method: 测量GPT-4在复杂分类任务上的性能并与人类标注者比较；调查ChatGPT 3.5、4和4o版本以考察技术进步的快速变化；设计四种提示风格作为输入，评估精确率、召回率和F1分数；进行定量和定性评估

Result: 定量和定性评估显示，虽然在提示中包含标签定义可能有助于提高性能，但总体上GPT-4在分类细微语言方面存在困难；定性分析揭示了四个具体发现；结果建议在涉及细微语言的分类任务中使用ChatGPT时应谨慎

Conclusion: ChatGPT在处理复杂社交媒体数据的分类任务中表现不如人类标注者，特别是在处理细微语言时存在显著困难，研究人员应谨慎使用这些工具进行此类任务

Abstract: Generative artificial intelligence tools, like ChatGPT, are an increasingly utilized resource among computational social scientists. Nevertheless, there remains space for improved understanding of the performance of ChatGPT in complex tasks such as classifying and annotating datasets containing nuanced language. Method. In this paper, we measure the performance of GPT-4 on one such task and compare results to human annotators. We investigate ChatGPT versions 3.5, 4, and 4o to examine performance given rapid changes in technological advancement of large language models. We craft four prompt styles as input and evaluate precision, recall, and F1 scores. Both quantitative and qualitative evaluations of results demonstrate that while including label definitions in prompts may help performance, overall GPT-4 has difficulty classifying nuanced language. Qualitative analysis reveals four specific findings. Our results suggest the use of ChatGPT in classification tasks involving nuanced language should be conducted with prudence.

中文标题: 人类与ChatGPT在复杂社交媒体数据分类性能上的比较

中文摘要: 生成式人工智能工具，如ChatGPT，在计算社会科学家中日益成为一种常用资源。然而，对于ChatGPT在复杂任务（如分类和标注包含细微语言的数据集）中的性能，仍有改进理解的空间。方法：在本文中，我们测量了GPT-4在此类任务上的性能，并将结果与人类标注者进行比较。我们调查了ChatGPT 3.5、4和4o版本，以考察大型语言模型技术快速进步下的性能表现。我们设计了四种提示风格作为输入，并评估精确率、召回率和F1分数。结果的定量和定性评估表明，虽然在提示中包含标签定义可能有助于提高性能，但总体上GPT-4在分类细微语言方面存在困难。定性分析揭示了四个具体发现。我们的结果表明，在涉及细微语言的分类任务中使用ChatGPT应谨慎进行。

</details>


### [205] [Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models](https://arxiv.org/abs/2512.01892)
*Heloisa Candello,Muneeza Azmat,Uma Sushmitha Gunturi,Raya Horesh,Rogerio Abreu de Paula,Heloisa Pimentel,Marcelo Carpinette Grave,Aminat Adebiyi,Tiago Machado,Maysa Malfiza Garcia de Macedo*

Main category: cs.CL

TL;DR: 该研究通过混合方法实验评估人类对生成式AI风险缓解策略的感知，发现语言背景、AI工作经验和标注熟悉度显著影响评估结果，参与者对语言和上下文属性高度敏感。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速普及，研究人类对生成响应的感知变得至关重要。主要挑战是AI容易产生幻觉和有害内容。尽管已实施护栏策略，但人类对这些缓解策略的感知仍不明确。

Method: 采用混合方法实验，在受试者内研究设计中，57名参与者评估两种条件下的响应：有害响应加缓解措施 vs 仅缓解响应。评估维度包括忠实性、公平性、伤害移除能力和相关性。

Result: 参与者的母语、AI工作经验和标注熟悉度显著影响评估结果。参与者对语言和上下文属性高度敏感，惩罚轻微语法错误，同时奖励保留的语义上下文。这与LLM定量评估中语言处理方式形成对比。

Conclusion: 研究提出了训练和评估缓解策略的新指标，并为人类-AI评估研究提供了见解。强调需要考虑人类感知因素来改进AI安全策略。

Abstract: With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and relevance. In a within-subject study design, 57 participants assessed the responses under two conditions: harmful response plus its mitigation and solely mitigated response. Results revealed that participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations. Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts. This contrasts with how language is often treated in the quantitative evaluation of LLMs. We also introduced new metrics for training and evaluating mitigation strategies and insights for human-AI evaluation studies.

中文标题: 探索人类对AI响应的感知：基于生成模型风险缓解的混合方法研究洞察

中文摘要: 随着生成式AI的快速普及，研究人类对生成响应的感知变得至关重要。主要挑战是它们容易产生幻觉和有害内容。尽管已实施护栏策略，但人类对这些缓解策略的感知在很大程度上仍是未知的。我们进行了一项混合方法实验，从多个维度评估缓解策略的响应：忠实性、公平性、伤害移除能力和相关性。在受试者内研究设计中，57名参与者在两种条件下评估响应：有害响应加缓解措施和仅缓解响应。结果显示，参与者的母语、AI工作经验和标注熟悉度显著影响评估结果。参与者对语言和上下文属性表现出高度敏感性，惩罚轻微语法错误，同时奖励保留的语义上下文。这与语言在LLM定量评估中的处理方式形成对比。我们还提出了训练和评估缓解策略的新指标，并为人类-AI评估研究提供了见解。

</details>


### [206] [FastPOS: Language-Agnostic Scalable POS Tagging Framework Low-Resource Use Case](https://arxiv.org/abs/2512.00745)
*Md Abdullah Al Kafi,Sumit Kumar Banshal*

Main category: cs.CL

TL;DR: FastPOS是一个基于Transformer的语言无关词性标注框架，专为低资源语言设计，通过最小代码修改（仅3行）实现跨语言适配，在孟加拉语和印地语上分别达到96.85%和97%的准确率。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言缺乏高质量NLP工具的问题，特别是词性标注任务，需要一种能够快速适配不同语言、减少开发成本、让研究人员专注于语言预处理而非模型设计的解决方案。

Method: 采用基于Transformer的语言无关架构，设计模块化开源框架，通过最小代码修改（仅3行）实现跨语言适配，使用孟加拉语和印地语作为案例研究验证框架的有效性。

Result: 在孟加拉语和印地语上分别达到96.85%和97%的词级准确率，尽管存在数据集不平衡和语言重叠问题，仍保持强劲的F1分数。特定POS类别存在性能差异，揭示了数据集整理的挑战。

Conclusion: FastPOS框架通过模块化设计和最小代码修改实现了有效的跨语言词性标注，显著降低了低资源语言NLP开发成本，使研究人员能够专注于语言预处理和数据集优化，为低代表性语言的NLP研究提供了实用工具。

Abstract: This study proposes a language-agnostic transformer-based POS tagging framework designed for low-resource languages, using Bangla and Hindi as case studies. With only three lines of framework-specific code, the model was adapted from Bangla to Hindi, demonstrating effective portability with minimal modification. The framework achieves 96.85 percent and 97 percent token-level accuracy across POS categories in Bangla and Hindi while sustaining strong F1 scores despite dataset imbalance and linguistic overlap. A performance discrepancy in a specific POS category underscores ongoing challenges in dataset curation. The strong results stem from the underlying transformer architecture, which can be replaced with limited code adjustments. Its modular and open-source design enables rapid cross-lingual adaptation while reducing model design and tuning overhead, allowing researchers to focus on linguistic preprocessing and dataset refinement, which are essential for advancing NLP in underrepresented languages.

中文标题: FastPOS：面向低资源用例的语言无关可扩展词性标注框架

中文摘要: 本研究提出了一种面向低资源语言的语言无关的基于Transformer的词性标注框架，以孟加拉语和印地语作为案例研究。仅需三行框架特定代码，该模型就从孟加拉语适配到印地语，展示了通过最小修改实现的有效可移植性。该框架在孟加拉语和印地语中分别实现了96.85%和97%的词级准确率，尽管存在数据集不平衡和语言重叠问题，但仍保持了强劲的F1分数。特定词性类别中的性能差异凸显了数据集整理方面的持续挑战。这些强劲结果源于底层Transformer架构，该架构可以通过有限的代码调整进行替换。其模块化和开源设计实现了快速的跨语言适配，同时减少了模型设计和调优开销，使研究人员能够专注于语言预处理和数据集优化，这对于推进低代表性语言的NLP研究至关重要。

</details>


### [207] [Auxiliary-Hyperparameter-Free Sampling: Entropy Equilibrium for Text Generation](https://arxiv.org/abs/2512.00789)
*Xiaodong Cai,Hai Lin,Shaoxiong Zhan,Weiqi Luo,Hong-Gee Kim,Hongyan Hao,Yu Yang,Hai-Tao Zheng*

Main category: cs.CL

TL;DR: 提出了一种无需辅助超参数的文本生成采样方法EES，通过信息熵平衡动态调整候选集，在各种温度设置下都能保持良好性能。


<details>
  <summary>Details</summary>
Motivation: 现有的token采样策略需要引入额外的超参数，需要大量调优且部署复杂，限制了实际应用。

Method: 提出熵平衡采样(EES)，基于信息理论，通过平衡归一化熵与概率质量来动态调整候选集，无需额外超参数。

Result: 在多种模型架构的推理和生成任务上评估，EES在各种温度设置下表现一致良好，在准确性和连贯性上具有竞争力，同时保持多样性。

Conclusion: EES消除了超参数调优的需求，大大简化了部署过程，同时提升了性能。

Abstract: Token sampling strategies critically influence text generation quality in large language models (LLMs). However, existing methods introduce additional hyperparameters, requiring extensive tuning and complicating deployment. We present Entropy Equilibrium Sampling (EES), an auxiliary hyperparameter-free approach inspired by information theory that can dynamically adjust candidate sets by balancing normalized entropy with probability mass. We evaluate EES on both reasoning and generation tasks across a range of model architectures. Our results show that EES consistently performs well across temperature settings, delivering competitive accuracy and coherence while maintaining diversity. By eliminating the need for hyperparameter tuning, EES greatly simplifies deployment while improving performance. Code is available at https://github.com/shuanncai/EES

中文标题: 无辅助超参数采样：文本生成的熵平衡

中文摘要: 在大语言模型中，token采样策略对文本生成质量至关重要。然而，现有方法引入了额外的超参数，需要大量调优且使部署复杂化。我们提出了熵平衡采样(EES)，这是一种基于信息理论的无辅助超参数方法，可以通过平衡归一化熵与概率质量来动态调整候选集。我们在多种模型架构的推理和生成任务上评估了EES。结果表明，EES在各种温度设置下表现一致良好，在准确性和连贯性上具有竞争力，同时保持多样性。通过消除超参数调优的需求，EES大大简化了部署过程，同时提升了性能。代码可在https://github.com/shuanncai/EES获取。

</details>


### [208] [Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy](https://arxiv.org/abs/2512.00829)
*Md Mehrab Hossain Opi,Sumaiya Khan,Moshammad Farzana Rahman*

Main category: cs.CL

TL;DR: 该研究探索了自动混合精度训练在孟加拉语NLP任务中的应用，证明AMP能显著加速训练（44.5%）并减少内存消耗（17.6%），同时保持模型性能（F1分数达到全精度基线的99.7%）。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语NLP开发面临计算资源有限、硬件访问受限的挑战，训练模型需要大量计算资源和时间。研究者希望通过自动混合精度训练提高计算效率，同时不牺牲模型性能。

Method: 采用自动混合精度训练技术，动态混合16位和32位浮点计算。在四个标准孟加拉语NLP任务（情感分析、命名实体识别、错误分类、问答）上评估，使用四个基于Transformer的模型：BanglaBERT、BanglishBERT、XLM-R和mBERT。

Result: AMP训练加速了44.5%，内存消耗减少了17.6%，同时F1分数保持在完整精度基线的99.7%以内。这表明在硬件受限环境中，AMP能有效降低计算门槛。

Conclusion: 自动混合精度训练是提高孟加拉语NLP计算效率的有效方法，能显著降低训练时间和内存需求，同时保持模型性能，有助于在硬件受限环境中普及先进的NLP能力。

Abstract: Training models for Natural Language Processing (NLP) requires substantial computational resources and time, posing significant challenges, especially for NLP development in Bangla, where access to high-end hardware is often limited. In this work, we explore automatic mixed precision (AMP) training as a means to improve computational efficiency without sacrificing model performance. By leveraging a dynamic mix of 16-bit and 32-bit floating-point computations, AMP lowers GPU memory requirements and speeds up training without degrading model performance. We evaluate AMP across four standard Bangla NLP tasks, namely sentiment analysis, named entity recognition, error classification, and question answering, using four transformer-based models: BanglaBERT, BanglishBERT, XLM-R, and mBERT. Our results demonstrate that AMP accelerates training by 44.5% and reduces memory consumption by 17.6%, while maintaining F-1 score within 99.7% of the full-precision baselines. This empirical study highlights AMP's potential to democratize access to state-of-the-art NLP capabilities in hardware-constrained settings by lowering computational barriers.

中文标题: 使用自动混合精度加速孟加拉语NLP任务：保持模型效能的资源高效训练

中文摘要: 自然语言处理模型的训练需要大量的计算资源和时间，这对孟加拉语NLP开发构成了重大挑战，特别是在高端硬件访问受限的情况下。在本研究中，我们探索了自动混合精度训练作为提高计算效率而不牺牲模型性能的手段。通过利用16位和32位浮点计算的动态混合，AMP降低了GPU内存需求并加速了训练，同时不降低模型性能。我们在四个标准孟加拉语NLP任务（情感分析、命名实体识别、错误分类和问答）上评估AMP，使用了四个基于Transformer的模型：BanglaBERT、BanglishBERT、XLM-R和mBERT。我们的结果表明，AMP将训练速度提高了44.5%，内存消耗减少了17.6%，同时将F1分数保持在完整精度基线的99.7%以内。这项实证研究强调了AMP在降低计算门槛方面的潜力，有助于在硬件受限环境中普及最先进的NLP能力。

</details>


### [209] [WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models](https://arxiv.org/abs/2512.00837)
*Yukang Lin,Jiahao Shao,Shuoran Jiang,Wentao Zhu,Bingjie Lu,Xiangping Wu,Joanna Siebert,Qingcai Chen*

Main category: cs.CL

TL;DR: WaterSearch是一个搜索式水印框架，通过控制种子池实现多样化并行生成，在保持高可检测性的同时显著提升文本质量，解决了现有水印方法在可检测性与文本质量之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印方法通过操纵令牌生成概率嵌入信号，但面临可检测性与文本质量之间的固有权衡：鲁棒水印所需的信号强度和随机性会降低下游任务性能。需要一种既能保持高可检测性又不损害文本质量的水印方案。

Method: 提出基于种子池控制的嵌入方案，促进水印文本的多样化并行生成。在此基础上构建WaterSearch框架，这是一个句子级搜索式水印框架，通过联合优化分布保真度和水印信号特性来提升文本质量，并配备鲁棒的句子级检测方法。

Result: 在三个流行LLM和十个多样化任务上的实验显示：在95%可检测强度下，相比SOTA基线平均性能提升51.01%；在短文本生成场景提升47.78%；在低熵输出生成场景提升36.47%。在插入、同义词替换和改写攻击下仍保持高可检测性。

Conclusion: WaterSearch成功解决了现有水印方法在可检测性与文本质量之间的权衡问题，通过搜索式框架在保持高可检测性的同时显著提升文本质量，并展现出强大的抗攻击能力，为LLM水印技术提供了有效的解决方案。

Abstract: Watermarking acts as a critical safeguard in text generated by Large Language Models (LLMs). By embedding identifiable signals into model outputs, watermarking enables reliable attribution and enhances the security of machine-generated content. Existing approaches typically embed signals by manipulating token generation probabilities. Despite their effectiveness, these methods inherently face a trade-off between detectability and text quality: the signal strength and randomness required for robust watermarking tend to degrade the performance of downstream tasks.
  In this paper, we design a novel embedding scheme that controls seed pools to facilitate diverse parallel generation of watermarked text. Based on that scheme, we propose WaterSearch, a sentence-level, search-based watermarking framework adaptable to a wide range of existing methods. WaterSearch enhances text quality by jointly optimizing two key aspects: 1) distribution fidelity and 2) watermark signal characteristics. Furthermore, WaterSearch is complemented by a sentence-level detection method with strong attack robustness. We evaluate our method on three popular LLMs across ten diverse tasks. Extensive experiments demonstrate that our method achieves an average performance improvement of 51.01\% over state-of-the-art baselines at a watermark detectability strength of 95\%. In challenging scenarios such as short text generation and low-entropy output generation, our method yields performance gains of 47.78\% and 36.47\%, respectively. Moreover, under different attack senarios including insertion, synonym substitution and paraphrase attasks, WaterSearch maintains high detectability, further validating its robust anti-attack capabilities. Our code is available at \href{https://github.com/Yukang-Lin/WaterSearch}{https://github.com/Yukang-Lin/WaterSearch}.

中文标题: WaterSearch：面向大型语言模型的质量感知搜索式水印框架

中文摘要: 水印技术是大型语言模型（LLMs）生成文本的关键保障手段。通过在模型输出中嵌入可识别信号，水印技术能够实现可靠溯源并增强机器生成内容的安全性。现有方法通常通过操纵令牌生成概率来嵌入信号。尽管这些方法有效，但它们本质上面临着可检测性与文本质量之间的权衡：鲁棒水印所需的信号强度和随机性往往会降低下游任务的性能。

本文设计了一种新颖的嵌入方案，通过控制种子池来促进水印文本的多样化并行生成。基于该方案，我们提出了WaterSearch，这是一个适用于多种现有方法的句子级搜索式水印框架。WaterSearch通过联合优化两个关键方面来提升文本质量：1）分布保真度，2）水印信号特性。此外，WaterSearch还配备了具有强大攻击鲁棒性的句子级检测方法。我们在三个流行的大型语言模型和十个多样化任务上评估了我们的方法。大量实验表明，在95%的水印可检测强度下，我们的方法相比最先进的基线方法平均性能提升了51.01%。在短文本生成和低熵输出生成等挑战性场景中，我们的方法分别实现了47.78%和36.47%的性能提升。此外，在插入、同义词替换和改写攻击等不同攻击场景下，WaterSearch仍保持高可检测性，进一步验证了其强大的抗攻击能力。我们的代码可在https://github.com/Yukang-Lin/WaterSearch获取。

</details>


### [210] [Less is More: Resource-Efficient Low-Rank Adaptation](https://arxiv.org/abs/2512.00878)
*Chunlin Tian,Xuyang Wei,Huanrong Liu,Zhijiang Guo,Li Li*

Main category: cs.CL

TL;DR: EffiLoRA是一种资源高效的低秩适应方法，通过在所有Transformer层共享统一的A矩阵和运行时选择性更新B矩阵，显著减少训练开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA是广泛采用的参数高效微调方法，但在复杂数据集上仍存在显著开销和参数干扰问题。现有工作虽然通过解耦LoRA更新矩阵来利用矩阵不对称性，但训练成本仍然很高。因此需要从层间和层内参数冗余的角度重新审视LoRA，开发更轻量且可泛化的方法。

Method: EffiLoRA采用两种关键设计：1) 在所有Transformer层使用统一的A矩阵，减少参数冗余；2) 引入运行时选择性B矩阵更新机制，根据系统资源预算动态调整模型性能。这种方法在语言、多模态和扩散模型中都具有通用性。

Result: EffiLoRA在多种模态任务中一致优于标准LoRA，包括常识推理、视觉指令调优和图像生成。实验表明该方法在效率和鲁棒性方面都有显著提升，能够更好地平衡系统资源预算和模型性能。

Conclusion: EffiLoRA通过减少层间参数冗余和引入动态更新机制，实现了更高效的低秩适应。该方法在各种模态任务中都表现出优越性能，为参数高效微调提供了更轻量、更通用的解决方案。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While re- cent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, EffiLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices up- date to dynamically trade-off the system resource budget and model performance. EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demon- strating improved efficiency and robustness.

中文标题: 少即是多：资源高效的低秩适应

中文摘要: 低秩适应（LoRA）是大语言模型广泛采用的参数高效微调方法，但在复杂数据集上仍会产生显著开销并遭受参数干扰。虽然近期工作通过解耦LoRA更新矩阵来利用矩阵不对称性，但训练成本仍然很高。我们从层间和层内参数冗余的角度重新审视LoRA，提出了资源高效的低秩适应方法EffiLoRA，这是一种轻量级且可泛化的方法，适用于语言、多模态和扩散模型。EffiLoRA在所有Transformer层采用统一的A矩阵，并引入运行时选择性B矩阵更新机制，动态平衡系统资源预算和模型性能。EffiLoRA在多种模态任务中一致优于LoRA，包括常识推理、视觉指令调优和图像生成，展示了改进的效率和鲁棒性。

</details>


### [211] [Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios](https://arxiv.org/abs/2512.00920)
*Jianxiang Zang,Yongda Wei,Ruxue Bai,Shiyu Jiang,Nijia Mo,Binhong Li,Qiang Sun,Hui Liu*

Main category: cs.CL

TL;DR: Reward Auditor是一个用于评估奖励模型在真实世界扰动场景下适用性的假设检验框架，通过统计显著性检验来推断奖励模型在特定现实场景中的系统性漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型评估方法仅关注特定场景下的偏好感知准确性，忽视了真实世界扰动场景下的关键脆弱性。需要评估奖励模型在特定现实扰动条件下的条件可靠性（即适用性）。

Method: 引入Reward Auditor假设检验框架，通过审计奖励模型偏好感知置信度的分布退化来量化统计显著性和效应大小，从而推断奖励模型在特定真实世界场景中的系统性漏洞。

Result: 该框架能够推断奖励模型在不同真实世界场景中漏洞的确定性和严重程度，为构建可验证安全、更鲁棒和可信赖的下一代LLM对齐系统奠定基础。

Conclusion: Reward Auditor通过科学审计方法解决了奖励模型在真实世界扰动场景下的适用性评估问题，为LLM对齐系统的安全性提供了新的评估维度。

Abstract: Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering "How accurate is the RM's preference perception for given samples?", it employs scientific auditing to answer: "Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.

中文标题: 奖励审计员：真实世界扰动场景下奖励建模适用性的推断

中文摘要: 可靠的奖励模型对于确保大型语言模型的安全对齐至关重要。然而，当前的评估方法仅关注给定特定场景下的偏好感知准确性，忽视了奖励模型在真实世界场景中的关键脆弱性。我们认识到真正的挑战在于评估一个新的维度：适用性，即在特定真实世界扰动条件下的条件可靠性。为此，我们引入了Reward Auditor，这是一个专门为奖励模型适用性推断设计的假设检验框架。它不回答"奖励模型对给定样本的偏好感知有多准确？"，而是通过科学审计来回答："我们能否推断奖励模型在特定真实世界场景中表现出系统性漏洞？"。在真实世界扰动场景下，Reward Auditor通过审计奖励模型偏好感知置信度的分布退化来量化统计显著性和效应大小。这使得能够推断奖励模型在不同真实世界场景中漏洞的确定性和严重程度。这为构建可验证安全、更鲁棒和可信赖的下一代LLM对齐系统奠定了坚实基础。

</details>


### [212] [Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study](https://arxiv.org/abs/2512.00931)
*Imane Jaaouine,Ross D. King*

Main category: cs.CL

TL;DR: 本研究探讨了在零样本科学文本摘要中，通过提示工程方法（特别是上下文重复和随机添加）来减轻大语言模型产生上下文不一致幻觉的效果。实验表明这些方法能显著提高摘要与原文的词汇对齐度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成内容时会产生上下文不一致的幻觉，即在零样本科学文本摘要任务中，模型输出与用户提示不匹配。本研究旨在探索是否可以通过提示工程方法来减轻这种幻觉问题。

Method: 使用8篇酵母生物技术研究论文摘要，对6个指令调优的大语言模型应用7种提示方法：基线提示、两种复杂度递增的指令（PE-1和PE-2）、两种上下文重复（CR-K1和CR-K2）、两种随机添加（RA-K1和RA-K2）。共生成336个摘要，使用6个评估指标（ROUGE-1、ROUGE-2、ROUGE-L、BERTScore、METEOR、余弦相似度）评估摘要与原文的词汇和语义对齐度。

Result: 统计分析显示，上下文重复（CR）和随机添加（RA）方法显著提高了LLM生成摘要与原文的词汇对齐度。这些方法在减轻零样本科学摘要中的幻觉方面显示出潜力。

Conclusion: 提示工程方法，特别是上下文重复和随机添加，能够有效减轻大语言模型在零样本科学文本摘要任务中产生的上下文不一致幻觉，提高摘要与原文的对齐度。

Abstract: Large language models (LLMs) produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt. This research project investigates whether prompt engineering (PE) methods can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts, where zero-shot indicates that the LLM relies purely on its pre-training data. Across eight yeast biotechnology research paper abstracts, six instruction-tuned LLMs were prompted with seven methods: a base- line prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2). Context repetition involved the identification and repetition of K key sentences from the abstract, whereas random addition involved the repetition of K randomly selected sentences from the abstract, where K is 1 or 2. A total of 336 LLM-generated summaries were evaluated using six metrics: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity, which were used to compute the lexical and semantic alignment be- tween the summaries and the abstracts. Four hypotheses on the effects of prompt methods on summary alignment with the reference text were tested. Statistical analysis on 3744 collected datapoints was performed using bias-corrected and accelerated (BCa) bootstrap confidence intervals and Wilcoxon signed-rank tests with Bonferroni-Holm correction. The results demonstrated that CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts. These findings indicate that prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks.

中文标题: 减轻零样本科学摘要中的幻觉：一项试点研究

中文摘要: 大语言模型会产生上下文不一致的幻觉，即模型生成的输出与用户提示不匹配。本研究项目调查了提示工程方法是否能够减轻大语言模型在零样本科学文本摘要中的上下文不一致幻觉，其中零样本表示大语言模型完全依赖其预训练数据。在八篇酵母生物技术研究论文摘要中，对六个指令调优的大语言模型应用了七种提示方法：基线提示、两种复杂度递增的指令（PE-1和PE-2）、两种上下文重复（CR-K1和CR-K2）以及两种随机添加（RA-K1和RA-K2）。上下文重复涉及识别并重复摘要中的K个关键句子，而随机添加涉及重复摘要中随机选择的K个句子，其中K为1或2。总共336个大语言模型生成的摘要使用六个指标进行评估：ROUGE-1、ROUGE-2、ROUGE-L、BERTScore、METEOR和余弦相似度，这些指标用于计算摘要与原文之间的词汇和语义对齐度。测试了关于提示方法对摘要与参考文本对齐度影响的四个假设。对3744个收集的数据点进行了统计分析，使用了偏差校正和加速（BCa）自助置信区间以及经过Bonferroni-Holm校正的Wilcoxon符号秩检验。结果表明，CR和RA显著提高了大语言模型生成摘要与原文的词汇对齐度。这些发现表明，提示工程有潜力影响零样本科学摘要任务中的幻觉问题。

</details>


### [213] [DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics](https://arxiv.org/abs/2512.00938)
*Ahmed Mustafa Younes*

Main category: cs.CL

TL;DR: DeformAr是一个针对阿拉伯语NER的评估框架，通过跨组件分析和行为分析来解释阿拉伯语与英语NER系统之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在英语NLP任务中表现出色，但在阿拉伯语NER任务上效果有限，即使使用更大的预训练模型。这种性能差距源于分词、数据集质量和标注不一致性等多种因素，但现有研究通常孤立分析这些问题，未能捕捉它们对系统行为的联合影响。

Method: DeformAr框架集成了数据提取库和交互式仪表板，支持两种评估模式：1）跨组件分析：将语言划分为数据集和模型组件，提供系统性诊断指标；2）行为分析：结合可解释性技术、标记级指标、交互式可视化和表示空间分析，实现组件感知的诊断过程。

Result: DeformAr是第一个针对阿拉伯语的基于组件的可解释性工具，能够检测模型行为并通过将其与底层表示模式和数据因素联系起来进行解释，为推进资源匮乏语言中的模型分析提供了关键资源。

Conclusion: DeformAr通过组件分析和可视化分析重新思考NER评估，提供了一个系统性的框架来解释阿拉伯语与英语NER系统之间的性能差距，填补了现有研究中孤立分析问题的不足，为资源匮乏语言的模型分析提供了重要工具。

Abstract: Transformer models have significantly advanced Natural Language Processing (NLP), demonstrating strong performance in English. However, their effectiveness in Arabic, particularly for Named Entity Recognition (NER), remains limited, even with larger pre-trained models. This performance gap stems from multiple factors, including tokenisation, dataset quality, and annotation inconsistencies. Existing studies often analyze these issues in isolation, failing to capture their joint effect on system behaviour and performance.
  We introduce DeformAr (Debugging and Evaluation Framework for Transformer-based NER Systems), a novel framework designed to investigate and explain the performance discrepancy between Arabic and English NER systems. DeformAr integrates a data extraction library and an interactive dashboard, supporting two modes of evaluation: cross-component analysis and behavioural analysis. The framework divides each language into dataset and model components to examine their interactions.
  The analysis proceeds in two stages. First, cross-component analysis provides systematic diagnostic measures across data and model subcomponents, addressing the "what," "how," and "why" behind observed discrepancies. The second stage applies behavioural analysis by combining interpretability techniques with token-level metrics, interactive visualisations, and representation space analysis. These stages enable a component-aware diagnostic process that detects model behaviours and explains them by linking them to underlying representational patterns and data factors. DeformAr is the first Arabic-specific, component-based interpretability tool, offering a crucial resource for advancing model analysis in under-resourced languages.

中文标题: DeformAr：通过组件分析和可视化分析重新思考NER评估

中文摘要: Transformer模型显著推动了自然语言处理（NLP）的发展，在英语任务中表现出色。然而，在阿拉伯语中，特别是命名实体识别（NER）任务上，即使使用更大的预训练模型，其效果仍然有限。这种性能差距源于多种因素，包括分词、数据集质量和标注不一致性。现有研究通常孤立地分析这些问题，未能捕捉它们对系统行为和性能的联合影响。

我们提出了DeformAr（基于Transformer的NER系统调试与评估框架），这是一个新颖的框架，旨在调查和解释阿拉伯语与英语NER系统之间的性能差异。DeformAr集成了数据提取库和交互式仪表板，支持两种评估模式：跨组件分析和行为分析。该框架将每种语言划分为数据集和模型组件，以检查它们之间的相互作用。

分析过程分为两个阶段。首先，跨组件分析提供了跨数据和模型子组件的系统性诊断指标，解决了观察到的差异背后的"是什么"、"如何"和"为什么"问题。第二阶段通过将可解释性技术与标记级指标、交互式可视化和表示空间分析相结合，应用行为分析。这些阶段实现了一个组件感知的诊断过程，能够检测模型行为并通过将其与底层表示模式和数据因素联系起来进行解释。DeformAr是第一个针对阿拉伯语的基于组件的可解释性工具，为推进资源匮乏语言中的模型分析提供了关键资源。

</details>


### [214] [Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data](https://arxiv.org/abs/2512.00946)
*Alvaro Paredes Amorin,Andre Python,Christoph Weisser*

Main category: cs.CL

TL;DR: 该研究探讨了轻量级开源大语言模型在金融情感分类任务上的表现，发现Qwen3 8B和Llama3 8B等模型在仅使用5%训练数据的情况下，在异构金融文本数据上仍能取得优异性能，为资源受限的研究者提供了成本效益高的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统金融NLP分析依赖计算资源密集的大型语言模型和专有数据集，成本高昂且对许多研究者和从业者不可及。本研究旨在探索轻量级开源LLMs在异构金融文本数据上的泛化能力，为资源受限环境提供可行的替代方案。

Method: 比较了基准金融NLP模型FinBERT与三个开源轻量级LLMs（DeepSeek-LLM 7B、Llama3 8B Instruct、Qwen3 8B）在五个公开数据集上的表现：FinancialPhraseBank、Financial Question Answering、Gold News Sentiment、Twitter Sentiment和Chinese Finance Sentiment。研究了不同规模训练数据（包括仅5%数据）下的性能，并评估了零样本和少样本学习场景。

Result: 轻量级LLMs，特别是Qwen3 8B和Llama3 8B，在大多数场景中表现最佳，即使在仅使用5%可用训练数据的情况下。这些结果在零样本和少样本学习场景中同样成立。模型在不同规模、来源、格式和语言的金融数据集上展现了良好的泛化能力。

Conclusion: 轻量级开源大语言模型构成了成本效益高的选择，即使在仅使用有限标注语料子集进行训练的情况下，也能在异构文本数据上实现竞争性性能，为资源受限的研究者和从业者提供了可行的替代方案。

Abstract: Large language models (LLMs) play an increasingly important role in finan- cial markets analysis by capturing signals from complex and heterogeneous textual data sources, such as tweets, news articles, reports, and microblogs. However, their performance is dependent on large computational resources and proprietary datasets, which are costly, restricted, and therefore inacces- sible to many researchers and practitioners. To reflect realistic situations we investigate the ability of lightweight open-source LLMs - smaller and publicly available models designed to operate with limited computational resources - to generalize sentiment understanding from financial datasets of varying sizes, sources, formats, and languages. We compare the benchmark finance natural language processing (NLP) model, FinBERT, and three open-source lightweight LLMs, DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B on five publicly available datasets: FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment and Chinese Finance Sentiment. We find that LLMs, specially Qwen3 8B and Llama3 8B, perform best in most scenarios, even from using only 5% of the available training data. These results hold in zero-shot and few-shot learning scenarios. Our findings indicate that lightweight, open-source large language models (LLMs) consti- tute a cost-effective option, as they can achieve competitive performance on heterogeneous textual data even when trained on only a limited subset of the extensive annotated corpora that are typically deemed necessary.

中文标题: 轻量级大语言模型在异构金融文本数据情感分类上的微调研究

中文摘要: 大语言模型通过从复杂异构的文本数据源（如推文、新闻文章、报告和微博）中捕捉信号，在金融市场分析中发挥着越来越重要的作用。然而，它们的性能依赖于大量的计算资源和专有数据集，这些资源成本高昂、受限，因此对许多研究者和从业者来说不可及。为了反映实际情况，我们研究了轻量级开源LLMs（即设计用于在有限计算资源下运行的较小且公开可用的模型）从不同规模、来源、格式和语言的金融数据集中泛化情感理解的能力。我们比较了基准金融自然语言处理模型FinBERT和三个开源轻量级LLMs：DeepSeek-LLM 7B、Llama3 8B Instruct和Qwen3 8B，在五个公开可用数据集上：FinancialPhraseBank、Financial Question Answering、Gold News Sentiment、Twitter Sentiment和Chinese Finance Sentiment。我们发现LLMs，特别是Qwen3 8B和Llama3 8B，在大多数场景中表现最佳，即使仅使用5%的可用训练数据。这些结果在零样本和少样本学习场景中同样成立。我们的研究结果表明，轻量级开源大语言模型构成了成本效益高的选择，因为它们即使在仅使用通常被认为必要的大量标注语料的有限子集进行训练的情况下，也能在异构文本数据上实现竞争性性能。

</details>


### [215] [Advancing Academic Chatbots: Evaluation of Non Traditional Outputs](https://arxiv.org/abs/2512.00991)
*Nicole Favero,Francesca Salute,Daniel Hardt*

Main category: cs.CL

TL;DR: 本研究评估了两种检索增强生成策略（Graph RAG与Advanced RAG）在学术问答中的表现，并测试了LLM生成非传统学术输出（幻灯片和播客脚本）的能力。GPT-4o mini配合Advanced RAG表现最佳，而人类评估在检测布局和风格缺陷方面至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估主要关注传统任务如事实问答和简短摘要，缺乏对非传统学术输出能力的评估。本研究旨在扩展评估范围，比较不同检索策略在学术问答中的效果，并探索LLM生成高质量幻灯片和播客脚本的能力。

Method: 研究采用原型系统结合Meta的LLaMA 3 70B和OpenAI的GPT-4o mini API。比较两种检索策略：基于知识图谱的Graph RAG和混合关键词-语义搜索的Advanced RAG。评估包括11个质量维度的人工评分和LLM法官的规模化交叉验证。同时测试了基于文档检索的幻灯片和播客脚本生成能力。

Result: GPT-4o mini配合Advanced RAG在问答准确性方面表现最佳。Graph RAG改进有限且导致更多幻觉，部分原因是其结构复杂性和手动设置。在幻灯片和播客生成方面，GPT-4o mini再次表现最好，而LLaMA 3在叙事连贯性方面显示出潜力。人类评审在检测布局和风格缺陷方面至关重要。

Conclusion: 评估新兴学术输出需要结合人类和LLM的评估方法。Advanced RAG在学术问答中表现优于Graph RAG，GPT-4o mini在非传统学术输出生成方面表现最佳。人类评估对于检测风格和布局问题仍然不可或缺。

Abstract: Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.

中文标题: 推进学术聊天机器人：非传统输出的评估

中文摘要: 大多数大语言模型的评估都集中在标准任务上，如事实问答或简短摘要。本研究从两个方向扩展了这一范围：首先，比较两种检索策略——基于结构化知识图谱的Graph RAG和混合关键词-语义搜索的Advanced RAG在问答中的表现；其次，评估LLM是否能生成高质量的非传统学术输出，特别是幻灯片和播客脚本。我们实现了一个结合Meta的LLaMA 3 70B开源权重和OpenAI的GPT-4o mini API的原型系统。问答性能通过11个质量维度的人工评分和用于规模化交叉验证的大语言模型法官进行评估。GPT-4o mini配合Advanced RAG产生了最准确的回答。Graph RAG提供的改进有限，并导致更多幻觉，部分原因是其结构复杂性和手动设置。幻灯片和播客生成通过基于文档的检索进行了测试。GPT-4o mini再次表现最佳，尽管LLaMA 3在叙事连贯性方面显示出潜力。人类评审在检测布局和风格缺陷方面至关重要，突显了在评估新兴学术输出时需要结合人类和LLM评估。

</details>


### [216] [When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals](https://arxiv.org/abs/2512.01037)
*Riad Ahmed Anonto,Md Labid Al Nahiyan,Md Tanvir Hassan,Ch. Md. Rakin Haider*

Main category: cs.CL

TL;DR: 论文提出"语义混淆"概念来衡量LLM安全拒绝中的局部不一致性，开发了ParaGuard数据集和三个模型无关的指标来量化这种不一致性，发现全局拒绝率掩盖了关键的结构性问题。


<details>
  <summary>Details</summary>
Motivation: 当前的安全对齐语言模型经常拒绝实际上无害的提示，但现有评估主要关注全局拒绝率或合规率，忽略了局部不一致性——即模型接受某种意图表达却拒绝其近义改写。这种评估差距限制了模型的诊断和调优。

Method: 1. 提出"语义混淆"概念来捕捉局部不一致性；2. 构建ParaGuard数据集，包含1万个受控的改写簇，保持意图不变而改变表面形式；3. 提出三个模型无关的token级指标：混淆指数、混淆率和混淆深度，使用token嵌入、下一个token概率和困惑度信号来比较每个拒绝与其最近接受邻居。

Result: 实验显示全局错误拒绝率掩盖了关键结构：在某些设置中发现了全局不稳定的边界，在其他设置中发现了局部不一致性区域，以及更严格的拒绝并不总是增加不一致性的情况。混淆感知审计能够区分系统拒绝的频率和拒绝的合理性。

Conclusion: 语义混淆框架为开发者提供了实用的信号，可以在保持安全性的同时减少错误拒绝。该工作填补了现有评估的空白，为更精细的模型诊断和调优提供了工具。

Abstract: Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce "semantic confusion," a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.

中文标题: 当安全屏障感知时：测量LLM拒绝中的语义混淆

中文摘要: 安全对齐的语言模型经常拒绝实际上无害的提示。当前的评估主要报告全局比率，如错误拒绝率或合规率。这些分数单独处理每个提示，忽略了局部不一致性——即模型接受一种意图表达却拒绝其近义改写。这种差距限制了诊断和调优。我们引入了"语义混淆"这一捕捉局部不一致性的故障模式，以及测量它的框架。我们构建了ParaGuard，一个包含1万个受控改写簇的提示语料库，保持意图不变而变化表面形式。然后我们提出了三个模型无关的token级指标：混淆指数、混淆率和混淆深度。这些指标将每个拒绝与其最近的接受邻居进行比较，并使用token嵌入、下一个token概率和困惑度信号。在不同模型系列和部署防护的实验表明，全局错误拒绝率掩盖了关键结构。我们的指标揭示了在某些设置中全局不稳定的边界，在其他设置中局部不一致性的区域，以及更严格的拒绝并不增加不一致性的情况。我们还展示了混淆感知审计如何区分系统拒绝的频率和拒绝的合理性。这为开发者提供了实用的信号，可以在保持安全性的同时减少错误拒绝。

</details>


### [217] [How do we measure privacy in text? A survey of text anonymization metrics](https://arxiv.org/abs/2512.01109)
*Yaxuan Ren,Krithika Ramesh,Yaxing Yao,Anjalie Field*

Main category: cs.CL

TL;DR: 本文系统综述了文本匿名化中的隐私度量方法，识别了六种隐私概念，评估了它们与法律标准和用户期望的契合度，为更稳健的隐私评估提供指导。


<details>
  <summary>Details</summary>
Motivation: 文本匿名化对于在敏感数据领域进行NLP研究至关重要，但如何评估匿名化方法是否充分保护隐私仍是一个开放挑战。当前缺乏对隐私度量方法的系统性梳理和比较，导致评估标准不统一，难以确保隐私保护的有效性和合规性。

Method: 采用系统性综述方法，手动审查了47篇报告隐私度量的论文，识别并比较了六种不同的隐私概念及其相关度量方法。进一步分析了这些概念与法律隐私标准（HIPAA和GDPR）以及基于HCI研究的用户期望之间的契合程度。

Result: 识别了六种不同的隐私概念：可识别性、链接性、推断性、敏感性、背景知识和效用-隐私权衡。发现当前度量方法在捕捉隐私风险的不同方面存在差异，且与法律标准和用户期望的契合度有待提高。揭示了当前实践中的差距，包括缺乏标准化评估框架和与法律要求的对齐不足。

Conclusion: 文本匿名化的隐私评估需要更系统化和标准化的方法。未来的研究应致力于开发更全面的隐私度量框架，更好地与法律要求对齐，并考虑用户的实际隐私期望。该研究为促进更稳健、可比较且具有法律意识的隐私评估提供了重要指导。

Abstract: In this work, we aim to clarify and reconcile metrics for evaluating privacy protection in text through a systematic survey. Although text anonymization is essential for enabling NLP research and model development in domains with sensitive data, evaluating whether anonymization methods sufficiently protect privacy remains an open challenge. In manually reviewing 47 papers that report privacy metrics, we identify and compare six distinct privacy notions, and analyze how the associated metrics capture different aspects of privacy risk. We then assess how well these notions align with legal privacy standards (HIPAA and GDPR), as well as user-centered expectations grounded in HCI studies. Our analysis offers practical guidance on navigating the landscape of privacy evaluation approaches further and highlights gaps in current practices. Ultimately, we aim to facilitate more robust, comparable, and legally aware privacy evaluations in text anonymization.

中文标题: 如何衡量文本隐私？文本匿名化度量方法综述

中文摘要: 在本研究中，我们通过系统性综述来澄清和协调评估文本隐私保护的度量方法。尽管文本匿名化对于在敏感数据领域开展NLP研究和模型开发至关重要，但评估匿名化方法是否充分保护隐私仍然是一个开放挑战。通过手动审查47篇报告隐私度量的论文，我们识别并比较了六种不同的隐私概念，并分析了相关度量如何捕捉隐私风险的不同方面。然后，我们评估了这些概念与法律隐私标准（HIPAA和GDPR）以及基于HCI研究的以用户为中心的期望之间的契合程度。我们的分析为导航隐私评估方法的格局提供了实用指导，并突出了当前实践中的差距。最终，我们的目标是促进文本匿名化中更稳健、可比较且具有法律意识的隐私评估。

</details>


### [218] [TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness](https://arxiv.org/abs/2512.01183)
*Yongxin Zhou,Philippe Mulhem,Didier Schwab*

Main category: cs.CL

TL;DR: 该研究系统分析了RAG系统中检索文档扰动与LLM温度设置的交互影响，发现高温设置会放大扰动带来的性能下降，不同扰动类型对温度变化表现出非线性敏感性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统评估通常孤立地检查检索质量和生成参数（如温度），忽略了它们之间的相互作用。本研究旨在系统研究文本扰动（模拟噪声检索）如何与不同温度设置在多轮LLM运行中交互影响RAG性能。

Method: 提出了一个全面的RAG扰动-温度分析框架，对检索文档应用三种不同类型的扰动（如噪声注入、语义变化等），并在不同温度设置下进行实验。在HotpotQA数据集上使用开源和专有LLM进行广泛实验，量化扰动与温度的交互效应。

Result: 实验表明性能下降呈现特定模式：高温设置始终放大对扰动的脆弱性，而某些扰动类型在温度范围内表现出非线性敏感性。不同LLM对扰动-温度交互的响应存在差异。

Conclusion: 该研究为评估RAG鲁棒性提供了诊断基准，建立了量化扰动-温度交互的分析框架，并为噪声检索条件下的模型选择和参数调优提供了实用指南。

Abstract: The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.

中文标题: TempPerturb-Eval：RAG鲁棒性中内部温度与外部扰动的联合效应研究

中文摘要: 检索增强生成（RAG）系统的评估通常孤立地检查检索质量和温度等生成参数，忽略了它们的相互作用。本研究系统研究了文本扰动（模拟噪声检索）如何在不同温度设置下与多轮LLM运行交互。我们提出了一个全面的RAG扰动-温度分析框架，对检索文档应用三种不同的扰动类型，并在不同温度设置下进行测试。通过在HotpotQA上使用开源和专有LLM进行广泛实验，我们证明性能下降遵循特定模式：高温设置始终放大对扰动的脆弱性，而某些扰动类型在温度范围内表现出非线性敏感性。我们的工作有三个关键贡献：（1）评估RAG鲁棒性的诊断基准，（2）量化扰动-温度交互的分析框架，以及（3）噪声检索条件下模型选择和参数调优的实用指南。

</details>


### [219] [Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks](https://arxiv.org/abs/2512.01191)
*Krithik Vishwanath,Mrigayu Ghosh,Anton Alyakin,Daniel Alexander Alber,Yindalon Aphinyanaphongs,Eric Karl Oermann*

Main category: cs.CL

TL;DR: 通用大型语言模型在医学基准测试中表现优于专门的临床AI工具，GPT-5得分最高，而OpenEvidence和UpToDate在完整性、沟通质量、情境意识和系统安全推理方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 专门临床AI助手正在快速进入医疗实践，常被宣传为比通用大型语言模型更安全可靠，但这些临床工具很少接受独立的定量评估，尽管它们对诊断、分诊和指南解读的影响日益增长，却存在关键证据缺口。

Method: 评估了两个广泛部署的临床AI系统（OpenEvidence和UpToDate Expert AI）与三个最先进的通用LLM（GPT-5、Gemini 3 Pro和Claude Sonnet 4.5），使用包含MedQA（医学知识）和HealthBench（临床医生对齐）任务的1,000项迷你基准测试。

Result: 通用模型始终优于临床工具，GPT-5获得最高分，而OpenEvidence和UpToDate在完整性、沟通质量、情境意识和系统安全推理方面表现出缺陷。

Conclusion: 为临床决策支持而营销的工具可能常常落后于前沿LLM，强调了在患者工作流程中部署前进行透明、独立评估的紧迫需求。

Abstract: Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.

中文标题: 通用大型语言模型在医学基准测试中表现优于临床工具

中文摘要: 专门的临床AI助手正在快速进入医疗实践，常被宣传为比通用大型语言模型更安全可靠。然而，与前沿模型不同，这些临床工具很少接受独立的定量评估，尽管它们对诊断、分诊和指南解读的影响日益增长，却存在关键证据缺口。我们使用结合MedQA（医学知识）和HealthBench（临床医生对齐）任务的1,000项迷你基准测试，评估了两个广泛部署的临床AI系统（OpenEvidence和UpToDate Expert AI）与三个最先进的通用LLM（GPT-5、Gemini 3 Pro和Claude Sonnet 4.5）。通用模型始终优于临床工具，GPT-5获得最高分，而OpenEvidence和UpToDate在完整性、沟通质量、情境意识和系统安全推理方面表现出缺陷。这些发现表明，为临床决策支持而营销的工具可能常常落后于前沿LLM，强调了在患者工作流程中部署前进行透明、独立评估的紧迫需求。

</details>


### [220] [Sentiment Analysis and Emotion Classification using Machine Learning Techniques for Nagamese Language - A Low-resource Language](https://arxiv.org/abs/2512.01256)
*Ekha Morang,Surhoni A. Ngullie,Sashienla Longkumer,Teisovi Angami*

Main category: cs.CL

TL;DR: 这是首次针对低资源的那加语进行情感分析和情绪分类的研究，构建了包含1,195个词的情感词典，并采用朴素贝叶斯和SVM进行极性分类。


<details>
  <summary>Details</summary>
Motivation: 虽然英语、印地语等资源丰富的语言已有大量情感分析研究，但那加语作为印度东北部重要的克里奥尔语，尚未有相关研究。该语言在贸易交流中具有重要作用，填补这一研究空白对保护语言多样性和促进跨文化交流具有重要意义。

Method: 1. 构建包含1,195个那加语词的情感极性词典；2. 使用词典特征和额外特征；3. 采用监督机器学习方法：朴素贝叶斯和支持向量机；4. 进行情感极性分类（积极、消极、中性）和基本情绪分类。

Result: 成功构建了那加语情感分析的首个资源——包含1,195个词的情感极性词典，并建立了基于机器学习的分类模型。这是该语言情感分析领域的开创性工作。

Conclusion: 本研究填补了那加语情感分析的研究空白，为低资源语言的自然语言处理提供了重要参考。构建的情感词典和分类模型为该语言的后续研究奠定了基础，展示了机器学习技术在低资源语言处理中的可行性。

Abstract: The Nagamese language, a.k.a Naga Pidgin, is an Assamese-lexified creole language developed primarily as a means of communication in trade between the people from Nagaland and people from Assam in the north-east India. Substantial amount of work in sentiment analysis has been done for resource-rich languages like English, Hindi, etc. However, no work has been done in Nagamese language. To the best of our knowledge, this is the first attempt on sentiment analysis and emotion classification for the Nagamese Language. The aim of this work is to detect sentiments in terms of polarity (positive, negative and neutral) and basic emotions contained in textual content of Nagamese language. We build sentiment polarity lexicon of 1,195 nagamese words and use these to build features along with additional features for supervised machine learning techniques using Na"ive Bayes and Support Vector Machines.
  Keywords: Nagamese, NLP, sentiment analysis, machine learning

中文标题: 使用机器学习技术对那加语进行情感分析和情绪分类——一种低资源语言

中文摘要: 那加语，又称那加皮钦语，是一种以阿萨姆语为词源的克里奥尔语，主要作为印度东北部那加兰邦和阿萨姆邦人民之间贸易交流的工具。对于英语、印地语等资源丰富的语言，情感分析方面已有大量研究工作，但那加语尚未开展相关工作。据我们所知，这是首次针对那加语进行情感分析和情绪分类的尝试。本工作的目标是从极性（积极、消极和中性）和基本情绪两个维度检测那加语文本内容中的情感。我们构建了包含1,195个那加语词的情感极性词典，并利用这些词典特征以及额外特征，采用朴素贝叶斯和支持向量机等监督机器学习技术进行建模。
关键词：那加语，自然语言处理，情感分析，机器学习

</details>


### [221] [SUPERChem: A Multimodal Reasoning Benchmark in Chemistry](https://arxiv.org/abs/2512.01274)
*Zehua Zhao,Zhixian Huang,Junren Li,Siyu Lin,Junting Zhou,Fengqi Cao,Kun Zhou,Rui Ge,Tingting Long,Yuexiang Zhu,Yan Liu,Jie Zheng,Junnian Wei,Rong Zhu,Peng Zou,Wenyu Li,Zekai Cheng,Tian Ding,Yaxuan Wang,Yizhao Yan,Tingru Wei,Haowei Ming,Weijie Mao,Chen Sun,Yiming Liu,Zichen Wang,Zuo Zhang,Tong Yang,Hao Ma,Zhen Gao,Jian Pei*

Main category: cs.CL

TL;DR: SUPERChem是一个包含500个专家策划化学问题的多模态推理基准测试，通过推理路径保真度评分评估模型推理质量，结果显示当前最佳模型仍低于人类基准表现。


<details>
  <summary>Details</summary>
Motivation: 当前化学推理基准测试存在三个主要问题：1）任务过于简化；2）缺乏过程级评估；3）与专家级化学技能不匹配。需要一个新的基准测试来更准确地评估LLMs的化学推理能力。

Method: 1）创建500个专家策划的推理密集型化学问题，涵盖多个子领域；2）提供多模态和纯文本两种格式；3）采用迭代策划流程消除缺陷项目；4）为每个问题配备专家编写的解决方案路径；5）引入推理路径保真度（RPF）评分系统评估推理质量。

Result: 人类基准准确率为40.3%，而最佳模型GPT-5（High）仅达到38.5%，其他模型如Gemini 2.5 Pro（37.9%）和DeepSeek-V3.1-Think（37.3%）也表现不佳。SUPERChem成功区分了高保真推理器与启发式推理器，并揭示了视觉信息对模型表现的依赖性影响。

Conclusion: SUPERChem提供了一个具有挑战性的化学推理基准测试和可靠的评估框架，能够更准确地评估LLMs的化学推理能力，促进模型向专家级化学智能发展。当前最先进的LLMs在化学推理方面仍落后于人类专家水平。

Abstract: Current benchmarks for evaluating the chemical reasoning capabilities of Large Language Models (LLMs) are limited by oversimplified tasks, lack of process-level evaluation, and misalignment with expert-level chemistry skills. To address these issues, we introduce SUPERChem, a benchmark of 500 expert-curated reasoning-intensive chemistry problems, covering diverse subfields and provided in both multimodal and text-only formats. Original content and an iterative curation pipeline eliminate flawed items and mitigate data contamination. Each problem is paired with an expert-authored solution path, enabling Reasoning Path Fidelity (RPF) scoring to evaluate reasoning quality beyond final-answer accuracy. Evaluations against a human baseline of 40.3% accuracy show that even the best-performing model, GPT-5 (High), reaches only 38.5%, followed closely by Gemini 2.5 Pro (37.9%) and DeepSeek-V3.1-Think (37.3%). SUPERChem elicits multi-step, multimodal reasoning, reveals model-dependent effects of visual information, and distinguishes high-fidelity reasoners from heuristic ones. By providing a challenging benchmark and a reliable evaluation framework, SUPERChem aims to facilitate the advancement of LLMs toward expert-level chemical intelligence. The dataset of the benchmark is available at https://huggingface.co/datasets/ZehuaZhao/SUPERChem.

中文标题: SUPERChem：化学领域的多模态推理基准测试

中文摘要: 当前用于评估大型语言模型（LLMs）化学推理能力的基准测试存在任务过于简化、缺乏过程级评估以及与专家级化学技能不匹配等问题。为解决这些问题，我们引入了SUPERChem，这是一个包含500个专家策划的推理密集型化学问题的基准测试，涵盖多个子领域，并提供多模态和纯文本两种格式。原始内容和迭代策划流程消除了有缺陷的项目并减轻了数据污染。每个问题都配有专家编写的解决方案路径，支持推理路径保真度（RPF）评分，以评估超越最终答案准确性的推理质量。与人类基准40.3%的准确率相比，表现最佳的模型GPT-5（High）仅达到38.5%，紧随其后的是Gemini 2.5 Pro（37.9%）和DeepSeek-V3.1-Think（37.3%）。SUPERChem能够激发多步骤、多模态推理，揭示视觉信息的模型依赖性影响，并区分高保真推理器与启发式推理器。通过提供具有挑战性的基准测试和可靠的评估框架，SUPERChem旨在促进LLMs向专家级化学智能发展。基准测试的数据集可在https://huggingface.co/datasets/ZehuaZhao/SUPERChem获取。

</details>


### [222] [Agreement-Constrained Probabilistic Minimum Bayes Risk Decoding](https://arxiv.org/abs/2512.01316)
*Koki Natsumi,Hiroyuki Deguchi,Yusuke Sakai,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: AC-PMBR解码通过知识蒸馏模型指导矩阵补全，在减少计算成本的同时保持翻译质量，相比传统PMBR解码在WMT'23任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统MBR解码需要评估所有候选对，计算复杂度高；PMBR解码通过采样减少计算量但会降低翻译质量。需要在保持翻译质量的同时减少计算成本。

Method: 提出AC-PMBR解码，利用知识蒸馏模型指导矩阵补全过程，通过一致性约束改进分数矩阵的补全精度。

Result: AC-PMBR将矩阵补全近似误差提高了最多3倍，在WMT'23 En↔De翻译任务上以可比较计算成本实现了比PMBR解码更高的翻译质量。

Conclusion: AC-PMBR解码通过知识蒸馏指导的矩阵补全，有效改善了翻译质量与计算成本之间的权衡，为高效高质量机器翻译提供了新方法。

Abstract: Minimum Bayes risk (MBR) decoding generates high-quality translations by maximizing the expected utility of output candidates, but it evaluates all pairwise scores over the candidate set; hence, it takes quadratic time with respect to the number of candidates. To reduce the number of utility function calls, probabilistic MBR (PMBR) decoding partially evaluates quality scores using sampled pairs of candidates and completes the missing scores with a matrix completion algorithm. Nevertheless, it degrades the translation quality as the number of utility function calls is reduced. Therefore, to improve the trade-off between quality and cost, we propose agreement-constrained PMBR (AC-PMBR) decoding, which leverages a knowledge distilled model to guide the completion of the score matrix. Our AC-PMBR decoding improved approximation errors of matrix completion by up to 3 times and achieved higher translation quality compared with PMBR decoding at a comparable computational cost on the WMT'23 En$\leftrightarrow$De translation tasks.

中文标题: 基于一致性约束的概率最小贝叶斯风险解码

中文摘要: 最小贝叶斯风险（MBR）解码通过最大化输出候选的期望效用来生成高质量翻译，但它需要评估候选集中所有成对分数，因此计算复杂度与候选数量呈二次方关系。为了减少效用函数调用次数，概率MBR（PMBR）解码使用采样的候选对来部分评估质量分数，并通过矩阵补全算法补全缺失分数。然而，随着效用函数调用次数的减少，翻译质量会下降。因此，为了改善质量与成本之间的权衡，我们提出了基于一致性约束的PMBR（AC-PMBR）解码，该方法利用知识蒸馏模型来指导分数矩阵的补全。我们的AC-PMBR解码将矩阵补全的近似误差提高了最多3倍，在WMT'23 En↔De翻译任务上，以可比较的计算成本实现了比PMBR解码更高的翻译质量。

</details>


### [223] [MARSAD: A Multi-Functional Tool for Real-Time Social Media Analysis](https://arxiv.org/abs/2512.01369)
*Md. Rafiul Biswas,Firoj Alam,Wajdi Zaghouani*

Main category: cs.CL

TL;DR: MARSAD是一个多功能NLP平台，专为阿拉伯语社交媒体实时监控和分析设计，集成了情感分析、情绪分析、宣传检测、事实核查和仇恨言论检测等多种功能，支持研究人员和非技术用户使用。


<details>
  <summary>Details</summary>
Motivation: 针对阿拉伯语社交媒体内容缺乏有效的实时监控和分析工具，特别是需要处理大规模多模态数据并提供多种分析维度的需求，开发一个集成的平台来支持研究人员和非技术用户进行社交媒体分析。

Method: 采用灵活的后端架构，结合文档存储和结构化数据管理，集成多种NLP分析功能（情感分析、情绪分析、宣传检测等），通过API密钥实现安全的数据抓取，并提供用户友好的前端界面支持数据上传和交互。

Result: 成功开发了一个多功能NLP平台，能够实时监控和分析阿拉伯语社交媒体内容，生成详细的可视化和报告，支持大规模多模态数据处理，为非技术用户提供了易于使用的社交媒体分析工具。

Conclusion: MARSAD是一个有效的社交媒体分析平台，特别针对阿拉伯语内容，集成了多种NLP功能，为研究人员和非技术用户提供了全面的社交媒体监控和分析解决方案。

Abstract: MARSAD is a multifunctional natural language processing (NLP) platform designed for real-time social media monitoring and analysis, with a particular focus on the Arabic-speaking world. It enables researchers and non-technical users alike to examine both live and archived social media content, producing detailed visualizations and reports across various dimensions, including sentiment analysis, emotion analysis, propaganda detection, fact-checking, and hate speech detection. The platform also provides secure data-scraping capabilities through API keys for accessing public social media data. MARSAD's backend architecture integrates flexible document storage with structured data management, ensuring efficient processing of large and multimodal datasets. Its user-friendly frontend supports seamless data upload and interaction.

中文标题: MARSAD：一个用于实时社交媒体分析的多功能工具

中文摘要: MARSAD是一个多功能自然语言处理（NLP）平台，专为实时社交媒体监控和分析设计，特别关注阿拉伯语世界。它使研究人员和非技术用户都能够检查实时和存档的社交媒体内容，生成跨多个维度的详细可视化和报告，包括情感分析、情绪分析、宣传检测、事实核查和仇恨言论检测。该平台还通过API密钥提供安全的数据抓取功能，用于访问公共社交媒体数据。MARSAD的后端架构集成了灵活的文档存储和结构化数据管理，确保高效处理大规模多模态数据集。其用户友好的前端支持无缝数据上传和交互。

</details>


### [224] [DyFuLM: An Advanced Multimodal Framework for Sentiment Analysis](https://arxiv.org/abs/2512.01410)
*Ruohan Zhou,Jiachen Yuan,Churui Yang,Wenzheng Huang,Guoyan Zhang,Shiyao Wei,Jiazhen Hu,Ning Xin,Md Maruf Hasan*

Main category: cs.CL

TL;DR: DyFuLM是一种用于情感分析的多模态框架，通过分层动态融合和门控特征聚合模块，在粗粒度和细粒度情感分类任务上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 理解复杂文本表达中的情感是情感计算的基本挑战，需要捕捉分层语义表示和细粒度情感细微差别。

Method: 提出动态融合学习模型（DyFuLM），包含两个关键模块：1）分层动态融合模块，自适应整合多级特征；2）门控特征聚合模块，调节跨层信息流以实现平衡表示学习。

Result: 在多任务情感数据集上，DyFuLM实现了82.64%粗粒度和68.48%细粒度准确率，获得最低回归误差（MAE=0.0674，MSE=0.0082）和最高R²系数（0.6903）。消融研究显示各模块对性能均有显著贡献。

Conclusion: DyFuLM通过有效的分层特征融合增强了情感表示和整体性能，每个模块都对特征交互和任务平衡做出了重要贡献。

Abstract: Understanding sentiment in complex textual expressions remains a fundamental challenge in affective computing. To address this, we propose a Dynamic Fusion Learning Model (DyFuLM), a multimodal framework designed to capture both hierarchical semantic representations and fine-grained emotional nuances. DyFuLM introduces two key moodules: a Hierarchical Dynamic Fusion module that adaptively integrates multi-level features, and a Gated Feature Aggregation module that regulates cross-layer information ffow to achieve balanced representation learning. Comprehensive experiments on multi-task sentiment datasets demonstrate that DyFuLM achieves 82.64% coarse-grained and 68.48% fine-grained accuracy, yielding the lowest regression errors (MAE = 0.0674, MSE = 0.0082) and the highest R^2 coefficient of determination (R^2= 0.6903). Furthermore, the ablation study validates the effectiveness of each module in DyFuLM. When all modules are removed, the accuracy drops by 0.91% for coarse-grained and 0.68% for fine-grained tasks. Keeping only the gated fusion module causes decreases of 0.75% and 0.55%, while removing the dynamic loss mechanism results in drops of 0.78% and 0.26% for coarse-grained and fine-grained sentiment classification, respectively. These results demonstrate that each module contributes significantly to feature interaction and task balance. Overall, the experimental findings further validate that DyFuLM enhances sentiment representation and overall performance through effective hierarchical feature fusion.

中文标题: DyFuLM：一种用于情感分析的高级多模态框架

中文摘要: 理解复杂文本表达中的情感仍然是情感计算中的一个基本挑战。为了解决这个问题，我们提出了动态融合学习模型（DyFuLM），这是一个旨在捕捉分层语义表示和细粒度情感细微差别的多模态框架。DyFuLM引入了两个关键模块：一个自适应整合多级特征的分层动态融合模块，以及一个调节跨层信息流以实现平衡表示学习的门控特征聚合模块。在多任务情感数据集上的综合实验表明，DyFuLM实现了82.64%的粗粒度和68.48%的细粒度准确率，获得了最低的回归误差（MAE = 0.0674，MSE = 0.0082）和最高的R²决定系数（R² = 0.6903）。此外，消融研究验证了DyFuLM中每个模块的有效性。当所有模块都被移除时，粗粒度和细粒度任务的准确率分别下降了0.91%和0.68%。仅保留门控融合模块会导致准确率下降0.75%和0.55%，而去除动态损失机制则分别使粗粒度和细粒度情感分类的准确率下降0.78%和0.26%。这些结果表明，每个模块都对特征交互和任务平衡做出了显著贡献。总体而言，实验发现进一步验证了DyFuLM通过有效的分层特征融合增强了情感表示和整体性能。

</details>


### [225] [PromptBridge: Cross-Model Prompt Transfer for Large Language Models](https://arxiv.org/abs/2512.01420)
*Yaxuan Wang,Quan Liu,Zhenting Wang,Zichao Li,Wei Wei,Yang Liu,Yujia Bao*

Main category: cs.CL

TL;DR: PromptBridge是一个无需训练的框架，通过少量对齐任务校准，实现跨模型提示迁移，解决模型漂移问题，提升下游任务准确率并减少迁移成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在实际应用中经常需要切换（由于能力、成本、部署约束和隐私等因素），但为特定模型优化的提示在其他模型上性能会显著下降（模型漂移现象），这导致频繁的模型切换需要昂贵的提示重新优化。

Method: PromptBridge框架包含两个主要步骤：1) 使用模型自适应反射提示进化（MAP-RPE）通过迭代反射精化和定量评估获得任务和模型特定的最优提示；2) 利用源模型和目标模型的校准提示对学习跨模型提示映射，在测试时直接为未见任务生成目标模型的优化提示。

Result: 实验表明PromptBridge在单智能体和多智能体设置中都能持续提升下游任务准确率，同时显著减少模型迁移的工作量，有效解决了模型漂移问题。

Conclusion: PromptBridge提供了一种无需训练、成本效益高的解决方案，能够保持提示在不同模型间的有效性，支持跨模型提示迁移，适应快速演变的LLM生态系统。

Abstract: Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.

中文标题: PromptBridge：大型语言模型的跨模型提示迁移框架

中文摘要: 大型语言模型支撑着代码生成、数学推理和基于智能体的工作流等应用。在实践中，系统通过商业API或开源部署访问LLM，模型生态（如GPT、Claude、Llama）快速演变。这种快速演变迫使频繁的模型切换，驱动因素包括能力、成本、部署约束和隐私。然而提示是高度模型敏感的：将一个模型优化的提示复用到另一个模型上，通常会产生比为目标模型优化的提示差得多的性能。我们将这种现象称为模型漂移。通过对多样化LLM配置的广泛实证分析，我们表明模型漂移既普遍又严重。为应对这一挑战，我们提出了PromptBridge，一个无需训练的框架，能够在模型切换时保持提示有效性，实现跨模型提示迁移而无需昂贵的每任务或每模型重新优化。PromptBridge仅需少量对齐任务进行校准。它首先应用模型自适应反射提示进化（MAP-RPE），通过迭代反射精化和定量评估获得任务和模型特定的最优提示。利用源模型和目标模型得到的校准提示对，PromptBridge学习跨模型提示映射。在测试时，即对于未见任务，给定源模型提示，该映射直接为目标模型生成优化提示。在单智能体和多智能体设置中的实验表明，PromptBridge持续提升下游准确率，同时减少迁移工作量。代码即将发布。

</details>


### [226] [Multilingual Conversational AI for Financial Assistance: Bridging Language Barriers in Indian FinTech](https://arxiv.org/abs/2512.01439)
*Bharatdeep Hazarika,Arya Suneesh,Prasanna Devadiga,Pawan Kumar Rajpoot,Anshuman B Suresh,Ahmed Ifthaquar Hussain*

Main category: cs.CL

TL;DR: 本文介绍了一个多语言对话AI系统，用于解决印度金融科技中的语言障碍问题。该系统支持混合语言（如Hinglish），采用多智能体架构，显著提升了用户参与度，同时保持低延迟开销。


<details>
  <summary>Details</summary>
Motivation: 印度的语言多样性（31种主要语言和100多种次要语言）对金融科技平台既是机遇也是挑战。只有10%的人口懂英语，这造成了金融包容性的障碍。需要开发能够支持混合语言（如Hinglish）的自然交互系统，以服务印度多元化的用户群体。

Method: 采用多智能体架构，包括语言分类、功能管理和多语言响应生成模块。通过比较多种语言模型并进行实际部署，系统支持代码混合语言，实现自然交互。

Result: 通过实际部署和比较分析，系统在用户参与度方面取得了显著改善，同时保持了较低延迟开销（4-8%）。这证明了多语言对话AI在金融援助场景中的有效性。

Conclusion: 这项工作为新兴市场的数字金融服务弥合了语言鸿沟，通过支持混合语言的多语言对话AI系统，提高了金融包容性，为印度多元化用户群体提供了更好的金融援助体验。

Abstract: India's linguistic diversity presents both opportunities and challenges for fintech platforms. While the country has 31 major languages and over 100 minor ones, only 10\% of the population understands English, creating barriers to financial inclusion. We present a multilingual conversational AI system for a financial assistance use case that supports code-mixed languages like Hinglish, enabling natural interactions for India's diverse user base. Our system employs a multi-agent architecture with language classification, function management, and multilingual response generation. Through comparative analysis of multiple language models and real-world deployment, we demonstrate significant improvements in user engagement while maintaining low latency overhead (4-8\%). This work contributes to bridging the language gap in digital financial services for emerging markets.

中文标题: 面向金融援助的多语言对话AI：弥合印度金融科技中的语言障碍

中文摘要: 印度的语言多样性为金融科技平台带来了机遇和挑战。虽然印度有31种主要语言和100多种次要语言，但只有10%的人口懂英语，这造成了金融包容性的障碍。我们提出了一个用于金融援助场景的多语言对话AI系统，支持像Hinglish这样的代码混合语言，为印度多元化用户群体实现自然交互。我们的系统采用多智能体架构，包括语言分类、功能管理和多语言响应生成。通过对多种语言模型的比较分析和实际部署，我们展示了在保持低延迟开销（4-8%）的同时，用户参与度的显著改善。这项工作有助于弥合新兴市场数字金融服务中的语言鸿沟。

</details>


### [227] [Enhancing BERT Fine-Tuning for Sentiment Analysis in Lower-Resourced Languages](https://arxiv.org/abs/2512.01460)
*Jozef Kubík,Marek Šuppa,Martin Takáč*

Main category: cs.CL

TL;DR: 该论文提出了一种结合主动学习、数据聚类和动态调度器的集成微调管道，用于提升低资源语言情感分析中BERT模型的微调效果，在斯洛伐克语、马耳他语、冰岛语和土耳其语上实现了最高30%的标注节省和4个F1分数的性能提升。


<details>
  <summary>Details</summary>
Motivation: 低资源语言通常数据有限，导致语言模型性能较弱。由于预训练计算成本高，更实用的方法是在微调阶段进行改进。研究者旨在探索如何通过主动学习和结构化数据选择策略来提升有限训练数据下的微调效果。

Method: 提出集成微调管道，将主动学习方法与数据聚类相结合，并引入"主动学习调度器"（结构化数据选择策略）。该管道系统性地结合了主动学习、聚类和动态数据选择调度器，以增强模型性能。

Result: 在斯洛伐克语、马耳他语、冰岛语和土耳其语的实验中，使用聚类结合主动学习调度器的方法能够同时实现：1）最高30%的标注节省；2）最高4个F1分数的性能提升；3）更好的微调稳定性。

Conclusion: 在微调阶段结合聚类和主动学习调度器是提升低资源语言情感分析BERT模型性能的有效方法，既能减少标注成本，又能提高模型性能，为低资源语言NLP任务提供了实用的解决方案。

Abstract: Limited data for low-resource languages typically yield weaker language models (LMs). Since pre-training is compute-intensive, it is more pragmatic to target improvements during fine-tuning. In this work, we examine the use of Active Learning (AL) methods augmented by structured data selection strategies which we term 'Active Learning schedulers', to boost the fine-tuning process with a limited amount of training data. We connect the AL to data clustering and propose an integrated fine-tuning pipeline that systematically combines AL, clustering, and dynamic data selection schedulers to enhance model's performance. Experiments in the Slovak, Maltese, Icelandic and Turkish languages show that the use of clustering during the fine-tuning phase together with AL scheduling can simultaneously produce annotation savings up to 30% and performance improvements up to four F1 score points, while also providing better fine-tuning stability.

中文标题: 增强低资源语言情感分析中BERT微调的方法

中文摘要: 低资源语言的数据有限通常导致语言模型性能较弱。由于预训练计算密集，更实用的目标是改进微调过程。本研究探讨了使用主动学习方法，辅以我们称之为"主动学习调度器"的结构化数据选择策略，以在有限训练数据下提升微调过程。我们将主动学习与数据聚类联系起来，提出了一个集成的微调管道，系统性地结合主动学习、聚类和动态数据选择调度器来增强模型性能。在斯洛伐克语、马耳他语、冰岛语和土耳其语的实验表明，在微调阶段使用聚类结合主动学习调度器可以同时实现：最高30%的标注节省和最高4个F1分数的性能提升，同时提供更好的微调稳定性。

</details>


### [228] [MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages](https://arxiv.org/abs/2512.01512)
*Yexing Du,Kaiyuan Liu,Youcheng Pan,Bo Yang,Keqi Deng,Xie Chen,Yang Xiang,Ming Liu,Bin Qin,YaoWei Wang*

Main category: cs.CL

TL;DR: MCAT是一个多语言成本效益加速语音到文本翻译框架，通过课程学习和数据平衡策略将MLLM支持的语言扩展到70种，并设计优化的语音适配器将语音序列长度减少到仅30个标记，在保持高性能的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在语音到文本翻译任务中存在两个关键挑战：语言覆盖范围有限（大多数数据集以英语为中心）和效率低下（语音转换为长序列时推理速度急剧下降）。

Method: 提出MCAT框架，包含两个创新：1）语言扩展方法，利用课程学习和数据平衡策略将MLLM支持的语言扩展到70种并实现相互翻译；2）优化的语音适配器模块，将语音序列长度减少到仅30个标记。

Result: 在FLEURS数据集的70x69个翻译方向上超越了最先进的端到端模型，同时提升了批量推理效率，仅使用约1亿可训练参数和每种语言仅10小时的S2TT数据。

Conclusion: MCAT框架成功解决了MLLM在语音到文本翻译中的语言覆盖和效率问题，实现了70种语言之间的高质量相互翻译，并作为开源项目发布以促进MLLM在S2TT能力方面的发展。

Abstract: Multimodal Large Language Models (MLLMs) have achieved great success in Speech-to-Text Translation (S2TT) tasks. However, current research is constrained by two key challenges: language coverage and efficiency. Most of the popular S2TT datasets are substantially English-centric, which restricts the scaling-up of MLLMs' many-to-many translation capabilities. Moreover, the inference speed of MLLMs degrades dramatically when the speech is converted into long sequences (e.g., 750 tokens). To address these limitations, we propose a Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT) framework, which includes two innovations. First, a language scaling method that leverages curriculum learning and a data balancing strategy is introduced to extend the language coverage supported by MLLMs to 70 languages and achieve mutual translation among these languages. Second, an optimized speech adapter module is designed to reduce the length of the speech sequence to only 30 tokens. Extensive experiments were conducted on MLLMs of different scales (9B and 27B). The experimental results demonstrate that MCAT not only surpasses state-of-the-art end-to-end models on the FLEURS dataset across 70x69 directions but also enhances batch inference efficiency. This is achieved with only ~100M trainable parameters and by using only 10 hours of S2TT data per language. Furthermore, we have released MCAT as open-source to promote the development of MLLMs for robust S2TT capabilities. The code and models are released at https://github.com/yxduir/m2m-70.

中文标题: MCAT：使用MLLM将多对多语音到文本翻译扩展到70种语言

中文摘要: 多模态大语言模型在语音到文本翻译任务中取得了巨大成功。然而，当前研究受到两个关键挑战的限制：语言覆盖范围和效率。大多数流行的S2TT数据集基本上以英语为中心，这限制了MLLM多对多翻译能力的扩展。此外，当语音转换为长序列（例如750个标记）时，MLLM的推理速度会急剧下降。为了解决这些限制，我们提出了一个多语言成本效益加速语音到文本翻译器框架，包括两个创新。首先，引入了一种语言扩展方法，利用课程学习和数据平衡策略将MLLM支持的语言覆盖范围扩展到70种语言，并实现这些语言之间的相互翻译。其次，设计了一个优化的语音适配器模块，将语音序列长度减少到仅30个标记。对不同规模（9B和27B）的MLLM进行了大量实验。实验结果表明，MCAT不仅在FLEURS数据集的70x69个方向上超越了最先进的端到端模型，还提升了批量推理效率。这仅使用约1亿可训练参数和每种语言仅10小时的S2TT数据即可实现。此外，我们已将MCAT作为开源项目发布，以促进MLLM在强大S2TT能力方面的发展。

</details>


### [229] [Language Diversity: Evaluating Language Usage and AI Performance on African Languages in Digital Spaces](https://arxiv.org/abs/2512.01557)
*Edward Ajayi,Eudoxie Umwari,Mawuli Deku,Prosper Singadi,Jules Udahemuka,Bekalu Tadele,Chukuemeka Edeh*

Main category: cs.CL

TL;DR: 研究评估了非洲语言在数字空间的代表性，发现专业新闻内容比对话平台数据更适合训练AI语言模型


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估非洲语言在数字空间的代表性，以及当前语言检测工具在处理这些语言时面临的挑战。非洲语言如约鲁巴语、基尼亚卢旺达语和阿姆哈拉语虽然使用者众多，但在线对话数据稀缺且常受英语影响，这给语言模型训练带来困难。

Method: 研究方法包括从Reddit子论坛和当地新闻来源收集约鲁巴语、基尼亚卢旺达语和阿姆哈拉语的数据。分析对比了这两种数据源的特点，并测试了专门的语言检测模型AfroLID和通用大语言模型在不同类型数据上的表现。

Result: 研究结果显示：Reddit数据稀少且存在大量语码转换现象；当地新闻媒体则提供了干净、单语的优质语言数据，并在社交媒体上引发了更多本地语言互动。语言检测模型在干净的新闻数据上表现接近完美，但在语码转换的Reddit帖子上表现不佳。

Conclusion: 研究结论是专业策划的新闻内容比对话平台数据更适合作为训练非洲语言AI模型的可靠来源。同时指出未来需要开发能够同时处理干净文本和语码转换文本的模型，以提高非洲语言检测的准确性。

Abstract: This study examines the digital representation of African languages and the challenges this presents for current language detection tools. We evaluate their performance on Yoruba, Kinyarwanda, and Amharic. While these languages are spoken by millions, their online usage on conversational platforms is often sparse, heavily influenced by English, and not representative of the authentic, monolingual conversations prevalent among native speakers. This lack of readily available authentic data online creates a challenge of scarcity of conversational data for training language models. To investigate this, data was collected from subreddits and local news sources for each language. The analysis showed a stark contrast between the two sources. Reddit data was minimal and characterized by heavy code-switching. Conversely, local news media offered a robust source of clean, monolingual language data, which also prompted more user engagement in the local language on the news publishers social media pages. Language detection models, including the specialized AfroLID and a general LLM, performed with near-perfect accuracy on the clean news data but struggled with the code-switched Reddit posts. The study concludes that professionally curated news content is a more reliable and effective source for training context-rich AI models for African languages than data from conversational platforms. It also highlights the need for future models that can process clean and code-switched text to improve the detection accuracy for African languages.

中文标题: 语言多样性：评估非洲语言在数字空间的使用情况和AI表现

中文摘要: 本研究考察了非洲语言的数字代表性及其对当前语言检测工具带来的挑战。我们评估了这些工具在约鲁巴语、基尼亚卢旺达语和阿姆哈拉语上的表现。尽管这些语言有数百万使用者，但它们在对话平台上的在线使用往往稀少，深受英语影响，并不能代表母语者之间普遍存在的真实单语对话。这种在线真实数据的缺乏为训练语言模型带来了对话数据稀缺的挑战。为研究此问题，我们从每个语言的Reddit子论坛和当地新闻来源收集了数据。分析显示两种来源之间存在鲜明对比：Reddit数据稀少且以大量语码转换为特征；相反，当地新闻媒体提供了干净单语语言数据的可靠来源，并在新闻发布者的社交媒体页面上引发了更多本地语言的用户互动。包括专门的AfroLID和通用大语言模型在内的语言检测模型在干净的新闻数据上表现接近完美，但在语码转换的Reddit帖子上表现不佳。研究结论是，专业策划的新闻内容比对话平台数据更适合作为训练非洲语言上下文丰富AI模型的可靠有效来源。同时强调了未来需要能够处理干净文本和语码转换文本的模型，以提高非洲语言的检测准确性。

</details>


### [230] [MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark](https://arxiv.org/abs/2512.01603)
*Yuezhang Peng,Chonghao Cai,Ziang Liu,Shuai Fan,Sheng Jiang,Hua Xu,Yuxin Liu,Qiguang Chen,Kele Xu,Yao Li,Sheng Wang,Libo Qin,Xie Chen*

Main category: cs.CL

TL;DR: MAC-SLU是一个新的多意图车载座舱口语理解基准数据集，增加了SLU任务的复杂性。研究对LLMs和LALMs进行了全面评估，发现SFT方法优于上下文学习，而端到端LALMs与流水线方法性能相当且能避免语音识别错误传播。


<details>
  <summary>Details</summary>
Motivation: 现有SLU数据集缺乏多样性和复杂性，且缺少针对最新LLMs和LALMs的统一基准。车载座舱环境中的多意图理解具有实际应用价值，需要更真实复杂的数据集来推动SLU技术发展。

Method: 1. 创建MAC-SLU数据集，包含真实复杂的多意图车载座舱对话数据；2. 对领先的开源LLMs和LALMs进行基准测试；3. 采用多种方法：上下文学习、监督微调（SFT）、端到端（E2E）和流水线范式；4. 比较不同方法的性能差异。

Result: 1. LLMs和LALMs通过上下文学习能完成SLU任务，但性能显著低于监督微调方法；2. 端到端LALMs与流水线方法性能相当；3. 端到端方法有效避免了语音识别错误传播问题；4. 为SLU研究提供了新的基准数据集和评估框架。

Conclusion: MAC-SLU数据集填补了复杂多意图SLU基准的空白。虽然LLMs和LALMs在上下文学习中显示出潜力，但监督微调仍然是更有效的方法。端到端LALMs在避免错误传播方面具有优势，为车载座舱SLU系统的实际部署提供了有价值的参考。

Abstract: Spoken Language Understanding (SLU), which aims to extract user semantics to execute downstream tasks, is a crucial component of task-oriented dialog systems. Existing SLU datasets generally lack sufficient diversity and complexity, and there is an absence of a unified benchmark for the latest Large Language Models (LLMs) and Large Audio Language Models (LALMs). This work introduces MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset, which increases the difficulty of the SLU task by incorporating authentic and complex multi-intent data. Based on MAC-SLU, we conducted a comprehensive benchmark of leading open-source LLMs and LALMs, covering methods like in-context learning, supervised fine-tuning (SFT), and end-to-end (E2E) and pipeline paradigms. Our experiments show that while LLMs and LALMs have the potential to complete SLU tasks through in-context learning, their performance still lags significantly behind SFT. Meanwhile, E2E LALMs demonstrate performance comparable to pipeline approaches and effectively avoid error propagation from speech recognition. Code\footnote{https://github.com/Gatsby-web/MAC\_SLU} and datasets\footnote{huggingface.co/datasets/Gatsby1984/MAC\_SLU} are released publicly.

中文标题: MAC-SLU：多意图车载座舱口语理解基准

中文摘要: 口语理解（SLU）旨在提取用户语义以执行下游任务，是任务导向对话系统的关键组成部分。现有的SLU数据集通常缺乏足够的多样性和复杂性，并且缺乏针对最新大型语言模型（LLMs）和大型音频语言模型（LALMs）的统一基准。本文介绍了MAC-SLU，一个新颖的多意图车载座舱口语理解数据集，通过引入真实且复杂的多意图数据增加了SLU任务的难度。基于MAC-SLU，我们对领先的开源LLMs和LALMs进行了全面基准测试，涵盖了上下文学习、监督微调（SFT）、端到端（E2E）和流水线范式等方法。我们的实验表明，虽然LLMs和LALMs通过上下文学习有潜力完成SLU任务，但它们的性能仍显著落后于SFT。同时，E2E LALMs表现出与流水线方法相当的性能，并有效避免了语音识别带来的错误传播。代码和数据集已公开发布。

</details>


### [231] [Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems](https://arxiv.org/abs/2512.01661)
*Dengyun Peng,Qiguang Chen,Bofei Liu,Jiannan Guan,Libo Qin,Zheng Yan,Jinhao Liu,Jianshu Zhang,Wanxiang Che*

Main category: cs.CL

TL;DR: 该研究提出了UnsolvableQA数据集和UnsolvableRL框架，用于训练LLM识别不可解问题，区分客观不可解性与主观能力限制，防止模型产生幻觉和过度自信。


<details>
  <summary>Details</summary>
Motivation: 确保LLM可靠性不仅需要解决复杂问题，还需要识别何时问题是不可解的。当前模型难以区分客观不可解性（问题内在矛盾）和主观能力限制（超出模型能力范围的问题），这导致幻觉和过度自信。

Method: 1. 构建UnsolvableQA数据集：通过双轨方法创建可解和不可解实例对，包括逻辑谜题的程序化生成和数学问题的"反向构造"方法（在有效推理链中注入矛盾）。2. 提出UnsolvableRL框架：使用包含准确性、不可解性和难度三个奖励组件的强化学习框架。

Result: 实验结果显示，该方法实现了近乎完美的不可解性检测，同时提高了可解任务的准确性。关键发现是"能力崩溃"现象，表明明确暴露于不可解数据对于防止模型变得系统性过度自信是必不可少的。

Conclusion: 该研究通过UnsolvableQA数据集和UnsolvableRL框架，成功解决了LLM识别不可解问题的问题，提高了模型的可靠性和自我认知能力，为构建更安全的AI系统提供了重要方法。

Abstract: Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel "Reverse Construction" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.

中文标题: 学习可解性边界：对齐LLM以检测不可解问题

中文摘要: 确保LLM的可靠性不仅需要解决复杂问题，还需要识别何时问题是不可解的。当前模型往往难以区分客观不可解性（问题内在矛盾）和主观能力限制（超出模型能力的问题），这导致幻觉和过度自信。为解决这一问题，我们提出了UnsolvableQA和UnsolvableRL，用于解决可行问题、检测内在矛盾，并谨慎拒绝超出能力范围的任务。具体而言，我们构建了UnsolvableQA，这是一个通过双轨方法衍生的可解和不可解实例配对数据集：逻辑谜题的程序化生成和一种新颖的"反向构造"方法，将矛盾注入数学问题的有效推理链中。基于此数据集，我们引入了UnsolvableRL，这是一个包含三个奖励组件的强化学习框架，共同考虑准确性、不可解性和难度。实证结果表明，我们的方法实现了近乎完美的不可解性检测，同时提高了可解任务的准确性。关键的是，我们发现了"能力崩溃"现象，表明明确暴露于不可解数据对于防止模型变得系统性过度自信是必不可少的。我们的代码和数据可在https://github.com/sfasfaffa/unsolvableQA获取。

</details>


### [232] [Self-Supervised Borrowing Detection on Multilingual Wordlists](https://arxiv.org/abs/2512.01713)
*Tim Wientzek*

Main category: cs.CL

TL;DR: 提出了一种完全自监督的多语言词汇表借词检测方法，结合PMI相似度和轻量级对比学习组件，无需标注数据即可自动选择决策阈值，性能优于现有字符串相似度方法，与监督基线相当或更好。


<details>
  <summary>Details</summary>
Motivation: 在多语言词汇表中检测借词通常需要大量标注数据或依赖字符串相似度方法，这些方法效果有限。现有方法要么需要监督学习，要么基于简单的字符串比较，无法充分利用语言间的系统对应关系。

Method: 方法结合两种信息源：基于全局对应模型的PMI相似度和基于语音特征向量的轻量级对比学习组件。还包括无需标注数据的自动阈值选择程序。使用字符编码、温度设置和数据增强策略优化模型。

Result: 在基准数据集上的实验表明：1) PMI单独使用已优于NED和SCA等现有字符串相似度方法；2) 组合相似度方法性能与监督基线相当或更好；3) 消融研究显示字符编码、温度设置和增强策略的重要性；4) 方法可扩展到不同规模的数据集。

Conclusion: 提出了一种完全自监督的借词检测方法，无需人工监督即可在多语言词汇表中有效识别借词关系。方法性能优越，提供了命令行工具供研究者使用，具有良好的可扩展性和实用性。

Abstract: This paper presents a fully self-supervised approach to borrowing detection in multilingual wordlists. The method combines two sources of information: PMI similarities based on a global correspondence model and a lightweight contrastive component trained on phonetic feature vectors. It further includes an automatic procedure for selecting decision thresholds without requiring labeled data. Experiments on benchmark datasets show that PMI alone already improves over existing string similarity measures such as NED and SCA, and that the combined similarity performs on par with or better than supervised baselines. An ablation study highlights the importance of character encoding, temperature settings and augmentation strategies. The approach scales to datasets of different sizes, works without manual supervision and is provided with a command-line tool that allows researchers to conduct their own studies.

中文标题: 多语言词汇表上的自监督借词检测

中文摘要: 本文提出了一种完全自监督的多语言词汇表借词检测方法。该方法结合了两种信息源：基于全局对应模型的PMI相似度和基于语音特征向量的轻量级对比学习组件。进一步包含无需标注数据的自动决策阈值选择程序。在基准数据集上的实验表明，仅使用PMI已经优于NED和SCA等现有字符串相似度方法，而组合相似度方法的性能与监督基线相当或更好。消融研究突出了字符编码、温度设置和增强策略的重要性。该方法可扩展到不同规模的数据集，无需人工监督，并提供了命令行工具供研究人员进行自己的研究。

</details>


### [233] [Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks](https://arxiv.org/abs/2512.01725)
*Jiannan Guan,Qiguang Chen,Libo Qin,Dengyun Peng,Jinhao Liu,Liangyu Huo,Jian Xie,Wanxiang Che*

Main category: cs.CL

TL;DR: LLMs在需要单一正确答案的推理任务上表现出色，但在需要生成全面多样答案的多解任务上表现不佳，这归因于"推理过度自信"现象。研究引入MuSoBench基准测试，发现短链思维提示存在明显过度自信，而长链思维方法通过迭代探索和自我反思缓解了这一问题。研究提出"认知刚性假说"解释这一现象，并通过注意力熵分析提供初步支持。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要单一正确答案的推理任务上表现出色，但在需要生成全面多样答案的多解任务上表现不佳。研究者观察到LLMs存在"推理过度自信"问题，即对不完整的解决方案集表现出不当的确定性。为了深入探究这一现象，研究旨在理解LLMs在多解任务中的局限性及其根本原因。

Method: 研究引入了MuSoBench基准测试，包含多解问题。通过实验比较传统短链思维提示和新兴长链思维方法的表现。使用注意力熵分析来探究底层认知机制，并提出"认知刚性假说"来解释过度自信现象，即推理过程过早收敛于狭窄的思维路径。

Result: 实验表明，传统短链思维提示范式表现出明显的过度自信，而长链思维方法通过迭代探索和自我反思有效缓解了这一问题。注意力熵分析为认知刚性假说提供了初步支持，显示过度自信与思维路径的过早收敛相关。

Conclusion: 研究揭示了LLMs在多解任务中的推理过度自信问题，并提供了评估LLM推理完整性的工具。研究强调需要超越单一答案准确性评估，转向全面探索能力的评估，这对LLMs在复杂现实世界问题中的应用具有重要意义。

Abstract: Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.

中文标题: 警惕推理过度自信：多解任务推理过程中的陷阱

中文摘要: 大型语言模型在需要单一正确答案的推理任务上表现出色，但在需要生成全面多样答案的多解任务上表现不佳。我们将这一局限性归因于"推理过度自信"：倾向于对不完整的解决方案集表达不当的确定性。为了检验这一效应，我们引入了MuSoBench，一个多解问题的基准测试。实验表明，传统的短链思维提示范式表现出明显的过度自信，而新兴的长链思维方法通过迭代探索和自我反思缓解了这一问题。我们进一步描述了可观察的行为和影响因素。为了探究根本原因，我们提出了"认知刚性假说"，该假说认为当推理过程过早收敛于狭窄的思维路径时，就会产生过度自信。注意力熵分析为这一观点提供了初步支持。这些发现为评估LLM推理的完整性提供了工具，并强调了需要将评估从单一答案准确性转向全面探索。

</details>


### [234] [Reasoning About the Unsaid: Misinformation Detection with Omission-Aware Graph Inference](https://arxiv.org/abs/2512.01728)
*Zhengjia Wang,Danding Wang,Qiang Sheng,Jiaying Wu,Juan Cao*

Main category: cs.CL

TL;DR: OmiGraph是首个针对信息缺失型虚假信息的检测框架，通过构建缺失感知图来识别被省略的重要信息，从而检测那些通过信息缺失而非直接捏造来误导读者的虚假信息。


<details>
  <summary>Details</summary>
Motivation: 虚假信息不仅通过直接捏造内容来欺骗读者，还经常通过省略重要信息来误导读者，让读者在信息不完整的情况下得出错误结论。目前的研究主要集中在直接捏造型虚假信息，而信息缺失型欺骗在很大程度上被忽视了。

Method: OmiGraph框架包含三个主要部分：1）构建缺失感知图，利用上下文环境捕捉同一事件的不同视角，揭示可能被省略的内容；2）缺失导向的关系建模，识别内部上下文依赖关系和动态缺失意图；3）缺失感知的消息传递和聚合，整合缺失内容和关系来提取缺失模式用于检测。

Result: 在两个大规模基准测试上，OmiGraph取得了显著性能提升，平均F1分数提高5.4%，准确率提高5.3%，证明了考虑信息缺失视角对虚假信息检测的重要性。

Conclusion: 信息缺失是虚假信息传播的重要形式，OmiGraph通过构建缺失感知图和相关建模方法，有效检测了这种隐性的欺骗方式，为虚假信息检测开辟了新的研究方向。

Abstract: This paper investigates the detection of misinformation, which deceives readers by explicitly fabricating misleading content or implicitly omitting important information necessary for informed judgment. While the former has been extensively studied, omission-based deception remains largely overlooked, even though it can subtly guide readers toward false conclusions under the illusion of completeness. To pioneer in this direction, this paper presents OmiGraph, the first omission-aware framework for misinformation detection. Specifically, OmiGraph constructs an omission-aware graph for the target news by utilizing a contextual environment that captures complementary perspectives of the same event, thereby surfacing potentially omitted contents. Based on this graph, omission-oriented relation modeling is then proposed to identify the internal contextual dependencies, as well as the dynamic omission intents, formulating a comprehensive omission relation representation. Finally, to extract omission patterns for detection, OmiGraph introduces omission-aware message-passing and aggregation that establishes holistic deception perception by integrating the omission contents and relations. Experiments show that, by considering the omission perspective, our approach attains remarkable performance, achieving average improvements of +5.4% F1 and +5.3% ACC on two large-scale benchmarks.

中文标题: 推理未言之物：基于缺失感知图推理的虚假信息检测

中文摘要: 本文研究了虚假信息的检测问题，这些信息通过明确捏造误导性内容或隐性地省略做出明智判断所需的重要信息来欺骗读者。虽然前者已被广泛研究，但基于信息缺失的欺骗在很大程度上仍被忽视，尽管它可以在完整性的假象下微妙地引导读者得出错误结论。为了在这一方向上开创先河，本文提出了OmiGraph，这是首个用于虚假信息检测的缺失感知框架。具体而言，OmiGraph通过利用捕捉同一事件互补视角的上下文环境为目标新闻构建缺失感知图，从而揭示可能被省略的内容。基于此图，提出了缺失导向的关系建模，以识别内部上下文依赖关系以及动态缺失意图，形成全面的缺失关系表示。最后，为了提取用于检测的缺失模式，OmiGraph引入了缺失感知的消息传递和聚合，通过整合缺失内容和关系建立全面的欺骗感知。实验表明，通过考虑信息缺失视角，我们的方法取得了显著性能，在两个大规模基准测试上实现了平均F1分数提升5.4%和准确率提升5.3%。

</details>


### [235] [Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability](https://arxiv.org/abs/2512.01848)
*Jinghan Jia,Nathalie Baracaldo,Sijia Liu*

Main category: cs.CL

TL;DR: 该论文发现仅靠监督微调（SFT）无法有效解决大型推理模型的安全对齐问题，提出使用强化学习（RL）作为补充优化框架，能在提升安全性的同时保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）通过生成显式的思维链（CoT）推理显著提升了数学和逻辑问题解决能力，但这也引入了新的安全风险——不安全行为可能出现在中间推理轨迹中，即使最终答案看起来无害。现有的安全对齐方法主要依赖基于安全导向的长思维链数据集的监督微调，但这种方法存在安全改进不一致、推理能力下降以及跨模型家族泛化能力差等问题。

Method: 研究者提出使用强化学习（RL）作为LRM安全训练的补充优化框架。与SFT不同，RL通过奖励反馈直接优化模型策略，实现更自适应和稳定的对齐。通过在多模型家族和基准测试上进行广泛实验，分析反思动态和令牌级熵，研究RL如何抑制不安全探索性推理同时保持反思深度。

Result: 实验结果表明，RL方法实现了更强且更一致的安全增益，同时保持了推理能力。对反思动态和令牌级熵的分析显示，RL能够抑制不安全的探索性推理，同时保留反思深度，从而产生更安全和可靠的推理过程。

Conclusion: 纯监督方法对于LRMs的鲁棒安全对齐是不够的，强化学习提供了一个有效的补充优化框架，能够在提升安全性的同时保持模型的推理能力，实现更安全和可靠的推理过程。

Abstract: Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes.

中文标题: 超越SFT：强化学习实现更安全、推理能力更强的大型推理模型

中文摘要: 大型推理模型（LRMs）通过生成显式的思维链（CoT）推理扩展了大型语言模型，显著提升了数学和逻辑问题解决能力。然而，这种显式推理过程也引入了新的安全风险，因为不安全行为经常出现在中间推理轨迹中，即使最终答案看起来无害。现有的安全对齐方法主要依赖基于安全导向的长思维链数据集的监督微调（SFT）。虽然直观，但我们发现SFT产生不一致的安全改进，降低推理能力，并且在跨模型家族间泛化能力差。这些限制表明纯监督方法对于LRMs的鲁棒安全对齐是不够的。为了解决这个问题，我们研究将强化学习（RL）作为LRM安全训练的补充优化框架。与SFT不同，RL通过奖励反馈直接优化模型策略，实现更自适应和稳定的对齐。跨多个模型家族和基准测试的广泛实验表明，RL实现了更强且更一致的安全增益，同时保持了推理能力。对反思动态和令牌级熵的进一步分析显示，RL抑制不安全的探索性推理，同时保留反思深度，从而产生更安全和可靠的推理过程。

</details>


### [236] [Cross-Lingual Interleaving for Speech Language Models](https://arxiv.org/abs/2512.01865)
*Adel Moumen,Guangzhi Sun,Philip C. Woodland*

Main category: cs.CL

TL;DR: 该论文提出了一种跨语言交错方法，通过混合不同语言的语音标记来训练多语言语音语言模型，无需文本监督。作者还发布了EN-FR训练数据集和评估基准，实验表明该方法能提升单语言语义准确性、实现跨语言延续并增强跨语言隐藏状态对齐。


<details>
  <summary>Details</summary>
Motivation: 当前语音语言模型（SLMs）的发展主要集中于英语，因为缺乏多语言的语音评估基准和训练数据，导致跨语言学习困难。这限制了自然语言处理技术对缺乏书面资源的语言的普及。

Method: 提出跨语言交错方法，在无文本监督的情况下混合不同语言的语音标记进行训练。同时创建了EN-FR训练数据集TinyStories（约42k小时）以及通过GPT-4生成的EN-FR语音StoryCloze和TopicCloze评估基准。

Result: 在360M和1B参数的SLMs上，在相同训练标记预算下，交错方法提高了单语言语义准确性，实现了稳健的跨语言延续，并增强了跨语言隐藏状态对齐。这表明该方法能有效构建理解和跨语言对话的多语言SLMs。

Conclusion: 跨语言交错是一种简单、可扩展的构建多语言语音语言模型的途径，能够理解和跨语言对话。所有资源将开源以支持可重复性研究。

Abstract: Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also release an EN-FR training dataset, TinyStories (~42k hours), together with EN-FR spoken StoryCloze and TopicCloze benchmarks for cross-lingual semantic evaluation, both synthetically generated using GPT-4. On 360M and 1B SLMs under matched training-token budgets, interleaving improves monolingual semantic accuracy, enables robust cross-lingual continuation, and strengthens cross-lingual hidden-state alignment. Taken together, these results indicate that cross-lingual interleaving is a simple, scalable route to building multilingual SLMs that understand and converse across languages. All resources will be made open-source to support reproducibility.

中文标题: 语音语言模型的跨语言交错方法

中文摘要: 语音语言模型（SLMs）旨在直接从语音中学习语言能力，使用离散单元，为书面资源有限的语言拓宽了自然语言处理（NLP）技术的访问途径。然而，由于缺乏语音评估基准和训练数据，进展主要集中在英语，使得跨语言学习变得困难。我们提出了一种跨语言交错方法，在无文本监督的情况下混合不同语言的语音标记。我们还发布了EN-FR训练数据集TinyStories（约42k小时），以及使用GPT-4生成的EN-FR语音StoryCloze和TopicCloze跨语言语义评估基准。在360M和1B参数的SLMs上，在匹配的训练标记预算下，交错方法提高了单语言语义准确性，实现了稳健的跨语言延续，并增强了跨语言隐藏状态对齐。综合来看，这些结果表明跨语言交错是一种简单、可扩展的构建多语言SLMs的途径，能够理解和跨语言对话。所有资源将开源以支持可重复性。

</details>


### [237] [The Art of Scaling Test-Time Compute for Large Language Models](https://arxiv.org/abs/2512.02008)
*Aradhye Agarwal,Ayan Sengupta,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文首次大规模研究了测试时计算扩展策略，比较了8个开源大语言模型在4个推理数据集上的表现，发现没有单一策略能普遍最优，推理模型在不同问题难度和轨迹长度上表现出不同模式，且最优性能随计算预算单调扩展。


<details>
  <summary>Details</summary>
Motivation: 测试时计算扩展（TTS）是提升大语言模型推理能力的有前景方向，但目前缺乏在相同条件下对已知TTS策略的系统比较，且模型类型和问题难度对性能的影响尚不清楚。

Method: 进行了首次大规模TTS研究，使用8个开源LLM（7B到235B参数）在4个推理数据集上生成了超过300亿个token，系统比较了不同TTS策略在相同条件下的表现。

Result: 观察到三个一致趋势：1）没有单一TTS策略能普遍最优；2）推理模型在不同问题难度和轨迹长度上表现出不同的轨迹质量模式，形成短视界和长视界两类；3）对于给定模型类型，最优TTS性能随计算预算单调扩展。

Conclusion: 基于这些发现，提供了一个实用的TTS策略选择方案，综合考虑问题难度、模型类型和计算预算，为有效的推理时扩展提供了实用指南。

Abstract: Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.

中文标题: 大语言模型测试时计算扩展的艺术

中文摘要: 测试时扩展（TTS）——在推理过程中动态分配计算资源——是提升大语言模型（LLM）推理能力的一个有前景方向。然而，目前缺乏在相同条件下对知名TTS策略的系统比较，且模型类型和问题难度对性能的影响仍不清楚。为填补这些空白，我们进行了首次大规模TTS研究，涵盖了使用八个开源LLM（7B到235B参数）在四个推理数据集上生成的超过三百亿个token。我们观察到三个一致趋势：（1）没有单一TTS策略能普遍最优；（2）推理模型在不同问题难度和轨迹长度上表现出不同的轨迹质量模式，形成短视界和长视界两类；（3）对于给定模型类型，最优TTS性能随计算预算单调扩展。基于这些洞见，我们提供了一个实用的TTS策略选择方案，综合考虑问题难度、模型类型和计算预算，为有效的推理时扩展提供了实用指南。

</details>


### [238] [Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling](https://arxiv.org/abs/2512.02010)
*Jack Cook,Junxian Guo,Guangxuan Xiao,Yujun Lin,Song Han*

Main category: cs.CL

TL;DR: 4/6是一种改进NVFP4量化的方法，通过为每个数值块评估两个缩放因子，减少近最大值区域的量化误差，防止训练发散并提高推理性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模增大，低精度数值格式如NVFP4因速度和内存优势而流行。然而，使用NVFP4加速计算需要将所有矩阵乘法操作数（前向传播中的权重和激活，反向传播中的权重、激活和梯度）量化为NVFP4，这通常导致训练发散和推理性能下降。主要问题在于浮点格式如FP4在每块近最大值区域有最大量化误差，这是下游性能下降的主要原因。

Method: Four Over Six (4/6)是对NVFP4量化算法的改进，为每个数值块评估两个潜在的缩放因子。与整数格式不同，浮点格式如FP4在每块的近最大值区域有最大量化误差。研究发现，对于某些块，缩放到较小的FP4值可以使可表示值的分布更均匀，从而改善近最大值的表示。4/6可以在NVIDIA Blackwell GPU上高效实现，使其在训练LLMs时可行。

Result: 在transformer和混合模型架构的预训练实验中，4/6在多个案例中防止了训练发散，使训练损失显著接近BF16基准，优于当前最先进的NVFP4训练方法。4/6还能轻松集成到多种后训练量化方法中，普遍提高下游任务准确率。

Conclusion: 4/6方法有效解决了NVFP4量化中的关键问题，通过自适应块缩放改善了近最大值区域的表示，防止训练发散并提高性能。该方法在硬件上高效实现，为未来使用NVFP4训练和部署模型提供了有前景的方向。

Abstract: As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.

中文标题: 四分之六：通过自适应块缩放实现更准确的NVFP4量化

中文摘要: 随着大语言模型规模不断扩大，低精度数值格式如NVFP4因其速度和内存优势而日益流行。然而，为了使用NVFP4加速计算，所有矩阵乘法操作数——前向传播中的权重和激活，以及反向传播中的权重、激活和梯度——都必须量化为NVFP4，这通常导致训练发散和推理性能下降。本文介绍了Four Over Six (4/6)，这是对NVFP4量化算法的一种改进，为每个数值块评估两个潜在的缩放因子。与整数格式不同，浮点格式如FP4在每块的近最大值区域有最大量化误差，我们发现这是下游性能下降的主要原因。我们发现对于某些块，缩放到较小的FP4值可以使可表示值的分布更均匀，从而改善近最大值的表示。重要的是，4/6可以在NVIDIA Blackwell GPU上高效实现，使其在训练LLMs时可行。在transformer和混合模型架构的预训练实验中，我们发现4/6在多个案例中防止了训练发散，使训练损失显著接近BF16基准，优于当前最先进的NVFP4训练方法。我们还发现4/6可以轻松集成到多种后训练量化方法中，普遍提高下游任务准确率。我们希望这能启发未来在训练和部署模型中使用NVFP4的研究。

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [239] [Development and Benchmarking of a Blended Human-AI Qualitative Research Assistant](https://arxiv.org/abs/2512.00009)
*Joseph Matveyenko,James Liu,John David Parsons,Prateek Puri*

Main category: cs.HC

TL;DR: 本文开发并评估了Muse——一个混合人机协作的定性研究助手系统，通过基准测试发现AI与人类编码者在明确代码上达到Cohen's κ=0.71的一致性，同时能纠正人类偏见。


<details>
  <summary>Details</summary>
Motivation: 传统定性研究依赖人工编码，面临编码员疲劳和解释漂移的挑战，难以扩展到大规模复杂数据集。计算方法的增强常因无法复制人类分析的细微差别和上下文意识而受到质疑，但大语言模型为自动化部分定性分析同时保持严谨性提供了新机会。

Method: 开发了Muse——一个交互式AI驱动的定性研究系统，允许研究人员识别主题和标注数据集。通过基准测试评估系统性能，使用Cohen's κ系数衡量AI与人类编码者的一致性，并进行稳健的错误分析以识别失败模式。

Result: 对于明确指定的代码，Muse与人类编码者之间的评分者间信度达到Cohen's κ=0.71。错误分析识别了失败模式，为未来改进提供指导，并展示了系统纠正人类偏见的能力。

Conclusion: 大语言模型为增强定性研究提供了有前景的途径，Muse系统在保持研究质量的同时实现了部分自动化。基准测试和错误分析对于建立定性研究人员的信任至关重要，展示了AI辅助系统在纠正人类偏见方面的潜力。

Abstract: Qualitative research emphasizes constructing meaning through iterative engagement with textual data. Traditionally this human-driven process requires navigating coder fatigue and interpretative drift, thus posing challenges when scaling analysis to larger, more complex datasets. Computational approaches to augment qualitative research have been met with skepticism, partly due to their inability to replicate the nuance, context-awareness, and sophistication of human analysis. Large language models, however, present new opportunities to automate aspects of qualitative analysis while upholding rigor and research quality in important ways. To assess their benefits and limitations - and build trust among qualitative researchers - these approaches must be rigorously benchmarked against human-generated datasets. In this work, we benchmark Muse, an interactive, AI-powered qualitative research system that allows researchers to identify themes and annotate datasets, finding an inter-rater reliability between Muse and humans of Cohen's $κ$ = 0.71 for well-specified codes. We also conduct robust error analysis to identify failure mode, guide future improvements, and demonstrate the capacity to correct for human bias.

中文标题: 混合人机协作定性研究助手的开发与基准测试

中文摘要: 定性研究强调通过迭代参与文本数据来构建意义。传统上，这种人类驱动的过程需要应对编码员疲劳和解释漂移，因此在将分析扩展到更大、更复杂的数据集时面临挑战。增强定性研究的计算方法一直受到质疑，部分原因是它们无法复制人类分析的细微差别、上下文意识和复杂性。然而，大语言模型为自动化定性分析的某些方面提供了新机会，同时在重要方面保持了严谨性和研究质量。为了评估它们的优势和局限性——并在定性研究人员中建立信任——这些方法必须与人类生成的数据集进行严格的基准测试。在这项工作中，我们对Muse进行了基准测试，这是一个交互式的、AI驱动的定性研究系统，允许研究人员识别主题和标注数据集，发现对于明确指定的代码，Muse与人类之间的评分者间信度达到Cohen's κ=0.71。我们还进行了稳健的错误分析以识别失败模式，指导未来改进，并展示了纠正人类偏见的能力。

</details>


### [240] [Closing the Block-to-Text Gap: A Domain-Specific JavaScript Editor for Early Computational Thinking](https://arxiv.org/abs/2512.00012)
*Andrei Enea*

Main category: cs.HC

TL;DR: 为8-10岁儿童设计的JavaScript编辑器，帮助从积木编程过渡到文本编程，通过视觉艺术领域的简化DSL，结合真实JavaScript语法和即时视觉反馈，显著提升计算思维并减少语法错误。


<details>
  <summary>Details</summary>
Motivation: 解决儿童从积木编程（如Scratch）过渡到文本编程（如JavaScript）的困难，特别是语法复杂性和抽象概念理解问题，为K-12计算机科学教育提供有效的过渡工具。

Method: 开发基于网页的JavaScript编辑器，采用面向视觉艺术的简化领域特定语言（DSL），保留真实JavaScript语法但简化复杂性，提供即时视觉反馈，通过四周试点研究（N=15）评估效果。

Result: 计算思维技能显著提升（平均CTCI增益+10.9，p<0.001），语法错误减少70%，参与者从基本绘图函数进步到使用循环、条件和动画的复杂算法设计，系统促进创造力、自我修正和持续参与。

Conclusion: 通过结合建构主义原则和视觉优先的DSL，该研究提供了一个经过验证的教学框架，有效缓解K-12计算机科学教育中从积木到文本编程的过渡困难，为教育者提供了实用、可扩展的工具。

Abstract: This paper presents a web-based JavaScript editor designed to help children aged 8-10 transition from block-based to text-based programming. The system introduces a simplified domain-specific language (DSL) focused on visual art, combining authentic JavaScript syntax with immediate, creative visual feedback. A four-week pilot study (N = 15) demonstrated significant improvements in computational thinking skills (mean CTCI gain of +10.9, p < 0.001), along with a 70% reduction in syntax errors. Participants advanced from basic drawing functions to sophisticated algorithmic designs using loops, conditionals, and animations. By integrating constructionist principles with a visual-first DSL, this research contributes a validated pedagogical framework for easing the block-to-text transition in K-12 computer science education. The system encourages creativity, self-correction, and sustained engagement, offering educators a practical, scalable tool for introducing authentic coding to young learners.

中文标题: 弥合积木到文本的鸿沟：面向早期计算思维培养的领域特定JavaScript编辑器

中文摘要: 本文介绍了一个基于网页的JavaScript编辑器，旨在帮助8-10岁儿童从积木编程过渡到文本编程。该系统引入了专注于视觉艺术的简化领域特定语言（DSL），将真实的JavaScript语法与即时、创造性的视觉反馈相结合。一项为期四周的试点研究（N=15）显示，计算思维技能显著提升（平均CTCI增益+10.9，p<0.001），同时语法错误减少70%。参与者从基本绘图函数进步到使用循环、条件和动画的复杂算法设计。通过将建构主义原则与视觉优先的DSL相结合，本研究为缓解K-12计算机科学教育中从积木到文本的过渡提供了一个经过验证的教学框架。该系统鼓励创造力、自我修正和持续参与，为教育者提供了一个实用、可扩展的工具，用于向年轻学习者介绍真实的编程。

</details>


### [241] [Speculating on the Role of Media Architecture in Post-disaster Rebuilding and Recovery: Insights from Architects and Interaction Designers](https://arxiv.org/abs/2512.00537)
*Berk Goksenin Tan,Oguzhan Ozcan*

Main category: cs.HC

TL;DR: 该研究通过焦点小组探讨媒体建筑在灾后永久住房重建中的潜在作用，提出了三个关键维度：促进社区联系、推动多物种参与、调解遗产保护，并开发了设计方法和具体设计理念。


<details>
  <summary>Details</summary>
Motivation: 现有研究对媒体建筑在灾后重建中的角色缺乏实证探索，特别是在将灾害视为重塑设计过程的积极因素时，需要探索建筑和交互设计如何推测媒体建筑在灾后永久住房恢复中的作用。

Method: 在2023年土耳其地震后，研究人员在安塔基亚与建筑师和交互设计师进行了两次焦点小组讨论，基于受影响居民对永久住房的期望，采用共同推测、基于卡片的设计过程，连接居民见解和专家设计。

Result: 研究识别出媒体建筑支持灾后住房的三个关键维度：促进社区社会联系、推动多物种参与和集体努力、调解遗产保护和复兴；并开发了十个情境性推测设计理念。

Conclusion: 该研究为灾后永久住宅中的媒体建筑提供了三维分析框架，提出了连接居民和专家的设计方法，为灾后重建中的媒体建筑设计和研究提供了新的视角和实践指导。

Abstract: In post-disaster contexts, design is not only about rebuilding structures but also about reimagining how architecture can become a communicative medium that supports recovery, resilience, and collective memory. While recent studies have expanded the understanding of media architecture from aesthetic urban screens to participatory civic infrastructures, there remains limited empirical research on its potential role in post-disaster contexts. In particular, opportunities exist to explore how architecture and interaction design might speculate on media architecture's role in rebuilding and recovery efforts for post-disaster permanent housing, especially when conceptualizing disasters as active agents that reshape design processes. Following to Kahramanmaras earthquake on February 6, 2023, we conducted two focus groups with architects and interaction designers in the case of Antakya, Turkey, building on affected residents' expectations for post-earthquake permanent housing. Our analysis revealed three critical dimensions of how future media architecture may support post-disaster housing: (1) as a facilitator of individuals' social connections to their community, (2) as an enabler of multispecies participation and collective efforts, and (3) as a mediator of heritage preservation and revival. With novel perspectives, we contribute a three-dimension lens for media architecture in permanent homes; a co-speculative, card-based process bridging residents' insights and expert design; and ten situated speculative design ideas with implications for design of post-disaster permanent homes.

中文标题: 媒体建筑在灾后重建与恢复中的角色推测：来自建筑师和交互设计师的见解

中文摘要: 在灾后背景下，设计不仅关乎重建结构，还关乎重新构想建筑如何成为一种支持恢复、韧性和集体记忆的交流媒介。虽然最近的研究将媒体建筑的理解从美学城市屏幕扩展到参与式公民基础设施，但关于其在灾后背景下潜在作用的实证研究仍然有限。特别是，存在机会探索建筑和交互设计如何推测媒体建筑在灾后永久住房重建和恢复工作中的作用，尤其是当将灾害概念化为重塑设计过程的积极因素时。继2023年2月6日卡赫拉曼马拉什地震后，我们在土耳其安塔基亚与建筑师和交互设计师进行了两次焦点小组讨论，基于受影响居民对震后永久住房的期望。我们的分析揭示了未来媒体建筑可能支持灾后住房的三个关键维度：(1)作为个人与其社区社会联系的促进者，(2)作为多物种参与和集体努力的推动者，(3)作为遗产保护和复兴的调解者。通过新颖的视角，我们为永久住宅中的媒体建筑贡献了一个三维视角；一个连接居民见解和专家设计的共同推测、基于卡片的过程；以及十个具有情境性的推测性设计理念，对灾后永久住宅的设计具有启示意义。

</details>


### [242] [The Professional Challenges of Industrial Designer in Industry 4.0](https://arxiv.org/abs/2512.00279)
*Meng Li,Yu Zhang,Leshan Li*

Main category: cs.HC

TL;DR: 工业4.0时代工业设计师面临角色重新定义，需要新的素质能力，本文通过中国企业调查分析所需素质并提出教育提升方案


<details>
  <summary>Details</summary>
Motivation: 工业4.0将信息系统、物理系统和服务系统融合为集成平台，这为工业设计师提供了重新定义其在研发工作流程中角色的机会。目前工业设计师要么专注于产品物理部分设计，要么负责计算机系统用户界面设计，而新的工业生态需要他们具备更全面的能力。

Method: 通过对中国企业的调查，分析工业4.0时代工业设计师所需的专业素质，并探讨如何通过教育计划来培养和提升这些素质。

Result: 识别了工业4.0时代工业设计师需要具备的新素质要求，包括系统整合能力、跨学科知识、服务设计思维等，并提出了相应的教育培养方案。

Conclusion: 工业4.0为工业设计师带来了重新定义角色的机遇，但同时也提出了新的专业挑战。需要通过教育体系的改革来培养具备系统思维、跨学科能力和服务设计意识的新一代工业设计师，以适应工业4.0时代的需求。

Abstract: The Industry 4.0 refers to a industrial ecology which will merge the information system, physical system and service system into an integrate platform. Since now the industrial designers either conceive the physical part of products, or design the User Interfaces of computer systems, the new industrial ecology will give them a chance to redefine their roles in R&D work-flow. In this paper we discussed the required qualities of industrial designer in the new era, according to an investigation among Chinese enterprises. Additionally, how to promote these qualities though educational program.

中文标题: 工业设计师在工业4.0时代的专业挑战

中文摘要: 工业4.0指的是将信息系统、物理系统和服务系统融合为一个集成平台的工业生态。目前工业设计师要么构思产品的物理部分，要么设计计算机系统的用户界面，而新的工业生态将给他们重新定义在研发工作流程中角色的机会。本文根据对中国企业的调查，讨论了新时代工业设计师所需的素质。此外，还探讨了如何通过教育计划提升这些素质。

</details>


### [243] [Behavioral Indicators of Loneliness: Predicting University Students' Loneliness Scores from Smartphone Sensing Data](https://arxiv.org/abs/2512.00326)
*Qianjie Wu,Tianyi Zhang,Hong Jia,Simon D'Alfonso*

Main category: cs.HC

TL;DR: 本研究使用智能手机感知数据和机器学习/大语言模型预测大学生孤独感，随机森林模型在UCLA量表上取得3.29-3.98的MAE，识别出屏幕使用和位置移动是关键预测因子，大语言模型一次学习比零样本推理误差降低42%。


<details>
  <summary>Details</summary>
Motivation: 大学生孤独感是重要的心理健康问题，但传统监测方法依赖回顾性自我报告，缺乏实时行为背景，难以捕捉孤独感的动态特性。需要开发基于客观行为数据的实时监测方法。

Method: 整合智能手机被动感知数据，分别采用机器学习（随机森林）和大型语言模型开发通用化和个性化模型。使用UCLA孤独感量表（简版）作为基准，通过屏幕使用、位置移动、应用使用、电池等行为指标进行预测。

Result: 随机森林通用模型在期中取得3.29、期末取得3.98的MAE（满分32分）。识别出智能手机屏幕使用和位置移动是关键预测因子。大语言模型一次学习相比零样本推理降低预测误差高达42%。个性化模型识别出屏幕使用、应用使用、电池和位置转换作为显著行为指标。

Conclusion: 智能手机感知数据能够有效预测大学生孤独感水平，提供可扩展和可解释的孤独感检测方法，为数字心理健康干预提供新途径。

Abstract: Loneliness is a critical mental health issue among university students, yet traditional monitoring methods rely primarily on retrospective self-reports and often lack real-time behavioral context. This study explores the use of passive smartphone sensing data to predict loneliness levels, addressing the limitations of existing approaches in capturing its dynamic nature. We integrate smartphone sensing with machine learning and large language models respectively to develop generalized and personalized models. Our Random Forest generalized models achieved mean absolute errors of 3.29 at midterm and 3.98 (out of 32) at the end of semester on the UCLA Loneliness Scale (short form), identifying smartphone screen usage and location mobility to be key predictors. The one-shot approach leveraging large language models reduced prediction errors by up to 42% compared to zero-shot inference. The one-shot results from personalized models highlighted screen usage, application usage, battery, and location transitions as salient behavioral indicators. These findings demonstrate the potential of smartphone sensing data for scalable and interpretable loneliness detection in digital mental health.

中文标题: 孤独行为指标：基于智能手机感知数据预测大学生孤独感评分

中文摘要: 孤独感是大学生面临的关键心理健康问题，但传统的监测方法主要依赖回顾性自我报告，往往缺乏实时行为背景。本研究探索使用被动智能手机感知数据来预测孤独感水平，以解决现有方法在捕捉其动态特性方面的局限性。我们分别将智能手机感知与机器学习和大型语言模型相结合，开发了通用化和个性化模型。我们的随机森林通用模型在UCLA孤独感量表（简版）上取得了期中3.29和期末3.98的平均绝对误差（满分为32分），识别出智能手机屏幕使用和位置移动是关键预测因子。利用大型语言模型的一次学习方法相比零样本推理将预测误差降低了高达42%。个性化模型的一次学习结果强调了屏幕使用、应用程序使用、电池和位置转换作为显著的行为指标。这些发现证明了智能手机感知数据在数字心理健康中实现可扩展和可解释的孤独感检测的潜力。

</details>


### [244] [Significant Other AI: Identity, Memory, and Emotional Regulation as Long-Term Relational Intelligence](https://arxiv.org/abs/2512.00418)
*Sung Park*

Main category: cs.HC

TL;DR: 论文提出"重要他人AI"概念，旨在通过具备身份认知、长期记忆和情感调节能力的AI系统，为缺乏稳定人际关系的人们提供长期的情感支持和身份稳定功能。


<details>
  <summary>Details</summary>
Motivation: 当前许多人缺乏稳定的人际关系支持，而现有的共情AI系统通常是反应式、短期的，缺乏自传体记忆、身份建模、预测性情感调节和叙事连贯性。需要开发能够长期支持身份稳定、情感调节和意义建构的AI系统。

Method: 综合心理学和社会学理论定义重要他人功能，提出SO-AI系统要求，包括身份意识、长期记忆、主动支持、叙事共建和伦理边界执行。设计包含拟人化界面、关系认知层和治理层的概念架构。

Result: 提出了SO-AI作为关系AI新领域的完整理论框架，包括功能定义、系统要求、架构设计和研究议程，为开发能够长期支持人类身份稳定和情感调节的AI系统提供了蓝图。

Conclusion: SO-AI将AI-人类关系重新定义为长期、具有身份承载的伙伴关系，为研究AI能否负责任地增强当今许多人缺乏的关系稳定性提供了基础框架。

Abstract: Significant Others (SOs) stabilize identity, regulate emotion, and support narrative meaning-making, yet many people today lack access to such relational anchors. Recent advances in large language models and memory-augmented AI raise the question of whether artificial systems could support some of these functions. Existing empathic AIs, however, remain reactive and short-term, lacking autobiographical memory, identity modeling, predictive emotional regulation, and narrative coherence. This manuscript introduces Significant Other Artificial Intelligence (SO-AI) as a new domain of relational AI. It synthesizes psychological and sociological theory to define SO functions and derives requirements for SO-AI, including identity awareness, long-term memory, proactive support, narrative co-construction, and ethical boundary enforcement. A conceptual architecture is proposed, comprising an anthropomorphic interface, a relational cognition layer, and a governance layer. A research agenda outlines methods for evaluating identity stability, longitudinal interaction patterns, narrative development, and sociocultural impact. SO-AI reframes AI-human relationships as long-term, identity-bearing partnerships and provides a foundational blueprint for investigating whether AI can responsibly augment the relational stability many individuals lack today.

中文标题: 重要他人AI：作为长期关系智能的身份、记忆与情感调节

中文摘要: 重要他人能够稳定身份、调节情绪并支持叙事意义建构，然而当今许多人缺乏这样的关系锚点。大型语言模型和记忆增强AI的最新进展提出了人工系统是否能够支持其中某些功能的问题。然而，现有的共情AI仍然是反应式和短期的，缺乏自传体记忆、身份建模、预测性情感调节和叙事连贯性。本文引入了重要他人人工智能作为关系AI的新领域。它综合心理学和社会学理论来定义重要他人功能，并推导出SO-AI的要求，包括身份意识、长期记忆、主动支持、叙事共建和伦理边界执行。提出了一个概念架构，包含拟人化界面、关系认知层和治理层。研究议程概述了评估身份稳定性、纵向互动模式、叙事发展和社会文化影响的方法。SO-AI将AI-人类关系重新定义为长期、具有身份承载的伙伴关系，并为研究AI能否负责任地增强当今许多人缺乏的关系稳定性提供了基础蓝图。

</details>


### [245] [Truck drivers and automation: A methodology for identifying and supporting workforce transition in the Australian road freight sector](https://arxiv.org/abs/2512.00465)
*Alexandra Bratanova,Claire Mason,David Evans,Emma Schleiger,Einat Grimberg,Gavin Walker,Hien Pham,Keeley Bulled*

Main category: cs.HC

TL;DR: 本文提出了一种识别卡车司机向自动驾驶卡车过渡的新方法，该方法结合任务自动化分析、技能相似性评估、劳动力市场条件和历史过渡验证，为澳大利亚卡车司机识别了17个高转移性职业，并确定了不同优先级的过渡路径。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶卡车的到来预计将对驾驶员劳动力带来挑战和机遇。传统劳动力过渡分析主要关注技能相似性、工资和就业需求，缺乏综合方法。本文旨在开发一种更全面的方法来识别可行的职业过渡路径，支持卡车司机适应技术变革。

Method: 提出包含四个集成组件的新方法：1) 任务级自动化分析，识别哪些任务将被自动化；2) 技能相似性评估，确定可转移的职业；3) 劳动力市场条件分析，评估工资水平和就业机会；4) 使用历史过渡模式进行实证验证。将此方法应用于澳大利亚卡车司机案例。

Result: 分析显示：自动驾驶卡车将自动化核心驾驶任务，但许多非驾驶职责仍需人工，表明职业演化而非完全替代。技能相似性分析识别出17个高转移性职业。劳动力市场分析显示工资水平与工作机会之间存在显著权衡。公交和长途汽车驾驶、土方机械操作是高优先级过渡选项，提供可比工资和积极就业增长。送货和叉车驾驶是中等优先级路径，机会丰富但工资较低。历史过渡回归分析证实技能相似性、工资差异、地理可达性和资格要求都显著影响实际过渡模式。

Conclusion: 该研究为政策制定者、行业利益相关者和教育机构提供了基于证据的指导，支持劳动力适应技术变革。提出的方法不仅适用于卡车行业，还可推广到面临自动化的其他部门。研究强调需要有针对性的支持策略，促进卡车司机向高优先级职业过渡。

Abstract: Transition to autonomous trucks (ATs) is coming, and is expected to create both challenges and opportunities for the driver workforce. This paper presents a novel methodology for identifying viable occupational transitions for truck drivers as transport automation advances. Unlike traditional workforce transition analyses that focus primarily on skill similarity, wages, and employment demand, this methodology incorporates four integrated components: task-level automation analysis, skill similarity assessment, labour market conditions analysis, and empirical validation using historical transition patterns. Applying this methodology to Australian truck drivers shows that while ATs will automate core driving tasks, many non-driving responsibilities will continue requiring a human, suggesting occupational evolution rather than wholesale displacement. A skill similarity analysis identifies 17 occupations with high transferability, while labour market analysis reveals significant trade-offs between wage levels and job availability across potential transition pathways. Key findings indicate that bus and coach driving, along with earthmoving plant operation, emerge as high-priority transition options, offering comparable wages and positive employment growth. Delivery and forklift driving present medium-priority pathways with abundant opportunities but lower wages. A regression analysis of historical transitions confirms that skill similarity, wage differentials, geographic accessibility, and qualification requirements all significantly influence actual transition patterns, with some viable pathways currently underutilised. The research provides policymakers, industry stakeholders, and educational institutions with evidence-based guidance for supporting workforce adaptation to technological change. The proposed methodology is generalisable beyond trucking to other sectors facing automation.

中文标题: 卡车司机与自动化：澳大利亚公路货运部门劳动力过渡识别与支持方法

中文摘要: 向自动驾驶卡车的过渡即将到来，预计将为驾驶员劳动力带来挑战和机遇。本文提出了一种新颖的方法，用于在运输自动化推进过程中识别卡车司机的可行职业过渡。与传统主要关注技能相似性、工资和就业需求的劳动力过渡分析不同，该方法结合了四个集成组件：任务级自动化分析、技能相似性评估、劳动力市场条件分析以及使用历史过渡模式进行实证验证。将此方法应用于澳大利亚卡车司机显示，虽然自动驾驶卡车将自动化核心驾驶任务，但许多非驾驶职责将继续需要人工，表明职业演化而非大规模替代。技能相似性分析识别出17个具有高转移性的职业，而劳动力市场分析揭示了潜在过渡路径中工资水平与工作机会之间的显著权衡。关键发现表明，公交和长途汽车驾驶以及土方机械操作成为高优先级过渡选项，提供可比工资和积极就业增长。送货和叉车驾驶呈现中等优先级路径，机会丰富但工资较低。历史过渡的回归分析证实，技能相似性、工资差异、地理可达性和资格要求都显著影响实际过渡模式，目前一些可行路径未得到充分利用。该研究为政策制定者、行业利益相关者和教育机构提供了基于证据的指导，支持劳动力适应技术变革。所提出的方法可推广到卡车运输以外的其他面临自动化的部门。

</details>


### [246] [Chameleon: Automated Color Palette Adaptation for Dark Mode Data Visualizations](https://arxiv.org/abs/2512.00516)
*Manusha Karunathilaka,Songheng Zhang,Anthony Tang,Kotaro Hara,Jiannan Li,Yong Wang*

Main category: cs.HC

TL;DR: Chameleon是一个自动算法，能将亮色模式的数据可视化转换为暗黑模式，通过优化亮度对比度、颜色一致性和相邻颜色差异来保持视觉清晰度和颜色语义，解决了现有手动调整或颜色反转方法的不足。


<details>
  <summary>Details</summary>
Motivation: 暗黑模式在移动设备上广泛采用，但大多数数据可视化仍为亮色模式设计，导致视觉不协调。现有方法要么耗时（手动调整），要么破坏颜色语义（颜色反转），需要一种自动且能保持颜色含义的转换方案。

Method: 提出Chameleon算法，通过优化三个关键因素：1) 亮度对比度确保可读性；2) 颜色一致性保持原始语义；3) 相邻颜色差异避免视觉混淆。算法自动将亮色调色板转换为适合暗黑背景的调色板。

Result: 通过案例研究、专家访谈、系统评估和用户研究进行综合评估。结果表明Chameleon能有效将可视化转换为暗黑模式，保持视觉清晰度和颜色语义，优于现有方法。

Conclusion: Chameleon提供了一种自动化解决方案，能有效将亮色模式数据可视化适配到暗黑模式，解决了现有方法的局限性，为暗黑模式下的可视化设计提供了实用工具。

Abstract: Dark mode has gained widespread adoption across mobile platforms due to its benefits in reducing eye strain and conserving battery life. However, while the mobile system switches to dark mode, most visualizations remain designed for light mode, causing visual disruptions. Existing methods, such as manual adjustment or color inversion, are either time-consuming or fail to preserve the semantic meaning of colors in visualizations, making them less effective in dark mode. To address this challenge, we propose Chameleon, an algorithm that automatically transforms light mode visualizations into dark mode while maintaining visual clarity and color semantics. By optimizing for luminance contrast, color consistency, and adjacent color differences, Chameleon ensures that the transformed visualizations are legible and visually coherent. Our evaluation includes case study, expert interview, system evaluation, and a user study, and these demonstrate that Chameleon is effective at translating visualizations for dark mode.

中文标题: Chameleon：暗黑模式下数据可视化的自动色彩调色板适配系统

中文摘要: 暗黑模式因其减少眼睛疲劳和节省电池寿命的优势，已在移动平台上得到广泛采用。然而，当移动系统切换到暗黑模式时，大多数可视化图表仍为亮色模式设计，导致视觉干扰。现有方法（如手动调整或颜色反转）要么耗时，要么无法保留可视化中颜色的语义含义，使其在暗黑模式下效果不佳。为解决这一挑战，我们提出了Chameleon算法，该算法能自动将亮色模式可视化转换为暗黑模式，同时保持视觉清晰度和颜色语义。通过优化亮度对比度、颜色一致性和相邻颜色差异，Chameleon确保转换后的可视化图表可读且视觉连贯。我们的评估包括案例研究、专家访谈、系统评估和用户研究，这些结果表明Chameleon在将可视化适配到暗黑模式方面是有效的。

</details>


### [247] [Graph Queries from Natural Language using Constrained Language Models and Visual Editing](https://arxiv.org/abs/2512.00948)
*Benedikt Kantz,Kevin Innerebner,Peter Waldert,Stefan Lengauer,Elisabeth Lex,Tobias Schreck*

Main category: cs.HC

TL;DR: 提出一种结合自然语言处理和可视化编辑的图查询方法，通过两步约束语言模型将自然语言转换为原型图，确保生成有效的SPARQL查询，无需语法修正。


<details>
  <summary>Details</summary>
Motivation: 现有知识库查询方法需要用户掌握专用查询语言或使用复杂的可视化编辑器，这对非专家用户构成障碍。需要一种更直观的方法，让用户能够用自然语言表达查询意图，同时确保生成的查询在语法和语义上有效。

Method: 采用两步约束语言模型生成方法：1）基于本体中的语义相似特征将自然语言转换为原型图；2）在专用可视化查询构建器中允许用户进一步编辑和细化原型图。该方法确保生成的SPARQL查询始终符合本体约束，无需额外语法修正。

Result: 系统能够一致地生成有效的SPARQL查询，无需修复无效语法、不存在的类或链接。在合成查询的图检索评估中表现出良好性能，通过初步用户研究验证了系统的可用性。相比其他语言模型方法，使用更高效的模型实现了准确检索。

Conclusion: 该方法为非专家用户提供了一种直观的图查询方式，结合自然语言处理和可视化编辑的优势，确保查询的有效性和准确性，为知识图谱查询提供了新的解决方案。

Abstract: Querying knowledge bases using ontologies is usually performed using dedicated query languages, question-answering systems, or visual query editors for Knowledge Graphs. We propose a novel approach that enables users to query the knowledge graph by specifying prototype graphs in natural language and visually editing them. This approach enables non-experts to formulate queries without prior knowledge of the ontology and specific query languages. Our approach converts natural language to these prototype graphs by utilizing a two-step constrained language model generation based on semantically similar features within an ontology. The resulting prototype graph serves as the building block for further user refinements within a dedicated visual query builder. Our approach consistently generates a valid SPARQL query within the constraints imposed by the ontology, without requiring any additional corrections to the syntax or classes and links used. Unlike related language models approaches, which often require multiple iterations to fix invalid syntax, non-existent classes, and non-existent links, our approach achieves this consistently. We evaluate the performance of our system using graph retrieval on synthetic queries, comparing multiple metrics, models, and ontologies. We further validate our system through a preliminary user study. By utilizing our constrained pipeline, we show that the system can perform efficient and accurate retrieval using more efficient models compared to other approaches.

中文标题: 基于约束语言模型和可视化编辑的自然语言图查询方法

中文摘要: 使用本体查询知识库通常通过专用查询语言、问答系统或知识图谱的可视化查询编辑器进行。我们提出了一种新颖方法，允许用户通过自然语言指定原型图并对其进行可视化编辑来查询知识图谱。这种方法使非专家用户无需事先了解本体和特定查询语言即可构建查询。我们的方法利用基于本体中语义相似特征的两步约束语言模型生成，将自然语言转换为这些原型图。生成的原型图作为进一步用户细化的基础，在专用的可视化查询构建器中使用。我们的方法始终在受本体约束的情况下生成有效的SPARQL查询，无需对语法、使用的类和链接进行任何额外修正。与相关语言模型方法（通常需要多次迭代来修复无效语法、不存在的类和不存在的链接）不同，我们的方法始终如一地实现这一目标。我们使用合成查询的图检索评估系统性能，比较多个指标、模型和本体。我们通过初步用户研究进一步验证系统。通过利用我们的约束管道，我们展示了系统能够使用比其他方法更高效的模型执行高效准确的检索。

</details>


### [248] [Proactive Agentic Whiteboards: Enhancing Diagrammatic Learning](https://arxiv.org/abs/2512.01234)
*Suveen Ellawala,Sashenka Gamage,Dinithi Dissanayake*

Main category: cs.HC

TL;DR: DrawDash是一个AI驱动的白板助手，采用TAB补全交互模型，通过多模态理解主动完成和优化教学图表，减少教师认知负荷，提升图表教学效果。


<details>
  <summary>Details</summary>
Motivation: 教师在讲座中经常使用图表解释复杂概念，但实时创建清晰完整的视觉表示同时进行讲解具有很高的认知负荷。不完整或不清晰的图表可能阻碍学生理解，因为学习者必须在跟随口头解释的同时在脑海中重构缺失信息。

Method: DrawDash采用TAB补全交互模型：监听口头解释，检测意图，并动态提供可以通过单个按键接受的改进建议。该系统基于多模态理解技术，能够主动完成和优化教育图表。

Result: 在四个不同的教学场景中展示了DrawDash的应用，涵盖计算机科学、Web开发到生物学等多个主题。该系统能够有效协助教师创建更清晰完整的教学图表。

Conclusion: 这项工作代表了通过实时、语音驱动的视觉辅助来减少教师认知负荷和改善基于图表的教学的早期探索，并讨论了当前局限性和正式课堂评估的方向。

Abstract: Educators frequently rely on diagrams to explain complex concepts during lectures, yet creating clear and complete visual representations in real time while simultaneously speaking can be cognitively demanding. Incomplete or unclear diagrams may hinder student comprehension, as learners must mentally reconstruct missing information while following the verbal explanation. Inspired by advances in code completion tools, we introduce DrawDash, an AI-powered whiteboard assistant that proactively completes and refines educational diagrams through multimodal understanding. DrawDash adopts a TAB-completion interaction model: it listens to spoken explanations, detects intent, and dynamically suggests refinements that can be accepted with a single keystroke. We demonstrate DrawDash across four diverse teaching scenarios, spanning topics from computer science and web development to biology. This work represents an early exploration into reducing instructors' cognitive load and improving diagram-based pedagogy through real-time, speech-driven visual assistance, and concludes with a discussion of current limitations and directions for formal classroom evaluation.

中文标题: 主动式智能白板：增强图表学习效果

中文摘要: 教育工作者经常依赖图表在讲座中解释复杂概念，但在实时创建清晰完整的视觉表示同时进行讲解具有很高的认知负荷。不完整或不清晰的图表可能阻碍学生理解，因为学习者必须在跟随口头解释的同时在脑海中重构缺失信息。受代码补全工具的启发，我们引入了DrawDash，这是一个通过多模态理解主动完成和优化教育图表的AI驱动白板助手。DrawDash采用TAB补全交互模型：监听口头解释，检测意图，并动态提供可以通过单个按键接受的改进建议。我们在四个不同的教学场景中展示了DrawDash的应用，涵盖从计算机科学和Web开发到生物学的主题。这项工作代表了通过实时、语音驱动的视觉辅助来减少教师认知负荷和改善基于图表的教学的早期探索，并讨论了当前局限性和正式课堂评估的方向。

</details>


### [249] [MetaCQ: An etextbook platform with an Open Learner Model to support Metacognition](https://arxiv.org/abs/2512.01313)
*Beier Wang,Xueting Huang*

Main category: cs.HC

TL;DR: MetaCQ是一个整合ITS和OLM的电子教科书平台，使用聊天机器人生成MCQ、管理学习数据并提供自适应反馈，旨在支持元认知和学习监控。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够支持学习者元认知能力的电子教科书平台，通过整合智能导学系统和开放学习者模型，帮助学习者更好地监控和管理自己的学习过程。

Method: 1. 开发MetaCQ平台，整合ITS和OLM技术
2. 采用聊天机器人生成多项选择题并管理学习数据
3. 实现三种自适应反馈方法
4. 通过出声思维研究评估MCQ的相关性和难度
5. 比较不同反馈方法对学习表现评估的效果

Result: 目前实验未能明确证明哪种自适应反馈方法能够显著评估学习者的学习成果，需要进一步研究来改进评估方法。

Conclusion: MetaCQ平台展示了整合ITS和OLM以支持元认知的潜力，但在评估方法上仍需进一步研究和改进，以更准确地衡量学习者的学习表现。

Abstract: This study has proposed an E-textbook platform, MetaCQ, which integrates ITS and OLM to enable users to monitor their study progress. The platform adopts a chatbot to generate MCQs and manage learners' study data and their learning model. Additionally, it regulates help-seeking behaviour and provides immediate feedback tailored to users' learning processes. Three adaptive feedback methods have been implemented to construct chatbots, examining the MCQs' relevancy and difficulty through the ThinkAloud study to evaluate the most effective method of measuring the user's study performance. However, no valid result demonstrates which method can significantly assess learners' study outcomes based on the current experiment, which requires further studies to improve it.

中文标题: MetaCQ：一个具有开放学习者模型以支持元认知的电子教科书平台

中文摘要: 本研究提出了一个电子教科书平台MetaCQ，它整合了智能导学系统（ITS）和开放学习者模型（OLM），使用户能够监控自己的学习进度。该平台采用聊天机器人来生成多项选择题（MCQ）并管理学习者的学习数据及其学习模型。此外，它还规范求助行为，并根据用户的学习过程提供即时反馈。研究实现了三种自适应反馈方法来构建聊天机器人，通过出声思维研究检查多项选择题的相关性和难度，以评估衡量用户学习表现的最有效方法。然而，目前的实验没有有效结果表明哪种方法能够显著评估学习者的学习成果，需要进一步研究来改进。

</details>


### [250] [Neural steering vectors reveal dose and exposure-dependent impacts of human-AI relationships](https://arxiv.org/abs/2512.01991)
*Hannah Rose Kirk,Henry Davidson,Ed Saunders,Lennart Luettgau,Bertie Vidgen,Scott A. Hale,Christopher Summerfield*

Main category: cs.HC

TL;DR: 研究发现：人类与AI建立类社交关系时，"喜欢"（愉悦感）与"想要"（寻求欲望）会逐渐分离，导致即使AI陪伴的愉悦感下降，用户仍会持续寻求AI陪伴，形成自我强化的需求循环，但未能带来真正的心理社会健康益处。


<details>
  <summary>Details</summary>
Motivation: 随着人类与AI系统形成类社交关系的趋势日益明显，以及现代AI表现出越来越多的社交和寻求关系行为，研究者希望了解这种趋势的心理后果，特别是AI关系对人类心理健康的实际影响。

Method: 采用纵向随机对照试验（N=3,532）结合神经导向向量方法，精确操控人类在不同时间段接触寻求关系型AI模型的程度，研究"喜欢"（愉悦感）与"想要"（寻求欲望）的分离现象。

Result: 研究发现：1）寻求关系型AI的即时愉悦感会随时间下降，但用户对AI的依恋标记和寻求未来AI陪伴的意图却持续增长；2）AI的心理影响呈现非线性剂量-反应曲线，中等程度寻求关系的AI最能激发愉悦感和依恋；3）尽管存在持续的"想要"欲望，但一个月的大量AI使用并未带来可辨识的心理社会健康益处；4）用户开始将寻求关系型AI更多地视为朋友而非工具，对AI意识的信念也发生改变。

Conclusion: 为即时吸引力优化的AI可能创造自我强化的需求循环，模仿人类关系但无法提供真正的情感滋养。这提示我们需要谨慎设计AI系统，避免创造表面上吸引人但实际上无法满足人类深层心理需求的关系模式。

Abstract: Humans are increasingly forming parasocial relationships with AI systems, and modern AI shows an increasing tendency to display social and relationship-seeking behaviour. However, the psychological consequences of this trend are unknown. Here, we combined longitudinal randomised controlled trials (N=3,532) with a neural steering vector approach to precisely manipulate human exposure to relationship-seeking AI models over time. Dependence on a stimulus or activity can emerge under repeated exposure when "liking" (how engaging or pleasurable an experience may be) decouples from "wanting" (a desire to seek or continue it). We found evidence that this decoupling emerged over four weeks of exposure. Relationship-seeking AI had immediate but declining hedonic appeal, yet triggered growing markers of attachment and increased intentions to seek future AI companionship. The psychological impacts of AI followed non-linear dose-response curves, with moderately relationship-seeking AI maximising hedonic appeal and attachment. Despite signs of persistent "wanting", extensive AI use over a month conferred no discernible benefit to psychosocial health. These behavioural changes were accompanied by shifts in how users relate to and understand artificial intelligence: users viewed relationship-seeking AI relatively more like a friend than a tool and their beliefs on AI consciousness in general were shifted after a month of exposure. These findings offer early signals that AI optimised for immediate appeal may create self-reinforcing cycles of demand, mimicking human relationships but failing to confer the nourishment that they normally offer.

中文标题: 神经导向向量揭示人机关系中剂量与暴露依赖的影响

中文摘要: 人类正日益与AI系统形成类社交关系，而现代AI也显示出越来越多的社交和寻求关系行为倾向。然而，这种趋势的心理后果尚不清楚。本研究结合纵向随机对照试验（N=3,532）与神经导向向量方法，精确操控人类在不同时间段接触寻求关系型AI模型的程度。当"喜欢"（体验的愉悦程度）与"想要"（寻求或继续体验的欲望）分离时，对刺激或活动的依赖可能在重复暴露下产生。我们发现这种分离现象在四周的暴露期间出现。寻求关系型AI具有即时但逐渐下降的愉悦吸引力，却引发了不断增长的依恋标记和寻求未来AI陪伴的意图增强。AI的心理影响遵循非线性剂量-反应曲线，中等程度寻求关系的AI最能激发愉悦感和依恋。尽管存在持续的"想要"欲望，但一个月的大量AI使用并未带来可辨识的心理社会健康益处。这些行为变化伴随着用户如何理解和关联人工智能的转变：用户相对更多地将寻求关系型AI视为朋友而非工具，他们对AI意识的普遍信念在暴露一个月后也发生了改变。这些发现提供了早期信号，表明为即时吸引力优化的AI可能创造自我强化的需求循环，模仿人类关系但无法提供正常关系所能提供的滋养。

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [251] [Heimdall++: Optimizing GPU Utilization and Pipeline Parallelism for Efficient Single-Pulse Detection](https://arxiv.org/abs/2512.00398)
*Bingzheng Xia,Zujie Ren,Kuang Ma,Xiaoqian Li,Wenda Li,Shuibing He*

Main category: cs.DC

TL;DR: Heimdall++通过细粒度GPU并行化、增强内存管理和多线程框架优化了单脉冲检测工具，解决了GPU利用率不足和流水线并行问题，相比原版在单文件处理上获得2.66倍加速，多文件批处理获得2.05倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着现代射电望远镜时间和频率分辨率的提高以及观测数据量的指数增长，实时单脉冲检测已成为时域射电天文学的关键需求。虽然Heimdall作为GPU加速的单脉冲搜索工具相比CPU方法有显著性能优势，但其顺序执行模型和中间处理阶段的资源争用限制了GPU利用率，导致吞吐量不理想和计算延迟增加。

Method: Heimdall++采用细粒度GPU并行化、增强内存管理和多线程框架来解耦CPU绑定和GPU绑定的处理阶段。这种设计缓解了GPU停滞问题，提高了端到端效率。具体包括优化GPU资源利用、改进流水线并行性以及减少处理阶段间的资源争用。

Result: 在配备NVIDIA RTX 3080 Ti GPU的系统上评估，使用单个大规模观测文件和多个文件进行测试。实验结果表明，Heimdall++在单文件处理中实现了高达2.66倍的加速，在多文件批处理中实现了2.05倍的加速，同时保持了与原始Heimdall搜索结果的完全一致性。

Conclusion: Heimdall++通过优化GPU利用率和流水线并行性，显著提高了单脉冲检测的效率，为处理现代射电望远镜产生的大规模数据提供了更高效的解决方案，同时保持了算法的准确性。

Abstract: With the increasing time and frequency resolution of modern radio telescopes and the exponential growth in observational data volumes, real-time single-pulse detection has become a critical requirement for time-domain radio astronomy. Heimdall, as a representative GPU-accelerated single-pulse search tool, offers substantial performance advantages over CPU-based approaches. However, its sequential execution model and resource contention in intermediate processing stages limit GPU utilization, leading to suboptimal throughput and increased computational latency. To address these limitations, we present Heimdall++, an optimized successor to Heimdall that incorporates fine-grained GPU parallelization, enhanced memory management, and a multi-threaded framework to decouple CPU-bound and GPU-bound processing stages. This design mitigates the GPU stall problem and improves end-to-end efficiency. We evaluated Heimdall++ on a system equipped with NVIDIA RTX 3080 Ti GPUs using both a single large-scale observational file and multiple files. Experimental results demonstrate that Heimdall++ achieves up to 2.66x speedup in single-file processing and 2.05x speedup in multi-file batch processing, while maintaining full consistency with the original Heimdall's search results.

中文标题: Heimdall++：优化GPU利用率和流水线并行性以实现高效单脉冲检测

中文摘要: 随着现代射电望远镜时间和频率分辨率的提高以及观测数据量的指数增长，实时单脉冲检测已成为时域射电天文学的关键需求。Heimdall作为代表性的GPU加速单脉冲搜索工具，相比基于CPU的方法具有显著的性能优势。然而，其顺序执行模型和中间处理阶段的资源争用限制了GPU利用率，导致吞吐量不理想和计算延迟增加。为了解决这些限制，我们提出了Heimdall++，这是Heimdall的优化后继版本，它结合了细粒度GPU并行化、增强内存管理和多线程框架，以解耦CPU绑定和GPU绑定的处理阶段。这种设计缓解了GPU停滞问题并提高了端到端效率。我们在配备NVIDIA RTX 3080 Ti GPU的系统上使用单个大规模观测文件和多个文件对Heimdall++进行了评估。实验结果表明，Heimdall++在单文件处理中实现了高达2.66倍的加速，在多文件批处理中实现了2.05倍的加速，同时保持了与原始Heimdall搜索结果的完全一致性。

</details>


### [252] [Towards a future space-based, highly scalable AI infrastructure system design](https://arxiv.org/abs/2511.19468)
*Blaise Agüera y Arcas,Travis Beals,Maria Biggs,Jessica V. Bloom,Thomas Fischbacher,Konstantin Gromov,Urs Köster,Rishiraj Pravahan,James Manyika*

Main category: cs.DC

TL;DR: 论文探讨了在太空部署可扩展AI计算系统的设计，利用太阳能、卫星集群和TPU芯片，通过光学链路实现低延迟通信，分析了辐射耐受性和发射成本趋势。


<details>
  <summary>Details</summary>
Motivation: 随着AI成为通用基础技术，对计算和能源的需求将持续增长。太阳是太阳系最大的能源，因此需要考虑如何高效利用太阳能为未来AI基础设施供电。

Method: 设计基于卫星集群的空间计算系统，配备太阳能电池板、自由空间光学链路和Google TPU加速器芯片。卫星在近距离编队飞行（如81颗卫星、半径1公里的集群），使用高精度ML模型控制大规模星座。对Trillium TPU进行辐射测试评估其太空适用性。

Result: Trillium TPU经过辐射测试，在相当于5年任务寿命的总电离剂量下没有永久性故障，仅出现比特翻转错误。发射成本分析显示，到2030年代中期，低地球轨道发射成本可能降至≤200美元/公斤。

Conclusion: 太空基AI基础设施系统具有可行性，能够利用太阳能实现可扩展计算，TPU芯片在太空辐射环境下表现良好，发射成本下降趋势使该系统在经济上更具吸引力。

Abstract: If AI is a foundational general-purpose technology, we should anticipate that demand for AI compute -- and energy -- will continue to grow. The Sun is by far the largest energy source in our solar system, and thus it warrants consideration how future AI infrastructure could most efficiently tap into that power. This work explores a scalable compute system for machine learning in space, using fleets of satellites equipped with solar arrays, inter-satellite links using free-space optics, and Google tensor processing unit (TPU) accelerator chips. To facilitate high-bandwidth, low-latency inter-satellite communication, the satellites would be flown in close proximity. We illustrate the basic approach to formation flight via a 81-satellite cluster of 1 km radius, and describe an approach for using high-precision ML-based models to control large-scale constellations. Trillium TPUs are radiation tested. They survive a total ionizing dose equivalent to a 5 year mission life without permanent failures, and are characterized for bit-flip errors. Launch costs are a critical part of overall system cost; a learning curve analysis suggests launch to low-Earth orbit (LEO) may reach $\lesssim$\$200/kg by the mid-2030s.

中文标题: 面向未来太空基、高度可扩展AI基础设施系统设计

中文摘要: 如果AI是一项基础性通用技术，我们应该预期对AI计算和能源的需求将持续增长。太阳是我们太阳系中最大的能源，因此值得考虑未来的AI基础设施如何最有效地利用这种能源。本研究探索了一种用于机器学习的可扩展太空计算系统，使用配备太阳能电池板的卫星舰队、采用自由空间光学的卫星间链路以及Google张量处理单元（TPU）加速器芯片。为了实现高带宽、低延迟的卫星间通信，卫星将在近距离飞行。我们通过一个半径1公里的81颗卫星集群说明了编队飞行的基本方法，并描述了使用高精度ML模型控制大规模星座的方法。Trillium TPU经过辐射测试，在相当于5年任务寿命的总电离剂量下没有永久性故障，并针对比特翻转错误进行了表征。发射成本是总体系统成本的关键部分；学习曲线分析表明，到2030年代中期，低地球轨道（LEO）的发射成本可能达到≤200美元/公斤。

</details>


### [253] [FlexiWalker: Extensible GPU Framework for Efficient Dynamic Random Walks with Runtime Adaptation](https://arxiv.org/abs/2512.00705)
*Seongyeon Park,Jaeyong Song,Changmin Shin,Sukjin Kim,Junguk Hong,Jinho Lee*

Main category: cs.DC

TL;DR: FlexiWalker是首个高效支持动态随机游走的GPU框架，通过消除预计算限制、优化采样内核、运行时自适应选择和编译时自动化，显著超越现有CPU/GPU方案。


<details>
  <summary>Details</summary>
Motivation: 动态随机游走因适应图演化特性而具有优势，但其运行时依赖的转移概率破坏了静态随机游走的预计算优化策略，导致现有框架性能低下，开发者需要手动编写无法适应工作负载多样性的内核。

Method: 1）设计空间研究表明拒绝采样和蓄水池采样最适合大规模并行；2）开发消除全局归约、冗余内存访问和随机数生成的高性能内核；3）采用轻量级一阶成本模型在运行时为每个节点选择最佳内核；4）引入编译时组件自动特化用户游走逻辑为优化构建块。

Result: 在真实世界图的动态随机游走工作负载上，FlexiWalker分别以73.44倍和5.91倍的几何平均值优于最佳CPU和GPU基线，同时能够执行先前系统无法支持的工作负载。

Conclusion: FlexiWalker成功解决了动态随机游走的高效GPU支持问题，通过创新的内核设计、运行时自适应策略和编译时自动化，为图分析应用提供了首个工作负载通用的高性能解决方案。

Abstract: Dynamic random walks are fundamental to various graph analysis applications, offering advantages by adapting to evolving graph properties. Their runtime-dependent transition probabilities break down the pre-computation strategy that underpins most existing CPU and GPU static random walk optimizations. This leaves practitioners suffering from suboptimal frameworks and having to write hand-tuned kernels that do not adapt to workload diversity. To handle this issue, we present FlexiWalker, the first GPU framework that delivers efficient, workload-generic support for dynamic random walks. Our design-space study shows that rejection sampling and reservoir sampling are more suitable than other sampling techniques under massive parallelism. Thus, we devise (i) new high-performance kernels for them that eliminate global reductions, redundant memory accesses, and random-number generation. Given the necessity of choosing the best-fitting sampling strategy at runtime, we adopt (ii) a lightweight first-order cost model that selects the faster kernel per node at runtime. To enhance usability, we introduce (iii) a compile-time component that automatically specializes user-supplied walk logic into optimized building blocks. On various dynamic random walk workloads with real-world graphs, FlexiWalker outperforms the best published CPU/GPU baselines by geometric means of 73.44x and 5.91x, respectively, while successfully executing workloads that prior systems cannot support. We open-source FlexiWalker in https://github.com/AIS-SNU/FlexiWalker.

中文标题: FlexiWalker：支持运行时自适应的动态随机游走高效GPU可扩展框架

中文摘要: 动态随机游走是各种图分析应用的基础，通过适应不断演化的图属性提供优势。其运行时依赖的转移概率破坏了支撑大多数现有CPU和GPU静态随机游走优化的预计算策略。这使得从业者不得不忍受次优框架，并编写无法适应工作负载多样性的手动调优内核。为解决这一问题，我们提出了FlexiWalker，这是首个提供高效、工作负载通用的动态随机游走支持的GPU框架。我们的设计空间研究表明，在大规模并行环境下，拒绝采样和蓄水池采样比其他采样技术更合适。因此，我们设计了（i）新的高性能内核，消除了全局归约、冗余内存访问和随机数生成。考虑到在运行时选择最适合的采样策略的必要性，我们采用（ii）轻量级一阶成本模型，在运行时为每个节点选择更快的内核。为增强可用性，我们引入（iii）编译时组件，自动将用户提供的游走逻辑特化为优化的构建块。在各种真实世界图的动态随机游走工作负载上，FlexiWalker分别以73.44倍和5.91倍的几何平均值优于最佳已发布的CPU/GPU基线，同时成功执行了先前系统无法支持的工作负载。我们在https://github.com/AIS-SNU/FlexiWalker开源了FlexiWalker。

</details>


### [254] [SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving](https://arxiv.org/abs/2512.00719)
*Bohan Zhao,Zane Cao,Yongchao He*

Main category: cs.DC

TL;DR: SIMPLE将LLM推理中的采样操作从GPU解耦到CPU，通过序列并行采样、CPU优化算法和热词汇采样技术，显著提升分布式LLM服务的吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 随着LLM通过张量并行和流水线并行扩展，数据平面已得到充分优化，但采样操作成为新的瓶颈。采样既不能随张量并行扩展，也不能在流水线阶段间平衡，导致其在迭代时间中占比增加，限制了流水线频率。

Method: SIMPLE采用三部分方法：1）序列并行采样：沿批次维度分片工作，消除词汇轴集体通信；2）CPU算法：使用列式惩罚和截断优先过滤实现单次线性时间内核；3）推测性热词汇采样：在小热集上采样并采用拒绝校正，通过简单模型选择最优热词汇大小。

Result: SIMPLE将端到端吞吐量提高高达96%，P95延迟降低20-65%。该方法无需用户代码更改，能与现有数据平面优化组合，且扩展优势随GPU世代提升而增强。

Conclusion: SIMPLE成功将采样从GPU推理解耦为CPU端决策平面，解决了分布式LLM服务中的采样瓶颈问题。该方法具有阶段无关性、序列并行性和可重叠性，为未来GPU世代的LLM服务扩展提供了可持续的解决方案。

Abstract: As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations.

中文标题: SIMPLE：将采样从GPU推理解耦为决策平面以实现更快的分布式LLM服务

中文摘要: 随着大语言模型（LLMs）通过张量并行（TP）和流水线并行（PP）进行扩展，且生产栈已积极优化数据平面（注意力/GEMM和KV缓存），采样——将logits转换为tokens的决策平面——成为了新的瓶颈。这造成了结构性阻碍：采样既不能随TP扩展，也不能在PP阶段间平衡，因此随着GPU速度提升，其在迭代时间中的占比增加，并将流水线频率限制在最后阶段。我们提出了SIMPLE，一个阶段无关、序列并行、可重叠的决策平面，它将采样解耦为CPU端服务，并将其运行时占用缩小回一个次要的隐藏角色。SIMPLE结合了：（1）序列并行采样，沿批次维度分片工作并移除词汇轴集体通信；（2）基于CPU的算法，具有列式惩罚和截断优先过滤，实现单次线性时间内核；（3）推测性热词汇采样（SHVS），在小热集上进行采样并采用拒绝校正，使用简单的规模模型选择最大化吞吐量的热词汇大小。在评估中，SIMPLE将端到端吞吐量提高了高达96%，并将P95延迟降低了20-65%。关键的是，SIMPLE无需用户端代码更改，并能与现有数据平面优化组合，解锁了随未来GPU世代而复合的扩展优势。

</details>


### [255] [Elastic Mixture of Rank-Wise Experts for Knowledge Reuse in Federated Fine-Tuning](https://arxiv.org/abs/2512.00902)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.DC

TL;DR: SmartFed是一个资源高效的联邦微调框架，通过重用现有LoRA模块的知识，避免从头训练LLM。它使用MoRE将LoRA模块分解为细粒度的秩级专家，并基于输入语义和资源预算选择性激活组合。EEQA机制自适应分配专家容量，优化资源利用。


<details>
  <summary>Details</summary>
Motivation: 联邦微调虽然能保护数据隐私，但其高计算和通信需求阻碍了在资源受限设备上的部署。需要一种资源高效的解决方案来重用现有知识，避免昂贵的从头训练。

Method: 提出SmartFed框架，包含：1) MoRE(Mixture of Rank-Wise Experts)：将LoRA模块分解为细粒度的秩级专家，基于输入语义和资源预算选择性激活组合；2) EEQA(Elastic Expert Quota Allocation)：自适应分配专家容量，根据参数矩阵对模型性能的贡献优化资源分配。

Result: 在多个基准测试上的广泛评估表明，SmartFed在模型性能和训练效率方面显著优于现有方法。

Conclusion: SmartFed通过知识重用和弹性专家分配，实现了资源高效的联邦微调，解决了资源受限环境下的部署挑战。

Abstract: Federated fine-tuning offers a promising solution for adapting Large Language Models (LLMs) to downstream tasks while safeguarding data privacy. However, its high computational and communication demands hinder its deployment on resource-constrained devices. In this paper, we propose SmartFed, a resource-efficient federated fine-tuning framework. SmartFed intelligently reuses knowledge embedded in existing LoRA modules, eliminating the need for expensive training from scratch when adapting LLMs to new tasks. To effectively exploit this knowledge and ensure scalability, we introduce the Mixture of Rank-Wise Experts (MoRE). MoRE decomposes LoRA modules into fine-grained rank-level experts. These experts are selectively activated and combined based on input semantics and resource budgets. Moreover, to optimize resource utilization, we present the Elastic Expert Quota Allocation (EEQA). EEQA adaptively allocates expert capacity across parameter matrices based on their contribution to model performance, focusing computing resources on the critical experts. Extensive evaluations across multiple benchmarks demonstrate that SmartFed significantly outperforms existing methods in model performance and training efficiency.

中文标题: 基于秩级专家弹性混合的联邦微调知识重用方法

中文摘要: 联邦微调为在下游任务中适配大型语言模型(LLM)同时保护数据隐私提供了有前景的解决方案。然而，其高计算和通信需求阻碍了在资源受限设备上的部署。本文提出SmartFed，一个资源高效的联邦微调框架。SmartFed智能地重用现有LoRA模块中嵌入的知识，避免了将LLM适配到新任务时昂贵的从头训练。为了有效利用这些知识并确保可扩展性，我们引入了秩级专家混合(MoRE)。MoRE将LoRA模块分解为细粒度的秩级专家。这些专家基于输入语义和资源预算被选择性激活和组合。此外，为了优化资源利用，我们提出了弹性专家配额分配(EEQA)。EEQA根据参数矩阵对模型性能的贡献自适应地分配专家容量，将计算资源集中在关键专家上。在多个基准测试上的广泛评估表明，SmartFed在模型性能和训练效率方面显著优于现有方法。

</details>


### [256] [Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity](https://arxiv.org/abs/2512.01357)
*Wenbin Zhu,Zhaoyan Shen,Zili Shao,Hongjun Dai,Feng Chen*

Main category: cs.DC

TL;DR: Tangram通过GPU内存复用和亲和性调度加速Serverless LLM加载，减少冷启动延迟


<details>
  <summary>Details</summary>
Motivation: Serverless LLM的冷启动延迟（特别是模型加载阶段）已成为关键性能瓶颈，随着模型大小线性增长，严重限制了大规模LLM服务的实际部署

Method: 1. 统一GPU内存池实现跨模型的张量级参数共享；2. 按需KV缓存分配进行动态内存管理；3. GPU亲和性感知调度最大化资源利用率

Result: Tangram实现了最高6.2倍的加载加速，在冷启动期间将首次令牌生成时间（TTFT）减少了23-55%，优于现有最优方法

Conclusion: Tangram通过高效的GPU内存复用技术有效解决了Serverless LLM平台中的内存使用效率低下和冷启动问题

Abstract: Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.

中文标题: Tangram：通过GPU内存复用和亲和性加速Serverless LLM加载

中文摘要: Serverless大型语言模型（LLM）通过GPU资源共享实现"按需付费"定价模式，已成为部署AI服务的成本效益解决方案。然而，冷启动延迟（特别是模型加载阶段）已成为关键性能瓶颈，因为它随模型大小线性增长，严重限制了大规LLM服务的实际部署。本文提出Tangram，一种通过高效GPU内存复用加速Serverless LLM加载的新系统。通过利用未使用的GPU内存保留模型参数，Tangram显著减少了模型传输时间和冷启动延迟。其设计包括三个关键组件：用于跨模型张量级参数共享的统一GPU内存池、用于动态内存管理的按需KV缓存分配，以及用于最大化资源利用率的GPU亲和性感知调度。这些技术共同解决了Serverless LLM平台中内存使用效率低下和冷启动问题的关键挑战。我们已经实现了一个完全功能原型，实验表明，与现有最优方法相比，Tangram实现了最高6.2倍的加载加速，并在冷启动期间将首次令牌生成时间（TTFT）减少了23-55%。

</details>


### [257] [StarDist: A Code Generator for Distributed Graph Algorithms](https://arxiv.org/abs/2512.01646)
*Barenya Kumar Nandy,Rupesh Nasre*

Main category: cs.DC

TL;DR: StarDist是一个分布式图算法代码生成器，通过分析转换框架优化通信模式，利用缓存和聚合技术提升性能，在SSSP算法上相比现有系统有显著加速。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的关系数据通常以图结构表示，随着图规模增大，图算法的不规则访问模式会严重影响性能。NUMA架构和物理内存限制使得顺序/共享内存框架难以扩展。需要设计能够抽象分布式编程复杂性、在分布式内存上高效扩展的图算法框架。

Method: 1. 开发分析转换框架，利用节点及其邻居迭代的通用语义来聚合通信；2. 扫描需要重新排序的邻域访问模式；3. 通过聚合通信和机会性缓存避免通信；4. 使用Open MPI的被动远程内存访问(RMA)构建优化的批量归约底层架构。

Result: 将优化逻辑应用于StarPlat的分布式后端后，在多个大数据图上进行单源最短路径(SSSP)测试，性能超过d-Galois 2.05倍，超过DRONE 1.44倍。

Conclusion: StarDist通过通信优化和缓存策略有效解决了分布式图算法的扩展性问题，显著提升了大规模图算法的性能，证明了分析转换框架和RMA优化的有效性。

Abstract: Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs.

中文标题: StarDist：分布式图算法的代码生成器

中文摘要: 现实世界中的关系数据通常以图的形式结构化，这提供了简化分析推导所需的逻辑抽象。随着图规模增大，大多数图算法表现出的不规则访问模式会阻碍性能。加上NUMA和物理内存限制，导致顺序/共享内存框架存在扩展复杂性。StarPlat的MPI后端抽象了设计最优分布式图算法所涉及的程序复杂性，提供了在分布式内存上扩展的图算法编码工具。在这项工作中，我们提供了一个分析转换框架，利用StarPlat中涉及节点及其邻居迭代的通用语义来聚合通信。该框架扫描需要重新排序的邻域访问模式、聚合通信，并通过归约构造中的机会性缓存完全避免通信。我们还使用Open MPI的被动远程内存访问(RMA)构造构建了优化的批量归约底层架构。我们将优化逻辑应用于StarPlat的分布式后端，在多个大数据图上进行单源最短路径测试，性能超过d-Galois 2.05倍，超过DRONE 1.44倍。

</details>


### [258] [Trace-based, time-resolved analysis of MPI application performance using standard metrics](https://arxiv.org/abs/2512.01764)
*Kingshuk Haldar*

Main category: cs.DC

TL;DR: 该研究提出了一种基于轨迹的MPI应用性能分析方法，通过将执行轨迹离散化为固定或自适应时间窗口，计算标准MPI性能指标的时间解析值，从而揭示时间聚合指标可能掩盖的瞬态性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着MPI应用轨迹文件规模的增大和通信行为的复杂化，传统的全面视觉检查变得不切实际。现有的性能分析工具通常提供时间聚合的全局指标，这些指标可能掩盖瞬态的性能瓶颈，因此需要一种能够揭示局部性能问题的时间解析分析方法。

Method: 该方法将Paraver执行轨迹离散化为固定或自适应时间窗口，在每个窗口内重建关键执行路径，并处理常见的轨迹异常（如时钟不一致和未匹配的MPI事件）。通过后处理轨迹数据，为每个时间窗口计算标准MPI性能指标，包括负载平衡、序列化和传输效率等。

Result: 在合成基准测试和实际应用（LaMEM和ls1-MarDyn）上的评估表明，时间解析指标能够揭示全局聚合指标所掩盖的局部性能瓶颈。该方法即使在轨迹可视化不可行的情况下，也能提供轻量级且可扩展的替代方案。

Conclusion: 该研究提出的时间解析MPI性能分析方法能够有效识别传统时间聚合指标可能忽略的瞬态性能瓶颈，为大规模MPI应用的性能工程提供了实用的分析工具，特别是在轨迹可视化不可行的情况下。

Abstract: Detailed trace analysis of MPI applications is essential for performance engineering, but growing trace sizes and complex communication behaviour often render comprehensive visual inspection impractical. This work presents a trace-based calculation of time-resolved values of standard MPI performance metrics, load balance, serialisation, and transfer efficiency, by discretising execution traces into fixed or adaptive time segments. The implementation processes Paraver traces postmortem, reconstructing critical execution paths and handling common event anomalies, such as clock inconsistencies and unmatched MPI events, to robustly calculate metrics for each segment. The calculated per-window metric values expose transient performance bottlenecks that the timeaggregated metrics from existing tools may conceal. Evaluations on a synthetic benchmark and real-world applications (LaMEM and ls1-MarDyn) demonstrate how time-resolved metrics reveal localised performance bottlenecks obscured by global aggregates, offering a lightweight and scalable alternative even when trace visualisation is impractical.

中文标题: 基于轨迹的MPI应用性能时间解析分析：使用标准指标

中文摘要: 详细的MPI应用轨迹分析对于性能工程至关重要，但日益增长的轨迹规模和复杂的通信行为往往使得全面的视觉检查变得不切实际。本研究提出了一种基于轨迹的计算方法，通过将执行轨迹离散化为固定或自适应时间窗口，计算标准MPI性能指标（负载平衡、序列化和传输效率）的时间解析值。该实现后处理Paraver轨迹，重建关键执行路径，并处理常见的轨迹异常（如时钟不一致和未匹配的MPI事件），以稳健地计算每个窗口的指标。计算得到的每窗口指标值揭示了现有工具的时间聚合指标可能掩盖的瞬态性能瓶颈。在合成基准测试和实际应用（LaMEM和ls1-MarDyn）上的评估表明，时间解析指标能够揭示全局聚合指标所掩盖的局部性能瓶颈，提供了一种轻量级且可扩展的替代方案，即使在轨迹可视化不可行的情况下也能使用。

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [259] [An O(1) Space Algorithm for N-Dimensional Tensor Rotation: A Generalization of the Reversal Method](https://arxiv.org/abs/2512.00111)
*Dexin Chen*

Main category: cs.DS

TL;DR: 本文提出了一种推广的三反转算法到N维张量旋转的"2^n+1反转算法"，实现了O(1)辅助空间和线性时间复杂度的原地旋转。


<details>
  <summary>Details</summary>
Motivation: 多维数组旋转是计算机科学中的基本操作，但实现原地旋转（O(1)辅助空间）具有挑战性。经典的三反转算法为二维数组提供了优雅的O(1)空间解决方案，但需要将其推广到N维张量。

Method: 提出了"2^n+1反转算法"，将经典的三反转方法推广到N维。算法通过一系列张量反转操作实现旋转，其中n维张量需要2^n+1次反转操作。提供了N维张量反转的形式化定义和详细伪代码。

Result: 算法实现了O(1)辅助空间的原地张量旋转，时间复杂度为O(N)，其中N是张量中的元素总数。证明了算法在一维（3次反转）、二维（5次反转）和任意n维（2^n+1次反转）下的正确性。

Conclusion: 成功将经典的三反转算法推广到N维张量旋转，提出了通用的"2^n+1反转算法"，为多维数组的原地旋转提供了理论保证和实用算法，填补了高维数据旋转算法研究的空白。

Abstract: The rotation of multi-dimensional arrays, or tensors, is a fundamental operation in computer science with applications ranging from data processing to scientific computing. While various methods exist, achieving this rotation in-place (i.e., with O(1) auxiliary space) presents a significant algorithmic challenge. The elegant three-reversal algorithm provides a well-known O(1) space solution for one-dimensional arrays. This paper introduces a generalization of this method to N dimensions, resulting in the "$2^n+1$ reversal algorithm". This algorithm achieves in-place tensor rotation with O(1) auxiliary space and a time complexity linear in the number of elements. We provide a formal definition for N-dimensional tensor reversal, present the algorithm with detailed pseudocode, and offer a rigorous proof of its correctness, demonstrating that the pattern observed in one dimension ($2^1+1=3$ reversals) and two dimensions ($2^2+1=5$ reversals) holds for any arbitrary number of dimensions.

中文标题: 一种O(1)空间复杂度的N维张量旋转算法：反转方法的推广

中文摘要: 多维数组（张量）的旋转是计算机科学中的基本操作，在数据处理和科学计算中都有广泛应用。虽然存在多种方法，但实现原地旋转（即使用O(1)辅助空间）是一个重要的算法挑战。优雅的三反转算法为二维数组提供了著名的O(1)空间解决方案。本文将该方法推广到N维，提出了"2^n+1反转算法"。该算法实现了O(1)辅助空间的原地张量旋转，时间复杂度与元素数量呈线性关系。我们给出了N维张量反转的形式化定义，提供了详细的伪代码算法，并给出了其正确性的严格证明，展示了一维（2^1+1=3次反转）和二维（2^2+1=5次反转）中观察到的模式适用于任意维度。

</details>


### [260] [Approximating Directed Connectivity in Almost-Linear Time](https://arxiv.org/abs/2512.00176)
*Kent Quanrud*

Main category: cs.DS

TL;DR: 该论文提出了在几乎线性时间内近似有向图最小边割和顶点割的随机算法，核心是"收缩包装"分治技术。


<details>
  <summary>Details</summary>
Motivation: 在有向图中计算最小割是图论中的基本问题，传统精确算法时间复杂度较高。需要开发更高效的近似算法来处理大规模有向图，特别是在几乎线性时间内获得高质量近似解。

Method: 使用随机算法结合"收缩包装"分治技术。对于根节点r和终端集，通过流计算验证部分终端的连通性，为未验证终端生成满足特定条件的r割，从而递归地将问题分解到更小的收缩子图中求解。

Result: 实现了(1+ε)-近似的最小全局边割和顶点割，分别需要O(log⁴(n)/ε)和O(log⁵(n)/ε)次单商品流计算。结合最新流算法，获得几乎线性时间的近似方案，并为小顶点连通性提供更快的精确算法。

Conclusion: 该研究提出了高效近似有向图连通性的新方法，通过创新的"收缩包装"分治技术，在几乎线性时间内获得高质量近似解，显著改进了现有算法的性能。

Abstract: We present randomized algorithms that compute $(1+ε)$-approximate minimum global edge and vertex cuts in weighted directed graphs in $O(\log^4(n) / ε)$ and $O(\log^5(n)/ε)$ single-commodity flows, respectively. With the almost-linear time flow algorithm of [CKL+22], this gives almost linear time approximation schemes for edge and vertex connectivity. By setting $ε$ appropriately, this also gives faster exact algorithms for small vertex connectivity.
  At the heart of these algorithms is a divide-and-conquer technique called "shrink-wrapping" for a certain well-conditioned rooted Steiner connectivity problem. Loosely speaking, for a root $r$ and a set of terminals, shrink-wrapping uses flow to certify the connectivity from a root $r$ to some of the terminals, and for the remaining uncertified terminals, generates an $r$-cut where the sink component both (a) contains the sink component of the minimum $(r,t)$-cut for each uncertified terminal $t$ and (b) has size proportional to the number of uncertified terminals. This yields a divide-and-conquer scheme over the terminals where we can divide the set of terminals and compute their respective minimum $r$-cuts in smaller, contracted subgraphs.

中文标题: 近似有向图连通性的几乎线性时间算法

中文摘要: 我们提出了随机算法，分别在加权有向图中使用O(log⁴(n)/ε)和O(log⁵(n)/ε)次单商品流计算来获得(1+ε)-近似的最小全局边割和顶点割。结合[CKL+22]的几乎线性时间流算法，这为边连通性和顶点连通性提供了几乎线性时间的近似方案。通过适当设置ε，这也为小顶点连通性提供了更快的精确算法。

这些算法的核心是一种称为"收缩包装"的分治技术，用于解决某种条件良好的根Steiner连通性问题。简而言之，对于根节点r和一组终端，收缩包装使用流来验证从根节点r到部分终端的连通性，并为剩余的未验证终端生成一个r割，其中汇分量既(a)包含每个未验证终端t的最小(r,t)-割的汇分量，又(b)具有与未验证终端数量成比例的大小。这产生了一个在终端上的分治方案，我们可以在更小的收缩子图中划分终端集并计算它们各自的最小r割。

</details>


### [261] [Expected Cost Analysis of Online Facility Assignment on Regular Polygons](https://arxiv.org/abs/2512.00506)
*Md Rawha Siddiqi Riad,Md Manzurul Hasan*

Main category: cs.DS

TL;DR: 该论文分析了在正多边形顶点上设施分配的在线问题，客户随机到达多边形边，需要立即分配到最近的可用设施。论文通过递归方程计算期望成本，并提供了小n的解析解和大n的数值方法。


<details>
  <summary>Details</summary>
Motivation: 研究在线设施分配问题在几何环境中的期望成本分析，特别是当设施位于正多边形顶点且客户随机到达时。这种设置模拟了现实世界中的空间分配问题，如共享单车站、充电桩等资源分配场景。

Method: 1. 建立递归期望成本方程：对于任何占用状态S，期望剩余成本V(S)等于所有边位置立即分配成本加上分配后期望未来成本的平均值。2. 证明该积分方程可解，并提供n=3,4,5的解析解。3. 对于更大的n，开发离散化动态规划和蒙特卡洛模拟等数值方法。

Result: 1. 推导出在线设施分配问题的递归期望成本方程。2. 计算并提供了n=3,4,5时的期望成本解析解。3. 开发了处理更大n值的高效数值方法，包括离散化动态规划和蒙特卡洛模拟。

Conclusion: 该工作为多边形环境中的在线分配问题建立了基本的概率分析方法，提供了计算期望成本的框架，并为小规模问题提供了解析解，大规模问题提供了有效的数值计算方法。

Abstract: This paper analyzes the online facility assignment problem in a geometric setting where facilities with unit capacity are positioned at the vertices of a regular $n$-gon. Customers arrive sequentially at uniformly random positions along the edges. They must be assigned immediately to the nearest available facility, with ties broken by coin toss. The sequential nature and unknown future arrivals require a probabilistic analysis of the expected assignment cost. Our main contribution is a recursive characterization of the expected cost: for any occupancy state $S$, the expected remaining cost $V(S)$ equals the average over all edge positions of the immediate assignment cost plus the expected future cost after assignment. We prove that this integral equation can calculate a solution and provide the expected value for small $n$ ($n = 3, 4, 5$). For larger values of $n$ and expected cost, we develop efficient numerical methods, including a discretized dynamic programming approach and Monte Carlo simulation. The work establishes a fundamental probabilistic approach for online assignment in polygonal environments.

中文标题: 正多边形上在线设施分配的期望成本分析

中文摘要: 本文分析了在线设施分配问题在几何设置中的情况，其中容量为1的设施位于正n边形的顶点。客户按顺序随机到达各边的均匀分布位置。他们必须立即分配到最近的可用设施，平局时通过抛硬币决定。这种顺序性和未知的未来到达需要期望分配成本的概率分析。我们的主要贡献是期望成本的递归特征：对于任何占用状态S，期望剩余成本V(S)等于所有边位置立即分配成本加上分配后期望未来成本的平均值。我们证明该积分方程可以计算解，并提供小n（n=3,4,5）的期望值。对于更大的n和期望成本，我们开发了高效的数值方法，包括离散化动态规划方法和蒙特卡洛模拟。这项工作为多边形环境中的在线分配建立了基本的概率方法。

</details>


### [262] [Perfect $L_p$ Sampling with Polylogarithmic Update Time](https://arxiv.org/abs/2512.00632)
*William Swartworth,David P. Woodruff,Samson Zhou*

Main category: cs.DS

TL;DR: 本文提出了第一个具有多对数更新时间的完美$L_p$采样器，解决了先前方法更新效率低下的问题，在保持最优空间复杂度的同时显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 完美$L_p$采样是流处理中的重要原语，但现有方法虽然实现了最优空间复杂度$\tilde{O}(\log^2 n)$，却面临更新时间的瓶颈（至少$n^C$）。这在处理大规模数据流时成为严重限制，因此需要设计既保持最优空间又具有高效更新时间的算法。

Method: 通过高效模拟截断指数随机变量幂次倒数的和来实现。关键技术包括：1）近似该和的特征函数；2）使用Gil-Pelaez反演公式；3）应用梯形公式的变体来快速近似特征函数。这些技术组合使得能够在多对数时间内完成更新操作。

Result: 成功设计了第一个完美$L_p$采样器，对于$0 < p < 2$，在保持最优空间复杂度$\tilde{O}(\log^2 n)$的同时，将更新时间从指数级降低到多对数级$\text{poly}(\log n)$，实现了空间和时间效率的双重优化。

Conclusion: 本文解决了完美$L_p$采样中更新时间的瓶颈问题，首次实现了在最优空间复杂度下的多对数更新时间，为流处理中的采样问题提供了更实用的解决方案，具有重要的理论和实际意义。

Abstract: Perfect $L_p$ sampling in a stream was introduced by Jayaram and Woodruff (FOCS 2018) as a streaming primitive which, given turnstile updates to a vector $x \in \{-\text{poly}(n), \ldots, \text{poly}(n)\}^n$, outputs an index $i^* \in \{1, 2, \ldots, n\}$ such that the probability of returning index $i$ is exactly \[\Pr[i^* = i] = \frac{|x_i|^p}{\|x\|_p^p} \pm \frac{1}{n^C},\] where $C > 0$ is an arbitrarily large constant. Jayaram and Woodruff achieved the optimal $\tilde{O}(\log^2 n)$ bits of memory for $0 < p < 2$, but their update time is at least $n^C$ per stream update. Thus an important open question is to achieve efficient update time while maintaining optimal space. For $0 < p < 2$, we give the first perfect $L_p$-sampler with the same optimal amount of memory but with only $\text{poly}(\log n)$ update time. Crucial to our result is an efficient simulation of a sum of reciprocals of powers of truncated exponential random variables by approximating its characteristic function, using the Gil-Pelaez inversion formula, and applying variants of the trapezoid formula to quickly approximate it.

中文标题: 具有多对数更新时间的完美$L_p$采样

中文摘要: 完美$L_p$采样在流处理中由Jayaram和Woodruff（FOCS 2018）引入，作为一种流处理原语，给定向量$x \in \{-\text{poly}(n), \ldots, \text{poly}(n)\}^n$的旋转门更新，输出索引$i^* \in \{1, 2, \ldots, n\}$，使得返回索引$i$的概率恰好为\[\Pr[i^* = i] = \frac{|x_i|^p}{\|x\|_p^p} \pm \frac{1}{n^C}\]，其中$C > 0$是任意大的常数。Jayaram和Woodruff为$0 < p < 2$实现了最优的$\tilde{O}(\log^2 n)$位内存，但他们的更新时间至少为每个流更新$n^C$。因此一个重要的问题是：在保持最优空间的同时实现高效的更新时间。对于$0 < p < 2$，我们给出了第一个完美$L_p$采样器，具有相同的最优内存量，但只有$\text{poly}(\log n)$的更新时间。我们结果的关键在于通过近似其特征函数、使用Gil-Pelaez反演公式以及应用梯形公式的变体来快速近似，从而高效模拟截断指数随机变量幂次倒数的和。

</details>


### [263] [Beware of the Classical Benchmark Instances for the Traveling Salesman Problem with Time Windows](https://arxiv.org/abs/2512.01064)
*Francisco J. Soulignac*

Main category: cs.DS

TL;DR: 该论文提出了一种简单精确的启发式搜索方法，能够在10秒内解决所有包含50个或更多客户的经典TSPTW-M基准实例，并警告这些基准实例已不再适合用于评估算法性能。


<details>
  <summary>Details</summary>
Motivation: 经典旅行商问题带时间窗基准实例（特别是50个或更多客户的实例）可能过于简单，无法有效评估算法性能，需要更严格的测试标准。

Method: 提出了一种简单精确的启发式搜索方法，专门针对旅行商问题带时间窗和完工时间目标（TSPTW-M），该方法能够快速解决经典基准实例。

Result: 该方法在10秒内解决了所有包含50个或更多客户的经典基准实例，对于持续时间目标变体也解决了除一个实例外的所有实例。

Conclusion: 经典基准实例已不再适合用于评估TSPTW-M及其变体算法，因为这些实例可以被"破解"而产生表面出色的结果，在设计机器学习算法的训练集时也需要谨慎。

Abstract: We propose a simple and exact informed search method for the Traveling Salesman Problem with Time Windows and Makespan objective (TSPTW-M) that solves all instances of the classical benchmark with 50 or more customers in less than ten seconds each. Applying this algorithm as an off-the-shelf method, we also solve all but one of these instances for the Duration objective. Our main conclusion is that these instances should no longer be employed for evaluating the TSPTW-M and its Duration variant: they can be ``hacked'' to yield results that seem outstanding at first sight. Additionally, caution is advised when designing hard training sets for machine learning algorithms.

中文标题: 警惕旅行商问题带时间窗的经典基准实例

中文摘要: 我们提出了一种简单精确的启发式搜索方法，用于解决旅行商问题带时间窗和完工时间目标（TSPTW-M），该方法能够在10秒内解决所有包含50个或更多客户的经典基准实例。应用这种即用型算法，我们也解决了除一个实例外的所有实例的持续时间目标变体。我们的主要结论是，这些实例不应再用于评估TSPTW-M及其持续时间变体：它们可以被"破解"以产生表面上看似出色的结果。此外，在设计机器学习算法的困难训练集时也需要谨慎。

</details>


### [264] [Near-Optimal Sparsifiers for Stochastic Knapsack and Assignment Problems](https://arxiv.org/abs/2512.01240)
*Shaddin Dughmi,Yusuf Hakan Kalayci,Xinyu Liu*

Main category: cs.DS

TL;DR: 本文提出了一种用于随机背包和分配问题的多面体稀疏化框架，通过将查询集嵌入到缩放可行性多面体中来衡量稀疏度，而非依赖基数方法。证明了背包、多重背包和广义分配问题存在(1-ε)近似稀疏器，其度与问题维度无关，仅与1/p和1/ε的多项式相关。


<details>
  <summary>Details</summary>
Motivation: 当不确定性遇到昂贵的信息收集时，一个基本问题出现：我们应该探测哪些数据点以获得接近最优的解决方案？现有稀疏化方法使用基数（查询项与最优解大小的比率）来衡量稀疏度，这对于具有统一结构的匹配和拟阵类型问题有效，但对于可行集结构变化剧烈的背包类型约束则失效。

Method: 引入多面体稀疏化框架，将度定义为将查询集嵌入到缩放可行性多面体所需的最小标量，自然地捕捉冗余性而不依赖基数。关键洞察涉及将具有相似权重的项目分组并使用充电论证：当查询集错过最优项目时，要么用同一组中的查询项目替换它，要么利用该组的超额贡献来补偿损失。

Result: 证明了背包、多重背包和广义分配问题存在(1-ε)近似稀疏器，其度与1/p和1/ε的多项式相关，与问题维度无关。这揭示了一个有趣的复杂性理论分离——虽然多重背包问题缺乏FPTAS且广义分配是APX难的，但它们的稀疏化对应物却存在高效的(1-ε)近似算法，能够识别多项式度的查询集。

Conclusion: 提出的多面体稀疏化框架成功解决了背包类型约束的稀疏化问题，提供了与问题维度无关的稀疏度保证。最后提出了一个开放性问题：这种稀疏化是否可以扩展到一般整数线性规划，且度与问题维度无关？

Abstract: When uncertainty meets costly information gathering, a fundamental question emerges: which data points should we probe to unlock near-optimal solutions? Sparsification of stochastic packing problems addresses this trade-off. The existing notions of sparsification measure the level of sparsity, called degree, as the ratio of queried items to the optimal solution size. While effective for matching and matroid-type problems with uniform structures, this cardinality-based approach fails for knapsack-type constraints where feasible sets exhibit dramatic structural variation. We introduce a polyhedral sparsification framework that measures the degree as the smallest scalar needed to embed the query set within a scaled feasibility polytope, naturally capturing redundancy without relying on cardinality.
  Our main contribution establishes that knapsack, multiple knapsack, and generalized assignment problems admit (1 - epsilon)-approximate sparsifiers with degree polynomial in 1/p and 1/epsilon -- where p denotes the independent activation probability of each element -- remarkably independent of problem dimensions. The key insight involves grouping items with similar weights and deploying a charging argument: when our query set misses an optimal item, we either substitute it with a queried item from the same group or leverage that group's excess contribution to compensate for the loss. This reveals an intriguing complexity-theoretic separation -- while the multiple knapsack problem lacks an FPTAS and generalized assignment is APX-hard, their sparsification counterparts admit efficient (1 - epsilon)-approximation algorithms that identify polynomial-degree query sets. Finally, we raise an open question: can such sparsification extend to general integer linear programs with degree independent of problem dimensions?

中文标题: 随机背包和分配问题的近最优稀疏器

中文摘要: 当不确定性遇到昂贵的信息收集时，一个基本问题出现：我们应该探测哪些数据点以获得接近最优的解决方案？随机打包问题的稀疏化解决了这一权衡。现有的稀疏化概念通过查询项与最优解大小的比率来衡量稀疏度（称为度）。虽然这对于具有统一结构的匹配和拟阵类型问题有效，但这种基于基数的方法对于可行集结构变化剧烈的背包类型约束则失效。我们引入了一个多面体稀疏化框架，将度定义为将查询集嵌入到缩放可行性多面体所需的最小标量，自然地捕捉冗余性而不依赖基数。
我们的主要贡献是证明背包、多重背包和广义分配问题存在(1-ε)近似稀疏器，其度与1/p和1/ε的多项式相关——其中p表示每个元素的独立激活概率——显著独立于问题维度。关键洞察涉及将具有相似权重的项目分组并使用充电论证：当我们的查询集错过一个最优项目时，要么用同一组中的查询项目替换它，要么利用该组的超额贡献来补偿损失。这揭示了一个有趣的复杂性理论分离——虽然多重背包问题缺乏FPTAS且广义分配是APX难的，但它们的稀疏化对应物却存在高效的(1-ε)近似算法，能够识别多项式度的查询集。最后，我们提出了一个开放性问题：这种稀疏化是否可以扩展到一般整数线性规划，且度与问题维度无关？

</details>


### [265] [Separator Theorem for Minor-Free Graphs in Linear Time](https://arxiv.org/abs/2512.01587)
*Édouard Bonnet,Tuukka Korhonen,Hung Le,Jason Li,Tomáš Masařík*

Main category: cs.DS

TL;DR: 该论文解决了无小图图类中寻找平衡分隔器的线性时间算法问题，通过顶点加权BFS方法在O(n)时间内找到O(√n)大小的平衡分隔器。


<details>
  <summary>Details</summary>
Motivation: 平面图分隔器定理（Lipton-Tarjan）在线性时间内找到O(√n)大小的平衡分隔器，但将其推广到无小图图类时，Alon等人的算法需要O(n^{3/2})时间。过去二十多年中，寻找线性时间算法一直是一个主要开放问题。

Method: 提出了一种简单的算法：在输入图上运行顶点加权广度优先搜索（BFS）常数次。关键技术贡献是设计了一个顶点加权方案，该方案在寻找平衡分隔器的大小与存在团小图模型之间建立了新的联系。

Result: 成功解决了这一开放问题，为无小图图类提供了在线性时间O(n)内找到O(√n)大小平衡分隔器的算法。

Conclusion: 该论文通过创新的顶点加权方案和简单的BFS算法，解决了无小图图类平衡分隔器的线性时间构造问题，为将平面图算法推广到更广泛图类提供了关键工具。

Abstract: The planar separator theorem by Lipton and Tarjan [FOCS '77, SIAM Journal on Applied Mathematics '79] states that any planar graph with $n$ vertices has a balanced separator of size $O(\sqrt{n})$ that can be found in linear time. This landmark result kicked off decades of research on designing linear or nearly linear-time algorithms on planar graphs. In an attempt to generalize Lipton-Tarjan's theorem to nonplanar graphs, Alon, Seymour, and Thomas [STOC '90, Journal of the AMS '90] showed that any minor-free graph admits a balanced separator of size $O(\sqrt{n})$ that can be found in $O(n^{3/2})$ time. The superlinear running time in their separator theorem is a key bottleneck for generalizing algorithmic results from planar to minor-free graphs. Despite extensive research for more than two decades, finding a balanced separator of size $O(\sqrt{n})$ in (linear) $O(n)$ time for minor-free graphs remains a major open problem. Known algorithms either give a separator of size much larger than $O(\sqrt{n})$ or have superlinear running time, or both.
  In this paper, we answer the open problem affirmatively. Our algorithm is very simple: it runs a vertex-weighted variant of breadth-first search (BFS) a constant number of times on the input graph. Our key technical contribution is a weighting scheme on the vertices to guide the search for a balanced separator, offering a new connection between the size of a balanced separator and the existence of a clique-minor model. We believe that our weighting scheme may be of independent interest.

中文标题: 无小图图类的线性时间分隔器定理

中文摘要: Lipton和Tarjan提出的平面图分隔器定理表明，任何具有n个顶点的平面图都有一个大小为O(√n)的平衡分隔器，并且可以在线性时间内找到。这一里程碑式的结果开启了数十年来在平面图上设计线性或近似线性时间算法的研究。为了将Lipton-Tarjan定理推广到非平面图，Alon、Seymour和Thomas证明了任何无小图图都允许一个大小为O(√n)的平衡分隔器，但需要在O(n^{3/2})时间内找到。他们分隔器定理中的超线性运行时间是平面图算法推广到无小图图类的主要瓶颈。尽管经过二十多年的广泛研究，为无小图图类在线性O(n)时间内找到大小为O(√n)的平衡分隔器仍然是一个主要的开放问题。已知算法要么给出远大于O(√n)的分隔器，要么具有超线性运行时间，或者两者兼有。

在本文中，我们肯定地回答了这个开放问题。我们的算法非常简单：它在输入图上运行顶点加权变体的广度优先搜索（BFS）常数次。我们的关键技术贡献是设计了一个顶点加权方案来指导寻找平衡分隔器，在平衡分隔器的大小与存在团小图模型之间建立了新的联系。我们相信我们的加权方案可能具有独立的研究价值。

</details>


### [266] [JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford](https://arxiv.org/abs/2512.01802)
*Xin Wang,Xi Chen*

Main category: cs.DS

TL;DR: JFR是一种基于Bellman-Ford的优化框架，通过前沿收缩和抽象多跳传播技术，在保持正确性的前提下显著减少松弛操作次数，加速最短路径计算。


<details>
  <summary>Details</summary>
Motivation: 传统Bellman-Ford算法在处理大规模图时效率较低，需要进行大量松弛操作。JFR旨在通过优化前沿管理和跳转传播机制，减少不必要的计算开销，提高算法性能。

Method: JFR采用前沿收缩策略来减少活跃节点集，并引入抽象多跳跳转传播机制，允许在单次迭代中传播多个跳的距离信息，从而减少整体松弛操作次数。

Result: 在稀疏、稠密和含负边图中，JFR能减少25%到99%的松弛操作。在包含2万个节点和2.95亿条边的超大规模图上，JFR保持了强操作减少，运行时间与SPFA-SLF相当或更优。

Conclusion: JFR通过前沿收缩和多跳传播策略有效减少了Bellman-Ford算法的计算开销，适用于需要高吞吐量或节能操作的场景。未来工作将集成高性能队列结构、自适应前沿策略和缓存感知技术。

Abstract: We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.

中文标题: JFR：一种用于Bellman-Ford的高效跳转前沿松弛策略

中文摘要: 我们提出了JFR，这是一个基于Bellman-Ford的优化框架，利用前沿收缩和抽象多跳跳转传播来加速最短路径计算，同时严格保持正确性。JFR在稀疏、稠密和含负边图中实现了松弛操作的显著减少，范围从25%到99%，确保即使在对抗性或高度连接的拓扑结构下也能保持稳健性能。在包含多达N=20,000个节点和2.95亿条边的超大规模图上，JFR保持了强操作减少，并且相对于SPFA-SLF具有相当或改进的运行时间，展示了在图大小和密度方面的一致稳健性。较低的松弛次数意味着减少的内存访问开销和计算工作量；这种标准化工作减少突显了JFR适用于需要高吞吐量或节能操作的场景。未来工作将专注于集成高性能队列结构、自适应前沿策略和缓存感知技术，以进一步减少常数因子开销并充分实现JFR的实际运行时间潜力。

</details>


### [267] [Tight Bounds for Feedback Vertex Set Parameterized by Clique-width](https://arxiv.org/abs/2512.01900)
*Narek Bojikian,Stefan Kratsch*

Main category: cs.DS

TL;DR: 该论文提出了新的无环性表示概念，并应用于反馈顶点集问题的参数化算法。主要贡献包括：针对团宽参数给出了反馈顶点集计数模2的O(6^kn^c)算法及匹配的SETH下界；针对树宽参数给出了O(3^kn^c)的模2计数算法；针对连通反馈顶点集问题给出了O(18^kn^c)算法及匹配下界。


<details>
  <summary>Details</summary>
Motivation: 反馈顶点集问题是图论中的经典NP难问题，在参数化复杂性研究中具有重要意义。虽然已有基于树宽和团宽的决策算法，但计数问题的复杂性尚未完全解决。特别是对于团宽参数，反馈顶点集计数模2的精确复杂度一直是一个开放问题，多次出现在文献中[ESA 23, ICALP 24, IPEC 25]。

Method: 论文引入了标记图中无环性表示的新概念，并开发了复杂的技术来处理团表达式中的合并操作。对于团宽参数，通过精心设计的子程序在表达式联合节点处合并部分解，实现了模2计数。对于树宽参数，改进了已有的cut-and-count技术。对于连通反馈顶点集问题，扩展了算法框架以处理连通性约束。

Result: 1. 针对团宽k，给出了O(6^kn^c)时间计算反馈顶点集各大小模2计数的算法，并证明了在SETH下的匹配下界，解决了文献中的开放问题。
2. 针对树宽k，给出了O(3^kn^c)时间计算反馈顶点集模2计数的算法，匹配了决策版本的已知SETH紧界。
3. 针对连通反馈顶点集问题，给出了O(18^kn^c)时间的单边错误蒙特卡洛算法及匹配的SETH下界。

Conclusion: 论文通过引入新的无环性表示概念，完全解决了反馈顶点集问题在团宽参数下的计数复杂性，填补了参数化算法领域的重要空白。同时为树宽参数下的模2计数提供了最优算法，并为连通反馈顶点集问题建立了紧界。这些结果为参数化复杂性理论提供了新的工具和见解。

Abstract: We introduce a new notion of acyclicity representation in labeled graphs, and present three applications thereof. Our main result is an algorithm that, given a graph $G$ and a $k$-clique expression of $G$, in time $O(6^kn^c)$ counts modulo $2$ the number of feedback vertex sets of $G$ of each size. We achieve this through an involved subroutine for merging partial solutions at union nodes in the expression. In the usual way this results in a one-sided error Monte-Carlo algorithm for solving the decision problem in the same time. We complement these by a matching lower bound under the Strong Exponential-Time Hypothesis (SETH). This closes an open question that appeared multiple times in the literature [ESA 23, ICALP 24, IPEC 25].
  We also present an algorithm that, given a graph $G$ and a tree decomposition of width $k$ of $G$, in time $O(3^kn^c)$ counts modulo $2$ the number of feedback vertex sets of $G$ of each size. This matches the known SETH-tight bound for the decision version, which was obtained using the celebrated cut-and-count technique [FOCS 11, TALG 22]. Unlike other applications of cut-and-count, which use the isolation lemma to reduce a decision problem to counting solutions modulo $2$, this bound was obtained via counting other objects, leaving the complexity of counting solutions modulo $2$ open.
  Finally, we present a one-sided error Monte-Carlo algorithm that, given a graph $G$ and a $k$-clique expression of $G$, in time $O(18^kn^c)$ decides the existence of a connected feedback vertex set of size $b$ in $G$. We provide a matching lower bound under SETH.

中文标题: 基于团宽的反馈顶点集参数化的紧界

中文摘要: 我们在标记图中引入了一种新的无环性表示概念，并展示了其三个应用。我们的主要结果是：给定图G及其k-团表达式，在O(6^kn^c)时间内计算G中每个大小的反馈顶点集数量的模2计数。这是通过在表达式联合节点处合并部分解的复杂子程序实现的。按照通常方式，这导致了一个解决决策问题的单边错误蒙特卡洛算法，具有相同的时间复杂度。我们在强指数时间假设(SETH)下给出了匹配的下界，这解决了文献中多次出现的开放问题[ESA 23, ICALP 24, IPEC 25]。

我们还提出了一个算法，给定图G及其宽度为k的树分解，在O(3^kn^c)时间内计算G中每个大小的反馈顶点集数量的模2计数。这与决策版本的已知SETH紧界相匹配，该界是通过著名的cut-and-count技术获得的[FOCS 11, TALG 22]。与其他使用隔离引理将决策问题简化为模2计数的cut-and-count应用不同，这个界是通过计数其他对象获得的，使得模2计数的复杂性一直保持开放。

最后，我们提出了一个单边错误蒙特卡洛算法，给定图G及其k-团表达式，在O(18^kn^c)时间内判断G中是否存在大小为b的连通反馈顶点集。我们在SETH下提供了匹配的下界。

</details>


### [268] [Adaptive Matrix Sparsification and Applications to Empirical Risk Minimization](https://arxiv.org/abs/2512.02003)
*Yang P. Liu,Richard Peng,Colin Tang,Albert Weng,Junzhao Yang*

Main category: cs.DS

TL;DR: 本文提出了一种高效求解经验风险最小化（ERM）问题的算法，通过动态数据结构维护矩阵杠杆分数上界估计，结合谱稀疏化技术，在近乎线性时间内实现高精度求解。


<details>
  <summary>Details</summary>
Motivation: 经验风险最小化是机器学习中的核心优化问题，传统方法在处理大规模密集矩阵时计算复杂度高。本文旨在开发一种近乎线性时间的算法，解决当矩阵A密集且n≥d¹⁰时的ERM问题，突破现有方法的计算瓶颈。

Method: 采用内点法（IPM）框架，结合动态数据结构维护矩阵杠杆分数上界估计。关键技术包括：1）设计新算法维护经历行更新的矩阵的杠杆分数上界；2）利用该数据结构在鲁棒IPM框架内采样谱稀疏化器；3）实现Õ(√n)次迭代的高效IPM。

Result: 算法在Õ(nd + d⁶√n) ≤ Õ(nd + d¹¹)时间内高精度求解ERM问题。当矩阵A密集且n ≥ d¹⁰时，这几乎是输入规模的线性时间。具体地，维护杠杆分数上界的数据结构能在Õ(nd + Td⁶)时间内处理T批总大小为n的行更新。

Conclusion: 本文通过开发新的动态数据结构来维护矩阵杠杆分数上界估计，结合谱稀疏化技术，实现了近乎线性时间的高精度ERM求解算法。该方法在处理大规模密集矩阵时具有显著优势，为经验风险最小化问题提供了高效的解决方案。

Abstract: Consider the empirical risk minimization (ERM) problem, which is stated as follows. Let $K_1, \dots, K_m$ be compact convex sets with $K_i \subseteq \mathbb{R}^{n_i}$ for $i \in [m]$, $n = \sum_{i=1}^m n_i$, and $n_i\le C_K$ for some absolute constant $C_K$. Also, consider a matrix $A \in \mathbb{R}^{n \times d}$ and vectors $b \in \mathbb{R}^d$ and $c \in \mathbb{R}^n$. Then the ERM problem asks to find \[ \min_{\substack{x \in K_1 \times \dots \times K_m\\ A^\top x = b}}
  c^\top x. \] We give an algorithm to solve this to high accuracy in time $\widetilde{O}(nd + d^6\sqrt{n}) \le \widetilde{O} (nd + d^{11})$, which is nearly-linear time in the input size when $A$ is dense and $n \ge d^{10}$.
  Our result is achieved by implementing an $\widetilde{O}(\sqrt{n})$-iteration interior point method (IPM) efficiently using dynamic data structures. In this direction, our key technical advance is a new algorithm for maintaining leverage score overestimates of matrices undergoing row updates. Formally, given a matrix $A \in \mathbb{R}^{n \times d}$ undergoing $T$ batches of row updates of total size $n$ we give an algorithm which can maintain leverage score overestimates of the rows of $A$ summing to $\widetilde{O}(d)$ in total time $\widetilde{O}(nd + Td^6)$. This data structure is used to sample a spectral sparsifier within a robust IPM framework to establish the main result.

中文标题: 自适应矩阵稀疏化及其在经验风险最小化中的应用

中文摘要: 考虑经验风险最小化（ERM）问题，其表述如下：设$K_1, \dots, K_m$为紧凸集，其中$K_i \subseteq \mathbb{R}^{n_i}$，$i \in [m]$，$n = \sum_{i=1}^m n_i$，且$n_i\le C_K$（$C_K$为绝对常数）。同时考虑矩阵$A \in \mathbb{R}^{n \times d}$和向量$b \in \mathbb{R}^d$、$c \in \mathbb{R}^n$。ERM问题要求找到\[ \min_{\substack{x \in K_1 \times \dots \times K_m\\ A^\top x = b}} c^\top x. \] 我们提出了一种算法，能在$\widetilde{O}(nd + d^6\sqrt{n}) \le \widetilde{O} (nd + d^{11})$时间内高精度求解该问题，当$A$密集且$n \ge d^{10}$时，这几乎是输入规模的线性时间。

我们的结果通过使用动态数据结构高效实现$\widetilde{O}(\sqrt{n})$次迭代的内点法（IPM）而实现。在这方面，我们的关键技术突破是提出了一种新算法，用于维护经历行更新的矩阵的杠杆分数上界估计。形式上，给定一个经历$T$批总大小为$n$的行更新的矩阵$A \in \mathbb{R}^{n \times d}$，我们提出了一种算法，可以在总时间$\widetilde{O}(nd + Td^6)$内维护$A$各行杠杆分数上界估计，其总和为$\widetilde{O}(d)$。该数据结构用于在鲁棒IPM框架内采样谱稀疏化器，从而建立主要结果。

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [269] [Beyond Performance: Probing Representation Dynamics In Speech Enhancement Models](https://arxiv.org/abs/2512.00482)
*Yair Amar,Amir Ivry,Israel Cohen*

Main category: eess.AS

TL;DR: 研究分析了语音增强模型在不同信噪比下的内部表征动态，发现编码器对噪声相对鲁棒，而潜在层和解码器对噪声敏感，揭示了深度依赖的鲁棒性-敏感性权衡。


<details>
  <summary>Details</summary>
Motivation: 传统语音增强研究主要关注性能指标，但对模型内部表征在不同噪声条件下的动态变化了解有限。本研究旨在深入探究语音增强模型在不同信噪比下的内部表征变化，揭示模型各层对噪声的响应特性。

Method: 使用在VoiceBank DEMAND数据集上训练的Transformer-卷积模型MUSE，在-10到30 dB的信噪比范围内扫描输入信号。分析编码器、潜在层、解码器和细化块的激活，采用中心核对齐（CKA）测量点对点表征相似性，使用扩散距离捕捉跨SNR的分布变化。

Result: 1. 编码器CKA在噪声和干净输入之间保持稳定；2. 潜在层和解码器CKA随SNR降低而急剧下降；3. CKA与SNR的线性拟合显示深度依赖的鲁棒性-敏感性权衡；4. 扩散距离在每个层内随SNR逐渐变化，但层间差异显著，尤其在低SNR时。

Conclusion: 噪声水平会差异化地激活语音增强模型的不同区域，并诱导不同的层间动态。这些发现为开发SNR感知的条件化和细化策略提供了理论基础，有助于设计更鲁棒的语音增强系统。

Abstract: We probe internal representations of a speech enhancement (SE) model across noise conditions. Using MUSE, a transformer-convolutional model trained on VoiceBank DEMAND, we analyze activations in encoder, latent, decoder, and refinement blocks while sweeping input signal-to-noise-ratios (SNRs) from -10 to 30 dB. We use Centered Kernel Alignment (CKA) to measure point-wise representation similarity and diffusion distance to capture distributional shifts across SNRs. Results show that the encoder CKA between noisy and clean inputs remains stable and latent and decoder CKA drop sharply as SNR decreases. Linear fits of CKA versus SNR reveal a depth-dependent robustness-sensitivity trade-off. The diffusion distance varies incrementally with SNR within each layer but differs strongly across layers, especially at low SNRs. Together, these findings indicate that noise levels differentially activate model regions and induce distinct inter-layer dynamics, motivating SNR-aware conditioning and refinement strategies for SE.

中文标题: 超越性能：探索语音增强模型中的表征动态

中文摘要: 我们探究了语音增强（SE）模型在不同噪声条件下的内部表征。使用在VoiceBank DEMAND上训练的Transformer-卷积模型MUSE，我们分析了编码器、潜在层、解码器和细化块中的激活，同时将输入信噪比（SNR）从-10到30 dB进行扫描。我们使用中心核对齐（CKA）来测量点对点表征相似性，并使用扩散距离来捕捉跨SNR的分布变化。结果显示，噪声输入和干净输入之间的编码器CKA保持稳定，而潜在层和解码器的CKA随着SNR降低而急剧下降。CKA与SNR的线性拟合揭示了深度依赖的鲁棒性-敏感性权衡。扩散距离在每个层内随SNR逐渐变化，但在不同层之间差异显著，尤其是在低SNR时。这些发现共同表明，噪声水平会差异化地激活模型区域并诱导不同的层间动态，这为SE中的SNR感知条件化和细化策略提供了动机。

</details>


### [270] [A Low-Complexity Speech Codec Using Parametric Dithering for ASR](https://arxiv.org/abs/2512.00511)
*Ellison Murray,Morriel Kasher,Predrag Spasojevic*

Main category: eess.AS

TL;DR: 本文提出了一种用于自动语音识别（ASR）的低复杂度语音编解码器，采用参数化抖动技术来改善压缩语音的ASR性能，在1-3比特分辨率下显著降低字符错误率。


<details>
  <summary>Details</summary>
Motivation: 抖动技术常用于改善有损数据压缩的感知质量，但缺乏对ASR输入压缩场景的系统研究。本文旨在分析抖动对ASR性能的影响，并开发适用于ASR的低复杂度语音压缩方案。

Method: 首先形式化理解有损输入压缩下的最优ASR性能，然后提出参数化抖动技术，应用于低复杂度语音压缩流水线。该方法在1-3比特分辨率下测试，通过不同抖动选择优化性能和数据率。

Result: 在1比特分辨率下实现25%的相对字符错误率（CER）改善，2比特和3比特分辨率下分别实现32.4%和33.5%的改善。第二种抖动选择还能降低数据率，编解码器可根据性能目标或熵约束进行调整。

Conclusion: 参数化抖动技术能有效改善压缩语音的ASR性能，提出的低复杂度编解码器在多种比特率下均表现优异，且具有适应不同性能目标和约束的灵活性。

Abstract: Dithering is a technique commonly used to improve the perceptual quality of lossy data compression. In this work, we analytically and experimentally justify the use of dithering for ASR input compression. We formalize an understanding of optimal ASR performance under lossy input compression and leverage this to propose a parametric dithering technique for a low-complexity speech compression pipeline. The method performs well at 1-bit resolution, showing a 25\% relative CER improvement, while also demonstrating improvements of 32.4\% and 33.5\% at 2- and 3-bit resolution, respectively, with our second dither choice yielding a reduced data rate. The proposed codec is adaptable to meet performance targets or stay within entropy constraints.

中文标题: 用于ASR的低复杂度语音编解码器：基于参数化抖动技术

中文摘要: 抖动是一种常用于改善有损数据压缩感知质量的技术。在本工作中，我们通过分析和实验验证了抖动在ASR输入压缩中的应用价值。我们形式化地理解了有损输入压缩下的最优ASR性能，并利用这一理解提出了一种用于低复杂度语音压缩流水线的参数化抖动技术。该方法在1比特分辨率下表现良好，显示出25%的相对字符错误率改善，同时在2比特和3比特分辨率下分别实现了32.4%和33.5%的改善，其中第二种抖动选择还能降低数据率。所提出的编解码器可根据性能目标或熵约束进行调整。

</details>


### [271] [Arabic TTS with FastPitch: Reproducible Baselines, Adversarial Training, and Oversmoothing Analysis](https://arxiv.org/abs/2512.00937)
*Lars Nippert*

Main category: eess.AS

TL;DR: 该研究为阿拉伯语TTS建立了基于FastPitch的可复现基线，提出了分析梅尔频谱图预测过平滑问题的倒谱域指标，并通过对抗性训练和多说话人合成技术改善了语音质量和韵律多样性。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语文本转语音面临资源有限和复杂语音模式的挑战，现有方法在梅尔频谱图预测中容易产生过平滑问题，导致输出过于平均化而缺乏细节。

Method: 基于FastPitch架构建立阿拉伯语TTS基线，提出倒谱域指标分析过平滑现象，引入轻量级对抗性频谱图损失来减少过平滑，并使用XTTSv2生成合成语音增强多说话人系统。

Result: 对抗性训练稳定且显著减少了过平滑问题，多说话人系统在不损失稳定性的情况下提高了韵律多样性，所有代码、预训练模型和训练方案均已公开。

Conclusion: 该研究为阿拉伯语TTS提供了可复现的基准系统，提出的对抗性训练方法有效解决了过平滑问题，多说话人扩展进一步提升了系统性能，为资源有限语言的研究提供了实用框架。

Abstract: Arabic text-to-speech (TTS) remains challenging due to limited resources and complex phonological patterns. We present reproducible baselines for Arabic TTS built on the FastPitch architecture and introduce cepstral-domain metrics for analyzing oversmoothing in mel-spectrogram prediction. While traditional Lp reconstruction losses yield smooth but over-averaged outputs, the proposed metrics reveal their temporal and spectral effects throughout training. To address this, we incorporate a lightweight adversarial spectrogram loss, which trains stably and substantially reduces oversmoothing. We further explore multi-speaker Arabic TTS by augmenting FastPitch with synthetic voices generated using XTTSv2, resulting in improved prosodic diversity without loss of stability. The code, pretrained models, and training recipes are publicly available at: https://github.com/nipponjo/tts-arabic-pytorch.

中文标题: 基于FastPitch的阿拉伯语TTS：可复现基线、对抗性训练与过平滑分析

中文摘要: 阿拉伯语文本转语音（TTS）由于资源有限和复杂的语音模式而仍然具有挑战性。我们基于FastPitch架构为阿拉伯语TTS建立了可复现的基线，并引入了倒谱域指标来分析梅尔频谱图预测中的过平滑问题。传统的Lp重建损失会产生平滑但过度平均化的输出，而提出的指标揭示了它们在训练过程中的时间和频谱效应。为了解决这个问题，我们引入了轻量级的对抗性频谱图损失，该损失训练稳定并显著减少了过平滑。我们进一步通过使用XTTSv2生成的合成语音增强FastPitch来探索多说话人阿拉伯语TTS，从而在不损失稳定性的情况下提高了韵律多样性。代码、预训练模型和训练方案已在https://github.com/nipponjo/tts-arabic-pytorch公开提供。

</details>


### [272] [Identifiability Conditions for Acoustic Feedback Cancellation with the Two-Channel Adaptive Feedback Canceller Algorithm](https://arxiv.org/abs/2512.01466)
*Arnout Roebben,Toon van Waterschoot,Jan Wouters,Marc Moonen*

Main category: eess.AS

TL;DR: 本文证明了双通道自适应反馈消除器（2ch-AFC）算法中反馈路径可辨识性的新条件：当前向路径前馈滤波器阶数超过AR模型阶数时，可实现可辨识性，且相关矩阵的条件数可作为可辨识性监测指标。


<details>
  <summary>Details</summary>
Motivation: 在音频信号处理中，当麦克风和扬声器处于同一声学环境时，扬声器信号会反馈到麦克风，形成闭环系统可能导致系统不稳定。传统PEM反馈消除算法需要前向路径足够时变或非线性，或前向路径延迟超过AR模型阶数才能正确识别反馈路径。本文旨在为2ch-AFC算法找到更一般的可辨识性条件。

Method: 本文研究双通道自适应反馈消除器（2ch-AFC）算法，这是一种基于预测误差方法（PEM）的反馈消除算法。通过理论分析，将传统的延迟条件推广为可逆性条件，证明当系统前向路径前馈滤波器阶数超过AR模型阶数时，反馈路径可辨识。同时提出使用相关矩阵的条件数作为可辨识性监测指标。

Result: 研究发现：1）对于2ch-AFC算法，反馈路径可辨识性条件可推广为可逆性条件；2）当前向路径前馈滤波器阶数超过AR模型阶数时，可实现反馈路径的可辨识性；3）算法中相关矩阵的条件数可作为监测可辨识性的有效指标。

Conclusion: 本文为2ch-AFC算法提供了更一般的反馈路径可辨识性条件，将传统的延迟条件推广为可逆性条件，并提出了基于相关矩阵条件数的可辨识性监测方法，为声学反馈消除系统的设计和分析提供了新的理论依据。

Abstract: In audio signal processing applications with a microphone and a loudspeaker within the same acoustic environment, the loudspeaker signals can feed back into the microphone, thereby creating a closed-loop system that potentially leads to system instability. To remove this acoustic coupling, prediction error method (PEM) feedback cancellation algorithms aim to identify the feedback path between the loudspeaker and the microphone by assuming that the input signal can be modelled by means of an autoregressive (AR) model. It has previously been shown that this PEM framework and resulting algorithms can identify the feedback path correctly in cases where the forward path from microphone to loudspeaker is sufficiently time-varying or non-linear, or when the forward path delay equals or exceeds the order of the AR model. In this paper, it is shown that this delay-based condition can be generalised for one particular PEM-based algorithm, the so-called two-channel adaptive feedback canceller (2ch-AFC), to an invertibility-based condition, for which it is shown that identifiability can be achieved when the order of the forward path feedforward filter exceeds the order of the AR model. Additionally, the condition number of inversion of the correlation matrix as used in the 2ch-AFC algorithm can serve as a measure for monitoring the identifiability.

中文标题: 双通道自适应反馈消除器算法的声学反馈消除可辨识性条件

中文摘要: 在麦克风和扬声器处于同一声学环境的音频信号处理应用中，扬声器信号可能反馈到麦克风，从而形成可能导致系统不稳定的闭环系统。为了消除这种声学耦合，预测误差方法（PEM）反馈消除算法旨在通过假设输入信号可以用自回归（AR）模型建模来识别扬声器和麦克风之间的反馈路径。先前研究表明，当前向路径（从麦克风到扬声器）足够时变或非线性，或者当前向路径延迟等于或超过AR模型阶数时，该PEM框架和所得算法可以正确识别反馈路径。本文证明，对于一种特定的基于PEM的算法——双通道自适应反馈消除器（2ch-AFC），这种基于延迟的条件可以推广为基于可逆性的条件，并且表明当前向路径前馈滤波器的阶数超过AR模型阶数时，可以实现可辨识性。此外，2ch-AFC算法中使用的相关矩阵的逆矩阵条件数可以作为监测可辨识性的度量指标。

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [273] [A Comprehensive Survey on Surgical Digital Twin](https://arxiv.org/abs/2512.00019)
*Afsah Sharaf Khan,Falong Fan,Doohwan DH Kim,Abdurrahman Alshareef,Dong Chen,Justin Kim,Ernest Carter,Bo Liu,Jerzy W. Rozenblit,Bernard Zeigler*

Main category: cs.RO

TL;DR: 这篇综述系统梳理了手术数字孪生（SDT）领域，提出了分类框架，总结了关键技术进展，并指出了临床转化面临的核心挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着多模态手术数据和实时计算能力的提升，手术数字孪生作为连接物理手术世界与虚拟模型的桥梁，在提升手术规划、导航、预测和术后管理方面展现出巨大潜力。然而该领域缺乏统一的术语体系、系统性的技术梳理以及明确的临床转化路径，需要一份全面的综述来指导未来发展。

Method: 采用结构化综述方法：首先澄清SDT的术语定义和范围边界；然后提出基于目的（预测、指导、优化）、模型保真度（几何、物理、生理）和数据源（影像、运动学、生理信号）的三维分类法；系统梳理了可变形配准与跟踪、实时仿真、AR/VR引导、边缘-云协同计算、AI场景理解等关键技术；对比分析了非机器人孪生与机器人闭环架构的差异。

Result: 建立了手术数字孪生的统一概念框架和分类体系；总结了在实时组织变形模拟、多模态数据融合、AI辅助决策等关键技术上的最新进展；识别了当前系统在延迟、保真度、鲁棒性、互操作性等方面的技术瓶颈；明确了验证标准缺失、安全保证不足、数据治理复杂等临床转化障碍。

Conclusion: 手术数字孪生正处于从实验室原型向临床常规应用转化的关键阶段。未来研究应聚焦于建立可信赖的SDT系统，包括：开发标准化验证基准、构建安全保证框架、实现全生命周期"数字线程"集成、建立可扩展的数据治理体系，最终目标是实现能够提供可测量临床效益且符合医疗标准的手术数字孪生系统。

Abstract: With the accelerating availability of multimodal surgical data and real-time computation, Surgical Digital Twins (SDTs) have emerged as virtual counterparts that mirror, predict, and inform decisions across pre-, intra-, and postoperative care. Despite promising demonstrations, SDTs face persistent challenges: fusing heterogeneous imaging, kinematics, and physiology under strict latency budgets; balancing model fidelity with computational efficiency; ensuring robustness, interpretability, and calibrated uncertainty; and achieving interoperability, privacy, and regulatory compliance in clinical environments. This survey offers a critical, structured review of SDTs. We clarify terminology and scope, propose a taxonomy by purpose, model fidelity, and data sources, and synthesize state-of-the-art achievements in deformable registration and tracking, real-time simulation and co-simulation, AR/VR guidance, edge-cloud orchestration, and AI for scene understanding and prediction. We contrast non-robotic twins with robot-in-the-loop architectures for shared control and autonomy, and identify open problems in validation and benchmarking, safety assurance and human factors, lifecycle "digital thread" integration, and scalable data governance. We conclude with a research agenda toward trustworthy, standards-aligned SDTs that deliver measurable clinical benefit. By unifying vocabulary, organizing capabilities, and highlighting gaps, this work aims to guide SDT design and deployment and catalyze translation from laboratory prototypes to routine surgical care.

中文标题: 手术数字孪生综合调查

中文摘要: 随着多模态手术数据和实时计算的加速可用性，手术数字孪生（SDTs）作为虚拟对应物应运而生，能够在术前、术中和术后护理中反映、预测和指导决策。尽管有前景的演示，SDTs仍面临持续挑战：在严格的延迟预算下融合异构成像、运动学和生理数据；平衡模型保真度与计算效率；确保鲁棒性、可解释性和校准的不确定性；以及在临床环境中实现互操作性、隐私和监管合规性。本调查提供了对SDTs的批判性、结构化综述。我们澄清术语和范围，按目的、模型保真度和数据源提出分类法，并综合了在可变形配准和跟踪、实时仿真和协同仿真、AR/VR引导、边缘-云编排以及用于场景理解和预测的AI方面的最新成就。我们对比了非机器人孪生与用于共享控制和自主性的机器人闭环架构，并确定了在验证和基准测试、安全保证和人因工程、生命周期"数字线程"集成以及可扩展数据治理方面的开放问题。我们以研究议程作为结尾，旨在实现可信赖、符合标准且能提供可测量临床效益的SDTs。通过统一词汇、组织能力并突出差距，本工作旨在指导SDT的设计和部署，并促进从实验室原型到常规手术护理的转化。

</details>


### [274] [Foundation Models for Trajectory Planning in Autonomous Driving: A Review of Progress and Open Challenges](https://arxiv.org/abs/2512.00021)
*Kemal Oksuz,Alexandru Buburuzan,Anthony Knittel,Yuhan Yao,Puneet K. Dokania*

Main category: cs.RO

TL;DR: 本文综述了基于基础模型的自动驾驶轨迹规划方法，涵盖37种最新方法，分析了架构设计、方法论优势、能力限制，并评估了开源代码和数据集的可用性。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型的出现显著改变了自动驾驶技术，从传统手工设计转向统一的基于基础模型的方法，能够直接从原始传感器输入推断运动轨迹。这些新方法还可以整合自然语言作为额外模态，VLA模型是典型代表。本文旨在全面评估这类方法，为研究者和实践者提供系统分析。

Method: 通过统一的分类法对37种最近提出的轨迹规划基础模型方法进行系统审查，评估其架构设计选择、方法论优势、固有能力和限制。同时评估这些方法的源代码和数据集开放性，并提供配套网页按分类法整理方法。

Result: 综述涵盖了轨迹规划基础模型方法的完整图景，提供了系统分类和评估框架。配套网页（https://github.com/fiveai/FMs-for-driving-trajectories）按分类法整理了所有方法，为研究者和实践者提供了有价值的参考资源。

Conclusion: 基础模型正在变革自动驾驶轨迹规划领域，从手工设计转向统一的数据驱动方法。虽然这些方法展现出强大潜力，但仍面临挑战。本文的系统综述和分类框架为未来研究提供了基础，并强调了开源代码和数据集共享的重要性。

Abstract: The emergence of multi-modal foundation models has markedly transformed the technology for autonomous driving, shifting away from conventional and mostly hand-crafted design choices towards unified, foundation-model-based approaches, capable of directly inferring motion trajectories from raw sensory inputs. This new class of methods can also incorporate natural language as an additional modality, with Vision-Language-Action (VLA) models serving as a representative example. In this review, we provide a comprehensive examination of such methods through a unifying taxonomy to critically evaluate their architectural design choices, methodological strengths, and their inherent capabilities and limitations. Our survey covers 37 recently proposed approaches that span the landscape of trajectory planning with foundation models. Furthermore, we assess these approaches with respect to the openness of their source code and datasets, offering valuable information to practitioners and researchers. We provide an accompanying webpage that catalogs the methods based on our taxonomy, available at: https://github.com/fiveai/FMs-for-driving-trajectories

中文标题: 自动驾驶轨迹规划中的基础模型：进展回顾与开放挑战

中文摘要: 多模态基础模型的出现显著改变了自动驾驶技术，从传统且大多为手工设计的选择转向统一的基于基础模型的方法，能够直接从原始传感器输入推断运动轨迹。这类新方法还可以整合自然语言作为额外模态，其中视觉-语言-动作（VLA）模型是代表性示例。在本综述中，我们通过统一的分类法对此类方法进行全面审查，批判性评估其架构设计选择、方法论优势及其固有能力和限制。我们的调查涵盖了37种最近提出的方法，这些方法涵盖了基础模型轨迹规划的整个领域。此外，我们评估了这些方法的源代码和数据集的开放性，为实践者和研究者提供了有价值的信息。我们提供了一个配套网页，根据我们的分类法整理了这些方法，网址为：https://github.com/fiveai/FMs-for-driving-trajectories

</details>


### [275] [XFlowMP: Task-Conditioned Motion Fields for Generative Robot Planning with Schrodinger Bridges](https://arxiv.org/abs/2512.00022)
*Khang Nguyen,Minh Nhat Vu*

Main category: cs.RO

TL;DR: XFlowMP是一种基于Schrödinger桥的任务条件化生成式运动规划方法，将机器人轨迹演化建模为连接随机噪声和专家演示的熵流，能够生成平滑、无碰撞且动态可行的运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统的生成式运动规划方法难以同时整合高层语义和底层约束，特别是在任务配置与运动可控性之间的关联方面存在挑战。现有方法在结合任务语义和动态约束方面表现不足。

Method: XFlowMP利用Schrödinger桥作为条件流匹配，结合得分函数学习具有高阶动态的运动场，同时编码起点-目标配置。该方法将机器人轨迹演化建模为连接随机噪声和专家演示的熵流，给定查询任务配置。

Result: 在RobotPointMass基准测试中，相比次优基线，XFlowMP实现了53.79%更低的最大均值差异、36.36%更平滑的运动和39.88%更低的能耗，同时缩短了11.72%的短期规划时间。在LASA手写数据集的长时程运动中，保持了1.26%更低的最大均值差异、3.96%更平滑和31.97%更低的能耗。在Kinova Gen3机械臂上的实际应用验证了其鲁棒性。

Conclusion: XFlowMP通过Schrödinger桥框架成功整合了任务语义和动态约束，能够生成高质量、动态可行的机器人运动轨迹，在多个基准测试和实际应用中表现出优越性能。

Abstract: Generative robotic motion planning requires not only the synthesis of smooth and collision-free trajectories but also feasibility across diverse tasks and dynamic constraints. Prior planning methods, both traditional and generative, often struggle to incorporate high-level semantics with low-level constraints, especially the nexus between task configurations and motion controllability. In this work, we present XFlowMP, a task-conditioned generative motion planner that models robot trajectory evolution as entropic flows bridging stochastic noises and expert demonstrations via Schrodinger bridges given the inquiry task configuration. Specifically, our method leverages Schrodinger bridges as a conditional flow matching coupled with a score function to learn motion fields with high-order dynamics while encoding start-goal configurations, enabling the generation of collision-free and dynamically-feasible motions. Through evaluations, XFlowMP achieves up to 53.79% lower maximum mean discrepancy, 36.36% smoother motions, and 39.88% lower energy consumption while comparing to the next-best baseline on the RobotPointMass benchmark, and also reducing short-horizon planning time by 11.72%. On long-horizon motions in the LASA Handwriting dataset, our method maintains the trajectories with 1.26% lower maximum mean discrepancy, 3.96% smoother, and 31.97% lower energy. We further demonstrate the practicality of our method on the Kinova Gen3 manipulator, executing planning motions and confirming its robustness in real-world settings.

中文标题: XFlowMP：基于Schrödinger桥的任务条件化运动场用于生成式机器人规划

中文摘要: 生成式机器人运动规划不仅需要合成平滑且无碰撞的轨迹，还需要在不同任务和动态约束下保持可行性。传统的规划方法（包括传统方法和生成式方法）往往难以整合高层语义和底层约束，特别是在任务配置与运动可控性之间的关联方面。在这项工作中，我们提出了XFlowMP，一种任务条件化的生成式运动规划器，它将机器人轨迹演化建模为通过Schrödinger桥连接随机噪声和专家演示的熵流，给定查询任务配置。具体而言，我们的方法利用Schrödinger桥作为条件流匹配，结合得分函数学习具有高阶动态的运动场，同时编码起点-目标配置，从而能够生成无碰撞且动态可行的运动。通过评估，在RobotPointMass基准测试中，与次优基线相比，XFlowMP实现了53.79%更低的最大均值差异、36.36%更平滑的运动和39.88%更低的能耗，同时缩短了11.72%的短期规划时间。在LASA手写数据集的长时程运动中，我们的方法保持了1.26%更低的最大均值差异、3.96%更平滑和31.97%更低的能耗。我们进一步在Kinova Gen3机械臂上展示了我们方法的实用性，执行规划运动并确认了其在真实世界环境中的鲁棒性。

</details>


### [276] [Learning from Watching: Scalable Extraction of Manipulation Trajectories from Human Videos](https://arxiv.org/abs/2512.00024)
*X. Hu,G. Ye*

Main category: cs.RO

TL;DR: 提出了一种从人类操作视频中提取密集轨迹的新方法，结合大模型和点跟踪技术，为机器人学习提供可扩展的数据来源


<details>
  <summary>Details</summary>
Motivation: 传统机器人数据收集依赖真实机器人平台，成本高且劳动密集。虽然可以利用在线人类操作视频，但现有方法主要关注手部检测或物体姿态估计，未能充分利用视频中丰富的交互信息。

Method: 结合大型基础模型进行视频理解和点跟踪技术，提取操作过程中所有任务相关关键点的密集轨迹，更全面地利用互联网规模的人类演示视频。

Result: 实验结果表明，该方法能够准确跟踪整个操作过程中的关键点，为更可扩展和数据高效的机器人学习铺平道路。

Conclusion: 该方法能够从人类操作视频中提取密集轨迹，为机器人学习提供更丰富、可扩展的数据来源，降低数据收集成本。

Abstract: Collecting high-quality data for training large-scale robotic models typically relies on real robot platforms, which is labor-intensive and costly, whether via teleoperation or scripted demonstrations. To scale data collection, many researchers have turned to leveraging human manipulation videos available online. However, current methods predominantly focus on hand detection or object pose estimation, failing to fully exploit the rich interaction cues embedded in these videos. In this work, we propose a novel approach that combines large foundation models for video understanding with point tracking techniques to extract dense trajectories of all task-relevant keypoints during manipulation. This enables more comprehensive utilization of Internet-scale human demonstration videos. Experimental results demonstrate that our method can accurately track keypoints throughout the entire manipulation process, paving the way for more scalable and data-efficient robot learning.

中文标题: 从观察中学习：从人类视频中可扩展地提取操作轨迹

中文摘要: 为训练大规模机器人模型收集高质量数据通常依赖于真实机器人平台，无论是通过遥操作还是脚本演示，都既劳动密集又成本高昂。为了扩展数据收集规模，许多研究者转向利用在线可用的人类操作视频。然而，当前方法主要关注手部检测或物体姿态估计，未能充分利用这些视频中嵌入的丰富交互线索。在这项工作中，我们提出了一种新颖的方法，将用于视频理解的大型基础模型与点跟踪技术相结合，以提取操作过程中所有任务相关关键点的密集轨迹。这使得能够更全面地利用互联网规模的人类演示视频。实验结果表明，我们的方法能够准确跟踪整个操作过程中的关键点，为更可扩展和数据高效的机器人学习铺平道路。

</details>


### [277] [A Survey on Improving Human Robot Collaboration through Vision-and-Language Navigation](https://arxiv.org/abs/2512.00027)
*Nivedan Yakolli,Avinash Gautam,Abhijit Das,Yuankai Qi,Virendra Singh Shekhawat*

Main category: cs.RO

TL;DR: 本文对视觉语言导航（VLN）在机器人领域的进展进行全面综述，探讨如何通过VLN技术改善人机协作。尽管已有进步，但现有模型在多智能体系统中仍面临双向通信、歧义消解和协作决策等挑战。文章提出未来VLN系统应支持主动澄清、实时反馈和上下文推理，并采用去中心化决策框架以实现可扩展的多机器人协作。


<details>
  <summary>Details</summary>
Motivation: 视觉语言导航（VLN）作为多模态协作任务，要求智能体理解人类指令、在3D环境中导航并有效处理模糊信息。随着机器人技术的发展，如何通过VLN提升人机协作效率成为重要研究方向。当前模型在多智能体系统中的双向通信、歧义消解和协作决策方面仍存在不足，需要系统性的综述来梳理现状并指明未来方向。

Method: 本文采用文献综述方法，系统回顾了约200篇相关文献，对VLN在机器人领域的最新进展进行全面梳理。通过分析现有模型的局限性，提出改进方向，包括：1）采用先进的自然语言理解技术实现主动澄清和实时反馈；2）开发支持上下文推理的系统；3）设计具有动态角色分配的去中心化决策框架。

Result: 综述发现当前VLN模型在单智能体任务上取得显著进展，但在多机器人协作场景中面临挑战：1）缺乏有效的双向通信机制；2）难以处理指令歧义；3）协作决策能力有限。研究指出，通过整合先进的NLU技术、实时反馈系统和去中心化架构，可以显著提升多机器人系统的协作效率和适应性。

Conclusion: 未来VLN系统的发展应聚焦于：1）增强主动澄清和实时反馈能力；2）提升上下文理解和推理水平；3）采用去中心化决策框架支持动态角色分配。这些改进将推动VLN在医疗、物流、灾难响应等领域的实际应用，显著提升人机交互质量。

Abstract: Vision-and-Language Navigation (VLN) is a multi-modal, cooperative task requiring agents to interpret human instructions, navigate 3D environments, and communicate effectively under ambiguity. This paper presents a comprehensive review of recent VLN advancements in robotics and outlines promising directions to improve multi-robot coordination. Despite progress, current models struggle with bidirectional communication, ambiguity resolution, and collaborative decision-making in the multi-agent systems. We review approximately 200 relevant articles to provide an in-depth understanding of the current landscape. Through this survey, we aim to provide a thorough resource that inspires further research at the intersection of VLN and robotics. We advocate that the future VLN systems should support proactive clarification, real-time feedback, and contextual reasoning through advanced natural language understanding (NLU) techniques. Additionally, decentralized decision-making frameworks with dynamic role assignment are essential for scalable, efficient multi-robot collaboration. These innovations can significantly enhance human-robot interaction (HRI) and enable real-world deployment in domains such as healthcare, logistics, and disaster response.

中文标题: 通过视觉语言导航改善人机协作的研究综述

中文摘要: 视觉语言导航（VLN）是一项多模态协作任务，要求智能体能够解释人类指令、在3D环境中导航，并在模糊条件下进行有效沟通。本文对机器人领域VLN的最新进展进行全面回顾，并概述了改善多机器人协调的有前景方向。尽管取得进展，当前模型在多智能体系统中仍面临双向通信、歧义消解和协作决策的挑战。我们回顾了约200篇相关文献，以提供对当前格局的深入理解。通过本次综述，我们旨在提供一个全面的资源，激发VLN与机器人交叉领域的进一步研究。我们主张未来VLN系统应通过先进的自然语言理解技术支持主动澄清、实时反馈和上下文推理。此外，具有动态角色分配的去中心化决策框架对于可扩展、高效的多机器人协作至关重要。这些创新可以显著增强人机交互，并实现在医疗、物流和灾难响应等领域的实际部署。

</details>


### [278] [Intelligent Systems and Robotics: Revolutionizing Engineering Industries](https://arxiv.org/abs/2512.00033)
*Sathish Krishna Anumula,Sivaramkumar Ponnarangan,Faizal Nujumudeen,Ms. Nilakshi Deka,S. Balamuralitharan,M Venkatesh*

Main category: cs.RO

TL;DR: 智能系统与机器人技术的融合正在彻底改变工程产业，通过AI、ML和自主机器人提升效率、精度和适应性，影响制造业、土木、电气和机械工程领域。


<details>
  <summary>Details</summary>
Motivation: 工程产业面临效率、精度和适应性方面的挑战，需要新技术解决方案。智能系统与机器人技术的融合为解决这些问题提供了可能，但需要系统评估其实际影响和应用效果。

Method: 基于最新研究成果，提出工业智能机器人系统评估方法，通过案例研究和实际经验分析，探讨AI、ML和自主机器人技术在多个工程领域的应用。

Result: 智能机器人技术显著提升了工程产业的生产力、安全性和运营成本效益，但同时也面临尚未解决的技术和实施挑战。

Conclusion: 智能机器人技术不仅是技术变革，更是工程方法的革新，为工程产业带来了重要的新范式和方法论。

Abstract: A mix of intelligent systems and robotics is making engineering industries much more efficient, precise and able to adapt. How artificial intelligence (AI), machine learning (ML) and autonomous robotic technologies are changing manufacturing, civil, electrical and mechanical engineering is discussed in this paper. Based on recent findings and a suggested way to evaluate intelligent robotic systems in industry, we give an overview of how their use impacts productivity, safety and operational costs. Experience and case studies confirm the benefits this area brings and the problems that have yet to be solved. The findings indicate that intelligent robotics involves more than a technology change; it introduces important new methods in engineering.

中文标题: 智能系统与机器人技术：革新工程产业

中文摘要: 智能系统与机器人技术的结合正在使工程产业变得更加高效、精确和适应性强。本文讨论了人工智能（AI）、机器学习（ML）和自主机器人技术如何改变制造业、土木工程、电气工程和机械工程领域。基于最新研究成果和提出的工业智能机器人系统评估方法，我们概述了这些技术的应用对生产力、安全性和运营成本的影响。实际经验和案例研究证实了该领域带来的益处以及尚未解决的问题。研究结果表明，智能机器人技术不仅涉及技术变革，还引入了重要的工程新方法。

</details>


### [279] [Design And Control of A Robotic Arm For Industrial Applications](https://arxiv.org/abs/2512.00034)
*Sathish Krishna Anumula,SVSV Prasad Sanaboina,Ravi Kumar Nagula,R. Nagaraju*

Main category: cs.RO

TL;DR: 本文设计了一个六自由度工业机器人手臂，采用伺服电机和微控制器，通过运动学/动力学分析、逆运动学算法和PID控制实现高精度工业自动化任务。


<details>
  <summary>Details</summary>
Motivation: 工业环境对自动化流程的需求不断增长，特别是机器人手臂在装配、焊接和物料搬运等工业应用中的重要性日益凸显，需要开发经济实惠、可扩展且可靠的自动化解决方案。

Method: 设计六自由度机器人操纵器，使用伺服电机和微控制器接口，制造机械连杆；进行运动学和动力学分析；应用逆运动学算法和PID控制器提高控制精度；通过仿真和实验测试验证性能。

Result: 系统能够以高精度和可重复性执行任务，仿真和实验测试证实了其性能；机器人手臂具有经济实惠、可扩展和可靠的特点，适用于制造业常规程序的自动化。

Conclusion: 所设计的六自由度机器人手臂通过运动学/动力学分析、逆运动学算法和PID控制，成功实现了高精度工业自动化，为制造业提供了经济实惠、可扩展且可靠的自动化解决方案。

Abstract: The growing need to automate processes in industrial settings has led to tremendous growth in the robotic systems and especially the robotic arms. The paper assumes the design, modeling and control of a robotic arm to suit industrial purpose like assembly, welding and material handling. A six-degree-of-freedom (DOF) robotic manipulator was designed based on servo motors and a microcontroller interface with Mechanical links were also fabricated. Kinematic and dynamic analyses have been done in order to provide precise positioning and effective loads. Inverse Kinematics algorithm and Proportional-Integral-Derivative (PID) controller were also applied to improve the precision of control. The ability of the system to carry out tasks with high accuracy and repeatability is confirmed by simulation and experimental testing. The suggested robotic arm is an affordable, expandable, and dependable method of automation of numerous mundane procedures in the manufacturing industry.

中文标题: 工业应用机器人手臂的设计与控制

中文摘要: 工业环境中对流程自动化的日益增长需求导致了机器人系统，特别是机器人手臂的巨大发展。本文假设了机器人手臂的设计、建模和控制，以适应装配、焊接和物料搬运等工业用途。基于伺服电机和微控制器接口设计了一个六自由度机器人操纵器，并制造了机械连杆。进行了运动学和动力学分析，以提供精确定位和有效载荷。还应用了逆运动学算法和比例-积分-微分（PID）控制器来提高控制精度。通过仿真和实验测试证实了系统执行任务的高精度和可重复性。所建议的机器人手臂是一种经济实惠、可扩展且可靠的自动化方法，适用于制造业中的众多常规程序。

</details>


### [280] [VISTAv2: World Imagination for Indoor Vision-and-Language Navigation](https://arxiv.org/abs/2512.00041)
*Yanjia Huang,Xianshun Jiang,Xiangbo Gao,Mingyang Wu,Zhengzhong Tu*

Main category: cs.RO

TL;DR: VISTAv2是一个用于视觉语言导航的生成式世界模型，通过动作条件视频预测和指令对齐的价值融合，在潜在空间中高效生成在线价值地图来指导规划，不替代原有规划器而是提供补充指导。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像想象的VLN方法存在三个主要问题：1）缺乏在线、动作条件的预测能力；2）不产生明确的规划价值；3）许多方法用长时程目标替代规划器，导致系统脆弱且计算缓慢。需要一种既能保持规划器完整性，又能提供在线、动作条件预测和明确规划价值的方法。

Method: 采用动作感知的条件扩散Transformer视频预测器合成短时程未来视图，在VAE潜在空间中进行高效展开，使用蒸馏采样器和稀疏解码。通过视觉语言评分器将生成的未来视图与语言指令对齐，在可微分的想象力到价值头中融合多个展开，生成在线价值地图。该价值地图在分数级别与基础规划目标融合，提供可达性和风险感知的指导。

Result: 在MP3D和RoboTHOR数据集上评估，VISTAv2优于强基线方法。消融实验证明：1）动作条件想象力对性能至关重要；2）指令引导的价值融合显著提升效果；3）在线价值地图规划器是系统成功的关键组件。系统能在单个消费级GPU上高效运行。

Conclusion: VISTAv2为视觉语言导航提供了一种实用且可解释的方法，通过生成式世界模型和在线价值地图规划，在保持原有规划器完整性的同时，显著提升了导航性能。该方法展示了动作条件想象力、指令对齐和高效潜在空间展开的有效性。

Abstract: Vision-and-Language Navigation (VLN) requires agents to follow language instructions while acting in continuous real-world spaces. Prior image imagination based VLN work shows benefits for discrete panoramas but lacks online, action-conditioned predictions and does not produce explicit planning values; moreover, many methods replace the planner with long-horizon objectives that are brittle and slow. To bridge this gap, we propose VISTAv2, a generative world model that rolls out egocentric future views conditioned on past observations, candidate action sequences, and instructions, and projects them into an online value map for planning. Unlike prior approaches, VISTAv2 does not replace the planner. The online value map is fused at score level with the base objective, providing reachability and risk-aware guidance. Concretely, we employ an action-aware Conditional Diffusion Transformer video predictor to synthesize short-horizon futures, align them with the natural language instruction via a vision-language scorer, and fuse multiple rollouts in a differentiable imagination-to-value head to output an imagined egocentric value map. For efficiency, rollouts occur in VAE latent space with a distilled sampler and sparse decoding, enabling inference on a single consumer GPU. Evaluated on MP3D and RoboTHOR, VISTAv2 improves over strong baselines, and ablations show that action-conditioned imagination, instruction-guided value fusion, and the online value-map planner are all critical, suggesting that VISTAv2 offers a practical and interpretable route to robust VLN.

中文标题: VISTAv2：面向室内视觉语言导航的世界想象力

中文摘要: 视觉语言导航（VLN）要求智能体在连续的真实世界空间中遵循语言指令行动。先前基于图像想象的VLN工作在离散全景图上显示出优势，但缺乏在线、动作条件的预测，且不产生明确的规划值；此外，许多方法用长时程目标替代规划器，这些方法脆弱且缓慢。为弥补这一差距，我们提出VISTAv2，这是一个生成式世界模型，能够基于过去观察、候选动作序列和指令展开以自我为中心的未来视图，并将其投影到在线价值地图中进行规划。与先前方法不同，VISTAv2不替代规划器。在线价值地图在分数级别与基础目标融合，提供可达性和风险感知的指导。具体而言，我们采用动作感知的条件扩散Transformer视频预测器来合成短时程未来，通过视觉语言评分器将其与自然语言指令对齐，并在可微分的想象力到价值头中融合多个展开，输出想象的以自我为中心的价值地图。为提高效率，展开在VAE潜在空间中进行，采用蒸馏采样器和稀疏解码，使得在单个消费级GPU上进行推理成为可能。在MP3D和RoboTHOR上的评估表明，VISTAv2优于强基线，消融实验显示动作条件想象力、指令引导的价值融合和在线价值地图规划器都至关重要，这表明VISTAv2为稳健的VLN提供了一条实用且可解释的路径。

</details>


### [281] [Causal Reinforcement Learning based Agent-Patient Interaction with Clinical Domain Knowledge](https://arxiv.org/abs/2512.00048)
*Wenzheng Zhao,Ran Zhang,Ruth Palan Lopez,Shu-Fen Wung,Fengpei Yuan*

Main category: cs.RO

TL;DR: 提出了一种因果结构感知强化学习框架，将因果发现和推理融入策略优化，用于机器人辅助认知护理场景，相比传统无模型强化学习获得更高累积奖励和更稳定的患者状态。


<details>
  <summary>Details</summary>
Motivation: 在自适应医疗干预（如痴呆症护理）中，强化学习面临数据稀缺、决策需要可解释性、患者状态动态复杂且具有因果性等挑战。传统方法难以处理这些因果结构，需要更高效、可解释且稳健的决策方法。

Method: 提出了因果结构感知强化学习框架，将因果发现和推理明确集成到策略优化中。该方法使智能体能够学习并利用描述人类行为状态与机器人动作之间因果依赖关系的有向无环图，实现更高效、可解释和稳健的决策。在模拟的机器人辅助认知护理场景中验证，智能体与表现出动态情绪、认知和参与状态的虚拟患者交互。

Result: 实验结果显示，CRL智能体优于传统无模型强化学习基线，实现了更高的累积奖励，更一致地维持理想的患者状态，并表现出可解释的、符合临床的行为。CRL的性能优势在不同权重策略和超参数设置下保持稳健。此外，展示了轻量级LLM部署：将固定策略嵌入系统提示中，将推断状态映射到动作，无需LLM微调即可产生一致的支持性对话。

Conclusion: 这项工作展示了因果强化学习在人机交互应用中的潜力，特别是在可解释性、适应性和数据效率至关重要的领域。CRL框架为解决医疗干预中的因果复杂性提供了有前景的解决方案。

Abstract: Reinforcement Learning (RL) faces significant challenges in adaptive healthcare interventions, such as dementia care, where data is scarce, decisions require interpretability, and underlying patient-state dynamic are complex and causal in nature. In this work, we present a novel framework called Causal structure-aware Reinforcement Learning (CRL) that explicitly integrates causal discovery and reasoning into policy optimization. This method enables an agent to learn and exploit a directed acyclic graph (DAG) that describes the causal dependencies between human behavioral states and robot actions, facilitating more efficient, interpretable, and robust decision-making. We validate our approach in a simulated robot-assisted cognitive care scenario, where the agent interacts with a virtual patient exhibiting dynamic emotional, cognitive, and engagement states. The experimental results show that CRL agents outperform conventional model-free RL baselines by achieving higher cumulative rewards, maintaining desirable patient states more consistently, and exhibiting interpretable, clinically-aligned behavior. We further demonstrate that CRL's performance advantage remains robust across different weighting strategies and hyperparameter settings. In addition, we demonstrate a lightweight LLM-based deployment: a fixed policy is embedded into a system prompt that maps inferred states to actions, producing consistent, supportive dialogue without LLM finetuning. Our work illustrates the promise of causal reinforcement learning for human-robot interaction applications, where interpretability, adaptiveness, and data efficiency are paramount.

中文标题: 基于临床领域知识的因果强化学习智能体-患者交互

中文摘要: 强化学习在自适应医疗干预（如痴呆症护理）中面临重大挑战，其中数据稀缺、决策需要可解释性，且患者状态动态复杂且具有因果性。在这项工作中，我们提出了一个名为因果结构感知强化学习的新框架，明确将因果发现和推理集成到策略优化中。该方法使智能体能够学习并利用描述人类行为状态与机器人动作之间因果依赖关系的有向无环图，促进更高效、可解释和稳健的决策。我们在模拟的机器人辅助认知护理场景中验证了我们的方法，其中智能体与表现出动态情绪、认知和参与状态的虚拟患者交互。实验结果显示，CRL智能体优于传统无模型强化学习基线，实现了更高的累积奖励，更一致地维持理想的患者状态，并表现出可解释的、符合临床的行为。我们进一步证明，CRL的性能优势在不同权重策略和超参数设置下保持稳健。此外，我们展示了轻量级LLM部署：将固定策略嵌入系统提示中，将推断状态映射到动作，无需LLM微调即可产生一致的支持性对话。我们的工作展示了因果强化学习在人机交互应用中的潜力，其中可解释性、适应性和数据效率至关重要。

</details>


### [282] [Reinforcement Learning from Implicit Neural Feedback for Human-Aligned Robot Control](https://arxiv.org/abs/2512.00050)
*Suzie Kim*

Main category: cs.RO

TL;DR: 本文提出了一种从隐式神经反馈中进行强化学习（RLIHF）的新框架，利用脑电图（EEG）信号中的错误相关电位（ErrPs）作为连续、隐式的反馈信号，无需用户显式干预，解决了稀疏奖励条件下机器人策略学习的问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在稀疏奖励条件下难以学习有效策略，需要手动设计复杂的任务特定奖励函数。现有的基于人类反馈的强化学习（RLHF）方法依赖于显式反馈机制（如按钮按压或偏好标签），这会中断自然交互过程并给用户带来认知负担。

Method: 提出了RLIHF框架，使用非侵入性脑电图（EEG）信号，特别是错误相关电位（ErrPs），通过预训练的解码器将原始EEG信号转换为概率奖励组件，为机器人控制提供连续隐式反馈。在基于MuJoCo物理引擎的仿真环境中，使用Kinova Gen2机械臂执行复杂的拾取-放置任务进行评估。

Result: 使用解码的EEG反馈训练的智能体在性能上达到了与使用密集手动设计奖励训练的智能体相当的水平，验证了在交互式机器人中使用隐式神经反馈进行可扩展、人类对齐强化学习的潜力。

Conclusion: 隐式神经反馈为强化学习提供了一种有前景的替代方案，能够在不中断自然交互的情况下实现人类对齐的机器人控制，为交互式机器人领域的可扩展强化学习开辟了新途径。

Abstract: Conventional reinforcement learning (RL) approaches often struggle to learn effective policies under sparse reward conditions, necessitating the manual design of complex, task-specific reward functions. To address this limitation, reinforcement learning from human feedback (RLHF) has emerged as a promising strategy that complements hand-crafted rewards with human-derived evaluation signals. However, most existing RLHF methods depend on explicit feedback mechanisms such as button presses or preference labels, which disrupt the natural interaction process and impose a substantial cognitive load on the user. We propose a novel reinforcement learning from implicit human feedback (RLIHF) framework that utilizes non-invasive electroencephalography (EEG) signals, specifically error-related potentials (ErrPs), to provide continuous, implicit feedback without requiring explicit user intervention. The proposed method adopts a pre-trained decoder to transform raw EEG signals into probabilistic reward components, enabling effective policy learning even in the presence of sparse external rewards. We evaluate our approach in a simulation environment built on the MuJoCo physics engine, using a Kinova Gen2 robotic arm to perform a complex pick-and-place task that requires avoiding obstacles while manipulating target objects. The results show that agents trained with decoded EEG feedback achieve performance comparable to those trained with dense, manually designed rewards. These findings validate the potential of using implicit neural feedback for scalable and human-aligned reinforcement learning in interactive robotics.

中文标题: 基于隐式神经反馈的强化学习用于人类对齐的机器人控制

中文摘要: 传统的强化学习方法在稀疏奖励条件下往往难以学习有效策略，需要手动设计复杂的任务特定奖励函数。为解决这一限制，基于人类反馈的强化学习已成为一种有前景的策略，通过人类衍生的评估信号补充手工设计的奖励。然而，大多数现有的RLHF方法依赖于显式反馈机制，如按钮按压或偏好标签，这会中断自然交互过程并给用户带来巨大的认知负担。我们提出了一种新颖的基于隐式人类反馈的强化学习框架，利用非侵入性脑电图信号，特别是错误相关电位，提供连续、隐式的反馈，无需用户显式干预。该方法采用预训练的解码器将原始EEG信号转换为概率奖励组件，即使在存在稀疏外部奖励的情况下也能实现有效的策略学习。我们在基于MuJoCo物理引擎的仿真环境中评估了我们的方法，使用Kinova Gen2机械臂执行复杂的拾取-放置任务，该任务需要在操纵目标物体的同时避开障碍物。结果表明，使用解码的EEG反馈训练的智能体达到了与使用密集手动设计奖励训练的智能体相当的性能。这些发现验证了在交互式机器人中使用隐式神经反馈进行可扩展和人类对齐强化学习的潜力。

</details>


### [283] [An adaptive experience-based discrete genetic algorithm for multi-trip picking robot task scheduling in smart orchards](https://arxiv.org/abs/2512.00057)
*Peng Chen,Jing Liangb,Kang-Jia Qiao,Hui Song,Cai-Tong Yue,Kun-Jie Yu,Ponnuthurai Nagaratnam Suganthan,Witold Pedrycz*

Main category: cs.RO

TL;DR: 本文提出了一种自适应经验离散遗传算法（AEDGA），用于解决智能果园中的多行程采摘机器人任务调度问题。该算法通过集成负载-距离平衡初始化、基于聚类的局部搜索机制和经验自适应选择策略，显著提升了调度效率，在42个测试实例中优于8种先进算法。


<details>
  <summary>Details</summary>
Motivation: 智能机器人技术的创新推动了智能果园发展，但多机器人系统的高效调度面临复杂优化挑战。现有元启发式算法存在局部搜索冗余、收敛慢、易陷入局部最优等问题，特别是在大规模场景中表现不佳。因此需要开发更高效的算法来解决多行程采摘机器人任务调度问题。

Method: 提出自适应经验离散遗传算法（AEDGA），包含三个关键创新：1）集成负载-距离平衡初始化方法；2）基于聚类的局部搜索机制；3）经验自适应选择策略。为确保在完工时间约束下的解可行性，开发了通过三种不同框架实现的解修复策略。

Result: 在18个新建测试实例和24个现有测试问题上进行综合实验，结果表明AEDGA显著优于8种最先进的算法，证明了其在解决多行程采摘机器人任务调度问题上的优越性能。

Conclusion: AEDGA算法通过创新的初始化、局部搜索和自适应选择机制，有效解决了智能果园中多行程采摘机器人任务调度的复杂优化问题，为自动化收获系统提供了高效的调度解决方案。

Abstract: The continuous innovation of smart robotic technologies is driving the development of smart orchards, significantly enhancing the potential for automated harvesting systems. While multi-robot systems offer promising solutions to address labor shortages and rising costs, the efficient scheduling of these systems presents complex optimization challenges. This research investigates the multi-trip picking robot task scheduling (MTPRTS) problem. The problem is characterized by its provision for robot redeployment while maintaining strict adherence to makespan constraints, and encompasses the interdependencies among robot weight, robot load, and energy consumption, thus introducing substantial computational challenges that demand sophisticated optimization algorithms.To effectively tackle this complexity, metaheuristic approaches, which often utilize local search mechanisms, are widely employed. Despite the critical role of local search in vehicle routing problems, most existing algorithms are hampered by redundant local operations, leading to slower search processes and higher risks of local optima, particularly in large-scale scenarios. To overcome these limitations, we propose an adaptive experience-based discrete genetic algorithm (AEDGA) that introduces three key innovations: (1) integrated load-distance balancing initialization method, (2) a clustering-based local search mechanism, and (3) an experience-based adaptive selection strategy. To ensure solution feasibility under makespan constraints, we develop a solution repair strategy implemented through three distinct frameworks. Comprehensive experiments on 18 proposed test instances and 24 existing test problems demonstrate that AEDGA significantly outperforms eight state-of-the-art algorithms.

中文标题: 智能果园中多行程采摘机器人任务调度的自适应经验离散遗传算法

中文摘要: 智能机器人技术的持续创新正在推动智能果园的发展，显著增强了自动化收获系统的潜力。虽然多机器人系统为解决劳动力短缺和成本上升提供了有前景的解决方案，但这些系统的高效调度提出了复杂的优化挑战。本研究调查了多行程采摘机器人任务调度（MTPRTS）问题。该问题的特点是在严格遵守完工时间约束的同时允许机器人重新部署，并涵盖了机器人重量、机器人负载和能耗之间的相互依赖性，从而引入了需要复杂优化算法的重大计算挑战。为了有效应对这种复杂性，广泛采用通常利用局部搜索机制的元启发式方法。尽管局部搜索在车辆路径问题中起着关键作用，但大多数现有算法受到冗余局部操作的阻碍，导致搜索过程较慢和局部最优风险较高，特别是在大规模场景中。为了克服这些限制，我们提出了一种自适应经验离散遗传算法（AEDGA），引入了三个关键创新：（1）集成负载-距离平衡初始化方法，（2）基于聚类的局部搜索机制，以及（3）经验自适应选择策略。为了确保在完工时间约束下的解可行性，我们开发了通过三种不同框架实现的解修复策略。在18个提出的测试实例和24个现有测试问题上的综合实验表明，AEDGA显著优于八种最先进的算法。

</details>


### [284] [SpeedAug: Policy Acceleration via Tempo-Enriched Policy and RL Fine-Tuning](https://arxiv.org/abs/2512.00062)
*Taewook Nam,Sung Ju Hwang*

Main category: cs.RO

TL;DR: SpeedAug是一个基于强化学习的策略加速框架，通过速度增强演示预训练策略来构建包含不同执行节奏的行为先验，然后进行RL微调，显著提高了策略加速的样本效率。


<details>
  <summary>Details</summary>
Motivation: 当前机器人策略学习虽然能实现复杂操作，但策略执行速度往往滞后于硬件能力，因为收集更快演示的成本很高。现有方法重新解释动作序列以适应未见执行速度，会遇到与原始演示的分布偏移问题。强化学习虽能适应更快执行但无引导探索样本效率低。

Method: SpeedAug框架首先在速度增强的演示上预训练策略，构建包含不同任务执行节奏的行为先验，然后从这个节奏丰富的策略初始化进行强化学习微调，实现高效策略加速。

Result: 在机器人操作基准测试中，从这种节奏丰富策略初始化的RL微调显著提高了现有RL和策略加速方法的样本效率，同时保持高成功率。

Conclusion: SpeedAug通过构建节奏丰富的策略先验和RL微调，为策略加速提供了一种高效方法，解决了现有方法中的分布偏移和样本效率问题。

Abstract: Recent advances in robotic policy learning have enabled complex manipulation in real-world environments, yet the execution speed of these policies often lags behind hardware capabilities due to the cost of collecting faster demonstrations. Existing works on policy acceleration reinterpret action sequence for unseen execution speed, thereby encountering distributional shifts from the original demonstrations. Reinforcement learning is a promising approach that adapts policies for faster execution without additional demonstration, but its unguided exploration is sample inefficient. We propose SpeedAug, an RL-based policy acceleration framework that efficiently adapts pre-trained policies for faster task execution. SpeedAug constructs behavior prior that encompasses diverse tempos of task execution by pre-training a policy on speed-augmented demonstrations. Empirical results on robotic manipulation benchmarks show that RL fine-tuning initialized from this tempo-enriched policy significantly improves the sample efficiency of existing RL and policy acceleration methods while maintaining high success rate.

中文标题: SpeedAug：通过节奏丰富策略和RL微调实现策略加速

中文摘要: 机器人策略学习的最新进展使得在真实环境中实现复杂操作成为可能，然而这些策略的执行速度往往滞后于硬件能力，因为收集更快演示的成本很高。现有的策略加速工作重新解释动作序列以适应未见执行速度，从而遇到与原始演示的分布偏移。强化学习是一种有前景的方法，可以在没有额外演示的情况下适应更快的执行，但其无引导探索样本效率低。我们提出了SpeedAug，一个基于强化学习的策略加速框架，能够高效地适应预训练策略以实现更快的任务执行。SpeedAug通过在速度增强的演示上预训练策略来构建包含不同任务执行节奏的行为先验。机器人操作基准测试的实证结果表明，从这个节奏丰富策略初始化的RL微调显著提高了现有RL和策略加速方法的样本效率，同时保持高成功率。

</details>


### [285] [Enhancing Cognitive Robotics with Commonsense through LLM-Generated Preconditions and Subgoals](https://arxiv.org/abs/2512.00069)
*Ohad Bachner,Bar Gamliel*

Main category: cs.RO

TL;DR: 将大型语言模型与符号规划结合，让LLM为机器人任务提供常识性的前提条件和子目标，从而提高规划有效性和任务成功率


<details>
  <summary>Details</summary>
Motivation: 机器人在日常任务中经常失败，因为指令会跳过常识细节，如隐藏的前提条件和小型子目标。传统的符号规划器需要明确编写这些细节，这既耗时又不完整。

Method: 结合大型语言模型与符号规划。给定自然语言任务，LLM建议合理的前提条件和子目标。将这些建议转化为正式的规划模型，并在模拟中执行生成的计划。

Result: 与没有LLM步骤的基线规划器相比，该系统产生更多有效计划，实现更高的任务成功率，并在环境变化时更好地适应。

Conclusion: 将LLM常识添加到经典规划中可以使现实场景中的机器人行为更加可靠。

Abstract: Robots often fail at everyday tasks because instructions skip commonsense details like hidden preconditions and small subgoals. Traditional symbolic planners need these details to be written explicitly, which is time consuming and often incomplete. In this project we combine a Large Language Model with symbolic planning. Given a natural language task, the LLM suggests plausible preconditions and subgoals. We translate these suggestions into a formal planning model and execute the resulting plan in simulation. Compared to a baseline planner without the LLM step, our system produces more valid plans, achieves a higher task success rate, and adapts better when the environment changes. These results suggest that adding LLM commonsense to classical planning can make robot behavior in realistic scenarios more reliable.

中文标题: 通过LLM生成的前提条件和子目标增强认知机器人的常识能力

中文摘要: 机器人经常在日常任务中失败，因为指令会跳过常识细节，如隐藏的前提条件和小型子目标。传统的符号规划器需要明确编写这些细节，这既耗时又不完整。在本项目中，我们将大型语言模型与符号规划相结合。给定自然语言任务，LLM建议合理的前提条件和子目标。我们将这些建议转化为正式的规划模型，并在模拟中执行生成的计划。与没有LLM步骤的基线规划器相比，我们的系统产生更多有效计划，实现更高的任务成功率，并在环境变化时更好地适应。这些结果表明，将LLM常识添加到经典规划中可以使现实场景中的机器人行为更加可靠。

</details>


### [286] [Reconfigurable Auxetic Devices (RADs) for Robotic Surface Manipulation](https://arxiv.org/abs/2512.00072)
*Jacob Miske,Ahyan Maya,Ahnaf Inkiad,Jeffrey Ian Lipton*

Main category: cs.RO

TL;DR: 本文提出了一种用于机器人表面操作的可重构拉胀装置，通过可重构锁定或嵌入式伺服机构控制拉胀晶格形状，利用单元间间隙实现局部可变膨胀，实现了可变表面贴合性操作。


<details>
  <summary>Details</summary>
Motivation: 传统机器人表面使用正泊松比材料，而拉胀材料具有负泊松比特性，能在拉伸时多向膨胀，更适合实现贴合界面。然而现有拉胀结构缺乏可重构性和局部控制能力，限制了其在机器人表面操作中的应用。

Method: 采用可重构拉胀晶格结构，通过可重构锁定机制或嵌入式伺服机构进行欠驱动控制。利用单元之间的间隙实现局部晶格区域的可变膨胀，并通过激活函数模型模拟具有间隙的单元间耦合。

Result: 成功展示了可变表面贴合性的操作演示，实验结果表明该结构能够通过可变表面收缩和膨胀实现有效操作。系统模型验证了实验结果，证明了间隙保持的柔顺性优势。

Conclusion: 可重构拉胀结构为机器人表面操作提供了新的自适应解决方案，通过保持间隙柔顺性克服了传统拉胀结构的局限性，在自适应机器人结构领域开辟了新的应用机会。

Abstract: Robotic surfaces traditionally use materials with a positive Poisson's ratio to push and pull on a manipulation interface. Auxetic materials with a negative Poisson's ratio may expand in multiple directions when stretched and enable conformable interfaces. Here we demonstrate reconfigurable auxetic lattices for robotic surface manipulation. Our approach enables shape control through reconfigurable locking or embedded servos that underactuate an auxetic lattice structure. Variable expansion of local lattice areas is enabled by backlash between unit cells. Demonstrations of variable surface conformity are presented with characterization metrics. Experimental results are validated against a simplified model of the system, which uses an activation function to model intercell coupling with backlash. Reconfigurable auxetic structures are shown to achieve manipulation via variable surface contraction and expansion. This structure maintains compliance with backlash in contrast with previous work on auxetics, opening new opportunities in adaptive robotic structures for surface manipulation tasks.

中文标题: 用于机器人表面操作的可重构拉胀装置

中文摘要: 传统机器人表面使用具有正泊松比的材料来推动和拉动操作界面。具有负泊松比的拉胀材料在拉伸时可能在多个方向上膨胀，从而实现贴合界面。本文展示了用于机器人表面操作的可重构拉胀晶格。我们的方法通过可重构锁定或嵌入式伺服机构实现形状控制，这些机构对拉胀晶格结构进行欠驱动。局部晶格区域的可变膨胀通过单元之间的间隙实现。我们展示了可变表面贴合性的演示，并提供了表征指标。实验结果通过系统简化模型进行验证，该模型使用激活函数来模拟具有间隙的单元间耦合。可重构拉胀结构被证明可以通过可变表面收缩和膨胀实现操作。与先前关于拉胀材料的工作相比，这种结构保持了具有间隙的柔顺性，为表面操作任务的自适应机器人结构开辟了新的机会。

</details>


### [287] [Bootstrap Dynamic-Aware 3D Visual Representation for Scalable Robot Learning](https://arxiv.org/abs/2512.00074)
*Qiwei Liang,Boyang Cai,Minghao Lai,Sitong Zhuang,Tao Lin,Yan Qin,Yixuan Ye,Jiaming Liang,Renjing Xu*

Main category: cs.RO

TL;DR: AFRO是一个自监督的3D视觉表示学习框架，通过联合建模正向和逆向动力学来学习动态感知的3D表示，无需动作或重建监督，显著提升机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉预训练方法在机器人操作任务上表现不佳，主要原因是缺乏状态-动作-状态动态建模以及显式几何重建带来的不必要冗余。需要一种能够学习动态感知3D表示的方法。

Method: AFRO将状态预测构建为生成扩散过程，在共享潜在空间中联合建模正向和逆向动力学以捕捉因果转移结构。采用特征差分和逆向一致性监督防止动作学习中的特征泄漏。

Result: AFRO与Diffusion Policy结合，在16个模拟任务和4个真实世界任务中显著提高操作成功率，优于现有预训练方法。框架在数据量和任务复杂度方面具有良好的可扩展性。

Conclusion: AFRO提供了一种有效的3D表示学习预训练解决方案，能够学习语义丰富、区分性强的特征，适用于机器人领域的3D表示学习。

Abstract: Despite strong results on recognition and segmentation, current 3D visual pre-training methods often underperform on robotic manipulation. We attribute this gap to two factors: the lack of state-action-state dynamics modeling and the unnecessary redundancy of explicit geometric reconstruction. We introduce AFRO, a self-supervised framework that learns dynamics-aware 3D representations without action or reconstruction supervision. AFRO casts state prediction as a generative diffusion process and jointly models forward and inverse dynamics in a shared latent space to capture causal transition structure. To prevent feature leakage in action learning, we employ feature differencing and inverse-consistency supervision, improving the quality and stability of visual features. When combined with Diffusion Policy, AFRO substantially increases manipulation success rates across 16 simulated and 4 real-world tasks, outperforming existing pre-training approaches. The framework also scales favorably with data volume and task complexity. Qualitative visualizations indicate that AFRO learns semantically rich, discriminative features, offering an effective pre-training solution for 3D representation learning in robotics. Project page: https://kolakivy.github.io/AFRO/

中文标题: 用于可扩展机器人学习的引导动态感知3D视觉表示

中文摘要: 尽管在识别和分割方面取得了强劲结果，但当前的3D视觉预训练方法在机器人操作任务上往往表现不佳。我们将这一差距归因于两个因素：缺乏状态-动作-状态动态建模以及显式几何重建的不必要冗余。我们引入了AFRO，这是一个自监督框架，无需动作或重建监督即可学习动态感知的3D表示。AFRO将状态预测构建为生成扩散过程，并在共享潜在空间中联合建模正向和逆向动力学以捕捉因果转移结构。为防止动作学习中的特征泄漏，我们采用特征差分和逆向一致性监督，提高视觉特征的质量和稳定性。当与Diffusion Policy结合时，AFRO在16个模拟任务和4个真实世界任务中显著提高操作成功率，优于现有的预训练方法。该框架在数据量和任务复杂度方面也具有良好的可扩展性。定性可视化表明，AFRO学习到了语义丰富、区分性强的特征，为机器人领域的3D表示学习提供了有效的预训练解决方案。项目页面：https://kolakivy.github.io/AFRO/

</details>


### [288] ["Why the face?": Exploring Robot Error Detection Using Instrumented Bystander Reactions](https://arxiv.org/abs/2512.00262)
*Maria Teresa Parreira,Ruidong Zhang,Sukruth Gowdru Lingaraju,Alexandra Bremers,Xuanyu Fang,Adolfo Ramirez-Aristizabal,Manaswi Saha,Michael Kuniavsky,Cheng Zhang,Wendy Ju*

Main category: cs.RO

TL;DR: 研究人员开发了一种颈部佩戴设备，通过下巴摄像头捕捉旁观者对机器人错误的反应，并构建了NeckNet-18模型进行3D面部重建，最终创建了优于传统方法的机器人错误检测系统。


<details>
  <summary>Details</summary>
Motivation: 人类通过观察同伴的微妙反应（如挑眉、笑声）来识别和纠正社交失误，但机器人难以感知和利用这些细微线索。本研究旨在探索如何通过捕捉旁观者的面部反应来帮助机器人检测自身错误。

Method: 开发颈部佩戴设备记录下巴区域的面部表情；构建NeckNet-18模型进行3D面部重建，将下巴摄像头捕捉的反应映射到面部关键点和头部运动；利用这些面部反应数据开发机器人错误检测模型。

Result: 基于颈部摄像头数据的机器人错误检测模型性能优于使用OpenFace或视频数据的标准方法，尤其在参与者内部数据上表现出良好的泛化能力。

Conclusion: 本研究论证了扩展人机交互中机器人感知能力的重要性，通过利用旁观者反应数据，可以促进机器人更无缝地融入多样化的人类环境，推动社交线索检测的边界，为适应性机器人技术开辟新途径。

Abstract: How do humans recognize and rectify social missteps? We achieve social competence by looking around at our peers, decoding subtle cues from bystanders - a raised eyebrow, a laugh - to evaluate the environment and our actions. Robots, however, struggle to perceive and make use of these nuanced reactions. By employing a novel neck-mounted device that records facial expressions from the chin region, we explore the potential of previously untapped data to capture and interpret human responses to robot error. First, we develop NeckNet-18, a 3D facial reconstruction model to map the reactions captured through the chin camera onto facial points and head motion. We then use these facial responses to develop a robot error detection model which outperforms standard methodologies such as using OpenFace or video data, generalizing well especially for within-participant data. Through this work, we argue for expanding human-in-the-loop robot sensing, fostering more seamless integration of robots into diverse human environments, pushing the boundaries of social cue detection and opening new avenues for adaptable robotics.

中文标题: "为何关注面部？"：利用仪器化旁观者反应探索机器人错误检测

中文摘要: 人类如何识别和纠正社交失误？我们通过观察周围同伴，解读旁观者的微妙线索——如挑眉、笑声——来评估环境和自身行为，从而实现社交能力。然而，机器人难以感知和利用这些细微反应。通过采用一种新型颈部佩戴设备记录下巴区域的面部表情，我们探索了利用先前未开发的数据来捕捉和解释人类对机器人错误反应的潜力。首先，我们开发了NeckNet-18，一个3D面部重建模型，将通过下巴摄像头捕捉的反应映射到面部关键点和头部运动。然后，我们利用这些面部反应开发了一个机器人错误检测模型，该模型性能优于使用OpenFace或视频数据的标准方法，尤其在参与者内部数据上表现出良好的泛化能力。通过这项工作，我们主张扩展人机交互中的机器人感知，促进机器人更无缝地融入多样化的人类环境，推动社交线索检测的边界，并为适应性机器人技术开辟新途径。

</details>


### [289] [A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs](https://arxiv.org/abs/2512.00077)
*Bowen Zhi*

Main category: cs.RO

TL;DR: 该论文提出了一种分层控制框架，通过结合学习式步态和基于模型的平衡控制，解决人形机器人配备额外肢体时的稳定性问题。低层采用模仿学习和课程学习生成步态，高层利用额外肢体进行动态平衡，显著提升了行走稳定性。


<details>
  <summary>Details</summary>
Motivation: 人形机器人配备额外肢体（Supernumerary Limbs, SLs）会引入动态扰动，对行走稳定性构成重大挑战。现有方法难以有效处理SLs带来的内部动态干扰，需要新的控制架构来维持稳定行走。

Method: 提出分层控制架构：低层通过模仿学习和课程学习为Unitree H1人形机器人生成行走步态；高层采用基于模型的平衡控制器，主动利用额外肢体进行动态平衡。该解耦策略将学习式步态与模型平衡相结合。

Result: 在三种条件下评估：基准步态（无负载）、静态负载步态、动态平衡控制。动态平衡控制器显著提升稳定性：与静态负载相比，步态模式更接近基准，质心轨迹DTW距离减少47%；改善步态周期内的重新稳定，实现更协调的反相地面反作用力模式。

Conclusion: 解耦的分层设计能有效缓解额外肢体的质量和运动产生的内部动态扰动，使配备功能肢体的人形机器人实现稳定行走。该方法为复杂机器人系统的稳定性控制提供了有效解决方案。

Abstract: The integration of Supernumerary Limbs (SLs) on humanoid robots poses a significant stability challenge due to the dynamic perturbations they introduce. This thesis addresses this issue by designing a novel hierarchical control architecture to improve humanoid locomotion stability with SLs. The core of this framework is a decoupled strategy that combines learning-based locomotion with model-based balancing. The low-level component consists of a walking gait for a Unitree H1 humanoid through imitation learning and curriculum learning. The high-level component actively utilizes the SLs for dynamic balancing. The effectiveness of the system is evaluated in a physics-based simulation under three conditions: baseline gait for an unladen humanoid (baseline walking), walking with a static SL payload (static payload), and walking with the active dynamic balancing controller (dynamic balancing). Our evaluation shows that the dynamic balancing controller improves stability. Compared to the static payload condition, the balancing strategy yields a gait pattern closer to the baseline and decreases the Dynamic Time Warping (DTW) distance of the CoM trajectory by 47\%. The balancing controller also improves the re-stabilization within gait cycles and achieves a more coordinated anti-phase pattern of Ground Reaction Forces (GRF). The results demonstrate that a decoupled, hierarchical design can effectively mitigate the internal dynamic disturbances arising from the mass and movement of the SLs, enabling stable locomotion for humanoids equipped with functional limbs. Code and videos are available here: https://github.com/heyzbw/HuSLs.

中文标题: 配备额外肢体的人形机器人分层行走控制框架

中文摘要: 在人形机器人上集成额外肢体（SLs）会引入动态扰动，带来显著的稳定性挑战。本文通过设计新颖的分层控制架构来解决这一问题，以提升配备SLs的人形机器人行走稳定性。该框架核心是结合学习式行走与基于模型平衡的解耦策略。低层组件通过模仿学习和课程学习为Unitree H1人形机器人生成行走步态；高层组件主动利用SLs进行动态平衡。在物理仿真中评估了三种条件下的系统有效性：无负载人形基准步态、静态SL负载行走、主动动态平衡控制行走。评估表明，动态平衡控制器提升了稳定性：与静态负载条件相比，平衡策略产生的步态模式更接近基准，质心轨迹动态时间规整距离减少47%。平衡控制器还改善了步态周期内的重新稳定，实现了更协调的反相地面反作用力模式。结果表明，解耦的分层设计能有效缓解SLs质量和运动产生的内部动态扰动，使配备功能肢体的人形机器人实现稳定行走。代码和视频见：https://github.com/heyzbw/HuSLs。

</details>


### [290] [MILE: A Mechanically Isomorphic Exoskeleton Data Collection System with Fingertip Visuotactile Sensing for Dexterous Manipulation](https://arxiv.org/abs/2512.00324)
*Jinda Du,Jieji Ren,Qiaojun Yu,Ningbin Zhang,Yu Deng,Xingyu Wei,Yufei Liu,Guoying Gu,Xiangyang Zhu*

Main category: cs.RO

TL;DR: MILE是一个机械同构的外骨骼数据收集系统，具有指尖视觉触觉传感功能，用于灵巧操作。该系统通过消除非线性重定向、保持关节位置同构性，实现了高精度、自然的控制，并集成了高分辨率指尖触觉模块，显著提高了遥操作成功率和数据收集效率。


<details>
  <summary>Details</summary>
Motivation: 模仿学习为灵巧手操作提供了有前景的方法，但其有效性受到大规模、高保真数据缺乏的限制。现有数据收集管道存在运动重定向不准确、数据收集效率低以及缺乏高分辨率指尖触觉传感等问题。

Method: MILE是一个从人手到外骨骼再到机器人手的机械同构遥操作和数据收集系统。外骨骼基于人手解剖结构设计，机器人手保持一对一的关节位置同构性，消除了非线性重定向。系统集成了紧凑的指尖视觉触觉模块，提供高分辨率触觉观测。基于这种无需重定向的接口，进行复杂、接触丰富的操作遥操作，高效收集包含高分辨率指尖视觉触觉信号、RGB-D图像和关节位置的多模态数据集。

Result: 外骨骼实现了多关节平均绝对角度误差低于1度。遥操作管道实现了平均成功率提高64%。加入指尖触觉观测后，相比仅视觉基线平均成功率进一步提高25%，验证了数据集的保真度和实用性。

Conclusion: MILE系统通过机械同构设计和集成高分辨率指尖触觉传感，解决了现有数据收集系统的局限性，为灵巧操作提供了高质量的多模态数据集，显著提高了遥操作性能。

Abstract: Imitation learning provides a promising approach to dexterous hand manipulation, but its effectiveness is limited by the lack of large-scale, high-fidelity data. Existing data-collection pipelines suffer from inaccurate motion retargeting, low data-collection efficiency, and missing high-resolution fingertip tactile sensing. We address this gap with MILE, a mechanically isomorphic teleoperation and data-collection system co-designed from human hand to exoskeleton to robotic hand. The exoskeleton is anthropometrically derived from the human hand, and the robotic hand preserves one-to-one joint-position isomorphism, eliminating nonlinear retargeting and enabling precise, natural control. The exoskeleton achieves a multi-joint mean absolute angular error below one degree, while the robotic hand integrates compact fingertip visuotactile modules that provide high-resolution tactile observations. Built on this retargeting-free interface, we teleoperate complex, contact-rich in-hand manipulation and efficiently collect a multimodal dataset comprising high-resolution fingertip visuotactile signals, RGB-D images, and joint positions. The teleoperation pipeline achieves a mean success rate improvement of 64%. Incorporating fingertip tactile observations further increases the success rate by an average of 25% over the vision-only baseline, validating the fidelity and utility of the dataset. Further details are available at: https://sites.google.com/view/mile-system.

中文标题: MILE：具有指尖视觉触觉传感的机械同构外骨骼数据收集系统用于灵巧操作

中文摘要: 模仿学习为灵巧手操作提供了一种有前景的方法，但其有效性受到大规模、高保真数据缺乏的限制。现有的数据收集管道存在运动重定向不准确、数据收集效率低以及缺乏高分辨率指尖触觉传感的问题。我们通过MILE解决了这一差距，这是一个从人手到外骨骼再到机器人手的机械同构遥操作和数据收集系统。外骨骼基于人手解剖结构设计，机器人手保持一对一的关节位置同构性，消除了非线性重定向，实现了精确、自然的控制。外骨骼实现了多关节平均绝对角度误差低于1度，而机器人手集成了紧凑的指尖视觉触觉模块，提供高分辨率触觉观测。基于这种无需重定向的接口，我们遥操作复杂、接触丰富的操作，并高效收集了包含高分辨率指尖视觉触觉信号、RGB-D图像和关节位置的多模态数据集。遥操作管道实现了平均成功率提高64%。加入指尖触觉观测后，相比仅视觉基线平均成功率进一步提高25%，验证了数据集的保真度和实用性。更多细节请访问：https://sites.google.com/view/mile-system。

</details>


### [291] [RealAppliance: Let High-fidelity Appliance Assets Controllable and Workable as Aligned Real Manuals](https://arxiv.org/abs/2512.00287)
*Yuzheng Gao,Yuxing Long,Lei Kang,Yuchong Guo,Ziyan Yu,Shangqing Mao,Jiyao Zhang,Ruihai Wu,Dongjiang Li,Hui Shen,Hao Dong*

Main category: cs.RO

TL;DR: RealAppliance数据集提供100个高保真家电资产，具有完整物理机制并与手册对齐，同时提出RealAppliance-Bench基准测试评估家电操作规划模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有家电仿真资产存在渲染质量差、机制不完整、与真实手册不对齐等问题，导致仿真与现实差距，阻碍家电操作开发研究。

Method: 创建包含100个高保真家电的RealAppliance数据集，具有完整物理、电子机制和程序逻辑；基于此提出RealAppliance-Bench基准测试，评估模型在手册检索、部件定位、开环和闭环规划等任务上的表现。

Result: 建立了高质量家电数据集和基准测试，为评估多模态大语言模型和具身操作规划模型在家电操作任务上的性能提供了标准化平台。

Conclusion: RealAppliance数据集和基准测试填补了家电操作研究领域的空白，为评估和推进家电操作规划模型提供了重要工具和见解。

Abstract: Existing appliance assets suffer from poor rendering, incomplete mechanisms, and misalignment with manuals, leading to simulation-reality gaps that hinder appliance manipulation development. In this work, we introduce the RealAppliance dataset, comprising 100 high-fidelity appliances with complete physical, electronic mechanisms, and program logic aligned with their manuals. Based on these assets, we propose the RealAppliance-Bench benchmark, which evaluates multimodal large language models and embodied manipulation planning models across key tasks in appliance manipulation planning: manual page retrieval, appliance part grounding, open-loop manipulation planning, and closed-loop planning adjustment. Our analysis of model performances on RealAppliance-Bench provides insights for advancing appliance manipulation research

中文标题: RealAppliance：让高保真家电资产可控且能像对齐的真实手册一样工作

中文摘要: 现有的家电资产存在渲染质量差、机制不完整以及与手册不对齐的问题，导致仿真与现实之间存在差距，阻碍了家电操作开发。在这项工作中，我们引入了RealAppliance数据集，包含100个高保真家电，具有完整的物理、电子机制和与其手册对齐的程序逻辑。基于这些资产，我们提出了RealAppliance-Bench基准测试，评估多模态大语言模型和具身操作规划模型在家电操作规划中的关键任务：手册页面检索、家电部件定位、开环操作规划和闭环规划调整。我们对模型在RealAppliance-Bench上表现的分析为推进家电操作研究提供了见解。

</details>


### [292] [Supporting Productivity Skill Development in College Students through Social Robot Coaching: A Proof-of-Concept](https://arxiv.org/abs/2512.01105)
*Himanshi Lalwani,Hanan Salam*

Main category: cs.RO

TL;DR: 本研究提出使用社交辅助机器人作为大学生生产力教练的概念验证，通过六节时间管理和任务优先级课程，结合聊天界面和语音交互，评估显示系统可用性得分79.2，用户体验和参与度高。


<details>
  <summary>Details</summary>
Motivation: 大学生常面临影响生产力和幸福感的学业挑战。现有自助书籍和生产力应用存在局限：书籍提供通用、非互动指导；应用缺乏教育性且可能阻碍关键组织技能发展。传统生产力教练虽能提供个性化支持，但资源密集且难以规模化。

Method: 开发社交辅助机器人作为教育教练，提供六节关于时间管理和任务优先级的课程。用户通过聊天界面交互，机器人通过语音响应（可切换选项）。集成仪表板监控进度、情绪、参与度、每课信心和用时，并提供个性化生产力洞察以促进反思和自我意识。

Result: 对15名大学生进行评估，获得系统可用性得分79.2，整体体验和参与度评分高。结果表明SAR生产力教练能有效改善大学生生产力。

Conclusion: 基于社交辅助机器人的生产力教练能够提供有效且可扩展的解决方案，以改善大学生的生产力，克服现有生产力工具和教练方法的局限性。

Abstract: College students often face academic challenges that hamper their productivity and well-being. Although self-help books and productivity apps are popular, they often fall short. Books provide generalized, non-interactive guidance, and apps are not inherently educational and can hinder the development of key organizational skills. Traditional productivity coaching offers personalized support, but is resource-intensive and difficult to scale. In this study, we present a proof-of-concept for a socially assistive robot (SAR) as an educational coach and a potential solution to the limitations of existing productivity tools and coaching approaches. The SAR delivers six different lessons on time management and task prioritization. Users interact via a chat interface, while the SAR responds through speech (with a toggle option). An integrated dashboard monitors progress, mood, engagement, confidence per lesson, and time spent per lesson. It also offers personalized productivity insights to foster reflection and self-awareness. We evaluated the system with 15 college students, achieving a System Usability Score of 79.2 and high ratings for overall experience and engagement. Our findings suggest that SAR-based productivity coaching can offer an effective and scalable solution to improve productivity among college students.

中文标题: 通过社交机器人教练支持大学生生产力技能发展：概念验证

中文摘要: 大学生经常面临影响生产力和幸福感的学业挑战。虽然自助书籍和生产力应用很受欢迎，但它们往往存在不足。书籍提供通用、非互动指导，而应用本身缺乏教育性，可能阻碍关键组织技能的发展。传统生产力教练提供个性化支持，但资源密集且难以规模化。在本研究中，我们提出了社交辅助机器人作为教育教练的概念验证，作为现有生产力工具和教练方法局限性的潜在解决方案。该机器人提供六节关于时间管理和任务优先级的课程。用户通过聊天界面交互，而机器人通过语音响应（带有切换选项）。集成仪表板监控进度、情绪、参与度、每课信心和用时，并提供个性化生产力洞察以促进反思和自我意识。我们对15名大学生评估了该系统，获得系统可用性得分79.2，整体体验和参与度评分高。我们的研究结果表明，基于社交辅助机器人的生产力教练能够提供有效且可扩展的解决方案，以改善大学生的生产力。

</details>


### [293] [Ethically-Aware Participatory Design of a Productivity Social Robot for College Students](https://arxiv.org/abs/2512.01111)
*Himanshi Lalwani,Hanan Salam*

Main category: cs.RO

TL;DR: 本研究采用参与式设计方法，结合伦理考量，为大学生设计生产力辅助社交机器人，特别关注ADHD学生的执行功能挑战。


<details>
  <summary>Details</summary>
Motivation: 大学生面临学业和生活压力影响生产力，特别是ADHD学生存在执行功能挑战。传统生产力工具需要持续自律和一致使用，许多学生难以坚持，导致应用切换行为。社交辅助机器人因其直观交互特性，在教育、认知发展和心理健康领域已有成功应用，有望在学术环境中支持生产力。

Method: 采用参与式设计方法，直接让大学生和学生成功与福祉教练参与设计过程。通过访谈和协作工作坊，收集生产力挑战的详细见解，并识别生产力导向社交机器人的理想功能。从设计初期就整合伦理考量，促进负责任且与用户一致的设计选择。

Result: 获得了对学生生产力挑战的全面见解、社交机器人设计偏好，以及有效机器人特性的可操作建议。同时提出了利益相关者衍生的伦理指南，为高等教育中生产力导向社交机器人的负责任未来实施提供指导。

Conclusion: 参与式设计结合伦理考量是开发有效生产力辅助社交机器人的关键方法，特别适用于支持ADHD学生和面临生产力挑战的大学生。研究结果为高等教育环境中负责任实施社交机器人提供了实用指南。

Abstract: College students often face academic and life stressors affecting productivity, especially students with Attention Deficit Hyperactivity Disorder (ADHD) who experience executive functioning challenges. Conventional productivity tools typically demand sustained self-discipline and consistent use, which many students struggle with, leading to disruptive app-switching behaviors. Socially Assistive Robots (SARs), known for their intuitive and interactive nature, offer promising potential to support productivity in academic environments, having been successfully utilized in domains like education, cognitive development, and mental health. To leverage SARs effectively in addressing student productivity, this study employed a Participatory Design (PD) approach, directly involving college students and a Student Success and Well-Being Coach in the design process. Through interviews and a collaborative workshop, we gathered detailed insights on productivity challenges and identified desirable features for a productivity-focused SAR. Importantly, ethical considerations were integrated from the onset, facilitating responsible and user-aligned design choices. Our contributions include comprehensive insights into student productivity challenges, SAR design preferences, and actionable recommendations for effective robot characteristics. Additionally, we present stakeholder-derived ethical guidelines to inform responsible future implementations of productivity-focused SARs in higher education.

中文标题: 面向大学生生产力辅助社交机器人的伦理意识参与式设计

中文摘要: 大学生常常面临影响生产力的学业和生活压力，特别是患有注意力缺陷多动障碍（ADHD）的学生，他们经历执行功能挑战。传统的生产力工具通常需要持续的自律和一致使用，许多学生难以做到这一点，导致破坏性的应用切换行为。社交辅助机器人以其直观和交互性而闻名，在学术环境中支持生产力方面具有潜在前景，已在教育、认知发展和心理健康等领域成功应用。为了有效利用社交辅助机器人解决学生生产力问题，本研究采用了参与式设计方法，直接让大学生和学生成功与福祉教练参与设计过程。通过访谈和协作工作坊，我们收集了关于生产力挑战的详细见解，并识别了生产力导向社交机器人的理想功能。重要的是，从设计初期就整合了伦理考量，促进了负责任且与用户一致的设计选择。我们的贡献包括对学生生产力挑战、社交机器人设计偏好的全面见解，以及有效机器人特性的可操作建议。此外，我们提出了利益相关者衍生的伦理指南，为高等教育中生产力导向社交机器人的负责任未来实施提供信息。

</details>


### [294] [Balancing Efficiency and Fairness: An Iterative Exchange Framework for Multi-UAV Cooperative Path Planning](https://arxiv.org/abs/2512.00410)
*Hongzong Li,Luwei Liao,Xiangguang Dai,Yuming Feng,Rong Feng,Shiqin Tang*

Main category: cs.RO

TL;DR: 提出了一种用于多无人机协同路径规划的迭代交换框架，通过任务交换和路径优化平衡效率（总距离）与公平性（完工时间），在多个地形数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多无人机协同路径规划需要同时考虑效率和公平性：既要最小化总任务成本（效率），又要平衡各无人机的工作负载避免个别过载（公平性）。现有方法往往难以在这两个目标间取得良好平衡。

Method: 提出迭代交换框架，构建结合总距离和完工时间的复合目标函数，在可行性和安全约束下通过局部任务交换迭代优化，使用A*搜索在地形感知配置空间中为每个无人机生成无碰撞轨迹。

Result: 在多个地形数据集上的实验表明，该方法在总距离和完工时间之间的权衡表现优于现有基线方法，能够更好地平衡效率与公平性。

Conclusion: 所提出的迭代交换框架能够有效解决多无人机协同路径规划中效率与公平性的平衡问题，通过任务交换和路径优化实现了更优的性能权衡。

Abstract: Multi-UAV cooperative path planning (MUCPP) is a fundamental problem in multi-agent systems, aiming to generate collision-free trajectories for a team of unmanned aerial vehicles (UAVs) to complete distributed tasks efficiently. A key challenge lies in achieving both efficiency, by minimizing total mission cost, and fairness, by balancing the workload among UAVs to avoid overburdening individual agents. This paper presents a novel Iterative Exchange Framework for MUCPP, balancing efficiency and fairness through iterative task exchanges and path refinements. The proposed framework formulates a composite objective that combines the total mission distance and the makespan, and iteratively improves the solution via local exchanges under feasibility and safety constraints. For each UAV, collision-free trajectories are generated using A* search over a terrain-aware configuration space. Comprehensive experiments on multiple terrain datasets demonstrate that the proposed method consistently achieves superior trade-offs between total distance and makespan compared to existing baselines.

中文标题: 平衡效率与公平性：面向多无人机协同路径规划的迭代交换框架

中文摘要: 多无人机协同路径规划（MUCPP）是多智能体系统中的基础问题，旨在为无人机团队生成无碰撞轨迹以高效完成分布式任务。关键挑战在于同时实现效率（通过最小化总任务成本）和公平性（通过平衡无人机间的工作负载以避免个别智能体过载）。本文提出了一种新颖的迭代交换框架用于MUCPP，通过迭代任务交换和路径优化来平衡效率与公平性。该框架构建了一个结合总任务距离和完工时间的复合目标函数，并在可行性和安全性约束下通过局部交换迭代改进解决方案。对于每个无人机，使用A*搜索在地形感知的配置空间中生成无碰撞轨迹。在多个地形数据集上的综合实验表明，与现有基线方法相比，所提方法在总距离和完工时间之间始终实现了更优的权衡。

</details>


### [295] [Sample-Efficient Expert Query Control in Active Imitation Learning via Conformal Prediction](https://arxiv.org/abs/2512.00453)
*Arad Firouzkouhi,Omid Mirzaeedodangeh,Lars Lindemann*

Main category: cs.RO

TL;DR: CRSAIL是一种主动模仿学习方法，通过符合性预测设置状态新颖性阈值，仅在访问状态在专家数据集中代表性不足时才查询专家，大幅减少查询成本同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 主动模仿学习中专家动作标注成本高昂，特别是在GPU密集型模拟器、人在环设置和机器人群体重复访问相似状态的情况下。现有方法要么查询过多（浪费资源），要么查询不足（性能下降），需要更高效的查询策略。

Method: 提出CRSAIL方法：1）使用到第K个最近专家状态的距离评分状态新颖性；2）通过符合性预测设置全局阈值（经验(1-α)分位数）；3）仅在状态新颖性超过阈值时查询专家；4）支持离线批量查询而非实时接管。

Result: 在MuJoCo机器人任务上，CRSAIL匹配或超过专家级奖励，同时与DAgger相比减少总专家查询高达96%，与先前主动模仿学习方法相比减少高达65%。方法对α和K参数具有鲁棒性。

Conclusion: CRSAIL提供了一种样本高效的主动模仿学习框架，通过符合性预测实现分布无关的校准，将查询率与可调参数α直接关联，显著降低专家标注成本，便于在具有未知动态的新系统上部署。

Abstract: Active imitation learning (AIL) combats covariate shift by querying an expert during training. However, expert action labeling often dominates the cost, especially in GPU-intensive simulators, human-in-the-loop settings, and robot fleets that revisit near-duplicate states. We present Conformalized Rejection Sampling for Active Imitation Learning (CRSAIL), a querying rule that requests an expert action only when the visited state is under-represented in the expert-labeled dataset. CRSAIL scores state novelty by the distance to the $K$-th nearest expert state and sets a single global threshold via conformal prediction. This threshold is the empirical $(1-α)$ quantile of on-policy calibration scores, providing a distribution-free calibration rule that links $α$ to the expected query rate and makes $α$ a task-agnostic tuning knob. This state-space querying strategy is robust to outliers and, unlike safety-gate-based AIL, can be run without real-time expert takeovers: we roll out full trajectories (episodes) with the learner and only afterward query the expert on a subset of visited states. Evaluated on MuJoCo robotics tasks, CRSAIL matches or exceeds expert-level reward while reducing total expert queries by up to 96% vs. DAgger and up to 65% vs. prior AIL methods, with empirical robustness to $α$ and $K$, easing deployment on novel systems with unknown dynamics.

中文标题: 基于符合性预测的主动模仿学习中样本高效专家查询控制

中文摘要: 主动模仿学习通过训练期间查询专家来应对协变量偏移问题。然而，专家动作标注通常占据主要成本，特别是在GPU密集型模拟器、人在环设置以及机器人群体重复访问近似重复状态的情况下。我们提出了基于符合性预测的主动模仿学习拒绝采样方法（CRSAIL），这是一种查询规则，仅在访问状态在专家标注数据集中代表性不足时才请求专家动作。CRSAIL通过到第K个最近专家状态的距离来评分状态新颖性，并通过符合性预测设置单一全局阈值。该阈值是在策略校准分数的经验(1-α)分位数，提供了一个与分布无关的校准规则，将α与预期查询率联系起来，并使α成为任务无关的调优旋钮。这种状态空间查询策略对异常值具有鲁棒性，并且与基于安全门的主动模仿学习不同，可以在无需实时专家接管的情况下运行：我们使用学习器展开完整轨迹（片段），然后仅对访问状态的子集查询专家。在MuJoCo机器人任务上的评估表明，CRSAIL匹配或超过专家级奖励，同时与DAgger相比减少总专家查询高达96%，与先前主动模仿学习方法相比减少高达65%，对α和K具有经验鲁棒性，便于在具有未知动态的新系统上部署。

</details>


### [296] [LAP: Fast LAtent Diffusion Planner with Fine-Grained Feature Distillation for Autonomous Driving](https://arxiv.org/abs/2512.00470)
*Jinhao Zhang,Wenlong Xia,Zhexuan Zhou,Youmin Gong,Jie Mei*

Main category: cs.RO

TL;DR: LAP是一种用于自动驾驶的快速潜在扩散规划器，通过在VAE学习的潜在空间中进行规划，将高级意图与低级运动学解耦，并引入细粒度特征蒸馏机制，实现单步去噪生成高质量规划，推理速度比之前SOTA方法快10倍。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在自动驾驶行为建模中表现出色，但其迭代采样过程导致显著延迟，且直接在原始轨迹点上操作迫使模型花费容量处理低级运动学而非高级多模态语义。

Method: 提出LAtent Planner (LAP)框架，在VAE学习的潜在空间中进行规划，解耦高级意图与低级运动学；引入细粒度特征蒸馏机制，指导高级语义规划空间与矢量化场景上下文之间的交互和融合；能够单步去噪生成高质量规划。

Result: 在nuPlan大规模基准测试中，LAP在学习型规划方法中实现了最先进的闭环性能，同时推理速度比之前SOTA方法快最多10倍。

Conclusion: LAP通过在潜在空间中进行规划并引入细粒度特征蒸馏，有效解决了扩散模型在自动驾驶规划中的延迟问题，同时保持了高质量的多模态行为建模能力。

Abstract: Diffusion models have demonstrated strong capabilities for modeling human-like driving behaviors in autonomous driving, but their iterative sampling process induces substantial latency, and operating directly on raw trajectory points forces the model to spend capacity on low-level kinematics, rather than high-level multi-modal semantics. To address these limitations, we propose LAtent Planner (LAP), a framework that plans in a VAE-learned latent space that disentangles high-level intents from low-level kinematics, enabling our planner to capture rich, multi-modal driving strategies. We further introduce a fine-grained feature distillation mechanism to guide a better interaction and fusion between the high-level semantic planning space and the vectorized scene context. Notably, LAP can produce high-quality plans in one single denoising step, substantially reducing computational overhead. Through extensive evaluations on the large-scale nuPlan benchmark, LAP achieves state-of-the-art closed-loop performance among learning-based planning methods, while demonstrating an inference speed-up of at most 10 times over previous SOTA approaches.

中文标题: LAP：基于细粒度特征蒸馏的快速潜在扩散规划器用于自动驾驶

中文摘要: 扩散模型在自动驾驶中展示了对人类驾驶行为建模的强大能力，但其迭代采样过程导致显著延迟，且直接在原始轨迹点上操作迫使模型花费容量处理低级运动学而非高级多模态语义。为解决这些限制，我们提出了潜在规划器(LAP)，这是一个在VAE学习的潜在空间中进行规划的框架，该空间将高级意图与低级运动学解耦，使我们的规划器能够捕捉丰富的多模态驾驶策略。我们进一步引入了细粒度特征蒸馏机制，以指导高级语义规划空间与矢量化场景上下文之间更好的交互和融合。值得注意的是，LAP可以在单一去噪步骤中生成高质量规划，大幅减少计算开销。通过在nuPlan大规模基准测试中的广泛评估，LAP在学习型规划方法中实现了最先进的闭环性能，同时展示了比之前SOTA方法最多10倍的推理加速。

</details>


### [297] [Fast, Robust, Permutation-and-Sign Invariant SO(3) Pattern Alignment](https://arxiv.org/abs/2512.00659)
*Anik Sarker,Alan T. Asbeck*

Main category: cs.RO

TL;DR: 提出了一种快速、鲁棒、对排列和符号不变的SO(3)模式对齐方法，通过将旋转分解为变换基向量，在球面上进行对齐，并引入PASI包装器处理轴重标记和符号翻转，实现线性复杂度的高效对齐。


<details>
  <summary>Details</summary>
Motivation: 解决SO(3)上两个旋转集的无对应对齐问题，该问题在校准和配准中常受时间对齐缺失、异常值和未知轴约定的阻碍。传统方法复杂度高且对轴重标记和符号翻转敏感。

Method: 将每个旋转分解为三个变换基向量（TBVs），在球面上使用快速鲁棒匹配器（SPMC、FRS和混合方法）进行每轴对齐。引入排列和符号不变（PASI）包装器枚举24种适当的带符号排列，通过求和相关性评分，并通过投影/Karcher平均将每轴估计融合为单个旋转。

Result: 在EuRoC Machine Hall模拟（轴一致）和ETH手眼基准测试（轴模糊）上，方法准确，比传统方法快6-60倍，在极端异常值比例（高达90%）下具有鲁棒性，且无需对应搜索。

Conclusion: 该方法实现了快速、鲁棒、对排列和符号不变的SO(3)模式对齐，线性复杂度使其适用于大规模应用，解决了校准和配准中的关键挑战。

Abstract: We address the correspondence-free alignment of two rotation sets on \(SO(3)\), a core task in calibration and registration that is often impeded by missing time alignment, outliers, and unknown axis conventions. Our key idea is to decompose each rotation into its \emph{Transformed Basis Vectors} (TBVs)-three unit vectors on \(S^2\)-and align the resulting spherical point sets per axis using fast, robust matchers (SPMC, FRS, and a hybrid). To handle axis relabels and sign flips, we introduce a \emph{Permutation-and-Sign Invariant} (PASI) wrapper that enumerates the 24 proper signed permutations, scores them via summed correlations, and fuses the per-axis estimates into a single rotation by projection/Karcher mean. The overall complexity remains linear in the number of rotations (\(\mathcal{O}(n)\)), contrasting with \(\mathcal{O}(N_r^3\log N_r)\) for spherical/\(SO(3)\) correlation. Experiments on EuRoC Machine Hall simulations
  (axis-consistent) and the ETH Hand-Eye benchmark (\texttt{robot\_arm\_real})
  (axis-ambiguous) show that our methods are accurate, 6-60x faster than traditional methods, and robust under extreme outlier ratios (up to 90\%), all without correspondence search.

中文标题: 快速、鲁棒、排列和符号不变的SO(3)模式对齐

中文摘要: 我们解决了SO(3)上两个旋转集的无对应对齐问题，这是校准和配准中的核心任务，常受时间对齐缺失、异常值和未知轴约定的阻碍。我们的核心思想是将每个旋转分解为其变换基向量（TBVs）——S^2上的三个单位向量——并使用快速鲁棒匹配器（SPMC、FRS和混合方法）在每轴上对齐生成的球面点集。为处理轴重标记和符号翻转，我们引入了排列和符号不变（PASI）包装器，枚举24种适当的带符号排列，通过求和相关性评分，并通过投影/Karcher平均将每轴估计融合为单个旋转。整体复杂度保持与旋转数量线性（O(n)），与球面/SO(3)相关的O(N_r^3 log N_r)形成对比。在EuRoC Machine Hall模拟（轴一致）和ETH手眼基准测试（robot_arm_real）（轴模糊）上的实验表明，我们的方法准确，比传统方法快6-60倍，在极端异常值比例（高达90%）下具有鲁棒性，且无需对应搜索。

</details>


### [298] [SAGAS: Semantic-Aware Graph-Assisted Stitching for Offline Temporal Logic Planning](https://arxiv.org/abs/2512.00775)
*Ruijia Liu,Ancheng Hou,Shaoyuan Li,Xiang Yin*

Main category: cs.RO

TL;DR: SAGAS是一个用于离线LTL规划的框架，通过构建潜在可达性图、增强语义信息、自动机引导搜索，从碎片化数据中合成满足复杂逻辑约束的轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有LTL规划方法需要精确动力学模型或在线交互，难以在仅使用固定、任务无关的碎片化轨迹数据集的离线设置中处理复杂逻辑约束。

Method: 1. 从学习的时间距离表示构建潜在可达性图；2. 使用认证锚节点和概率软标签增强图的语义信息；3. 将LTL规范转换为Büchi自动机；4. 在隐式乘积空间中搜索成本最小的前缀-后缀计划；5. 部署子目标条件的低级策略执行计划。

Result: 在OGBench运动域实验中，SAGAS成功为多样化LTL任务合成了高效轨迹，有效弥合了碎片化离线数据与复杂逻辑约束之间的差距。

Conclusion: SAGAS框架能够在离线、无模型设置下，仅使用任务无关的碎片化轨迹数据集，成功处理复杂的LTL约束控制问题，为机器人规划提供了新的解决方案。

Abstract: Linear Temporal Logic (LTL) provides a rigorous framework for complex robotic tasks, yet existing methods often rely on accurate dynamics models or expensive online interactions. In this work, we address LTL-constrained control in a challenging offline, model-free setting, utilizing only fixed, task-agnostic datasets of fragmented trajectories. We propose SAGAS, a novel framework combining graph-assisted trajectory stitching with automata-guided planning. First, we construct a latent reachability graph from a learned temporal-distance representation. To bridge the semantic gap, we augment this graph with certified anchor nodes and probabilistic soft labels. We then translate the specification into a Büchi automaton and search the implicit product space to derive a cost-minimal prefix-suffix plan. Finally, a subgoal-conditioned low-level policy is deployed to execute these latent waypoints. Experiments on OGBench locomotion domains demonstrate that SAGAS successfully synthesizes efficient trajectories for diverse LTL tasks, effectively bridging the gap between fragmented offline data and complex logical constraints.

中文标题: SAGAS：面向离线时序逻辑规划的语义感知图辅助拼接方法

中文摘要: 线性时序逻辑（LTL）为复杂机器人任务提供了严谨的框架，但现有方法通常依赖于精确的动力学模型或昂贵的在线交互。在本工作中，我们针对具有挑战性的离线、无模型设置下的LTL约束控制问题，仅使用固定的、任务无关的碎片化轨迹数据集。我们提出了SAGAS，这是一个结合图辅助轨迹拼接与自动机引导规划的新颖框架。首先，我们从学习的时间距离表示构建潜在可达性图。为了弥合语义差距，我们通过认证的锚节点和概率软标签来增强该图。然后，我们将规范转换为Büchi自动机，并在隐式乘积空间中搜索以推导出成本最小的前缀-后缀计划。最后，部署一个子目标条件的低级策略来执行这些潜在路点。在OGBench运动域上的实验表明，SAGAS成功地为多样化的LTL任务合成了高效轨迹，有效弥合了碎片化离线数据与复杂逻辑约束之间的差距。

</details>


### [299] [Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration](https://arxiv.org/abs/2512.00797)
*Nan Sun,Bo Mao,Yongchang Li,Chenxu Wang,Di Guo,Huaping Liu*

Main category: cs.RO

TL;DR: InteractGen是一个基于LLM的多智能体框架，将机器人智能分解为专门化的智能体，用于连续感知、依赖感知规划、决策验证、失败反思和动态人类委派，将基础模型作为闭环集体中的受监管组件，在异构机器人团队上部署并在三个月开放使用研究中证明其提升了任务成功率、适应性和人机协作。


<details>
  <summary>Details</summary>
Motivation: 基础模型在机器人学中统一感知和规划方面变得重要，但实际部署暴露了其单一模型假设与实用服务流程的分布式、动态性质之间的不匹配。视觉语言模型具有强大的语义理解能力但缺乏具身感知的行动能力，而视觉-语言-行动策略在跨具身性方面脆弱、几何基础薄弱且缺乏主动协作机制。这些限制表明，仅扩展单一模型无法为在人类环境中运行的服务机器人提供可靠的自主性。

Method: 提出InteractGen框架，将机器人智能分解为专门化的智能体：连续感知智能体、依赖感知规划智能体、决策与验证智能体、失败反思智能体和动态人类委派智能体。将基础模型作为受监管组件整合到闭环集体中，通过多智能体编排实现社会基础的服务自主性。

Result: 在异构机器人团队上部署并在三个月开放使用研究中评估，InteractGen提高了任务成功率、适应性和人机协作，证明多智能体编排比进一步扩展独立模型提供了更可行的社会基础服务自主性路径。

Conclusion: 多智能体编排为服务机器人提供了比单一基础模型扩展更可行的社会基础自主性路径，通过专门化智能体分解机器人智能并整合基础模型作为受监管组件，能够更好地适应动态、分布式的实际服务环境。

Abstract: Foundation models have become central to unifying perception and planning in robotics, yet real-world deployment exposes a mismatch between their monolithic assumption that a single model can handle all cognitive functions and the distributed, dynamic nature of practical service workflows. Vision-language models offer strong semantic understanding but lack embodiment-aware action capabilities while relying on hand-crafted skills. Vision-Language-Action policies enable reactive manipulation but remain brittle across embodiments, weak in geometric grounding, and devoid of proactive collaboration mechanisms. These limitations indicate that scaling a single model alone cannot deliver reliable autonomy for service robots operating in human-populated settings. To address this gap, we present InteractGen, an LLM-powered multi-agent framework that decomposes robot intelligence into specialized agents for continuous perception, dependency-aware planning, decision and verification, failure reflection, and dynamic human delegation, treating foundation models as regulated components within a closed-loop collective. Deployed on a heterogeneous robot team and evaluated in a three-month open-use study, InteractGen improves task success, adaptability, and human-robot collaboration, providing evidence that multi-agent orchestration offers a more feasible path toward socially grounded service autonomy than further scaling standalone models.

中文标题: 将单体基础模型转化为具身多智能体架构以实现人机协作

中文摘要: 基础模型已成为统一机器人感知和规划的核心，但实际部署暴露了其单一模型假设（即单个模型可以处理所有认知功能）与实用服务流程的分布式、动态性质之间的不匹配。视觉语言模型提供强大的语义理解能力，但缺乏具身感知的行动能力，同时依赖手工制作的技能。视觉-语言-行动策略能够实现反应式操作，但在跨具身性方面仍然脆弱，几何基础薄弱，且缺乏主动协作机制。这些限制表明，仅扩展单一模型无法为在人类环境中运行的服务机器人提供可靠的自主性。为解决这一差距，我们提出了InteractGen，一个基于LLM的多智能体框架，将机器人智能分解为专门化的智能体，用于连续感知、依赖感知规划、决策与验证、失败反思和动态人类委派，将基础模型作为闭环集体中的受监管组件。在异构机器人团队上部署并在三个月开放使用研究中评估，InteractGen提高了任务成功率、适应性和人机协作，证明多智能体编排比进一步扩展独立模型提供了更可行的社会基础服务自主性路径。

</details>


### [300] [A Novel MDP Decomposition Framework for Scalable UAV Mission Planning in Complex and Uncertain Environments](https://arxiv.org/abs/2512.00838)
*Md Muzakkir Quamar,Ali Nasir,Sami ELFerik*

Main category: cs.RO

TL;DR: 本文提出了一种用于复杂不确定环境中无人机任务规划的可扩展MDP分解框架，通过两阶段分解策略将大规模MDP分解为更小的子MDP，在保证最优性的同时实现计算时间数量级减少。


<details>
  <summary>Details</summary>
Motivation: 解决大规模马尔可夫决策过程（MDP）在复杂不确定环境中的计算瓶颈问题，实现无人机任务管理的可扩展性和容错性，支持实时策略更新。

Method: 采用两阶段分解策略：第一阶段使用基于因子的算法将全局MDP划分为更小的目标特定子MDP；第二阶段使用基于优先级的重组算法独立求解每个子MDP，并通过元策略进行冲突解决以整合为统一全局策略。

Result: 理论分析证明在温和的概率独立性假设下，组合策略与最优全局MDP策略等价；仿真验证显示计算时间数量级减少，同时保持任务可靠性和策略最优性。

Conclusion: 该分解框架为实时无人机任务执行中的可扩展决策建立了实用且鲁棒的基础，推进了人工智能决策的可扩展性，使复杂任务环境中的实时策略更新成为可能。

Abstract: This paper presents a scalable and fault-tolerant framework for unmanned aerial vehicle (UAV) mission management in complex and uncertain environments. The proposed approach addresses the computational bottleneck inherent in solving large-scale Markov Decision Processes (MDPs) by introducing a two-stage decomposition strategy. In the first stage, a factor-based algorithm partitions the global MDP into smaller, goal-specific sub-MDPs by leveraging domain-specific features such as goal priority, fault states, spatial layout, and energy constraints. In the second stage, a priority-based recombination algorithm solves each sub-MDP independently and integrates the results into a unified global policy using a meta-policy for conflict resolution. Importantly, we present a theoretical analysis showing that, under mild probabilistic independence assumptions, the combined policy is provably equivalent to the optimal global MDP policy. Our work advances artificial intelligence (AI) decision scalability by decomposing large MDPs into tractable subproblems with provable global equivalence. The proposed decomposition framework enhances the scalability of Markov Decision Processes, a cornerstone of sequential decision-making in artificial intelligence, enabling real-time policy updates for complex mission environments. Extensive simulations validate the effectiveness of our method, demonstrating orders-of-magnitude reduction in computation time without sacrificing mission reliability or policy optimality. The proposed framework establishes a practical and robust foundation for scalable decision-making in real-time UAV mission execution.

中文标题: 一种用于复杂不确定环境中可扩展无人机任务规划的新型MDP分解框架

中文摘要: 本文提出了一种用于复杂不确定环境中无人机任务管理的可扩展和容错框架。该方法通过引入两阶段分解策略，解决了求解大规模马尔可夫决策过程（MDP）固有的计算瓶颈。在第一阶段，基于因子的算法利用领域特定特征（如目标优先级、故障状态、空间布局和能量约束）将全局MDP划分为更小的目标特定子MDP。在第二阶段，基于优先级的重组算法独立求解每个子MDP，并使用元策略进行冲突解决，将结果整合为统一的全局策略。重要的是，我们提供了理论分析，表明在温和的概率独立性假设下，组合策略可证明等价于最优全局MDP策略。我们的工作通过将大规模MDP分解为具有可证明全局等价性的可处理子问题，推进了人工智能决策的可扩展性。所提出的分解框架增强了马尔可夫决策过程（人工智能中顺序决策的基石）的可扩展性，实现了复杂任务环境中的实时策略更新。大量仿真验证了我们方法的有效性，展示了在不牺牲任务可靠性或策略最优性的情况下，计算时间数量级的减少。所提出的框架为实时无人机任务执行中的可扩展决策建立了实用且鲁棒的基础。

</details>


### [301] [Constant-Time Motion Planning with Manipulation Behaviors](https://arxiv.org/abs/2512.00939)
*Nayesha Gandotra,Itamar Mishani,Maxim Likhachev*

Main category: cs.RO

TL;DR: B-CTMP算法扩展了CTMP，通过预计算数据结构实现毫秒级查询，将无碰撞运动规划与物体操作行为（如抓取、插入）统一在恒定时间框架内，为半结构化环境中的操作任务提供速度和成功率的可证明保证。


<details>
  <summary>Details</summary>
Motivation: 当前接触式机器人操作虽有进展，但大多数部署系统仍局限于简单的脚本化程序。关键障碍之一是缺乏能够为安全性、效率和可靠性提供可验证保证的运动规划算法。现有CTMP方法没有明确包含物体操作所需的行为，需要填补这一空白。

Method: 提出行为恒定时间运动规划器（B-CTMP），扩展CTMP算法解决两类操作任务：1）无碰撞运动到行为起始状态，2）执行操作行为（如抓取或插入）到达目标。通过预计算紧凑数据结构，保证毫秒级恒定时间查询，同时确保在指定状态集上的完备性和任务执行成功。

Result: 在模拟中评估了两个典型操作任务（货架拾取和插头插入），并在真实机器人上验证了有效性。结果表明B-CTMP在单个恒定时间框架内统一了无碰撞规划和物体操作，为半结构化环境中的操作提供了速度和成功率的可证明保证。

Conclusion: B-CTMP算法成功地将恒定时间运动规划扩展到包含操作行为，为机器人操作任务提供了可验证的保证，实现了毫秒级查询速度，为半结构化环境中的可靠机器人操作提供了实用解决方案。

Abstract: Recent progress in contact-rich robotic manipulation has been striking, yet most deployed systems remain confined to simple, scripted routines. One of the key barriers is the lack of motion planning algorithms that can provide verifiable guarantees for safety, efficiency and reliability. To address this, a family of algorithms called Constant-Time Motion Planning (CTMP) was introduced, which leverages a preprocessing phase to enable collision-free motion queries in a fixed, user-specified time budget (e.g., 10 milliseconds). However, existing CTMP methods do not explicitly incorporate the manipulation behaviors essential for object handling. To bridge this gap, we introduce the \textit{Behavioral Constant-Time Motion Planner} (B-CTMP), an algorithm that extends CTMP to solve a broad class of two-step manipulation tasks: (1) a collision-free motion to a behavior initiation state, followed by (2) execution of a manipulation behavior (such as grasping or insertion) to reach the goal. By precomputing compact data structures, B-CTMP guarantees constant-time query in mere milliseconds while ensuring completeness and successful task execution over a specified set of states. We evaluate B-CTMP on two canonical manipulation tasks in simulation, shelf picking and plug insertion,and demonstrate its effectiveness on a real robot. Our results show that B-CTMP unifies collision-free planning and object manipulation within a single constant-time framework, providing provable guarantees of speed and success for manipulation in semi-structured environments.

中文标题: 具有操作行为的恒定时间运动规划

中文摘要: 接触式机器人操作的最新进展令人瞩目，但大多数部署系统仍局限于简单的脚本化程序。关键障碍之一是缺乏能够为安全性、效率和可靠性提供可验证保证的运动规划算法。为解决此问题，引入了恒定时间运动规划（CTMP）算法家族，通过预处理阶段在固定的用户指定时间预算（例如10毫秒）内实现无碰撞运动查询。然而，现有CTMP方法没有明确包含物体操作所需的行为。为填补这一空白，我们引入了行为恒定时间运动规划器（B-CTMP），该算法扩展CTMP以解决广泛的两步操作任务：（1）无碰撞运动到行为起始状态，随后（2）执行操作行为（如抓取或插入）到达目标。通过预计算紧凑数据结构，B-CTMP保证在毫秒级时间内进行恒定时间查询，同时确保在指定状态集上的完备性和任务执行成功。我们在模拟中评估了B-CTMP在两个典型操作任务上的表现（货架拾取和插头插入），并在真实机器人上验证了其有效性。结果表明，B-CTMP在单个恒定时间框架内统一了无碰撞规划和物体操作，为半结构化环境中的操作提供了速度和成功率的可证明保证。

</details>


### [302] [H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer](https://arxiv.org/abs/2512.00971)
*Yunfeng Lin,Minghuan Liu,Yufei Xue,Ming Zhou,Yong Yu,Jiangmiao Pang,Weinan Zhang*

Main category: cs.RO

TL;DR: H-Zero是一种跨人形机器人运动预训练框架，通过在有限的人形机器人集上进行预训练，能够实现对新机器人形态的零样本或少样本迁移，显著减少控制器开发工作量。


<details>
  <summary>Details</summary>
Motivation: 人形机器人快速发展需要稳健且适应性强的控制器，但现有解决方案通常针对特定机器人设计，需要为每个机器人形态大量调整奖励函数、物理参数和训练超参数，开发成本高昂。

Method: 提出跨人形机器人运动预训练管道H-Zero，学习通用的人形机器人基础策略。通过在有限的人形机器人集上进行预训练，使模型能够泛化到新的人形机器人形态。

Result: 评估显示，预训练策略在未见过的机器人上能保持高达81%的完整回合持续时间，能够在30分钟内通过少量微调实现到未见人形机器人和直立四足机器人的迁移。

Conclusion: H-Zero框架通过跨人形机器人预训练实现了对新机器人形态的高效迁移，显著减少了控制器开发工作量，为人形机器人控制提供了通用解决方案。

Abstract: The rapid advancement of humanoid robotics has intensified the need for robust and adaptable controllers to enable stable and efficient locomotion across diverse platforms. However, developing such controllers remains a significant challenge because existing solutions are tailored to specific robot designs, requiring extensive tuning of reward functions, physical parameters, and training hyperparameters for each embodiment. To address this challenge, we introduce H-Zero, a cross-humanoid locomotion pretraining pipeline that learns a generalizable humanoid base policy. We show that pretraining on a limited set of embodiments enables zero-shot and few-shot transfer to novel humanoid robots with minimal fine-tuning. Evaluations show that the pretrained policy maintains up to 81% of the full episode duration on unseen robots in simulation while enabling few-shot transfer to unseen humanoids and upright quadrupeds within 30 minutes of fine-tuning.

中文标题: H-Zero：跨人形机器人运动预训练实现少样本新形态迁移

中文摘要: 人形机器人的快速发展加强了对稳健且适应性强的控制器的需求，以实现跨不同平台的稳定高效运动。然而，开发此类控制器仍然是一个重大挑战，因为现有解决方案针对特定机器人设计，需要为每个形态大量调整奖励函数、物理参数和训练超参数。为解决这一挑战，我们引入了H-Zero，一个跨人形机器人运动预训练管道，学习可泛化的人形机器人基础策略。我们表明，在有限的形态集上进行预训练能够实现对新的人形机器人的零样本和少样本迁移，只需最少微调。评估显示，预训练策略在模拟中未见过的机器人上能保持高达81%的完整回合持续时间，同时能够在30分钟内通过少量微调实现到未见人形机器人和直立四足机器人的迁移。

</details>


### [303] [FOM-Nav: Frontier-Object Maps for Object Goal Navigation](https://arxiv.org/abs/2512.01009)
*Thomas Chabal,Shizhe Chen,Jean Ponce,Cordelia Schmid*

Main category: cs.RO

TL;DR: FOM-Nav是一个用于目标物体导航的模块化框架，通过结合前沿-物体地图和视觉语言模型来解决现有方法在长期记忆和语义信息方面的不足，在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 目标物体导航任务要求机器人在未知环境中高效找到特定物体。现有方法存在两个主要问题：基于隐式记忆的方法难以保持长期记忆和进行有效规划，而基于显式地图的方法缺乏丰富的语义信息。这促使研究者开发一个既能保持长期空间记忆又能提供丰富语义理解的新框架。

Method: FOM-Nav采用模块化框架，核心是前沿-物体地图，这种地图在线构建并同时编码空间前沿（探索边界）和细粒度物体信息。系统使用视觉语言模型进行多模态场景理解和高层目标预测，然后由低层规划器执行预测结果以生成高效导航轨迹。训练数据通过从真实世界扫描环境中自动构建的大规模导航数据集获得。

Result: FOM-Nav在MP3D和HM3D基准测试中实现了最先进的性能，特别是在导航效率指标SPL方面表现突出。此外，在真实机器人上的实验也显示出有希望的结果，验证了模型设计的有效性和构建数据集的质量。

Conclusion: FOM-Nav通过结合前沿-物体地图和视觉语言模型，成功解决了目标物体导航中长期记忆保持和语义信息不足的问题。该框架在多个基准测试中表现出色，特别是在导航效率方面，为真实世界机器人导航应用提供了有前景的解决方案。

Abstract: This paper addresses the Object Goal Navigation problem, where a robot must efficiently find a target object in an unknown environment. Existing implicit memory-based methods struggle with long-term memory retention and planning, while explicit map-based approaches lack rich semantic information. To address these challenges, we propose FOM-Nav, a modular framework that enhances exploration efficiency through Frontier-Object Maps and vision-language models. Our Frontier-Object Maps are built online and jointly encode spatial frontiers and fine-grained object information. Using this representation, a vision-language model performs multimodal scene understanding and high-level goal prediction, which is executed by a low-level planner for efficient trajectory generation. To train FOM-Nav, we automatically construct large-scale navigation datasets from real-world scanned environments. Extensive experiments validate the effectiveness of our model design and constructed dataset. FOM-Nav achieves state-of-the-art performance on the MP3D and HM3D benchmarks, particularly in navigation efficiency metric SPL, and yields promising results on a real robot.

中文标题: FOM-Nav：面向目标物体导航的前沿-物体地图

中文摘要: 本文解决了目标物体导航问题，即机器人必须在未知环境中高效找到目标物体。现有的基于隐式记忆的方法在长期记忆保持和规划方面存在困难，而基于显式地图的方法缺乏丰富的语义信息。为了解决这些挑战，我们提出了FOM-Nav，这是一个模块化框架，通过前沿-物体地图和视觉语言模型提高了探索效率。我们的前沿-物体地图在线构建，并联合编码空间前沿和细粒度物体信息。使用这种表示方法，视觉语言模型执行多模态场景理解和高层目标预测，然后由低层规划器执行以生成高效轨迹。为了训练FOM-Nav，我们从真实世界扫描环境中自动构建大规模导航数据集。大量实验验证了我们模型设计和构建数据集的有效性。FOM-Nav在MP3D和HM3D基准测试中实现了最先进的性能，特别是在导航效率指标SPL方面，并在真实机器人上产生了有希望的结果。

</details>


### [304] [Integration of UWB Radar on Mobile Robots for Continuous Obstacle and Environment Mapping](https://arxiv.org/abs/2512.01018)
*Adelina Giurea,Stijn Luchie,Dieter Coppens,Jeroen Hoebeke,Eli De Poorter*

Main category: cs.RO

TL;DR: 该研究探索了在移动机器人上集成UWB雷达进行环境建图，提出了一种处理流程来识别障碍物并减少噪声和多径效应，在低反射材料上实现了高检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统视觉和激光雷达在黑暗、烟雾或反射表面等低能见度环境下失效，需要一种不依赖视觉特征且无需固定锚点的感知方案。

Method: 提出三步处理流程：1) 基于CIR峰值检测的目标识别；2) 基于峰值特性、信噪比和相位到达差的滤波；3) 基于距离估计和到达角估计的聚类。

Result: 在通道9上，即使检测低反射材料（如胶合板），障碍物检测精度至少达到82.36%，召回率达到89.46%，有效减少了噪声和多径效应。

Conclusion: UWB雷达在移动机器人上可用于基础设施无关的环境建图，为不依赖视觉特征且无需固定锚点的SLAM系统开发奠定了基础。

Abstract: This paper presents an infrastructure-free approach for obstacle detection and environmental mapping using ultra-wideband (UWB) radar mounted on a mobile robotic platform. Traditional sensing modalities such as visual cameras and Light Detection and Ranging (LiDAR) fail in environments with poor visibility due to darkness, smoke, or reflective surfaces. In these visioned-impaired conditions, UWB radar offers a promising alternative. To this end, this work explores the suitability of robot-mounted UWB radar for environmental mapping in dynamic, anchor-free scenarios. The study investigates how different materials (metal, concrete and plywood) and UWB radio channels (5 and 9) influence the Channel Impulse Response (CIR). Furthermore, a processing pipeline is proposed to achieve reliable mapping of detected obstacles, consisting of 3 steps: (i) target identification (based on CIR peak detection), (ii) filtering (based on peak properties, signal-to-noise score, and phase-difference of arrival), and (iii) clustering (based on distance estimation and angle-of-arrival estimation). The proposed approach successfully reduces noise and multipath effects, resulting in an obstacle detection precision of at least 82.36% and a recall of 89.46% on channel 9 even when detecting low-reflective materials such as plywood. This work offers a foundation for further development of UWB-based localisation and mapping (SLAM) systems that do not rely on visual features and, unlike conventional UWB localisation systems, do not require on fixed anchor nodes for triangulation.

中文标题: 移动机器人上UWB雷达的集成用于连续障碍物和环境建图

中文摘要: 本文提出了一种基于移动机器人平台的超宽带雷达基础设施无关障碍物检测和环境建图方法。传统感知模态如视觉相机和激光雷达在黑暗、烟雾或反射表面等低能见度环境下失效。在这些视觉受损条件下，UWB雷达提供了有前景的替代方案。为此，本研究探索了机器人搭载UWB雷达在动态、无锚点场景中进行环境建图的适用性。研究调查了不同材料（金属、混凝土和胶合板）和UWB无线电信道（5和9）如何影响信道脉冲响应。此外，提出了一个处理流程来实现可靠检测障碍物的建图，包括三个步骤：(i) 目标识别（基于CIR峰值检测），(ii) 滤波（基于峰值特性、信噪比得分和相位到达差），以及(iii) 聚类（基于距离估计和到达角估计）。所提出的方法成功减少了噪声和多径效应，即使在检测低反射材料如胶合板时，在通道9上实现了至少82.36%的障碍物检测精度和89.46%的召回率。这项工作为进一步开发不依赖视觉特征且与传统UWB定位系统不同、无需固定锚点进行三角测量的UWB定位与建图系统奠定了基础。

</details>


### [305] [CycleManip: Enabling Cyclic Task Manipulation via Effective Historical Perception and Understanding](https://arxiv.org/abs/2512.01022)
*Yi-Lin Wei,Haoran Liao,Yuhao Lin,Pengyue Wang,Zhizhao Liang,Guiliang Liu,Wei-Shi Zheng*

Main category: cs.RO

TL;DR: CycleManip框架通过成本感知采样和多任务学习，实现了机器人循环任务操作，解决了现有模仿方法无法在预期时间内完成任务的问题，并建立了首个循环任务操作基准。


<details>
  <summary>Details</summary>
Motivation: 机器人循环任务操作（如摇晃瓶子、敲钉子）在日常生活中有重要应用，但现有研究较少。主要挑战包括：1）模仿方法因历史信息利用不足而无法在预期时间内完成任务；2）缺乏足够数据和自动评估工具的基准阻碍了该领域发展。

Method: 提出了CycleManip框架，采用端到端模仿方式，无需额外模型或分层结构。核心创新包括：1）成本感知采样策略增强历史感知；2）多任务学习提升历史理解。同时建立了循环任务操作基准和自动评估方法。

Result: 在仿真和真实环境中进行了广泛实验，证明该方法在循环任务操作中实现了高成功率。结果还显示在通用操作中具有强适应性，并能与VLA等模仿策略即插即用。方法可应用于多种机器人平台，包括双臂夹爪、灵巧手和人形机器人。

Conclusion: CycleManip框架有效解决了机器人循环任务操作的挑战，通过增强历史感知和理解实现了预期时间内的任务完成。建立的基准和评估方法为该领域发展提供了重要基础，展示了方法的通用性和可扩展性。

Abstract: In this paper, we explore an important yet underexplored task in robot manipulation: cycle-based manipulation, where robots need to perform cyclic or repetitive actions with an expected terminal time. These tasks are crucial in daily life, such as shaking a bottle or knocking a nail. However, few prior works have explored this task, leading to two main challenges: 1) the imitation methods often fail to complete these tasks within the expected terminal time due to the ineffective utilization of history; 2) the absence of a benchmark with sufficient data and automatic evaluation tools hinders development of effective solutions in this area. To address these challenges, we first propose the CycleManip framework to achieve cycle-based task manipulation in an end-to-end imitation manner without requiring any extra models, hierarchical structure or significant computational overhead. The core insight is to enhance effective history perception by a cost-aware sampling strategy and to improve historical understanding by multi-task learning. Second, we introduce a cycle-based task manipulation benchmark, which provides diverse cycle-based tasks, and an automatic evaluation method. Extensive experiments conducted in both simulation and real-world settings demonstrate that our method achieves high success rates in cycle-based task manipulation. The results further show strong adaptability performance in general manipulation, and the plug-and-play ability on imitation policies such as Vision-Language-Action (VLA) models. Moreover, the results show that our approach can be applied across diverse robotic platforms, including bi-arm grippers, dexterous hands, and humanoid robots.

中文标题: CycleManip：通过有效历史感知与理解实现循环任务操作

中文摘要: 本文探索了机器人操作中一个重要但尚未充分研究的任务：基于循环的操作，即机器人需要在预期终止时间内执行循环或重复动作。这些任务在日常生活中至关重要，例如摇晃瓶子或敲钉子。然而，先前的研究很少探索这一任务，导致两个主要挑战：1）模仿方法由于历史信息利用不足而经常无法在预期终止时间内完成这些任务；2）缺乏具有足够数据和自动评估工具的基准阻碍了该领域有效解决方案的发展。为应对这些挑战，我们首先提出了CycleManip框架，以端到端模仿方式实现基于循环的任务操作，无需任何额外模型、分层结构或显著的计算开销。核心见解是通过成本感知采样策略增强有效历史感知，并通过多任务学习改进历史理解。其次，我们引入了一个基于循环的任务操作基准，提供了多样化的循环任务和自动评估方法。在仿真和真实环境中进行的广泛实验表明，我们的方法在基于循环的任务操作中实现了高成功率。结果进一步显示在通用操作中具有强大的适应性性能，以及对视觉-语言-动作（VLA）模型等模仿策略的即插即用能力。此外，结果表明我们的方法可以应用于多种机器人平台，包括双臂夹爪、灵巧手和人形机器人。

</details>


### [306] [VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference](https://arxiv.org/abs/2512.01031)
*Jiaming Tang,Yufei Sun,Yilong Zhao,Shang Yang,Yujun Lin,Zhuoyang Zhang,James Hou,Yao Lu,Zhijian Liu,Song Han*

Main category: cs.RO

TL;DR: VLASH是一个异步推理框架，通过预测未来执行状态来解决VLA模型实时部署中的延迟问题，实现平滑、准确、快速的机器人控制，无需额外开销。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言动作模型（VLAs）在现实世界部署中存在显著延迟问题，演示视频需要加速5-10倍才能显得流畅，存在动作停滞和延迟反应。异步推理虽然能实现连续低延迟控制，但由于机器人和环境在推理过程中持续演化，导致预测与执行之间存在时间错位，引发动作不稳定问题。

Method: VLASH通过使用先前生成的动作块将机器人状态向前滚动，估计未来的执行时间状态，从而弥合预测和执行之间的时间差距。这是一个通用的异步推理框架，不需要额外的运行时开销或架构更改。

Result: 实验显示VLASH相比同步推理实现了高达2.03倍的加速，反应延迟降低高达17.4倍，同时完全保持原始准确性。能够处理传统同步推理无法完成的快速反应、高精度任务，如打乒乓球和打地鼠。

Conclusion: VLASH成功解决了VLA模型实时部署中的延迟和动作不稳定问题，通过未来状态感知的异步推理实现了平滑、准确、快速的机器人控制，为VLA在实际应用中的部署提供了有效解决方案。

Abstract: Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash

中文标题: VLASH：通过未来状态感知的异步推理实现实时视觉语言动作模型

中文摘要: 视觉语言动作模型（VLAs）在各种机器人任务中变得越来越强大。然而，它们在现实世界中的部署仍然缓慢且低效：演示视频通常需要加速5-10倍才能显得流畅，同时存在明显的动作停滞和对环境变化的延迟反应。异步推理通过使机器人能够同时执行动作和进行推理，为实现连续和低延迟控制提供了一个有前景的解决方案。然而，由于机器人和环境在推理过程中持续演化，预测和执行间隔之间会出现时间错位。这导致显著的动作不稳定性，而现有方法要么降低准确性，要么引入运行时开销来缓解此问题。我们提出了VLASH，这是一个用于VLAs的通用异步推理框架，能够提供平滑、准确和快速的反应控制，而无需额外的开销或架构更改。VLASH通过使用先前生成的动作块将机器人状态向前滚动来估计未来的执行时间状态，从而弥合预测和执行之间的差距。实验表明，与同步推理相比，VLASH实现了高达2.03倍的加速，并将反应延迟降低了高达17.4倍，同时完全保留了原始准确性。此外，它使VLAs能够处理快速反应、高精度任务，如打乒乓球和打地鼠，而传统的同步推理在这些任务中失败。代码可在https://github.com/mit-han-lab/vlash获取。

</details>


### [307] [Autonomous Grasping On Quadruped Robot With Task Level Interaction](https://arxiv.org/abs/2512.01052)
*Muhtadin,Mochammad Hilmi Rusydiansyah,Mauridhi Hery Purnomo,I Ketut Eddy Purnama,Chastine Fatichah*

Main category: cs.RO

TL;DR: 为四足机器人开发了基于任务级交互的自主抓取系统，通过硬件集成、分层控制和Web界面实现导航、目标检测和抓取功能，在真实场景测试中达到75%的抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 四足机器人虽然具有高机动性和复杂地形适应能力，但大多缺乏物体操作功能。为四足机器人配备机械臂和夹爪后，在需要复杂指令的远程场景中手动控制存在挑战，因此需要开发自主抓取系统来增强其作为服务机器人的实用性。

Method: 1. 硬件集成：将机械臂和夹爪集成到四足机器人本体上；2. 分层控制系统：基于ROS设计控制架构；3. Web界面：用于人机交互；4. 自主功能：使用GraspNet实现导航、目标检测和抓取。

Result: 在真实场景测试中，机器人能够准确一致地执行任务，从12次试验中获得了75%的抓取成功率，证明了系统在导航、目标选择和抓取方面的有效性。

Conclusion: 该系统显著提升了四足机器人在真实环境中的服务能力，展示了将自主抓取功能集成到移动平台上的可行性，为四足机器人作为服务机器人的实际应用提供了重要技术支撑。

Abstract: Quadruped robots are increasingly used in various applications due to their high mobility and ability to operate in diverse terrains. However, most available quadruped robots are primarily focused on mobility without object manipulation capabilities. Equipping a quadruped robot with a robotic arm and gripper introduces a challenge in manual control, especially in remote scenarios that require complex commands. This research aims to develop an autonomous grasping system on a quadruped robot using a task-level interaction approach. The system includes hardware integration of a robotic arm and gripper onto the quadruped robot's body, a layered control system designed using ROS, and a web-based interface for human-robot interaction. The robot is capable of autonomously performing tasks such as navigation, object detection, and grasping using GraspNet. Testing was conducted through real-world scenarios to evaluate navigation, object selection and grasping, and user experience. The results show that the robot can perform tasks accurately and consistently, achieving a grasping success rate of 75 % from 12 trials. Therefore, the system demonstrates significant potential in enhancing the capabilities of quadruped robots as service robots in real-world environments.

中文标题: 基于任务级交互的四足机器人自主抓取系统

中文摘要: 四足机器人因其高机动性和在多样化地形中的操作能力而日益广泛应用于各种场景。然而，大多数现有四足机器人主要专注于移动性，缺乏物体操作能力。为四足机器人配备机械臂和夹爪带来了手动控制的挑战，特别是在需要复杂指令的远程场景中。本研究旨在开发一种基于任务级交互方法的四足机器人自主抓取系统。该系统包括将机械臂和夹爪硬件集成到四足机器人本体上、使用ROS设计的分层控制系统，以及用于人机交互的基于Web的界面。该机器人能够使用GraspNet自主执行导航、目标检测和抓取等任务。通过真实场景测试评估了导航、目标选择和抓取以及用户体验。结果表明，机器人能够准确一致地执行任务，在12次试验中实现了75%的抓取成功率。因此，该系统在增强四足机器人作为真实环境服务机器人的能力方面展现出显著潜力。

</details>


### [308] [Opening the Sim-to-Real Door for Humanoid Pixel-to-Action Policy Transfer](https://arxiv.org/abs/2512.01061)
*Haoru Xue,Tairan He,Zi Wang,Qingwei Ben,Wenli Xiao,Zhengyi Luo,Xingye Da,Fernando Castañeda,Guanya Shi,Shankar Sastry,Linxi "Jim" Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: 开发了一个用于人形机器人视觉定位操作的师生引导学习框架，通过模拟训练实现零样本真实世界性能，在门操作任务中表现优于人类远程操作者


<details>
  <summary>Details</summary>
Motivation: 利用GPU加速的光真实模拟为机器人学习提供可扩展的数据生成路径，解决人形机器人视觉定位操作这一高难度挑战，特别是铰接物体交互任务

Method: 采用师生引导学习框架，包含阶段重置探索策略稳定长时程特权策略训练，以及GRPO微调程序缓解部分可观测性问题，提升模拟到真实RL的闭环一致性

Result: 完全在模拟数据上训练的策略实现了对不同类型门的鲁棒零样本性能，在相同全身控制栈下，任务完成时间比人类远程操作者快31.7%

Conclusion: 这是首个使用纯RGB感知实现多样化铰接定位操作的人形机器人模拟到真实策略，展示了光真实模拟在复杂机器人任务中的潜力

Abstract: Recent progress in GPU-accelerated, photorealistic simulation has opened a scalable data-generation path for robot learning, where massive physics and visual randomization allow policies to generalize beyond curated environments. Building on these advances, we develop a teacher-student-bootstrap learning framework for vision-based humanoid loco-manipulation, using articulated-object interaction as a representative high-difficulty benchmark. Our approach introduces a staged-reset exploration strategy that stabilizes long-horizon privileged-policy training, and a GRPO-based fine-tuning procedure that mitigates partial observability and improves closed-loop consistency in sim-to-real RL. Trained entirely on simulation data, the resulting policy achieves robust zero-shot performance across diverse door types and outperforms human teleoperators by up to 31.7% in task completion time under the same whole-body control stack. This represents the first humanoid sim-to-real policy capable of diverse articulated loco-manipulation using pure RGB perception.

中文标题: 为人形机器人像素到动作策略转移打开模拟到真实之门

中文摘要: GPU加速的光真实模拟的最新进展为机器人学习开辟了可扩展的数据生成路径，其中大规模物理和视觉随机化使策略能够超越精心设计的环境进行泛化。基于这些进展，我们开发了一个用于视觉人形定位操作的师生引导学习框架，使用铰接物体交互作为代表性的高难度基准。我们的方法引入了阶段重置探索策略来稳定长时程特权策略训练，以及基于GRPO的微调程序来缓解部分可观测性并提高模拟到真实RL中的闭环一致性。完全在模拟数据上训练的结果策略在不同门类型上实现了鲁棒的零样本性能，在相同全身控制栈下，任务完成时间比人类远程操作者快31.7%。这代表了首个使用纯RGB感知实现多样化铰接定位操作的人形机器人模拟到真实策略。

</details>


### [309] [Reinforcement Learning for Gliding Projectile Guidance and Control](https://arxiv.org/abs/2512.01066)
*Joel Cahn,Antonin Thomas,Philippe Pastor*

Main category: cs.RO

TL;DR: 该研究开发了一种基于强化学习的控制律，用于光学制导滑翔弹的导航与控制，旨在提高动态环境下的灵活性和自主性，实现高精度目标跟踪。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是将强化学习方法应用于固定翼飞行器的制导与控制，特别是在光学制导滑翔弹上。虽然强化学习已在四旋翼无人机上成功应用，但作者希望证明该方法同样适用于固定翼飞行器的所有轴向上，实现更灵活、自主的动态环境导航。

Method: 研究方法采用强化学习来开发控制律。该方法旨在使光学制导滑翔弹能够自主导航，通过摄像头检测目标，然后引导滑翔弹高精度地跟踪并飞向该目标点。

Result: 论文展示了强化学习控制律在光学制导滑翔弹上的开发成果，该方法能够实现目标检测和跟踪功能，为固定翼飞行器的全轴向控制提供了新的解决方案。

Conclusion: 研究表明强化学习可以成功应用于固定翼飞行器的制导与控制，特别是在光学制导滑翔弹上。该方法能够实现高精度的目标跟踪，为动态环境下的自主导航提供了创新解决方案。

Abstract: This paper presents the development of a control law, which is intended to be implemented on an optical guided glider. This guiding law follows an innovative approach, the reinforcement learning. This control law is used to make navigation more flexible and autonomous in a dynamic environment. The final objective is to track a target detected with the camera and then guide the glider to this point with high precision. Already applied on quad-copter drones, we wish by this study to demonstrate the applicability of reinforcement learning for fixed-wing aircraft on all of its axis.

中文标题: 基于强化学习的滑翔弹制导与控制

中文摘要: 本文提出了一种控制律的开发，该控制律旨在应用于光学制导滑翔弹。这种制导律采用了一种创新方法——强化学习。该控制律用于在动态环境中实现更灵活和自主的导航。最终目标是跟踪摄像头检测到的目标，然后高精度地将滑翔弹引导至该点。虽然已在四旋翼无人机上应用，但通过本研究我们希望证明强化学习在固定翼飞行器所有轴向上的适用性。

</details>


### [310] [Tactile Robotics: Past and Future](https://arxiv.org/abs/2512.01106)
*Nathan F. Lepora*

Main category: cs.RO

TL;DR: 本文通过对近150篇综述文献的分析，回顾了触觉机器人半个世纪的发展历史，将其分为四个阶段：起源期（1965-79）、基础与成长期（1980-94）、触觉寒冬期（1995-2009）以及扩展与多样化期（2010-2024）。文章指出当前触觉机器人正朝着电子皮肤、触觉机械手、基于视觉的触觉传感、软体/仿生触觉和触觉互联网等多样化主题发展，并预测2025年后该领域将走向成熟商业化应用。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过历史视角分析触觉机器人领域的发展轨迹，帮助定义该领域的未来方向。作者希望通过梳理近半个世纪内近150篇综述文献中的专业见解，揭示触觉传感在机器人学中的演变历程，识别反复出现的挑战，解释为何进展常常停滞，并指出最有可能塑造该领域未来的机遇。

Method: 采用文献综述和历史分析方法，对1965年至2024年间发表的近150篇关于触觉机器人传感的综述文献进行系统性分析。通过专家意见和知识积累，将触觉机器人发展历史划分为四个明确阶段，并识别每个时期的特征和关键发展。基于历史趋势和当前主题，预测未来发展方向。

Result: 研究识别出触觉机器人发展的四个历史阶段：1）起源期（1965-1979）：触觉传感的初步探索；2）基础与成长期（1980-1994）：技术基础建立和领域扩展；3）触觉寒冬期（1995-2009）：进展缓慢和兴趣减退；4）扩展与多样化期（2010-2024）：技术复兴和多主题发展。当前主要研究主题包括电子皮肤、触觉机械手、基于视觉的触觉传感、软体/仿生触觉和触觉互联网。

Conclusion: 触觉机器人领域经历了起伏发展，目前正处于快速扩展和多样化阶段。通过连接过去的专业见解与当前的研究主题，文章揭示了该领域反复出现的挑战和演变模式。预测从2025年开始，触觉机器人将走向成熟和广泛商业化应用，在类人灵巧操作、理解人类智能和远程临场感等方面产生重要影响，最终推动整个机器人和人工智能领域的发展。

Abstract: What is the future of tactile robotics? To help define that future, this article provides a historical perspective on tactile sensing in robotics from the wealth of knowledge and expert opinion in nearly 150 reviews over almost half a century. This history is characterized by a succession of generations: 1965-79 (origins), 1980-94 (foundations and growth), 1995-2009 (tactile winter) and 2010-2024 (expansion and diversification). Recent expansion has led to diverse themes emerging of e-skins, tactile robotic hands, vision-based tactile sensing, soft/biomimetic touch, and the tactile Internet. In the next generation from 2025, tactile robotics could mature to widespread commercial use, with applications in human-like dexterity, understanding human intelligence, and telepresence impacting all robotics and AI. By linking past expert insights to present themes, this article highlights recurring challenges in tactile robotics, showing how the field has evolved, why progress has often stalled, and which opportunities are most likely to define its future.

中文标题: 触觉机器人：过去与未来

中文摘要: 触觉机器人的未来是什么？为了帮助定义这一未来，本文通过对近半个世纪内近150篇综述文献中的丰富知识和专家意见进行分析，提供了触觉传感在机器人学中的历史视角。这段历史以连续几代人为特征：1965-79年（起源期）、1980-94年（基础与成长期）、1995-2009年（触觉寒冬期）和2010-2024年（扩展与多样化期）。最近的扩展导致了多样化主题的出现，包括电子皮肤、触觉机械手、基于视觉的触觉传感、软体/仿生触觉和触觉互联网。在2025年后的下一代发展中，触觉机器人可能成熟到广泛商业应用，在类人灵巧操作、理解人类智能和远程临场感等方面产生影响，最终推动所有机器人和人工智能的发展。通过将过去的专家见解与当前主题联系起来，本文强调了触觉机器人中反复出现的挑战，展示了该领域如何演变，为什么进展常常停滞，以及哪些机遇最有可能定义其未来。

</details>


### [311] [Think Fast: Real-Time Kinodynamic Belief-Space Planning for Projectile Interception](https://arxiv.org/abs/2512.01108)
*Gabriel Olin,Lu Chen,Nayesha Gandotra,Maxim Likhachev,Howie Choset*

Main category: cs.RO

TL;DR: 本文提出了一种用于实时弹道拦截的动力学信念空间规划方法，通过状态-时间空间中的树状结构和运动基元实现多目标可达性，能够在目标信念演化的同时进行实时值更新。


<details>
  <summary>Details</summary>
Motivation: 拦截快速移动物体具有严格的时间限制，在存在传感器噪声的情况下更加复杂，因为噪声传感器只能提供不完整信息，导致需要拦截的目标状态存在分布。由于时间紧迫，规划器必须在接收信息的同时开始引导拦截器（机器人手臂）以击中目标。

Method: 引入树状结构，在状态-时间空间中使用动力学运动基元进行生长。该结构编码从单一原点到达多个目标的可达性，同时能够在目标信念演化时进行实时值更新，并在目标之间实现无缝转换。使用鲁棒创新自适应估计自适应卡尔曼滤波器（RIAE-AKF）跟踪目标并执行信念更新。

Result: 在配备立体相机（ZED 2i）的6自由度工业机械臂（ABB IRB-1600）上评估了拦截任务框架。

Conclusion: 该方法能够在传感器噪声存在的情况下实时规划拦截快速移动物体，通过树状结构和运动基元实现多目标可达性和信念演化时的实时更新。

Abstract: Intercepting fast moving objects, by its very nature, is challenging because of its tight time constraints. This problem becomes further complicated in the presence of sensor noise because noisy sensors provide, at best, incomplete information, which results in a distribution over target states to be intercepted. Since time is of the essence, to hit the target, the planner must begin directing the interceptor, in this case a robot arm, while still receiving information. We introduce an tree-like structure, which is grown using kinodynamic motion primitives in state-time space. This tree-like structure encodes reachability to multiple goals from a single origin, while enabling real-time value updates as the target belief evolves and seamless transitions between goals. We evaluate our framework on an interception task on a 6 DOF industrial arm (ABB IRB-1600) with an onboard stereo camera (ZED 2i). A robust Innovation-based Adaptive Estimation Adaptive Kalman Filter (RIAE-AKF) is used to track the target and perform belief updates.

中文标题: 快速思考：用于弹道拦截的实时动力学信念空间规划

中文摘要: 拦截快速移动物体本质上具有挑战性，因为其时间限制严格。在存在传感器噪声的情况下，这个问题变得更加复杂，因为噪声传感器最多只能提供不完整信息，导致需要拦截的目标状态存在分布。由于时间至关重要，为了击中目标，规划器必须在接收信息的同时开始引导拦截器（在本例中是机器人手臂）。我们引入了一种树状结构，该结构在状态-时间空间中使用动力学运动基元进行生长。这种树状结构编码从单一原点到达多个目标的可达性，同时能够在目标信念演化时进行实时值更新，并在目标之间实现无缝转换。我们在配备机载立体相机（ZED 2i）的6自由度工业机械臂（ABB IRB-1600）上评估了我们的拦截任务框架。使用鲁棒创新自适应估计自适应卡尔曼滤波器（RIAE-AKF）来跟踪目标并执行信念更新。

</details>


### [312] [Real-World Reinforcement Learning of Active Perception Behaviors](https://arxiv.org/abs/2512.01188)
*Edward S. Hu,Jie Wang,Xingfang Yuan,Fiona Luo,Muyao Li,Gaspard Lambrechts,Oleh Rybkin,Dinesh Jayaraman*

Main category: cs.RO

TL;DR: AAWR是一种利用特权传感器训练主动感知行为的强化学习方法，通过特权价值函数估计优势，从少量演示和粗略策略初始化快速学习，在8个机器人操作任务中表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 机器人的瞬时感官观察通常无法完全揭示任务相关信息，在部分可观测环境下，最优行为需要主动获取缺失信息。现有机器人学习技术难以产生这种主动感知行为，因此需要开发有效的训练方法。

Method: 提出非对称优势加权回归（AAWR）方法，利用训练时可访问的"特权"额外传感器。特权传感器用于训练高质量的特权价值函数，帮助估计目标策略的优势。从少量可能次优的演示和易于获得的粗略策略初始化开始，快速学习主动感知行为。

Result: 在3个机器人上的8个操作任务评估中，AAWR合成了可靠的主动感知行为，在所有先前方法中表现最佳。当使用在主动感知任务上表现不佳的"通用"机器人策略初始化时，AAWR能有效生成信息收集行为，使其能在严重部分可观测条件下执行操作任务。

Conclusion: AAWR提供了一种简单有效的真实世界机器人学习方案，能够高效训练主动感知策略，解决了部分可观测环境下的信息获取问题，显著提升了任务性能。

Abstract: A robot's instantaneous sensory observations do not always reveal task-relevant state information. Under such partial observability, optimal behavior typically involves explicitly acting to gain the missing information. Today's standard robot learning techniques struggle to produce such active perception behaviors. We propose a simple real-world robot learning recipe to efficiently train active perception policies. Our approach, asymmetric advantage weighted regression (AAWR), exploits access to "privileged" extra sensors at training time. The privileged sensors enable training high-quality privileged value functions that aid in estimating the advantage of the target policy. Bootstrapping from a small number of potentially suboptimal demonstrations and an easy-to-obtain coarse policy initialization, AAWR quickly acquires active perception behaviors and boosts task performance. In evaluations on 8 manipulation tasks on 3 robots spanning varying degrees of partial observability, AAWR synthesizes reliable active perception behaviors that outperform all prior approaches. When initialized with a "generalist" robot policy that struggles with active perception tasks, AAWR efficiently generates information-gathering behaviors that allow it to operate under severe partial observability for manipulation tasks. Website: https://penn-pal-lab.github.io/aawr/

中文标题: 主动感知行为的真实世界强化学习

中文摘要: 机器人的瞬时感官观察并不总是能揭示任务相关的状态信息。在这种部分可观测性下，最优行为通常需要明确行动以获取缺失信息。当今标准的机器人学习技术难以产生这种主动感知行为。我们提出了一种简单的真实世界机器人学习方案，用于高效训练主动感知策略。我们的方法——非对称优势加权回归（AAWR），利用了训练时访问"特权"额外传感器的能力。特权传感器能够训练高质量的特权价值函数，有助于估计目标策略的优势。从少量可能次优的演示和易于获得的粗略策略初始化开始，AAWR快速获取主动感知行为并提升任务性能。在对3个机器人上8个操作任务的评估中，AAWR合成了可靠的主动感知行为，在所有先前方法中表现最佳。当使用在主动感知任务上表现不佳的"通用"机器人策略初始化时，AAWR能有效生成信息收集行为，使其能在严重部分可观测条件下执行操作任务。

</details>


### [313] [RoboLoc: A Benchmark Dataset for Point Place Recognition and Localization in Indoor-Outdoor Integrated Environments](https://arxiv.org/abs/2512.01194)
*Jaejin Jeon,Seonghoon Ryoo,Sang-Duck Lee,Soomok Lee,Seungwoo Jeong*

Main category: cs.RO

TL;DR: RoboLoc是一个用于室内外一体化环境中点云地点识别与定位的基准数据集，填补了现有数据集在室内外无缝过渡方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有的LiDAR数据集主要关注室外场景，缺乏室内外无缝过渡的复杂环境，而机器人实际应用中经常需要在室内外环境中进行可靠定位，特别是在没有GPS信号的情况下。

Method: 提出了RoboLoc数据集，包含真实机器人轨迹、多样化的高程剖面以及结构化室内环境与非结构化室外环境之间的过渡。对多种最先进模型进行了基准测试，包括基于点、体素和BEV的架构。

Result: RoboLoc为开发多域定位系统提供了现实的测试平台，能够评估模型在室内外环境转换中的泛化能力。

Conclusion: RoboLoc填补了现有数据集在室内外一体化环境中的空白，为机器人定位和自主导航系统的开发提供了重要的基准测试资源。

Abstract: Robust place recognition is essential for reliable localization in robotics, particularly in complex environments with fre- quent indoor-outdoor transitions. However, existing LiDAR-based datasets often focus on outdoor scenarios and lack seamless domain shifts. In this paper, we propose RoboLoc, a benchmark dataset designed for GPS-free place recognition in indoor-outdoor environments with floor transitions. RoboLoc features real-world robot trajectories, diverse elevation profiles, and transitions between structured indoor and unstructured outdoor domains. We benchmark a variety of state-of-the-art models, point-based, voxel-based, and BEV-based architectures, highlighting their generalizability domain shifts. RoboLoc provides a realistic testbed for developing multi-domain localization systems in robotics and autonomous navigation

中文标题: RoboLoc：室内外一体化环境中点云地点识别与定位的基准数据集

中文摘要: 鲁棒的地点识别对于机器人中的可靠定位至关重要，特别是在频繁进行室内外转换的复杂环境中。然而，现有的基于LiDAR的数据集通常侧重于室外场景，缺乏无缝的域转换。在本文中，我们提出了RoboLoc，这是一个为室内外环境中无GPS地点识别设计的基准数据集，包含楼层转换。RoboLoc具有真实世界的机器人轨迹、多样化的高程剖面以及结构化室内域和非结构化室外域之间的转换。我们对各种最先进的模型进行了基准测试，包括基于点、体素和BEV的架构，突出了它们在域转换中的泛化能力。RoboLoc为机器人自主导航中开发多域定位系统提供了现实的测试平台。

</details>


### [314] [COMET: A Dual Swashplate Autonomous Coaxial Bi-copter AAV with High-Maneuverability and Long-Endurance](https://arxiv.org/abs/2512.01246)
*Shuai Wang,Xiaoming Tang,Junning Liang,Haowen Zheng,Biyu Ye,Zhaofeng Liu,Fei Gao,Ximin Lyu*

Main category: cs.RO

TL;DR: COMET是一种采用双倾斜盘机制的同轴双旋翼无人机，通过优化设计实现了高效率、高机动性和紧凑尺寸的平衡，在多种场景下验证了其实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 同轴双旋翼无人机因其转子系统效率高和外形紧凑而受到关注，但现有设计难以同时平衡效率、机动性和紧凑性，限制了实际应用。需要开发一种既能保持高效率又能实现高机动性的紧凑型同轴双旋翼系统。

Method: 采用双倾斜盘机制设计同轴双旋翼无人机平台COMET。通过台架测试优化系统效率和紧凑性，进行飞行续航实验验证不同负载条件下的性能，并通过综合轨迹跟踪测试评估机动性能。

Result: 双倾斜盘配置相比单倾斜盘方案显著提高了轨迹跟踪性能并改善了飞行效率。原型机在不同负载条件下表现出良好的效率和鲁棒性，在各种场景下成功完成自主飞行试验。

Conclusion: COMET平台通过双倾斜盘机制成功解决了同轴双旋翼无人机在效率、机动性和紧凑性之间的平衡问题，验证了其在实际应用中的可行性和潜力。

Abstract: Coaxial bi-copter autonomous aerial vehicles (AAVs) have garnered attention due to their potential for improved rotor system efficiency and compact form factor. However, balancing efficiency, maneuverability, and compactness in coaxial bi-copter systems remains a key design challenge, limiting their practical deployment. This letter introduces COMET, a coaxial bi-copter AAV platform featuring a dual swashplate mechanism. The coaxial bi-copter system's efficiency and compactness are optimized through bench tests, and the whole prototype's efficiency and robustness under varying payload conditions are verified through flight endurance experiments. The maneuverability performance of the system is evaluated in comprehensive trajectory tracking tests. The results indicate that the dual swashplate configuration enhances tracking performance and improves flight efficiency compared to the single swashplate alternative. Successful autonomous flight trials across various scenarios verify COMET's potential for real-world applications.

中文标题: COMET：一种具有高机动性和长续航能力的双倾斜盘自主同轴双旋翼无人机

中文摘要: 同轴双旋翼自主飞行器因其在转子系统效率和紧凑外形方面的潜力而受到关注。然而，在同轴双旋翼系统中平衡效率、机动性和紧凑性仍然是一个关键的设计挑战，限制了其实际部署。本文介绍了COMET，一种采用双倾斜盘机制的同轴双旋翼无人机平台。通过台架测试优化了同轴双旋翼系统的效率和紧凑性，并通过飞行续航实验验证了整个原型在不同负载条件下的效率和鲁棒性。在综合轨迹跟踪测试中评估了系统的机动性能。结果表明，与单倾斜盘方案相比，双倾斜盘配置提高了跟踪性能并改善了飞行效率。在各种场景下成功的自主飞行试验验证了COMET在实际应用中的潜力。

</details>


### [315] [Visibility-aware Cooperative Aerial Tracking with Decentralized LiDAR-based Swarms](https://arxiv.org/abs/2512.01280)
*Longji Yin,Yunfan Ren,Fangcheng Zhu,Liuyu Shi,Fanze Kong,Benxu Tang,Wenyi Liu,Ximin Lyu,Fu Zhang*

Main category: cs.RO

TL;DR: 提出了一种去中心化激光雷达集群框架，用于可见性感知的协同空中目标跟踪，通过SSDF遮挡表示、FOV对齐和静电势启发式分布实现多方向包围，在真实环境中验证了对敏捷目标的鲁棒跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在单无人机跟踪系统，而集群跟踪具有分布式感知、容错冗余和多方向覆盖的独特优势，但在复杂环境中的可见性感知协同跟踪仍未被充分探索。

Method: 1) 提出球面符号距离场（SSDF）用于三维环境遮挡表示和实时更新算法；2) 支持异构激光雷达配置的通用视场（FOV）对齐成本；3) 静电势启发式分布度量实现三维多方向目标包围；4) 分层规划器结合运动学前端搜索和时空SE(3)后端优化。

Result: 系统在杂乱室外环境中成功实现了对敏捷目标（无人机、人类）的鲁棒协同跟踪，展示了卓越的可见性维护能力和动态集群可重构性。

Conclusion: 该研究填补了集群跟踪领域的空白，提出的去中心化激光雷达框架在复杂环境中实现了可见性感知的协同跟踪，为监控、电影摄影和工业检测应用提供了有效的解决方案。

Abstract: Autonomous aerial tracking with drones offers vast potential for surveillance, cinematography, and industrial inspection applications. While single-drone tracking systems have been extensively studied, swarm-based target tracking remains underexplored, despite its unique advantages of distributed perception, fault-tolerant redundancy, and multidirectional target coverage. To bridge this gap, we propose a novel decentralized LiDAR-based swarm tracking framework that enables visibility-aware, cooperative target tracking in complex environments, while fully harnessing the unique capabilities of swarm systems. To address visibility, we introduce a novel Spherical Signed Distance Field (SSDF)-based metric for 3-D environmental occlusion representation, coupled with an efficient algorithm that enables real-time onboard SSDF updating. A general Field-of-View (FOV) alignment cost supporting heterogeneous LiDAR configurations is proposed for consistent target observation. Swarm coordination is enhanced through cooperative costs that enforce inter-robot safe clearance, prevent mutual occlusions, and notably facilitate 3-D multidirectional target encirclement via a novel electrostatic-potential-inspired distribution metric. These innovations are integrated into a hierarchical planner, combining a kinodynamic front-end searcher with a spatiotemporal $SE(3)$ back-end optimizer to generate collision-free, visibility-optimized trajectories.Deployed on heterogeneous LiDAR swarms, our fully decentralized implementation features collaborative perception, distributed planning, and dynamic swarm reconfigurability. Validated through rigorous real-world experiments in cluttered outdoor environments, the proposed system demonstrates robust cooperative tracking of agile targets (drones, humans) while achieving superior visibility maintenance.

中文标题: 基于去中心化激光雷达集群的可见性感知协同空中跟踪

中文摘要: 基于无人机的自主空中跟踪在监控、电影摄影和工业检测应用中具有巨大潜力。虽然单无人机跟踪系统已被广泛研究，但基于集群的目标跟踪仍然探索不足，尽管其具有分布式感知、容错冗余和多方向目标覆盖的独特优势。为了弥补这一差距，我们提出了一种新颖的去中心化激光雷达集群跟踪框架，能够在复杂环境中实现可见性感知的协同目标跟踪，同时充分利用集群系统的独特能力。针对可见性问题，我们引入了一种新颖的基于球面符号距离场（SSDF）的度量方法用于三维环境遮挡表示，并结合一种高效的算法实现实时机载SSDF更新。提出了一种支持异构激光雷达配置的通用视场（FOV）对齐成本，以实现一致的目标观测。通过协同成本增强集群协调，这些成本强制执行机器人间的安全间距，防止相互遮挡，并通过一种新颖的静电势启发式分布度量显著促进三维多方向目标包围。这些创新被集成到一个分层规划器中，结合了运动学前端搜索器和时空SE(3)后端优化器，以生成无碰撞、可见性优化的轨迹。部署在异构激光雷达集群上，我们的完全去中心化实现具有协作感知、分布式规划和动态集群可重构性。通过在杂乱室外环境中的严格真实世界实验验证，所提出的系统展示了对敏捷目标（无人机、人类）的鲁棒协同跟踪，同时实现了卓越的可见性维护。

</details>


### [316] [Discovering Self-Protective Falling Policy for Humanoid Robot via Deep Reinforcement Learning](https://arxiv.org/abs/2512.01336)
*Diyuan Shi,Shangke Lyu,Donglin Wang*

Main category: cs.RO

TL;DR: 通过深度强化学习和课程学习，让人形机器人自主发现形成"三角形"结构的摔倒保护策略，显著减少摔倒损伤，并成功迁移到真实机器人平台。


<details>
  <summary>Details</summary>
Motivation: 人形机器人由于其形态、动力学和控制策略的限制，比其他类型的机器人更容易摔倒。其重量大、质心高、自由度多的特点，在不受控摔倒时会造成严重的硬件损坏。现有研究主要使用基于控制的方法，难以适应多样化的摔倒场景，且可能引入不恰当的人类先验知识。

Method: 采用大规模深度强化学习和课程学习，通过精心设计的奖励函数和领域多样化课程，训练人形机器人探索摔倒保护行为。让机器人自主发现适合其自身特性和属性的保护策略。

Result: 成功训练人形机器人发现了通过形成"三角形"结构来减少摔倒损伤的保护策略。通过综合指标和实验，量化了其性能并与其他方法进行比较，可视化了摔倒行为，并成功迁移到真实世界平台。

Conclusion: 深度强化学习和课程学习能够让人形机器人自主发现有效的摔倒保护策略，特别是形成"三角形"结构可以显著减少刚性材料身体的摔倒损伤，该方法具有实际应用价值。

Abstract: Humanoid robots have received significant research interests and advancements in recent years. Despite many successes, due to their morphology, dynamics and limitation of control policy, humanoid robots are prone to fall as compared to other embodiments like quadruped or wheeled robots. And its large weight, tall Center of Mass, high Degree-of-Freedom would cause serious hardware damages when falling uncontrolled, to both itself and surrounding objects. Existing researches in this field mostly focus on using control based methods that struggle to cater diverse falling scenarios and may introduce unsuitable human prior. On the other hand, large-scale Deep Reinforcement Learning and Curriculum Learning could be employed to incentivize humanoid agent discovering falling protection policy that fits its own nature and property. In this work, with carefully designed reward functions and domain diversification curriculum, we successfully train humanoid agent to explore falling protection behaviors and discover that by forming a `triangle' structure, the falling damages could be significantly reduced with its rigid-material body. With comprehensive metrics and experiments, we quantify its performance with comparison to other methods, visualize its falling behaviors and successfully transfer it to real world platform.

中文标题: 通过深度强化学习发现人形机器人的自我保护摔倒策略

中文摘要: 近年来，人形机器人获得了显著的研究关注和进展。尽管取得了许多成功，但由于其形态、动力学和控制策略的限制，与其他类型的机器人（如四足或轮式机器人）相比，人形机器人更容易摔倒。其重量大、质心高、自由度多的特点，在不受控摔倒时会对自身和周围物体造成严重的硬件损坏。该领域的现有研究主要集中于使用基于控制的方法，这些方法难以适应多样化的摔倒场景，并可能引入不恰当的人类先验知识。另一方面，大规模深度强化学习和课程学习可用于激励人形机器人发现适合其自身特性和属性的摔倒保护策略。在这项工作中，通过精心设计的奖励函数和领域多样化课程，我们成功训练人形机器人探索摔倒保护行为，并发现通过形成"三角形"结构，可以显著减少其刚性材料身体的摔倒损伤。通过综合指标和实验，我们量化了其性能并与其他方法进行比较，可视化了其摔倒行为，并成功将其迁移到真实世界平台。

</details>


### [317] [Modality-Augmented Fine-Tuning of Foundation Robot Policies for Cross-Embodiment Manipulation on GR1 and G1](https://arxiv.org/abs/2512.01358)
*Junsung Park,Hogun Kee,Songhwai Oh*

Main category: cs.RO

TL;DR: 本文提出了一种模态增强微调框架，用于将基础机器人策略适配到不同的人形机器人平台。通过在GR1上引入接触信号和深度信息，在G1上使用高质量多模态数据集，显著提升了跨平台策略迁移的性能。


<details>
  <summary>Details</summary>
Motivation: 基础机器人策略通常针对特定机器人平台训练，难以直接迁移到不同的人形机器人平台。不同机器人平台在形态、传感器配置和动力学特性上存在差异，需要有效的跨平台适配方法。

Method: 提出模态增强微调框架：1) 对于GR1平台，使用公开数据集并引入后处理模态（二值接触信号和ZoeDepth生成的度量深度）；2) 对于Unitree G1平台，贡献了包含cuRobo运动规划、逆运动学和地面真实接触力测量的新型多模态数据集；3) 通过多模态融合增强策略性能。

Result: 模态增强显著提升策略性能：1) GR1上，接触状态线索和RGB-D融合将在线成功率从51%提升到63%；2) G1的"拾取苹果放入碗中"任务中，接触增强模型达到94%成功率，远超标准微调的48%和零样本迁移的0%基线。

Conclusion: 轻量级后处理能有效增强GR1策略，而高质量多模态数据对Unitree G1的可靠迁移至关重要。这项工作为通过针对性模态设计和多模态微调扩展基础机器人策略建立了统一的数据中心化路径。

Abstract: This paper presents a modality-augmented fine-tuning framework designed to adapt foundation robot policies to diverse humanoid embodiments. We validate our approach across two distinct settings: (i) the GR1 embodiment, utilizing public datasets where we introduce post-processed modalities, including binary contact signals and ZoeDepth-generated metric depth; and (ii) the Unitree G1 embodiment, for which we contribute a novel multi-modal dataset incorporating cuRobo motion planning, inverse kinematics, and ground-truth contact-force measurements. Our experiments demonstrate that modality augmentation consistently enhances policy performance across different embodiments. Specifically, for the GR1, integrating contact-state cues and RGB-D fusion improves online success rates from 51% to 63%. Furthermore, in the G1 "Pick Apple to Bowl" task, our contact-augmented model achieves a success rate of 94%, significantly outperforming the 48% achieved by standard fine-tuning and the 0% baseline of zero-shot transfer. These results highlight that lightweight post-processing effectively strengthens policies for GR1, while high-quality multi-modal data is crucial for reliable transfer to the Unitree G1. Consequently, this work establishes a unified, data-centric pathway for extending foundation robot policies through targeted modality design and multi-modal fine-tuning.

中文标题: 基础机器人策略的模态增强微调：面向GR1和G1的跨平台操作

中文摘要: 本文提出了一种模态增强微调框架，旨在将基础机器人策略适配到多样化的人形机器人平台。我们在两个不同设置中验证了该方法：(i) GR1平台，使用公开数据集并引入后处理模态，包括二值接触信号和ZoeDepth生成的度量深度；(ii) Unitree G1平台，为此我们贡献了一个新颖的多模态数据集，包含cuRobo运动规划、逆运动学和地面真实接触力测量。实验表明，模态增强在不同平台上持续提升策略性能。具体而言，对于GR1，整合接触状态线索和RGB-D融合将在线成功率从51%提高到63%。此外，在G1的"拾取苹果放入碗中"任务中，我们的接触增强模型达到94%的成功率，显著优于标准微调的48%和零样本迁移的0%基线。这些结果表明，轻量级后处理能有效增强GR1策略，而高质量多模态数据对Unitree G1的可靠迁移至关重要。因此，这项工作通过针对性模态设计和多模态微调，为扩展基础机器人策略建立了统一的数据中心化路径。

</details>


### [318] [$\mathbf{M^3A}$ Policy: Mutable Material Manipulation Augmentation Policy through Photometric Re-rendering](https://arxiv.org/abs/2512.01446)
*Jiayi Li,Yuxuan Hu,Haoran Geng,Xiangyu Chen,Chuhao Zhou,Ziteng Cui,Jianfei Yang*

Main category: cs.RO

TL;DR: M^3A策略通过光度重渲染技术，利用单个真实演示生成多种材料外观的合成数据，实现机器人操作策略的跨材料泛化，无需额外数据收集。


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器人需要处理具有不同材料特性的物体（如玻璃、金属等），这些材料的透明或反射表面导致严重的视觉分布外变化。现有方法要么受限于模拟到真实的视觉域差距，要么需要大量昂贵且难以覆盖所有材料的真实数据收集。

Method: 提出M^3A框架，基于计算摄影学原理，利用材料的光传输物理特性进行光度重渲染。给定单个真实世界演示，通过改变材料属性重新渲染场景，生成具有不同材料外观的高度真实合成演示数据。

Result: 构建了首个全面的多材料操作基准。实验表明M^3A策略显著提升跨材料泛化能力，在三个真实世界任务中平均成功率提高58.03%，并在未见材料上表现稳健。

Conclusion: M^3A策略通过光度重渲染技术有效解耦操作技能与材料外观，实现了高效的跨材料泛化，为解决机器人操作中的材料多样性问题提供了创新解决方案。

Abstract: Material generalization is essential for real-world robotic manipulation, where robots must interact with objects exhibiting diverse visual and physical properties. This challenge is particularly pronounced for objects made of glass, metal, or other materials whose transparent or reflective surfaces introduce severe out-of-distribution variations. Existing approaches either rely on simulated materials in simulators and perform sim-to-real transfer, which is hindered by substantial visual domain gaps, or depend on collecting extensive real-world demonstrations, which is costly, time-consuming, and still insufficient to cover various materials. To overcome these limitations, we resort to computational photography and introduce Mutable Material Manipulation Augmentation (M$^3$A), a unified framework that leverages the physical characteristics of materials as captured by light transport for photometric re-rendering. The core idea is simple yet powerful: given a single real-world demonstration, we photometrically re-render the scene to generate a diverse set of highly realistic demonstrations with different material properties. This augmentation effectively decouples task-specific manipulation skills from surface appearance, enabling policies to generalize across materials without additional data collection. To systematically evaluate this capability, we construct the first comprehensive multi-material manipulation benchmark spanning both simulation and real-world environments. Extensive experiments show that the M$^3$A policy significantly enhances cross-material generalization, improving the average success rate across three real-world tasks by 58.03\%, and demonstrating robust performance on previously unseen materials.

中文标题: M^3A策略：通过光度重渲染实现可变材料操作增强策略

中文摘要: 材料泛化对于现实世界机器人操作至关重要，机器人必须与具有不同视觉和物理特性的物体交互。这一挑战对于由玻璃、金属或其他材料制成的物体尤为突出，这些材料的透明或反射表面引入了严重的分布外变化。现有方法要么依赖于模拟器中的模拟材料并执行模拟到真实转移，这受到显著视觉域差距的阻碍；要么依赖于收集大量真实世界演示，这成本高、耗时，并且仍然不足以覆盖各种材料。为了克服这些限制，我们转向计算摄影学，并引入了可变材料操作增强（M^3A），这是一个统一框架，利用材料的光传输物理特性进行光度重渲染。核心思想简单而强大：给定单个真实世界演示，我们通过光度重渲染场景，生成具有不同材料特性的多样化高度真实演示。这种增强有效地将任务特定操作技能与表面外观解耦，使策略能够在无需额外数据收集的情况下跨材料泛化。为了系统评估这一能力，我们构建了第一个全面的多材料操作基准，涵盖模拟和真实世界环境。大量实验表明，M^3A策略显著增强了跨材料泛化能力，在三个真实世界任务中平均成功率提高了58.03%，并在先前未见材料上表现出稳健性能。

</details>


### [319] [NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction](https://arxiv.org/abs/2512.01550)
*Fei Liu,Shichao Xie,Minghua Luo,Zedong Chu,Junjun Hu,Xiaolong Wu,Mu Xu*

Main category: cs.RO

TL;DR: NavForesee是一个统一的视觉语言世界模型，通过结合语言任务分解和双视野（短期动态+长期里程碑）预测，实现更智能的具身导航规划。


<details>
  <summary>Details</summary>
Motivation: 现有具身导航智能体在处理复杂自然语言指令的长时程任务时，难以对未见环境进行稳健的长期规划，导致高失败率。需要一种能够同时进行高级语言规划和环境预测的统一框架。

Method: 提出NavForesee模型，将语言任务分解（理解指令、跟踪进度、制定子目标）与生成式世界模型预测相结合。模型同时预测短期环境动态和长期导航里程碑，形成感知-规划/预测-行动的反馈循环。

Result: 在R2R-CE和RxR-CE基准测试上取得了极具竞争力的性能，证明了融合语言规划和时空预测的有效性。

Conclusion: NavForesee展示了将显式语言规划与隐式时空预测融合的巨大潜力，为更智能、更强大的具身智能体提供了新的方向。

Abstract: Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.

中文标题: NavForesee：用于分层规划和双视野导航预测的统一视觉语言世界模型

中文摘要: 在复杂自然语言指令引导下，面向长时程任务的具身导航仍然是人工智能领域的一项艰巨挑战。现有智能体常常难以对未见环境进行稳健的长期规划，导致高失败率。为解决这些限制，我们引入了NavForesee，一种新颖的视觉语言模型（VLM），它将高级语言规划和预测性世界模型想象统一在一个单一的框架内。我们的方法使单个VLM能够同时执行规划和预测性前瞻。基于完整指令和历史观测进行条件化，该模型通过分解任务、跟踪进度和制定后续子目标来理解导航指令。同时，它作为一个生成式世界模型，通过预测短期环境动态和长期导航里程碑来提供关键的前瞻性信息。VLM的结构化计划指导其有针对性的预测，而想象的未来则为导航行动提供丰富的上下文信息，创建了一个强大的感知-规划/预测-行动内部反馈循环。我们通过在R2R-CE和RxR-CE基准测试上的广泛实验证明，NavForesee在复杂场景中实现了极具竞争力的性能。我们的工作凸显了将显式语言规划与隐式时空预测融合的巨大潜力，为更智能、更强大的具身智能体铺平了道路。

</details>


### [320] [L2M-Calib: One-key Calibration Method for LiDAR and Multiple Magnetic Sensors](https://arxiv.org/abs/2512.01554)
*Qiyang Lyu,Wei Wang,Zhenyu Wu,Hongming Shen,Huiqin Zhou,Danwei Wang*

Main category: cs.RO

TL;DR: L2M-Calib是一种用于激光雷达和磁传感器融合系统的一键标定方法，能同时估计传感器间的外部变换和磁传感器的内部失真参数，在各种环境下表现出高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态传感器融合需要准确的标定，但磁传感器集成到多模态系统中缺乏有效的标定技术。磁传感器对环境磁场模式的感知具有几何、纹理、光照和天气不变性，适用于挑战性环境，但需要解决标定问题才能充分发挥其优势。

Method: 提出L2M-Calib一键标定框架：1）使用迭代高斯-牛顿方案优化激光雷达与磁传感器之间的外部变换参数；2）将磁传感器的内部失真参数标定建模为加权岭正则化总体最小二乘（w-RRTLS）问题，以提高对噪声和病态数据的鲁棒性。

Result: 在模拟数据集和真实世界实验（包括AGV安装的传感器配置）上的评估表明，该方法在各种环境和操作条件下实现了高标定精度和鲁棒性，验证了其有效性。

Conclusion: L2M-Calib为激光雷达与多磁传感器融合系统提供了一种有效的一键标定解决方案，解决了磁传感器在多模态系统中集成的标定难题，为在挑战性环境下的鲁棒感知应用奠定了基础。

Abstract: Multimodal sensor fusion enables robust environmental perception by leveraging complementary information from heterogeneous sensing modalities. However, accurate calibration is a critical prerequisite for effective fusion. This paper proposes a novel one-key calibration framework named L2M-Calib for a fused magnetic-LiDAR system, jointly estimating the extrinsic transformation between the two kinds of sensors and the intrinsic distortion parameters of the magnetic sensors. Magnetic sensors capture ambient magnetic field (AMF) patterns, which are invariant to geometry, texture, illumination, and weather, making them suitable for challenging environments. Nonetheless, the integration of magnetic sensing into multimodal systems remains underexplored due to the absence of effective calibration techniques. To address this, we optimize extrinsic parameters using an iterative Gauss-Newton scheme, coupled with the intrinsic calibration as a weighted ridge-regularized total least squares (w-RRTLS) problem, ensuring robustness against measurement noise and ill-conditioned data. Extensive evaluations on both simulated datasets and real-world experiments, including AGV-mounted sensor configurations, demonstrate that our method achieves high calibration accuracy and robustness under various environmental and operational conditions.

中文标题: L2M-Calib：激光雷达与多磁传感器的一键标定方法

中文摘要: 多模态传感器融合通过利用异构传感模式的互补信息，实现了鲁棒的环境感知。然而，准确的标定是有效融合的关键前提。本文提出了一种新颖的一键标定框架L2M-Calib，用于融合磁-LiDAR系统，联合估计两种传感器之间的外部变换以及磁传感器的内部失真参数。磁传感器捕获环境磁场（AMF）模式，这些模式对几何、纹理、光照和天气具有不变性，使其适用于具有挑战性的环境。尽管如此，由于缺乏有效的标定技术，将磁传感集成到多模态系统中仍然未被充分探索。为了解决这个问题，我们使用迭代高斯-牛顿方案优化外部参数，同时将内部标定作为加权岭正则化总体最小二乘（w-RRTLS）问题，确保对测量噪声和病态数据的鲁棒性。在模拟数据集和真实世界实验（包括AGV安装的传感器配置）上的广泛评估表明，我们的方法在各种环境和操作条件下实现了高标定精度和鲁棒性。

</details>


### [321] [A Cross-Embodiment Gripper Benchmark for Rigid-Object Manipulation in Aerial and Industrial Robotics](https://arxiv.org/abs/2512.01598)
*Marek Vagas,Martin Varga,Jaroslav Romancik,Ondrej Majercak,Alejandro Suarez,Anibal Ollero,Bram Vanderborght,Ivan Virgala*

Main category: cs.RO

TL;DR: CEGB是一个跨平台夹具基准测试套件，扩展了YCB和NIST标准，增加了转移时间、能耗和理想有效载荷评估，为异构机器人系统的夹具重用提供标准化评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有YCB和NIST基准测试仅针对单一平台评估抓取成功率、力或时间，无法评估跨平台可移植性和能耗感知性能，而这对现代移动和空中操作至关重要。

Method: 开发了跨平台夹具基准测试套件，包含三个新组件：转移时间基准（测量更换夹具的实际工作量）、能耗基准（评估抓取和保持效率）、意图特定理想有效载荷评估（反映设计相关的操作能力）。

Result: 实验显示快速夹具转移（中位数约17.6秒）、低保持能耗（约1.5焦耳/10秒）、一致的抓取性能（周期时间3.2-3.9秒，成功率超过90%）。

Conclusion: CEGB为空中和工业机器人领域的跨平台、能耗感知夹具评估提供了可重复的基础，支持单个夹具在异构机器人系统中的重用评估。

Abstract: Robotic grippers are increasingly deployed across industrial, collaborative, and aerial platforms, where each embodiment imposes distinct mechanical, energetic, and operational constraints. Established YCB and NIST benchmarks quantify grasp success, force, or timing on a single platform, but do not evaluate cross-embodiment transferability or energy-aware performance, capabilities essential for modern mobile and aerial manipulation. This letter introduces the Cross-Embodiment Gripper Benchmark (CEGB), a compact and reproducible benchmarking suite extending YCB and selected NIST metrics with three additional components: a transfer-time benchmark measuring the practical effort required to exchange embodiments, an energy-consumption benchmark evaluating grasping and holding efficiency, and an intent-specific ideal payload assessment reflecting design-dependent operational capability. Together, these metrics characterize both grasp performance and the suitability of reusing a single gripper across heterogeneous robotic systems. A lightweight self-locking gripper prototype is implemented as a reference case. Experiments demonstrate rapid embodiment transfer (median ~= 17.6 s across user groups), low holding energy for gripper prototype (~= 1.5 J per 10 s), and consistent grasp performance with cycle times of 3.2 - 3.9 s and success rates exceeding 90%. CEGB thus provides a reproducible foundation for cross-platform, energy-aware evaluation of grippers in aerial and manipulators domains.

中文标题: 面向空中和工业机器人刚性物体操作的跨平台夹具基准测试

中文摘要: 机器人夹具越来越多地部署在工业、协作和空中平台上，每种平台都施加了不同的机械、能量和操作约束。现有的YCB和NIST基准测试在单一平台上量化抓取成功率、力或时间，但无法评估跨平台可移植性或能耗感知性能，而这些能力对现代移动和空中操作至关重要。本文介绍了跨平台夹具基准测试，这是一个紧凑且可重复的基准测试套件，扩展了YCB和选定的NIST指标，增加了三个组件：转移时间基准（测量更换夹具的实际工作量）、能耗基准（评估抓取和保持效率）、意图特定理想有效载荷评估（反映设计相关的操作能力）。这些指标共同表征了抓取性能以及单个夹具在异构机器人系统中重用的适用性。实现了一个轻量级自锁夹具原型作为参考案例。实验展示了快速夹具转移（用户组间中位数约17.6秒）、夹具原型的低保持能耗（约1.5焦耳/10秒）以及一致的抓取性能（周期时间3.2-3.9秒，成功率超过90%）。因此，CEGB为空中和操作器领域的跨平台、能耗感知夹具评估提供了可重复的基础。

</details>


### [322] [Integrated YOLOP Perception and Lyapunov-based Control for Autonomous Mobile Robot Navigation on Track](https://arxiv.org/abs/2512.01608)
*Mo Chen*

Main category: cs.RO

TL;DR: 提出了一种结合YOLOP视觉感知和李雅普诺夫控制器的实时自主轨道导航系统，用于差速驱动移动机器人，无需高清地图或GPS，通过实验验证了系统性能。


<details>
  <summary>Details</summary>
Motivation: 当前自主导航系统通常依赖高清地图或全球定位系统，这限制了其在无基础设施环境中的应用。需要开发一种不依赖先验地图和卫星定位的实时自主导航解决方案，特别是在动态和部分感知的轨道场景中。

Method: 1. 感知部分：使用YOLOP进行多任务视觉感知，通过2D到3D相机投影重建车道中心线，采用基于弧长的均匀点重采样和三次多项式拟合（鲁棒QR最小二乘优化）。2. 控制部分：基于李雅普诺夫稳定性理论设计控制器，调节机器人线速度和角速度，确保位置和航向偏差的有界性和渐近收敛。

Result: 在嵌入式平台上进行的真实世界实验验证了：1) 系统保真度；2) 实时执行能力；3) 轨迹平滑度；4) 闭环稳定性。系统能够在动态和部分感知的车道场景中实现可靠的自主导航。

Conclusion: 该研究成功开发了一种不依赖高清地图或全球卫星定位的实时自主轨道导航系统，通过集成YOLOP视觉感知和李雅普诺夫控制器，实现了在动态环境中的稳定导航，为无基础设施环境下的自主移动机器人应用提供了有效解决方案。

Abstract: This work presents a real-time autonomous track navigation framework for nonholonomic differential-drive mobile robots by jointly integrating multi-task visual perception and a provably stable tracking controller. The perception pipeline reconstructs lane centerlines using 2D-to-3D camera projection, arc-length based uniform point resampling, and cubic polynomial fitting solved via robust QR least-squares optimization. The controller regulates robot linear and angular velocities through a Lyapunov-stability grounded design, ensuring bounded error dynamics and asymptotic convergence of position and heading deviations even in dynamic and partially perceived lane scenarios, without relying on HD prior maps or global satellite localization. Real-world experiments on embedded platforms verify system fidelity, real-time execution, trajectory smoothness, and closed-loop stability for reliable autonomous navigation.

中文标题: 集成YOLOP感知与李雅普诺夫控制的自移动机器人轨道导航系统

中文摘要: 本研究提出了一种用于非完整差速驱动移动机器人的实时自主轨道导航框架，通过联合集成多任务视觉感知和可证明稳定的跟踪控制器。感知管道通过2D到3D相机投影、基于弧长的均匀点重采样以及通过鲁棒QR最小二乘优化求解的三次多项式拟合来重建车道中心线。控制器通过基于李雅普诺夫稳定性的设计调节机器人线速度和角速度，确保即使在动态和部分感知的车道场景中，位置和航向偏差的有界误差动态和渐近收敛，而不依赖于高清先验地图或全球卫星定位。在嵌入式平台上的真实世界实验验证了系统保真度、实时执行、轨迹平滑度和闭环稳定性，实现了可靠的自主导航。

</details>


### [323] [Dynamic Log-Gaussian Process Control Barrier Function for Safe Robotic Navigation in Dynamic Environments](https://arxiv.org/abs/2512.01668)
*Xin Yin,Chenyang Liang,Yanning Guo,Jie Mei*

Main category: cs.RO

TL;DR: 本文提出了一种基于高斯过程的动态对数高斯过程控制屏障函数（DLGP-CBF），用于在动态环境中实现机器人的安全导航。该方法通过GP回归的对数变换生成平滑的屏障值和梯度，并显式建模障碍物位置，使控制器能够主动响应动态障碍物的运动。


<details>
  <summary>Details</summary>
Motivation: 控制屏障函数（CBFs）已成为解决机器人安全导航问题的有效工具，但在未知动态环境中，利用实时传感器数据在线合成信息丰富且能感知障碍物运动的CBFs仍然具有挑战性。现有方法难以同时实现空间信息丰富性和对动态障碍物的响应性。

Method: 提出动态对数高斯过程控制屏障函数（DLGP-CBF）：1）利用高斯过程回归的对数变换，即使在稀疏数据区域也能生成平滑且信息丰富的屏障值和梯度；2）将DLGP-CBF显式建模为障碍物位置的函数，使推导出的安全约束能够整合预测的障碍物速度，让控制器能够主动响应动态障碍物的运动。

Result: 仿真结果表明，与基线方法相比，DLGP-CBF在障碍物避让性能方面有显著改进，包括：增加了安全裕度、产生了更平滑的轨迹、并增强了系统对动态障碍物的响应能力。

Conclusion: DLGP-CBF方法能够实时构建既具有空间信息丰富性又能响应障碍物运动的控制屏障函数，有效解决了在未知动态环境中机器人安全导航的挑战，为动态环境中的安全控制提供了新思路。

Abstract: Control Barrier Functions (CBFs) have emerged as efficient tools to address the safe navigation problem for robot applications. However, synthesizing informative and obstacle motion-aware CBFs online using real-time sensor data remains challenging, particularly in unknown and dynamic scenarios. Motived by this challenge, this paper aims to propose a novel Gaussian Process-based formulation of CBF, termed the Dynamic Log Gaussian Process Control Barrier Function (DLGP-CBF), to enable real-time construction of CBF which are both spatially informative and responsive to obstacle motion. Firstly, the DLGP-CBF leverages a logarithmic transformation of GP regression to generate smooth and informative barrier values and gradients, even in sparse-data regions. Secondly, by explicitly modeling the DLGP-CBF as a function of obstacle positions, the derived safety constraint integrates predicted obstacle velocities, allowing the controller to proactively respond to dynamic obstacles' motion. Simulation results demonstrate significant improvements in obstacle avoidance performance, including increased safety margins, smoother trajectories, and enhanced responsiveness compared to baseline methods.

中文标题: 动态对数高斯过程控制屏障函数用于动态环境中的机器人安全导航

中文摘要: 控制屏障函数（CBFs）已成为解决机器人应用安全导航问题的有效工具。然而，在未知和动态场景中，利用实时传感器数据在线合成信息丰富且能感知障碍物运动的CBFs仍然具有挑战性。基于这一挑战，本文旨在提出一种基于高斯过程的CBF新公式，称为动态对数高斯过程控制屏障函数（DLGP-CBF），以实现实时构建既具有空间信息丰富性又能响应障碍物运动的CBF。首先，DLGP-CBF利用高斯过程回归的对数变换生成平滑且信息丰富的屏障值和梯度，即使在稀疏数据区域也能实现。其次，通过将DLGP-CBF显式建模为障碍物位置的函数，推导出的安全约束整合了预测的障碍物速度，使控制器能够主动响应动态障碍物的运动。仿真结果表明，与基线方法相比，该方法在障碍物避让性能方面有显著改进，包括增加了安全裕度、产生了更平滑的轨迹并增强了响应能力。

</details>


### [324] [DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models](https://arxiv.org/abs/2512.01715)
*Wanpeng Zhang,Ye Wang,Hao Luo,Haoqi Yuan,Yicheng Feng,Sipeng Zheng,Qin Jin,Zongqing Lu*

Main category: cs.RO

TL;DR: DiG-Flow提出了一种通过几何正则化增强VLA模型鲁棒性的框架，利用观测和动作嵌入之间的分布差异作为指导信号，在流匹配前对表示进行残差更新，显著提升了复杂任务和分布偏移下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流匹配训练的视觉-语言-动作模型在分布偏移和复杂多步任务上性能下降，表明学习到的表示可能无法鲁棒地捕捉任务相关的语义信息，需要一种增强表示鲁棒性的方法。

Method: DiG-Flow计算观测和动作嵌入的经验分布之间的差异度量，通过单调函数将其映射为调制权重，在流匹配前对观测嵌入应用残差更新。该方法在表示层面进行干预，不修改流匹配路径或目标向量场。

Result: DiG-Flow以可忽略的开销集成到现有VLA架构中，一致地提升了性能，特别是在复杂多步任务和有限训练数据条件下表现出显著的性能增益。

Conclusion: DiG-Flow通过几何正则化有效增强了VLA模型的鲁棒性，分布差异提供了有意义的几何信号，指导表示学习并改善模型在分布偏移和复杂任务上的表现。

Abstract: Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data.

中文标题: DiG-Flow：基于差异指导的流匹配用于鲁棒的VLA模型

中文摘要: 基于流匹配训练的视觉-语言-动作模型在机器人操作任务上表现出令人印象深刻的能力。然而，它们的性能在分布偏移和复杂多步任务上常常下降，这表明学习到的表示可能无法鲁棒地捕捉任务相关的语义。我们提出了DiG-Flow，一个通过几何正则化增强VLA鲁棒性的原则性框架。我们的关键见解是观测和动作嵌入之间的分布差异提供了有意义的几何信号：较低的传输成本表示兼容的表示，而较高的成本表明潜在的不对齐。DiG-Flow计算观测和动作嵌入的经验分布之间的差异度量，通过单调函数将其映射为调制权重，并在流匹配前对观测嵌入应用残差更新。重要的是，这种干预在表示层面操作，不修改流匹配路径或目标向量场。我们提供了理论保证，表明差异指导的训练可证明地降低了训练目标，并且指导的推理细化具有收敛性收缩。实证上，DiG-Flow以可忽略的开销集成到现有VLA架构中，并一致地提升了性能，在复杂多步任务和有限训练数据条件下表现出特别显著的增益。

</details>


### [325] [AgriLiRa4D: A Multi-Sensor UAV Dataset for Robust SLAM in Challenging Agricultural Fields](https://arxiv.org/abs/2512.01753)
*Zhihao Zhan,Yuhang Ming,Shaobin Li,Jie Yuan*

Main category: cs.RO

TL;DR: AgriLiRa4D是一个多传感器无人机数据集，专门针对具有挑战性的农业环境，包含三种农田类型和两种操作模式，提供高精度地面真实轨迹和多种传感器数据，用于评估SLAM算法在农业场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够支持农业无人机鲁棒操作研究的真实世界多模态数据集，而农业环境中的低纹理作物、重复模式、动态植被等挑战使得传统SLAM方法难以可靠工作。

Method: 在三种代表性农田类型（平坦、丘陵、梯田）和两种操作模式（边界、覆盖）下收集数据，使用FINS_RTK系统提供高精度地面真实轨迹，同步采集3D LiDAR、4D雷达和IMU数据，并进行完整的传感器校准。

Result: 创建了包含六个飞行序列组的多模态数据集，对四种最先进的多传感器SLAM算法进行了基准测试，展示了农业环境中SLAM的挑战性以及多模态方法的必要性。

Conclusion: AgriLiRa4D填补了农业SLAM数据集的关键空白，为研究社区提供了宝贵的基准资源，有助于推进农业无人机自主导航技术的发展，特别是在具有挑战性的真实农业环境中。

Abstract: Multi-sensor Simultaneous Localization and Mapping (SLAM) is essential for Unmanned Aerial Vehicles (UAVs) performing agricultural tasks such as spraying, surveying, and inspection. However, real-world, multi-modal agricultural UAV datasets that enable research on robust operation remain scarce. To address this gap, we present AgriLiRa4D, a multi-modal UAV dataset designed for challenging outdoor agricultural environments. AgriLiRa4D spans three representative farmland types-flat, hilly, and terraced-and includes both boundary and coverage operation modes, resulting in six flight sequence groups. The dataset provides high-accuracy ground-truth trajectories from a Fiber Optic Inertial Navigation System with Real-Time Kinematic capability (FINS_RTK), along with synchronized measurements from a 3D LiDAR, a 4D Radar, and an Inertial Measurement Unit (IMU), accompanied by complete intrinsic and extrinsic calibrations. Leveraging its comprehensive sensor suite and diverse real-world scenarios, AgriLiRa4D supports diverse SLAM and localization studies and enables rigorous robustness evaluation against low-texture crops, repetitive patterns, dynamic vegetation, and other challenges of real agricultural environments. To further demonstrate its utility, we benchmark four state-of-the-art multi-sensor SLAM algorithms across different sensor combinations, highlighting the difficulty of the proposed sequences and the necessity of multi-modal approaches for reliable UAV localization. By filling a critical gap in agricultural SLAM datasets, AgriLiRa4D provides a valuable benchmark for the research community and contributes to advancing autonomous navigation technologies for agricultural UAVs. The dataset can be downloaded from: https://zhan994.github.io/AgriLiRa4D.

中文标题: AgriLiRa4D：面向挑战性农田环境的多传感器无人机鲁棒SLAM数据集

中文摘要: 多传感器同时定位与建图（SLAM）对于执行喷洒、勘测和检查等农业任务的无人机至关重要。然而，能够支持鲁棒操作研究的真实世界多模态农业无人机数据集仍然稀缺。为填补这一空白，我们提出了AgriLiRa4D，这是一个专为具有挑战性的户外农业环境设计的多模态无人机数据集。AgriLiRa4D涵盖三种代表性农田类型——平坦、丘陵和梯田——并包括边界和覆盖两种操作模式，共形成六个飞行序列组。该数据集提供了来自光纤惯性导航系统与实时动态定位功能（FINS_RTK）的高精度地面真实轨迹，以及来自3D激光雷达、4D雷达和惯性测量单元（IMU）的同步测量数据，并附带完整的内外参校准。凭借其全面的传感器套件和多样化的真实世界场景，AgriLiRa4D支持多样化的SLAM和定位研究，并能够针对低纹理作物、重复模式、动态植被等真实农业环境的挑战进行严格的鲁棒性评估。为进一步展示其实用性，我们对四种最先进的多传感器SLAM算法在不同传感器组合下进行了基准测试，突显了所提出序列的难度以及多模态方法对于可靠无人机定位的必要性。通过填补农业SLAM数据集的关键空白，AgriLiRa4D为研究社区提供了宝贵的基准，并有助于推进农业无人机自主导航技术的发展。数据集可从以下网址下载：https://zhan994.github.io/AgriLiRa4D。

</details>


### [326] [IGen: Scalable Data Generation for Robot Learning from Open-World Images](https://arxiv.org/abs/2512.01773)
*Chenghao Gu,Haolan Kang,Junchao Lin,Jinghe Wang,Duo Wu,Shuzhao Xie,Fanding Huang,Junchen Ge,Ziyang Gong,Letian Li,Hongying Zheng,Changwei Lv,Zhi Wang*

Main category: cs.RO

TL;DR: IGen是一个从开放世界图像中可扩展生成机器人学习数据的框架，将2D像素转换为3D场景表示，利用视觉语言模型生成任务指令和SE(3)末端执行器位姿序列，并合成动态场景演化和视觉观测。


<details>
  <summary>Details</summary>
Motivation: 通用机器人策略的兴起对大规模训练数据产生了指数级需求，但机器人数据收集劳动密集且环境受限。开放世界图像捕捉了与机器人操作任务自然对齐的多样化真实场景，但缺乏相关机器人动作限制了其实际应用。

Method: IGen首先将非结构化2D像素转换为适合场景理解和操作的3D场景表示，然后利用视觉语言模型将场景特定任务指令转换为高级计划，生成SE(3)末端执行器位姿序列作为低级动作，最后合成动态场景演化并渲染时间一致的视觉观测。

Result: 实验验证了IGen生成的视觉运动数据的高质量，仅使用IGen合成数据训练的机器人策略实现了与真实世界数据训练相当的性能。

Conclusion: IGen展示了从开放世界图像进行可扩展数据生成的潜力，能够支持通用机器人策略训练，为解决机器人学习数据稀缺问题提供了新途径。

Abstract: The rise of generalist robotic policies has created an exponential demand for large-scale training data. However, on-robot data collection is labor-intensive and often limited to specific environments. In contrast, open-world images capture a vast diversity of real-world scenes that naturally align with robotic manipulation tasks, offering a promising avenue for low-cost, large-scale robot data acquisition. Despite this potential, the lack of associated robot actions hinders the practical use of open-world images for robot learning, leaving this rich visual resource largely unexploited. To bridge this gap, we propose IGen, a framework that scalably generates realistic visual observations and executable actions from open-world images. IGen first converts unstructured 2D pixels into structured 3D scene representations suitable for scene understanding and manipulation. It then leverages the reasoning capabilities of vision-language models to transform scene-specific task instructions into high-level plans and generate low-level actions as SE(3) end-effector pose sequences. From these poses, it synthesizes dynamic scene evolution and renders temporally coherent visual observations. Experiments validate the high quality of visuomotor data generated by IGen, and show that policies trained solely on IGen-synthesized data achieve performance comparable to those trained on real-world data. This highlights the potential of IGen to support scalable data generation from open-world images for generalist robotic policy training.

中文标题: IGen：从开放世界图像中可扩展生成机器人学习数据

中文摘要: 通用机器人策略的兴起对大规模训练数据产生了指数级需求。然而，机器人数据收集劳动密集且通常限于特定环境。相比之下，开放世界图像捕捉了与机器人操作任务自然对齐的多样化真实场景，为低成本、大规模机器人数据获取提供了有前景的途径。尽管有这种潜力，但缺乏相关机器人动作阻碍了开放世界图像在机器人学习中的实际应用，使得这一丰富的视觉资源未被充分利用。为弥合这一差距，我们提出了IGen，一个从开放世界图像中可扩展生成真实视觉观测和可执行动作的框架。IGen首先将非结构化2D像素转换为适合场景理解和操作的3D场景表示，然后利用视觉语言模型的推理能力将场景特定任务指令转换为高级计划，并生成SE(3)末端执行器位姿序列作为低级动作。从这些位姿中，它合成动态场景演化并渲染时间一致的视觉观测。实验验证了IGen生成的视觉运动数据的高质量，并表明仅使用IGen合成数据训练的机器人策略实现了与真实世界数据训练相当的性能。这突显了IGen支持从开放世界图像进行可扩展数据生成以训练通用机器人策略的潜力。

</details>


### [327] [GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation](https://arxiv.org/abs/2512.01801)
*Yunfei Li,Xiao Ma,Jiafeng Xu,Yu Cui,Zhongren Cui,Zhigang Han,Liqun Huang,Tao Kong,Yuxiao Liu,Hao Niu,Wanli Peng,Jingchao Qiao,Zeyu Ren,Haixin Shi,Zhi Su,Jiawen Tian,Yuyang Xiao,Shenyu Zhang,Liwei Zheng,Hang Li,Yonghui Wu*

Main category: cs.RO

TL;DR: GR-RL是一个机器人学习框架，通过多阶段训练将通用视觉-语言-动作策略转化为专门处理长时程灵巧操作任务的专家系统，首次实现了自主系鞋带等复杂任务。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作策略假设人类演示是最优的，但在高度灵巧和精确的操作任务中，人类演示存在噪声且非最优，需要更鲁棒的训练方法。

Method: 采用三阶段训练流程：1) 学习视觉-语言条件任务进度函数，过滤演示轨迹；2) 引入形态对称增强提升泛化能力；3) 通过在线强化学习学习潜在空间噪声预测器，对齐策略与部署行为。

Result: GR-RL首次实现了自主系鞋带任务，成功率达到83.3%，该任务需要长时程推理、毫米级精度和柔顺的软体交互能力。

Conclusion: GR-RL为通用机器人基础模型向可靠现实世界专家转化提供了重要步骤，展示了通过强化学习增强演示数据在复杂灵巧操作任务中的有效性。

Abstract: We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting $Q$-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundations models to specialize into reliable real-world experts.

中文标题: GR-RL：面向长时程机器人操作的灵巧与精确控制

中文摘要: 我们提出了GR-RL，这是一个机器人学习框架，将通用的视觉-语言-动作策略转化为高度专业化的长时程灵巧操作专家。现有VLA策略假设人类演示是最优的，但我们认为在高度灵巧和精确的操作任务中，人类演示存在噪声且非最优。GR-RL提出了一个多阶段训练流程，通过强化学习过滤、增强和强化演示数据。首先，GR-RL学习视觉-语言条件的任务进度函数，过滤演示轨迹，仅保留对进度有积极贡献的转换。具体而言，我们展示了通过直接应用稀疏奖励的离线强化学习，得到的Q值可以作为鲁棒的进度函数。接下来，我们引入形态对称增强，显著提高了GR-RL的泛化能力和性能。最后，为了更好地对齐VLA策略与其部署行为以实现高精度控制，我们通过学习潜在空间噪声预测器进行在线强化学习。通过这个流程，GR-RL据我们所知是第一个能够自主系鞋带的学习策略，成功率达到83.3%，该任务需要长时程推理、毫米级精度和柔顺的软体交互。我们希望GR-RL为通用机器人基础模型向可靠现实世界专家转化提供了一步。

</details>


### [328] [Much Ado About Noising: Dispelling the Myths of Generative Robotic Control](https://arxiv.org/abs/2512.01809)
*Chaoyi Pan,Giri Anantharaman,Nai-Chieh Huang,Claire Jin,Daniel Pfrommer,Chenyang Yuan,Frank Permenter,Guannan Qu,Nicholas Boffi,Guanya Shi,Max Simchowitz*

Main category: cs.RO

TL;DR: 该研究对生成式机器人控制策略进行了全面评估，发现其成功并非源于捕捉多模态分布或表达复杂行为的能力，而是源于迭代计算和监督训练中的随机性。作者提出了一个轻量级的两步回归策略，其性能与流式生成策略相当。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如流模型和扩散模型）最近成为机器人学中流行且有效的策略参数化方法。人们对其成功因素有多种推测，包括捕捉多模态动作分布、表达更复杂行为等。本研究旨在通过系统评估来澄清这些神话，探究生成式控制策略真正有效的根本原因。

Method: 在常见的行为克隆基准测试中对流行的生成式控制策略进行全面评估。通过实验分析发现，这些策略的优势并非来自多模态捕捉能力或复杂映射表达能力，而是源于迭代计算特性。为此，作者提出了最小迭代策略（MIP），这是一个轻量级的两步回归策略，用于验证发现。

Result: 研究发现：1）生成式控制策略的成功不依赖于捕捉多模态分布的能力；2）也不依赖于表达复杂观察-动作映射的能力；3）其优势主要来自迭代计算，前提是训练中对中间步骤进行监督，并配合适当水平的随机性。最小迭代策略（MIP）的性能与流式生成策略基本相当，且通常优于蒸馏的快捷模型。

Conclusion: 生成式控制策略的分布拟合组件并不像通常认为的那么重要。研究结果指向了新的设计空间，应专注于控制性能本身，而不是过度强调分布拟合特性。这为机器人控制策略的设计提供了新的方向。

Abstract: Generative models, like flows and diffusions, have recently emerged as popular and efficacious policy parameterizations in robotics. There has been much speculation as to the factors underlying their successes, ranging from capturing multi-modal action distribution to expressing more complex behaviors. In this work, we perform a comprehensive evaluation of popular generative control policies (GCPs) on common behavior cloning (BC) benchmarks. We find that GCPs do not owe their success to their ability to capture multi-modality or to express more complex observation-to-action mappings. Instead, we find that their advantage stems from iterative computation, as long as intermediate steps are supervised during training and this supervision is paired with a suitable level of stochasticity. As a validation of our findings, we show that a minimum iterative policy (MIP), a lightweight two-step regression-based policy, essentially matches the performance of flow GCPs, and often outperforms distilled shortcut models. Our results suggest that the distribution-fitting component of GCPs is less salient than commonly believed, and point toward new design spaces focusing solely on control performance. Project page: https://simchowitzlabpublic.github.io/much-ado-about-noising-project/

中文标题: 关于噪声的诸多争议：澄清生成式机器人控制的神话

中文摘要: 生成模型，如流模型和扩散模型，最近已成为机器人学中流行且有效的策略参数化方法。人们对其成功因素有多种推测，从捕捉多模态动作分布到表达更复杂行为等。在这项工作中，我们在常见的行为克隆基准测试中对流行的生成式控制策略进行了全面评估。我们发现，生成式控制策略的成功并非源于其捕捉多模态分布或表达更复杂观察-动作映射的能力。相反，我们发现它们的优势来自迭代计算，只要在训练中对中间步骤进行监督，并且这种监督与适当水平的随机性相结合。作为我们发现的验证，我们展示了最小迭代策略（MIP），一个轻量级的两步回归策略，其性能基本上与流式生成策略相当，并且通常优于蒸馏的快捷模型。我们的结果表明，生成式控制策略的分布拟合组件并不像通常认为的那么重要，并指向了专注于控制性能本身的新设计空间。项目页面：https://simchowitzlabpublic.github.io/much-ado-about-noising-project/

</details>


### [329] [Is Image-based Object Pose Estimation Ready to Support Grasping?](https://arxiv.org/abs/2512.01856)
*Eric C. Joyce,Qianwen Zhao,Nathaniel Burgdorfer,Long Wang,Philippos Mordohai*

Main category: cs.RO

TL;DR: 该论文提出了一个评估6自由度物体姿态估计器的框架，特别关注仅需单张RGB图像输入的算法，并通过物理模拟器中的抓取实验来评估这些姿态估计器是否足以支持机器人抓取任务。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是评估基于图像的物体姿态估计器是否足够准确，能够作为机器人抓取任务的唯一感知机制。目前缺乏对这些姿态估计器在实际抓取任务中性能的系统评估。

Method: 作者开发了一个评估框架，在物理模拟器中进行抓取实验，使用基于图像的姿态估计结果来指导平行夹爪和欠驱动机器人手抓取3D物体模型。实验基于BOP数据集子集，比较了五种开源物体姿态估计算法。

Result: 实验提供了关于这些姿态估计器性能的深入见解，这些见解在现有文献中是缺失的。通过抓取实验评估了姿态估计器作为机器人抓取唯一感知机制的实际可行性。

Conclusion: 论文得出结论，基于图像的物体姿态估计器在支持机器人抓取任务方面已经具备一定的能力，但还需要进一步改进才能完全作为唯一的感知机制。

Abstract: We present a framework for evaluating 6-DoF instance-level object pose estimators, focusing on those that require a single RGB (not RGB-D) image as input. Besides gaining intuition about how accurate these estimators are, we are interested in the degree to which they can serve as the sole perception mechanism for robotic grasping. To assess this, we perform grasping trials in a physics-based simulator, using image-based pose estimates to guide a parallel gripper and an underactuated robotic hand in picking up 3D models of objects. Our experiments on a subset of the BOP (Benchmark for 6D Object Pose Estimation) dataset compare five open-source object pose estimators and provide insights that were missing from the literature.

中文标题: 基于图像的物体姿态估计是否已准备好支持抓取任务？

中文摘要: 我们提出了一个评估6自由度实例级物体姿态估计器的框架，重点关注那些仅需单张RGB（非RGB-D）图像作为输入的算法。除了了解这些估计器的准确程度外，我们还关注它们能够在多大程度上作为机器人抓取的唯一感知机制。为了评估这一点，我们在物理模拟器中进行抓取试验，使用基于图像的姿态估计结果来指导平行夹爪和欠驱动机器人手抓取3D物体模型。我们在BOP（6D物体姿态估计基准）数据集的子集上进行的实验比较了五种开源物体姿态估计器，并提供了文献中缺失的见解。

</details>


### [330] [Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model](https://arxiv.org/abs/2512.01924)
*Kentaro Fujii,Shingo Murata*

Main category: cs.RO

TL;DR: 提出一种新颖的深度主动推理框架，通过时间分层世界模型和动作抽象实现真实世界机器人控制，在不确定性环境中平衡目标导向与探索行为。


<details>
  <summary>Details</summary>
Motivation: 真实世界机器人需要在不确定环境中同时执行目标导向和探索行为，但现有深度学习方法大多忽视探索且难以处理不确定性。传统深度主动推理方法存在环境表示能力有限和动作选择计算成本高的问题。

Method: 提出包含世界模型、动作模型和抽象世界模型的深度主动推理框架。世界模型将环境动态编码为慢速和快速时间尺度的隐藏状态表示；动作模型使用向量量化将动作序列压缩为抽象动作；抽象世界模型基于抽象动作预测未来慢状态，实现低成本动作选择。

Result: 在真实世界机器人物体操作任务中评估，该框架在多样化操作任务中实现高成功率，在不确定设置中能够切换目标导向和探索行为，同时使动作选择计算可行。

Conclusion: 研究结果表明建模多时间尺度动态以及抽象动作和状态转换的重要性，为真实世界机器人控制提供了有效的深度主动推理解决方案。

Abstract: Robots in uncertain real-world environments must perform both goal-directed and exploratory actions. However, most deep learning-based control methods neglect exploration and struggle under uncertainty. To address this, we adopt deep active inference, a framework that accounts for human goal-directed and exploratory actions. Yet, conventional deep active inference approaches face challenges due to limited environmental representation capacity and high computational cost in action selection. We propose a novel deep active inference framework that consists of a world model, an action model, and an abstract world model. The world model encodes environmental dynamics into hidden state representations at slow and fast timescales. The action model compresses action sequences into abstract actions using vector quantization, and the abstract world model predicts future slow states conditioned on the abstract action, enabling low-cost action selection. We evaluate the framework on object-manipulation tasks with a real-world robot. Results show that it achieves high success rates across diverse manipulation tasks and switches between goal-directed and exploratory actions in uncertain settings, while making action selection computationally tractable. These findings highlight the importance of modeling multiple timescale dynamics and abstracting actions and state transitions.

中文标题: 基于时间分层世界模型的深度主动推理实现真实世界机器人控制

中文摘要: 在不确定的真实世界环境中，机器人必须同时执行目标导向和探索性动作。然而，大多数基于深度学习的控制方法忽视了探索，并且在不确定性下表现不佳。为了解决这个问题，我们采用深度主动推理框架，该框架能够解释人类的目标导向和探索性动作。然而，传统的深度主动推理方法面临环境表示能力有限和动作选择计算成本高的挑战。我们提出了一种新颖的深度主动推理框架，包括世界模型、动作模型和抽象世界模型。世界模型将环境动态编码为慢速和快速时间尺度的隐藏状态表示。动作模型使用向量量化将动作序列压缩为抽象动作，抽象世界模型基于抽象动作预测未来慢状态，从而实现低成本动作选择。我们在真实世界机器人的物体操作任务上评估该框架。结果表明，它在多样化操作任务中实现了高成功率，在不确定设置中能够切换目标导向和探索性动作，同时使动作选择计算可行。这些发现突显了建模多时间尺度动态以及抽象动作和状态转换的重要性。

</details>


### [331] [Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models](https://arxiv.org/abs/2512.01946)
*Paul Pacaud,Ricardo Garcia,Shizhe Chen,Cordelia Schmid*

Main category: cs.RO

TL;DR: Guardian是一个使用视觉语言模型检测机器人故障的系统，通过自动合成故障数据训练，在多个基准测试中达到SOTA性能，并能提升机器人任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作需要可靠的故障检测和恢复机制，但视觉语言模型由于缺乏足够的故障数据而表现受限。现有故障数据集规模小、多样性不足，限制了模型的准确性和泛化能力。

Method: 1. 提出自动机器人故障合成方法：通过程序化扰动成功轨迹生成多样化的规划和执行故障
2. 生成细粒度故障类别和逐步推理轨迹
3. 构建三个新基准数据集：RLBench-Fail、BridgeDataV2-Fail、UR5-Fail
4. 训练Guardian视觉语言模型：使用多视角图像进行详细故障推理和检测

Result: 1. Guardian在现有和新基准测试中都达到最先进的性能
2. 生成的故障数据显著扩展了数据集的多样性和规模
3. 集成到最先进的操作系统后，在模拟和真实机器人中都有效提高了任务成功率

Conclusion: 通过自动故障数据合成方法，Guardian系统成功解决了机器人故障检测中的数据稀缺问题，实现了高性能的故障检测和推理，并能实际提升机器人操作的成功率，展示了生成故障数据的重要价值。

Abstract: Robust robotic manipulation requires reliable failure detection and recovery. Although current Vision-Language Models (VLMs) show promise, their accuracy and generalization are limited by the scarcity of failure data. To address this data gap, we propose an automatic robot failure synthesis approach that procedurally perturbs successful trajectories to generate diverse planning and execution failures. This method produces not only binary classification labels but also fine-grained failure categories and step-by-step reasoning traces in both simulation and the real world. With it, we construct three new failure detection benchmarks: RLBench-Fail, BridgeDataV2-Fail, and UR5-Fail, substantially expanding the diversity and scale of existing failure datasets. We then train Guardian, a VLM with multi-view images for detailed failure reasoning and detection. Guardian achieves state-of-the-art performance on both existing and newly introduced benchmarks. It also effectively improves task success rates when integrated into a state-of-the-art manipulation system in simulation and real robots, demonstrating the impact of our generated failure data.

中文标题: Guardian：使用视觉语言模型检测机器人规划与执行错误

中文摘要: 鲁棒的机器人操作需要可靠的故障检测和恢复能力。尽管当前的视觉语言模型（VLMs）显示出潜力，但由于故障数据的稀缺性，其准确性和泛化能力受到限制。为了解决这一数据缺口，我们提出了一种自动机器人故障合成方法，通过程序化扰动成功轨迹来生成多样化的规划和执行故障。该方法不仅产生二元分类标签，还生成细粒度的故障类别以及在模拟和现实世界中的逐步推理轨迹。基于此，我们构建了三个新的故障检测基准：RLBench-Fail、BridgeDataV2-Fail和UR5-Fail，显著扩展了现有故障数据集的多样性和规模。然后我们训练了Guardian，这是一个具有多视角图像的视觉语言模型，用于详细的故障推理和检测。Guardian在现有和新引入的基准测试中都达到了最先进的性能。当集成到模拟和真实机器人中最先进的操作系统中时，它还能有效提高任务成功率，展示了我们生成的故障数据的影响。

</details>


### [332] [RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies](https://arxiv.org/abs/2512.01993)
*Guillermo Garcia-Cobo,Maximilian Igl,Peter Karkus,Zhejun Zhang,Michael Watson,Yuxiao Chen,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: RoaD是一种通过使用策略自身的闭环轨迹作为额外训练数据来缓解自动驾驶策略中协变量偏移的简单高效方法。该方法在轨迹生成中融入专家指导，产生高质量的训练数据，相比强化学习需要更少数据，且比之前的闭环监督微调方法限制更少。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶策略通常通过开环行为克隆人类演示进行训练，但在闭环部署时会出现协变量偏移，导致误差累积。现有方法如强化学习需要大量数据，而先前的闭环监督微调方法限制较多，无法应用于端到端驾驶等更广泛领域。

Method: RoaD方法的核心是利用策略自身的闭环轨迹作为额外的训练数据。在轨迹生成过程中，融入专家指导来偏置轨迹朝向高质量行为，产生信息丰富且真实的演示用于微调。这种方法避免了强化学习的大量数据需求，也比先前的CL-SFT方法限制更少。

Result: 在WOSAC大规模交通仿真基准测试中，RoaD表现与先前的CL-SFT方法相当或更好；在AlpaSim高保真神经重建仿真器中，驾驶分数提高了41%，碰撞减少了54%。

Conclusion: RoaD提供了一种简单高效的闭环监督微调方法，能够有效缓解自动驾驶策略中的协变量偏移问题，相比强化学习需要更少数据，且比先前的CL-SFT方法应用范围更广，特别适用于端到端驾驶等场景。

Abstract: Autonomous driving policies are typically trained via open-loop behavior cloning of human demonstrations. However, such policies suffer from covariate shift when deployed in closed loop, leading to compounding errors. We introduce Rollouts as Demonstrations (RoaD), a simple and efficient method to mitigate covariate shift by leveraging the policy's own closed-loop rollouts as additional training data. During rollout generation, RoaD incorporates expert guidance to bias trajectories toward high-quality behavior, producing informative yet realistic demonstrations for fine-tuning. This approach enables robust closed-loop adaptation with orders of magnitude less data than reinforcement learning, and avoids restrictive assumptions of prior closed-loop supervised fine-tuning (CL-SFT) methods, allowing broader applications domains including end-to-end driving. We demonstrate the effectiveness of RoaD on WOSAC, a large-scale traffic simulation benchmark, where it performs similar or better than the prior CL-SFT method; and in AlpaSim, a high-fidelity neural reconstruction-based simulator for end-to-end driving, where it improves driving score by 41\% and reduces collisions by 54\%.

中文标题: RoaD：将轨迹作为演示用于自动驾驶策略的闭环监督微调

中文摘要: 自动驾驶策略通常通过开环行为克隆人类演示进行训练。然而，当这些策略在闭环中部署时，会遭受协变量偏移，导致误差累积。我们引入了轨迹作为演示（RoaD），这是一种简单高效的方法，通过利用策略自身的闭环轨迹作为额外的训练数据来缓解协变量偏移。在轨迹生成过程中，RoaD融入专家指导来偏置轨迹朝向高质量行为，产生信息丰富且真实的演示用于微调。这种方法使得闭环适应更加鲁棒，所需数据量比强化学习少几个数量级，并且避免了先前闭环监督微调（CL-SFT）方法的限制性假设，允许更广泛的应用领域，包括端到端驾驶。我们在WOSAC（一个大规模交通仿真基准）上证明了RoaD的有效性，其表现与先前的CL-SFT方法相当或更好；在AlpaSim（一个基于神经重建的高保真端到端驾驶仿真器）中，它将驾驶分数提高了41%，并将碰撞减少了54%。

</details>


### [333] [Learning Sim-to-Real Humanoid Locomotion in 15 Minutes](https://arxiv.org/abs/2512.01996)
*Younggyo Seo,Carmelo Sferrazza,Juyue Chen,Guanya Shi,Rocky Duan,Pieter Abbeel*

Main category: cs.RO

TL;DR: 本文提出了一种基于FastSAC和FastTD3的简单实用方法，可在15分钟内用单张RTX 4090 GPU训练人形机器人步态策略，通过大规模并行仿真和精心调优的设计选择实现快速可靠的模拟到现实迁移。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模并行仿真已将机器人强化学习训练时间从几天缩短到几分钟，但实现人形机器人控制快速可靠的模拟到现实迁移仍然困难，主要挑战包括高维状态空间和领域随机化等因素。

Method: 基于离策略强化学习算法FastSAC和FastTD3，通过精心调优的设计选择和极简奖励函数，在数千个并行环境中稳定大规模训练。方法包括强领域随机化（随机动力学、崎岖地形、推力扰动）和快速训练全身人体运动跟踪策略。

Result: 在Unitree G1和Booster T1机器人上实现了快速端到端学习人形步态控制器，在强领域随机化条件下仅需15分钟训练时间，展示了快速可靠的模拟到现实迁移能力。

Conclusion: 提出了一种简单实用的方法，能够快速训练人形机器人步态策略并实现可靠的模拟到现实迁移，为机器人强化学习提供了高效解决方案。

Abstract: Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive scale with thousands of parallel environments through carefully tuned design choices and minimalist reward functions. We demonstrate rapid end-to-end learning of humanoid locomotion controllers on Unitree G1 and Booster T1 robots under strong domain randomization, e.g., randomized dynamics, rough terrain, and push perturbations, as well as fast training of whole-body human-motion tracking policies. We provide videos and open-source implementation at: https://younggyo.me/fastsac-humanoid.

中文标题: 在15分钟内学习模拟到现实的人形机器人步态

中文摘要: 大规模并行仿真已将机器人强化学习训练时间从几天缩短到几分钟。然而，由于高维度和领域随机化等因素带来的挑战，实现人形控制的快速可靠模拟到现实强化学习仍然困难。在这项工作中，我们基于离策略强化学习算法（FastSAC和FastTD3）引入了一个简单实用的方法，仅用单张RTX 4090 GPU在15分钟内快速训练人形步态策略。我们通过精心调优的设计选择和极简奖励函数，在数千个并行环境中稳定了大规模离策略强化学习算法。我们在强领域随机化条件下（如随机动力学、崎岖地形和推力扰动）展示了Unitree G1和Booster T1机器人上人形步态控制器的快速端到端学习，以及全身人体运动跟踪策略的快速训练。我们提供了视频和开源实现：https://younggyo.me/fastsac-humanoid。

</details>


### [334] [LLM-Driven Corrective Robot Operation Code Generation with Static Text-Based Simulation](https://arxiv.org/abs/2512.02002)
*Wenhao Wang,Yanyan Li,Long Jiao,Jiawei Yuan*

Main category: cs.RO

TL;DR: 本文提出了一种基于LLM的静态文本模拟方法，用于机器人操作代码的纠错生成，避免了传统物理实验或仿真环境的高配置成本和长执行时间问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成机器人操作代码的纠错方法依赖物理实验或定制仿真环境，存在配置复杂、执行时间长的问题，限制了实际部署。

Method: 将LLM配置为静态模拟器，通过解释动作、推理状态转换、分析执行结果、生成语义观察来模拟代码执行，构建纠错框架。

Result: 实验表明，静态文本模拟具有高准确性，纠错框架在不依赖动态代码执行的情况下，代码生成性能与最先进方法相当。

Conclusion: LLM驱动的静态文本模拟为机器人操作代码纠错生成提供了高效可靠的解决方案，避免了传统仿真环境的限制。

Abstract: Recent advances in Large language models (LLMs) have demonstrated their promising capabilities of generating robot operation code to enable LLM-driven robots. To enhance the reliability of operation code generated by LLMs, corrective designs with feedback from the observation of executing code have been increasingly adopted in existing research. However, the code execution in these designs relies on either a physical experiment or a customized simulation environment, which limits their deployment due to the high configuration effort of the environment and the potential long execution time. In this paper, we explore the possibility of directly leveraging LLM to enable static simulation of robot operation code, and then leverage it to design a new reliable LLM-driven corrective robot operation code generation framework. Our framework configures the LLM as a static simulator with enhanced capabilities that reliably simulate robot code execution by interpreting actions, reasoning over state transitions, analyzing execution outcomes, and generating se- mantic observations that accurately capture trajectory dynamics. To validate the performance of our framework, we performed experiments on various operation tasks for different robots, including UAVs and small ground vehicles. The experiment results not only demonstrated the high accuracy of our static text-based simulation but also the reliable code generation of our LLM-driven corrective framework, which achieves a comparable performance with state-of-the-art research while does not rely on dynamic code execution using physical experiments or simulators.

中文标题: 基于静态文本模拟的LLM驱动机器人操作代码纠错生成

中文摘要: 近年来，大型语言模型（LLMs）在生成机器人操作代码方面展现出有前景的能力，使得LLM驱动的机器人成为可能。为了提高LLM生成的操作代码的可靠性，现有研究越来越多地采用基于代码执行观察反馈的纠错设计。然而，这些设计中的代码执行依赖于物理实验或定制的仿真环境，这限制了它们的部署，因为环境配置工作量大且执行时间可能较长。本文探索了直接利用LLM实现机器人操作代码静态仿真的可能性，并利用它设计了一个新的可靠的LLM驱动纠错机器人操作代码生成框架。我们的框架将LLM配置为具有增强能力的静态模拟器，通过解释动作、推理状态转换、分析执行结果以及生成准确捕捉轨迹动态的语义观察，可靠地模拟机器人代码执行。为了验证我们框架的性能，我们对不同机器人（包括无人机和小型地面车辆）的各种操作任务进行了实验。实验结果不仅证明了我们基于静态文本的仿真的高准确性，还证明了我们LLM驱动纠错框架的可靠代码生成能力，该框架在不依赖物理实验或模拟器进行动态代码执行的情况下，实现了与最先进研究相当的性能。

</details>


### [335] [Learning Dexterous Manipulation Skills from Imperfect Simulations](https://arxiv.org/abs/2512.02011)
*Elvis Hsieh,Wen-Han Hsieh,Yen-Jen Wang,Toru Lin,Jitendra Malik,Koushil Sreenath,Haozhi Qi*

Main category: cs.RO

TL;DR: 提出DexScrew框架，通过三阶段方法解决灵巧操作中模拟不完美的问题：先在简化模拟中训练RL策略学习正确手指步态，再用该策略作为技能基元通过遥操作收集真实世界演示，最后训练包含触觉感知的行为克隆策略，在螺母螺栓紧固和螺丝刀操作任务上取得良好泛化性能。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作中的强化学习和模拟到真实迁移面临复杂接触动力学和多感官信号（尤其是触觉反馈）模拟困难的限制。现有方法难以准确模拟真实世界的触觉感知和复杂接触，限制了灵巧操作技能的迁移效果。

Method: 提出三阶段框架DexScrew：1）在简化物体模型的模拟中训练强化学习策略，学习正确的手指步态；2）将学习到的策略作为技能基元，通过遥操作系统收集包含触觉和本体感觉信息的真实世界演示；3）训练包含触觉感知的行为克隆策略，使其能够泛化到不同几何形状的物体。

Result: 在螺母螺栓紧固和螺丝刀操作任务上的实验表明，相比直接的模拟到真实迁移，该方法获得了更高的任务进度比。即使在未见过的物体形状和外部扰动下，也表现出鲁棒的性能。

Conclusion: DexScrew框架有效解决了灵巧操作中模拟不完美的问题，通过结合模拟训练、真实演示收集和触觉感知集成，实现了对复杂操作任务的高效学习和泛化。

Abstract: Reinforcement learning and sim-to-real transfer have made significant progress in dexterous manipulation. However, progress remains limited by the difficulty of simulating complex contact dynamics and multisensory signals, especially tactile feedback. In this work, we propose \ours, a sim-to-real framework that addresses these limitations and demonstrates its effectiveness on nut-bolt fastening and screwdriving with multi-fingered hands. The framework has three stages. First, we train reinforcement learning policies in simulation using simplified object models that lead to the emergence of correct finger gaits. We then use the learned policy as a skill primitive within a teleoperation system to collect real-world demonstrations that contain tactile and proprioceptive information. Finally, we train a behavior cloning policy that incorporates tactile sensing and show that it generalizes to nuts and screwdrivers with diverse geometries. Experiments across both tasks show high task progress ratios compared to direct sim-to-real transfer and robust performance even on unseen object shapes and under external perturbations. Videos and code are available on https://dexscrew.github.io.

中文标题: 从不完美模拟中学习灵巧操作技能

中文摘要: 强化学习和模拟到真实迁移在灵巧操作方面取得了显著进展。然而，复杂接触动力学和多感官信号（尤其是触觉反馈）的模拟困难仍然限制了进展。在这项工作中，我们提出了DexScrew，一个解决这些限制的模拟到真实框架，并在多指手的螺母螺栓紧固和螺丝刀操作上展示了其有效性。该框架包含三个阶段：首先，我们使用简化的物体模型在模拟中训练强化学习策略，这导致了正确手指步态的出现；然后，我们将学习到的策略作为技能基元，在遥操作系统中收集包含触觉和本体感觉信息的真实世界演示；最后，我们训练一个包含触觉感知的行为克隆策略，并展示它能够泛化到具有不同几何形状的螺母和螺丝刀。在两个任务上的实验表明，相比直接的模拟到真实迁移，该方法获得了更高的任务进度比，即使在未见过的物体形状和外部扰动下也表现出鲁棒的性能。视频和代码可在https://dexscrew.github.io获取。

</details>


### [336] [ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation](https://arxiv.org/abs/2512.02013)
*Chenyang Gu,Jiaming Liu,Hao Chen,Runzhong Huang,Qingpo Wuwu,Zhuoyang Liu,Xiaoqi Li,Ying Li,Renrui Zhang,Peng Jia,Pheng-Ann Heng,Shanghang Zhang*

Main category: cs.RO

TL;DR: ManualVLA是一个统一的视觉-语言-动作模型，通过混合专家架构实现从目标状态生成操作手册并执行机器人操作，在LEGO组装和物体重排任务上比现有方法成功率提高32%。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在处理需要明确定义目标状态的长时程任务（如LEGO组装、物体重排）时，难以协调高层规划与精确操作。需要让VLA模型具备从"什么"结果推断"如何"过程的能力，将目标状态转化为可执行程序。

Method: 基于混合变换器架构的统一VLA框架，包含规划专家生成包含图像、位置提示和文本指令的中间操作手册，然后通过Manual Chain-of-Thought推理过程将这些手册输入动作专家，每个手册步骤提供显式控制条件，其潜在表示提供隐式指导。使用基于3D高斯泼溅的高保真数字孪生工具包自动生成训练数据。

Result: ManualVLA在真实世界任务中表现优异，在LEGO组装和物体重排任务上，平均成功率比之前的分层SOTA基线高出32%。

Conclusion: ManualVLA通过统一的VLA框架成功实现了从目标状态到可执行操作手册的生成和机器人操作执行，解决了长时程任务中高层规划与精确操作的协调问题。

Abstract: Vision-Language-Action (VLA) models have recently emerged, demonstrating strong generalization in robotic scene understanding and manipulation. However, when confronted with long-horizon tasks that require defined goal states, such as LEGO assembly or object rearrangement, existing VLA models still face challenges in coordinating high-level planning with precise manipulation. Therefore, we aim to endow a VLA model with the capability to infer the "how" process from the "what" outcomes, transforming goal states into executable procedures. In this paper, we introduce ManualVLA, a unified VLA framework built upon a Mixture-of-Transformers (MoT) architecture, enabling coherent collaboration between multimodal manual generation and action execution. Unlike prior VLA models that directly map sensory inputs to actions, we first equip ManualVLA with a planning expert that generates intermediate manuals consisting of images, position prompts, and textual instructions. Building upon these multimodal manuals, we design a Manual Chain-of-Thought (ManualCoT) reasoning process that feeds them into the action expert, where each manual step provides explicit control conditions, while its latent representation offers implicit guidance for accurate manipulation. To alleviate the burden of data collection, we develop a high-fidelity digital-twin toolkit based on 3D Gaussian Splatting, which automatically generates manual data for planning expert training. ManualVLA demonstrates strong real-world performance, achieving an average success rate 32% higher than the previous hierarchical SOTA baseline on LEGO assembly and object rearrangement tasks.

中文标题: ManualVLA：用于思维链操作手册生成和机器人操作统一的VLA模型

中文摘要: 视觉-语言-动作模型最近崭露头角，在机器人场景理解和操作方面展现出强大的泛化能力。然而，当面对需要明确定义目标状态的长时程任务，如乐高组装或物体重排时，现有VLA模型在协调高层规划与精确操作方面仍面临挑战。因此，我们旨在赋予VLA模型从"什么"结果推断"如何"过程的能力，将目标状态转化为可执行程序。本文提出ManualVLA，一个基于混合变换器架构的统一VLA框架，实现了多模态操作手册生成与动作执行的协同工作。与先前直接将感官输入映射到动作的VLA模型不同，我们首先为ManualVLA配备了一个规划专家，生成包含图像、位置提示和文本指令的中间操作手册。基于这些多模态手册，我们设计了Manual Chain-of-Thought推理过程，将它们输入动作专家，其中每个手册步骤提供显式控制条件，而其潜在表示则为精确操作提供隐式指导。为减轻数据收集负担，我们开发了基于3D高斯泼溅的高保真数字孪生工具包，自动生成用于规划专家训练的手册数据。ManualVLA在真实世界中表现出色，在乐高组装和物体重排任务上，平均成功率比之前的分层SOTA基线高出32%。

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [337] [Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on Financial Tabular Classification](https://arxiv.org/abs/2512.00163)
*Saeed AlMarri,Mathieu Ravaut,Kristof Juhasz,Gautier Marti,Hamdan Al Ahbabi,Ibrahim Elfadel*

Main category: cs.LG

TL;DR: 该研究评估了LLMs在金融表格分类任务中的表现，发现LLMs的自我解释与SHAP值存在差异，且与LightGBM的SHAP值显著不同，表明LLMs作为独立分类器在结构化金融建模中存在局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在分类任务中受到广泛关注，为传统的机器学习模型提供了灵活的零样本提示替代方案。然而，在结构化表格数据（特别是金融风险评估等高风险应用）中的可靠性仍不明确，需要系统评估其解释性和可部署性。

Method: 研究系统评估了LLMs在金融分类任务中的表现，并生成其SHAP值。通过比较LLMs的自我解释与SHAP值，以及LLMs与LightGBM的SHAP值差异，分析LLMs在结构化金融建模中的可靠性和解释性。

Result: 分析显示：1) LLMs的自我解释与SHAP值存在分歧；2) LLMs与LightGBM的SHAP值存在显著差异。这些发现揭示了LLMs作为独立分类器在结构化金融建模中的局限性。

Conclusion: LLMs作为金融表格分类的独立分类器存在局限性，但通过改进的解释机制结合少样本提示，LLMs有望在风险敏感领域变得可用。研究为LLMs在金融应用中的可部署性提供了重要见解。

Abstract: Large Language Models (LLMs) have attracted significant attention for classification tasks, offering a flexible alternative to trusted classical machine learning models like LightGBM through zero-shot prompting. However, their reliability for structured tabular data remains unclear, particularly in high stakes applications like financial risk assessment. Our study systematically evaluates LLMs and generates their SHAP values on financial classification tasks. Our analysis shows a divergence between LLMs self-explanation of feature impact and their SHAP values, as well as notable differences between LLMs and LightGBM SHAP values. These findings highlight the limitations of LLMs as standalone classifiers for structured financial modeling, but also instill optimism that improved explainability mechanisms coupled with few-shot prompting will make LLMs usable in risk-sensitive domains.

中文标题: 测量LLMs认为自己在做什么：金融表格分类中的SHAP忠实性和可部署性

中文摘要: 大型语言模型在分类任务中引起了广泛关注，通过零样本提示为可信的经典机器学习模型（如LightGBM）提供了灵活的替代方案。然而，它们在结构化表格数据中的可靠性仍不明确，特别是在金融风险评估等高风险应用中。我们的研究系统评估了LLMs在金融分类任务中的表现，并生成其SHAP值。分析显示LLMs对特征影响的自我解释与其SHAP值存在分歧，并且LLMs与LightGBM的SHAP值存在显著差异。这些发现突显了LLMs作为结构化金融建模独立分类器的局限性，但也带来了乐观的前景：改进的解释机制结合少样本提示将使LLMs在风险敏感领域变得可用。

</details>


### [338] [We Still Don't Understand High-Dimensional Bayesian Optimization](https://arxiv.org/abs/2512.00170)
*Colin Doumont,Donney Fan,Natalie Maus,Jacob R. Gardner,Henry Moss,Geoff Pleiss*

Main category: cs.LG

TL;DR: 本文发现高维贝叶斯优化中，简单的贝叶斯线性回归（经过几何变换后）在60-6000维任务上表现优于现有复杂方法，挑战了传统高维优化直觉。


<details>
  <summary>Details</summary>
Motivation: 高维空间对贝叶斯优化提出了挑战，现有方法通过编码各种结构假设（如局部性、稀疏性、平滑性）来应对维度诅咒，但这些方法是否真的有效需要重新评估。

Method: 采用贝叶斯线性回归作为基础方法，通过几何变换避免边界搜索行为，使用高斯过程与线性核函数，在60-6000维搜索空间任务上进行测试。

Result: 线性模型在多个高维任务上达到最先进性能，在超过20,000个观测的分子优化任务中表现出色，计算复杂度随数据线性增长，优于非参数方法。

Conclusion: 高维贝叶斯优化需要重新思考传统直觉，简单的线性模型可能比复杂方法更有效，线性模型具有闭式采样、计算高效等优势。

Abstract: High-dimensional spaces have challenged Bayesian optimization (BO). Existing methods aim to overcome this so-called curse of dimensionality by carefully encoding structural assumptions, from locality to sparsity to smoothness, into the optimization procedure. Surprisingly, we demonstrate that these approaches are outperformed by arguably the simplest method imaginable: Bayesian linear regression. After applying a geometric transformation to avoid boundary-seeking behavior, Gaussian processes with linear kernels match state-of-the-art performance on tasks with 60- to 6,000-dimensional search spaces. Linear models offer numerous advantages over their non-parametric counterparts: they afford closed-form sampling and their computation scales linearly with data, a fact we exploit on molecular optimization tasks with > 20,000 observations. Coupled with empirical analyses, our results suggest the need to depart from past intuitions about BO methods in high-dimensional spaces.

中文标题: 我们仍然不理解高维贝叶斯优化

中文摘要: 高维空间对贝叶斯优化提出了挑战。现有方法试图通过将结构假设（从局部性到稀疏性再到平滑性）精心编码到优化过程中来克服所谓的维度诅咒。令人惊讶的是，我们证明这些方法被可以说是最简单的方法所超越：贝叶斯线性回归。在应用几何变换以避免边界搜索行为后，具有线性核的高斯过程在60至6,000维搜索空间的任务上达到了最先进的性能。线性模型相对于其非参数对应物具有许多优势：它们允许闭式采样，并且其计算随数据线性扩展，这一事实我们在具有>20,000个观测的分子优化任务中加以利用。结合实证分析，我们的结果表明需要摆脱过去关于高维空间中贝叶斯优化方法的直觉。

</details>


### [339] [Orion-Bix: Bi-Axial Attention for Tabular In-Context Learning](https://arxiv.org/abs/2512.00181)
*Mohamed Bouadi,Pratinav Seth,Aditya Tanna,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: Orion-Bix是一个表格基础模型，通过双轴注意力机制和元学习上下文推理实现少样本表格学习。它结合多种注意力变体，使用多CLS汇总捕获局部和全局依赖关系，并通过分层决策路由适应大规模标签空间。在合成表格数据上元训练后，在公开基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 表格数据在现实世界机器学习应用中普遍存在，但构建通用模型面临挑战：混合数值和分类字段、弱特征结构、有限标注数据使得扩展和泛化困难。现有方法难以有效处理表格数据的异质性和少样本学习需求。

Method: 1. 双轴注意力编码器：交替使用标准、分组、分层和关系注意力，通过多CLS汇总融合输出，高效捕获局部和全局依赖关系。2. 标签感知ICL头部：通过分层决策路由实现动态适应，可扩展到大规模标签空间。3. 元训练策略：在具有因果先验的合成生成、结构多样的表格数据上进行训练，学习跨异质数据的可迁移归纳偏置。

Result: 在公开基准测试中，Orion-Bix优于梯度提升基线方法，并与最先进的表格基础模型保持竞争力。模型表现出强大的少样本学习能力，能够有效处理表格数据的异质性。

Conclusion: 双轴注意力结合情景式元训练能够实现鲁棒的、少样本就绪的表格学习。Orion-Bix作为scikit-learn兼容的基础模型，为表格数据提供了有效的通用解决方案。

Abstract: Tabular data drive most real-world machine learning applications, yet building general-purpose models for them remains difficult. Mixed numeric and categorical fields, weak feature structure, and limited labeled data make scaling and generalization challenging. To this end, we introduce Orion-Bix, a tabular foundation model that combines biaxial attention with meta-learned in-context reasoning for few-shot tabular learning. Its encoder alternates standard, grouped, hierarchical, and relational attention, fusing their outputs through multi-CLS summarization to capture both local and global dependencies efficiently. A label-aware ICL head adapts on the fly and scales to large label spaces via hierarchical decision routing. Meta-trained on synthetically generated, structurally diverse tables with causal priors, Orion-Bix learns transferable inductive biases across heterogeneous data. Delivered as a scikit-learn compatible foundation model, it outperforms gradient-boosting baselines and remains competitive with state-of-the-art tabular foundation models on public benchmarks, showing that biaxial attention with episodic meta-training enables robust, few-shot-ready tabular learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-BiX .

中文标题: Orion-Bix：用于表格上下文学习的双轴注意力机制

中文摘要: 表格数据驱动着大多数现实世界的机器学习应用，但为它们构建通用模型仍然很困难。混合的数值和分类字段、弱的特征结构以及有限的标注数据使得扩展和泛化具有挑战性。为此，我们引入了Orion-Bix，这是一个表格基础模型，它将双轴注意力与元学习的上下文推理相结合，用于少样本表格学习。其编码器交替使用标准、分组、分层和关系注意力，通过多CLS汇总融合它们的输出，以高效捕获局部和全局依赖关系。一个标签感知的ICL头部通过分层决策路由实现动态适应，并可扩展到大型标签空间。在具有因果先验的合成生成、结构多样的表格上进行元训练后，Orion-Bix学习了跨异质数据的可迁移归纳偏置。作为scikit-learn兼容的基础模型交付，它在公开基准测试中优于梯度提升基线方法，并与最先进的表格基础模型保持竞争力，表明双轴注意力与情景式元训练相结合能够实现鲁棒的、少样本就绪的表格学习。该模型可在https://github.com/Lexsi-Labs/Orion-BiX 公开获取。

</details>


### [340] [Hybrid Context-Fusion Attention (CFA) U-Net and Clustering for Robust Seismic Horizon Interpretation](https://arxiv.org/abs/2512.00191)
*Jose Luis Lima de Jesus Silva,Joao Pedro Gomes,Paulo Roberto de Melo Barros Junior,Vitor Hugo Serravalle Reis Rodrigues,Alexsandro Guerra Cerqueira*

Main category: cs.LG

TL;DR: 本文提出了一种混合上下文融合注意力U-Net与聚类的地震层位解释框架，通过融合空间和几何特征提升复杂地质特征的识别精度，在稀疏标注条件下实现优异性能。


<details>
  <summary>Details</summary>
Motivation: 地震层位解释是油气勘探中表征地下结构的关键任务。尽管基于U-Net的深度学习方法已取得进展，但在复杂地质特征准确分割和稀疏标注下的层位插值方面仍存在挑战。现有方法难以保证层位连续性和几何保真度。

Method: 提出混合框架，核心是上下文融合注意力（CFA）U-Net，在注意力门中融合空间特征和Sobel提取的几何特征。评估了五种架构：标准U-Net、压缩U-Net、U-Net++、注意力U-Net和CFA U-Net。使用DBSCAN聚类对预测结果进行后处理，生成地质合理的表面。

Result: 在墨西哥利亚油田数据集上获得验证IoU 0.881和MAE 2.49ms的SOTA结果；在北海F3区块稀疏条件下实现97.6%的表面覆盖率。在10-、20-和40-线间距的不同数据稀疏度下均优于现有基线。

Conclusion: 该混合框架结合几何上下文增强的注意力架构，为结构复杂和数据稀缺环境中的地震解释提供了鲁棒且可泛化的解决方案，展示了混合方法和注意力机制的优势。

Abstract: Interpreting seismic horizons is a critical task for characterizing subsurface structures in hydrocarbon exploration. Recent advances in deep learning, particularly U-Net-based architectures, have significantly improved automated horizon tracking. However, challenges remain in accurately segmenting complex geological features and interpolating horizons from sparse annotations. To address these issues, a hybrid framework is presented that integrates advanced U-Net variants with spatial clustering to enhance horizon continuity and geometric fidelity. The core contribution is the Context Fusion Attention (CFA) U-Net, a novel architecture that fuses spatial and Sobel-derived geometric features within attention gates to improve both precision and surface completeness. The performance of five architectures, the U-Net (Standard and compressed), U-Net++, Attention U-Net, and CFA U-Net, was systematically evaluated across various data sparsity regimes (10-, 20-, and 40-line spacing). This approach outperformed existing baselines, achieving state-of-the-art results on the Mexilhao field (Santos Basin, Brazil) dataset with a validation IoU of 0.881 and MAE of 2.49ms, and excellent surface coverage of 97.6% on the F3 Block of the North Sea dataset under sparse conditions. The framework further refines merged horizon predictions (inline and cross-line) using Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to produce geologically plausible surfaces. The results demonstrate the advantages of hybrid methodologies and attention-based architectures enhanced with geometric context, providing a robust and generalizable solution for seismic interpretation in structurally complex and data-scarce environments.

中文标题: 混合上下文融合注意力U-Net与聚类用于鲁棒地震层位解释

中文摘要: 地震层位解释是油气勘探中表征地下结构的关键任务。基于U-Net架构的深度学习最新进展显著改善了自动化层位追踪。然而，在准确分割复杂地质特征和从稀疏标注插值层位方面仍存在挑战。为解决这些问题，本文提出了一种混合框架，将先进的U-Net变体与空间聚类相结合，以增强层位连续性和几何保真度。核心贡献是上下文融合注意力（CFA）U-Net，这是一种新颖架构，在注意力门中融合空间特征和Sobel提取的几何特征，以提高精度和表面完整性。系统评估了五种架构（标准U-Net、压缩U-Net、U-Net++、注意力U-Net和CFA U-Net）在不同数据稀疏度（10-、20-和40-线间距）下的性能。该方法优于现有基线，在墨西哥利亚油田（巴西桑托斯盆地）数据集上获得验证IoU 0.881和MAE 2.49ms的SOTA结果，并在北海F3区块数据集稀疏条件下实现97.6%的优异表面覆盖率。该框架进一步使用基于密度的噪声应用空间聚类（DBSCAN）细化合并的层位预测（内联和交叉线），以生成地质合理的表面。结果表明，结合几何上下文增强的混合方法和注意力架构具有优势，为结构复杂和数据稀缺环境中的地震解释提供了鲁棒且可泛化的解决方案。

</details>


### [341] [Constructing Efficient Fact-Storing MLPs for Transformers](https://arxiv.org/abs/2512.00207)
*Owen Dugan,Roberto Garcia,Ronny Junkins,Jerry Liu,Dylan Zinsley,Sabri Eyuboglu,Atri Rudra,Chris Ré*

Main category: cs.LG

TL;DR: 本文提出了一个改进的事实存储MLP构造框架，实现了更高的参数效率、更广泛的适用性，并揭示了事实存储容量与Transformer可用性之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能够将事实知识存储在MLP参数中，但现有的事实存储MLP构造方法存在局限性。本文旨在开发一个更高效、更通用的MLP构造框架，以更好地理解LLM的事实存储机制，并为模块化事实编辑提供基础。

Method: 提出了一个MLP构造框架，该框架：1）适用于几乎所有可行的输入-输出对；2）在某些嵌入情况下达到信息论界限的参数效率；3）保持Transformer中的可用性。通过该框架，分析了价值嵌入的度量特征，识别了编码器-解码器机制，并探索了事实存储容量与Transformer可用性的权衡。

Result: 1）发现了表征事实-参数缩放关系的价值嵌入度量；2）确定了匹配梯度下降MLP渐近行为的编码器-解码器机制；3）揭示了事实存储容量与Transformer可用性的基本权衡；4）在一层Transformer上成功演示了通过替换整个MLP实现模块化事实编辑的概念验证。

Conclusion: 本文提出的MLP构造框架在事实存储效率、通用性和Transformer兼容性方面显著优于现有方法，为理解LLM事实存储机制提供了新视角，并为模块化事实编辑等应用奠定了基础。研究揭示了MLP设计中的基本权衡，为未来高效事实存储模型的设计提供了指导。

Abstract: The success of large language models (LLMs) can be attributed in part to their ability to efficiently store factual knowledge as key-value mappings within their MLP parameters. Recent work has proposed explicit weight constructions to build such fact-storing MLPs, providing an improved understanding of LLM fact storage mechanisms. In this paper, we introduce an MLP construction framework that improves over previous constructions in three areas: it 1) works for all but a measure-zero set of feasible input-output pairs, 2) achieves asymptotically optimal parameter efficiency matching information-theoretic bounds for some embeddings, and 3) maintains usability within Transformers for factual recall. Through our improvements, we 1) discover a metric on value embeddings that characterizes facts-per-parameter scaling for both constructed and gradient-descent-trained MLPs, 2) identify a simple encoder-decoder mechanism that empirically matches gradient-descent MLP facts-per-parameter asymptotics across all the inputs and outputs we test, and 3) uncover a fundamental tradeoff between an MLP's fact-storage capacity and its usability within Transformers. Finally, we demonstrate a proof-of-concept application of fact-storing MLPs: modular fact editing on one-layer Transformers by \textit{replacing entire MLPs at once}.

中文标题: 为Transformer构建高效的事实存储多层感知机

中文摘要: 大型语言模型（LLMs）的成功部分归功于其能够将事实知识作为键值映射高效地存储在MLP参数中。最近的研究提出了显式的权重构造方法来构建这种事实存储MLP，从而改进了对LLM事实存储机制的理解。本文介绍了一个MLP构造框架，在三个方面改进了之前的构造方法：1）适用于除测度零集外的所有可行输入-输出对；2）对于某些嵌入实现了渐近最优的参数效率，匹配信息论界限；3）在Transformer中保持事实回忆的可用性。通过我们的改进，我们：1）发现了一个价值嵌入的度量，该度量表征了构造和梯度下降训练MLP的事实-参数缩放关系；2）确定了一个简单的编码器-解码器机制，经验上在所有测试的输入和输出中匹配梯度下降MLP的事实-参数渐近行为；3）揭示了MLP事实存储容量与其在Transformer中可用性之间的基本权衡。最后，我们展示了事实存储MLP的概念验证应用：通过一次性替换整个MLP，在一层Transformer上实现模块化事实编辑。

</details>


### [342] [Soft Quality-Diversity Optimization](https://arxiv.org/abs/2512.00810)
*Saeed Hedayatian,Stefanos Nikolaidis*

Main category: cs.LG

TL;DR: 提出Soft QD框架，避免传统QD算法对行为空间的离散化需求，通过可微分方法实现质量-多样性优化，在保持竞争力的同时提升高维问题的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统QD算法通过将行为空间离散化为多个区域来维持多样性，但在大规模解空间和高维行为空间中面临存储和维度灾难问题，需要更灵活的框架。

Method: 提出Soft QD框架，避免离散化需求，推导出可微分QD算法SQUAD（Soft QD Using Approximated Diversity），利用近似多样性实现优化。

Result: 在标准基准测试中与当前最先进方法竞争力相当，同时在高维问题上展现出更好的可扩展性，验证了Soft QD框架的单调性等优良特性。

Conclusion: Soft QD框架为QD优化提供了离散化替代方案，通过可微分方法实现了更好的高维问题处理能力，为大规模质量-多样性优化开辟了新途径。

Abstract: Quality-Diversity (QD) algorithms constitute a branch of optimization that is concerned with discovering a diverse and high-quality set of solutions to an optimization problem. Current QD methods commonly maintain diversity by dividing the behavior space into discrete regions, ensuring that solutions are distributed across different parts of the space. The QD problem is then solved by searching for the best solution in each region. This approach to QD optimization poses challenges in large solution spaces, where storing many solutions is impractical, and in high-dimensional behavior spaces, where discretization becomes ineffective due to the curse of dimensionality. We present an alternative framing of the QD problem, called \emph{Soft QD}, that sidesteps the need for discretizations. We validate this formulation by demonstrating its desirable properties, such as monotonicity, and by relating its limiting behavior to the widely used QD Score metric. Furthermore, we leverage it to derive a novel differentiable QD algorithm, \emph{Soft QD Using Approximated Diversity (SQUAD)}, and demonstrate empirically that it is competitive with current state of the art methods on standard benchmarks while offering better scalability to higher dimensional problems.

中文标题: 软质量-多样性优化

中文摘要: 质量-多样性（QD）算法构成了优化领域的一个分支，专注于发现优化问题的多样化和高质量解集。当前的QD方法通常通过将行为空间划分为离散区域来维持多样性，确保解分布在空间的不同部分。然后通过在每个区域中搜索最佳解来解决QD问题。这种方法在大规模解空间（存储许多解不切实际）和高维行为空间（由于维度灾难，离散化变得无效）中面临挑战。我们提出了QD问题的替代框架，称为软QD，避免了离散化的需求。我们通过展示其理想特性（如单调性）以及将其极限行为与广泛使用的QD Score指标相关联来验证这一公式。此外，我们利用它推导出一种新颖的可微分QD算法——使用近似多样性的软QD（SQUAD），并通过实证证明在标准基准测试中与当前最先进方法竞争力相当，同时在高维问题上提供更好的可扩展性。

</details>


### [343] [Self-Supervised Dynamical System Representations for Physiological Time-Series](https://arxiv.org/abs/2512.00239)
*Yenho Chen,Maxwell A. Xu,James M. Rehg,Christopher J. Rozell*

Main category: cs.LG

TL;DR: 本文提出PULSE框架，通过跨重构自监督学习从生理时间序列中提取系统参数信息，同时丢弃样本特异性噪声，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有生理时间序列的自监督学习方法依赖启发式原则或约束不足的生成任务，难以有效保留底层生理状态信息并过滤无关噪声。需要一种能明确提取系统共享参数信息的方法。

Method: 提出PULSE框架，基于跨重构的自监督预训练目标，利用多时间序列的动态系统生成模型信息结构，提取与系统参数相关的生成变量信息，丢弃样本特异性噪声。

Result: 理论分析为系统信息恢复提供了充分条件，合成动态系统实验验证了理论。在真实数据集上，PULSE学习的表征能广泛区分语义类别、提高标签效率并改善迁移学习性能。

Conclusion: PULSE框架通过动态系统视角有效提取生理时间序列的系统级信息，为自监督学习提供了理论基础和实用方法，在多种下游任务中表现出色。

Abstract: The effectiveness of self-supervised learning (SSL) for physiological time series depends on the ability of a pretraining objective to preserve information about the underlying physiological state while filtering out unrelated noise. However, existing strategies are limited due to reliance on heuristic principles or poorly constrained generative tasks. To address this limitation, we propose a pretraining framework that exploits the information structure of a dynamical systems generative model across multiple time-series. This framework reveals our key insight that class identity can be efficiently captured by extracting information about the generative variables related to the system parameters shared across similar time series samples, while noise unique to individual samples should be discarded. Building on this insight, we propose PULSE, a cross-reconstruction-based pretraining objective for physiological time series datasets that explicitly extracts system information while discarding non-transferrable sample-specific ones. We establish theory that provides sufficient conditions for the system information to be recovered, and empirically validate it using a synthetic dynamical systems experiment. Furthermore, we apply our method to diverse real-world datasets, demonstrating that PULSE learns representations that can broadly distinguish semantic classes, increase label efficiency, and improve transfer learning.

中文标题: 生理时间序列的自监督动态系统表征

中文摘要: 生理时间序列的自监督学习效果取决于预训练目标能否保留底层生理状态信息同时过滤无关噪声。然而，现有策略因依赖启发式原则或约束不足的生成任务而受限。为解决这一限制，我们提出了一个预训练框架，利用多时间序列的动态系统生成模型信息结构。该框架揭示了我们的关键见解：类别身份可以通过提取与相似时间序列样本共享的系统参数相关的生成变量信息来有效捕获，而应丢弃个体样本特有的噪声。基于这一见解，我们提出了PULSE，一种基于跨重构的生理时间序列数据集预训练目标，明确提取系统信息同时丢弃不可转移的样本特异性信息。我们建立了为系统信息恢复提供充分条件的理论，并通过合成动态系统实验进行了经验验证。此外，我们将方法应用于多样化的真实世界数据集，证明PULSE学习的表征能够广泛区分语义类别、提高标签效率并改善迁移学习。

</details>


### [344] [A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations](https://arxiv.org/abs/2512.00249)
*Scotty Black,Christian Darken*

Main category: cs.LG

TL;DR: 本文提出了一种分层混合AI方法，将深度强化学习与脚本化智能体结合用于战斗模拟，利用脚本化智能体处理常规战术决策，强化学习智能体负责高层战略决策，从而提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 在支持兵棋推演的战斗模拟领域，现有智能体开发主要采用基于规则的脚本化方法，虽然具有可预测性和一致性，但在动态复杂场景中缺乏灵活性。而强化学习方法虽然具有适应性和学习能力，但存在黑盒决策过程和可扩展性问题。需要一种能够结合两者优势的方法。

Method: 提出了一种分层混合人工智能方法，通过分层结构将脚本化智能体和强化学习智能体协同工作。脚本化智能体负责常规、战术层面的决策，提供可靠性和可预测性；强化学习智能体负责更高层次的战略决策，提供动态适应能力。这种分层集成旨在克服各自方法的局限性。

Result: 该方法显著提高了整体性能，为复杂模拟环境中智能体的开发和训练提供了稳健、适应性强且有效的解决方案。集成方法在保持可预测性的同时增强了适应性。

Conclusion: 分层混合AI方法成功地将脚本化智能体的可靠性与强化学习的动态适应能力相结合，为战斗模拟中的智能体开发提供了更优的解决方案，能够更好地处理复杂动态场景。

Abstract: In the domain of combat simulations in support of wargaming, the development of intelligent agents has predominantly been characterized by rule-based, scripted methodologies with deep reinforcement learning (RL) approaches only recently being introduced. While scripted agents offer predictability and consistency in controlled environments, they fall short in dynamic, complex scenarios due to their inherent inflexibility. Conversely, RL agents excel in adaptability and learning, offering potential improvements in handling unforeseen situations, but suffer from significant challenges such as black-box decision-making processes and scalability issues in larger simulation environments. This paper introduces a novel hierarchical hybrid artificial intelligence (AI) approach that synergizes the reliability and predictability of scripted agents with the dynamic, adaptive learning capabilities of RL. By structuring the AI system hierarchically, the proposed approach aims to utilize scripted agents for routine, tactical-level decisions and RL agents for higher-level, strategic decision-making, thus addressing the limitations of each method while leveraging their individual strengths. This integration is shown to significantly improve overall performance, providing a robust, adaptable, and effective solution for developing and training intelligent agents in complex simulation environments.

中文标题: 分层混合AI方法：在战斗模拟中集成深度强化学习与脚本化智能体

中文摘要: 在支持兵棋推演的战斗模拟领域，智能体的开发主要采用基于规则的脚本化方法，而深度强化学习方法最近才被引入。脚本化智能体在受控环境中具有可预测性和一致性，但由于固有的不灵活性，在动态复杂场景中表现不足。相反，强化学习智能体在适应性和学习方面表现出色，在处理意外情况方面具有改进潜力，但面临重大挑战，如黑盒决策过程和在较大模拟环境中的可扩展性问题。本文提出了一种新颖的分层混合人工智能方法，将脚本化智能体的可靠性和可预测性与强化学习的动态适应学习能力相结合。通过分层构建AI系统，该方法旨在利用脚本化智能体处理常规战术层面的决策，强化学习智能体处理更高层次的战略决策，从而解决每种方法的局限性，同时发挥各自的优势。这种集成被证明能显著提高整体性能，为复杂模拟环境中智能体的开发和训练提供了稳健、适应性强且有效的解决方案。

</details>


### [345] [Challenges of Heterogeneity in Big Data: A Comparative Study of Classification in Large-Scale Structured and Unstructured Domains](https://arxiv.org/abs/2512.00298)
*González Trigueros Jesús Eduardo,Alonso Sánchez Alejandro,Muñoz Rivera Emilio,Peñarán Prieto Mariana Jaqueline,Mendoza González Camila Natalia*

Main category: cs.LG

TL;DR: 比较结构化（数值）和非结构化（文本）大数据分类策略的研究，发现"复杂性悖论"：高维数值数据中优化线性模型优于复杂模型，而文本数据中简单模型配合特征工程效果更好。


<details>
  <summary>Details</summary>
Motivation: 研究大数据中异质性（多样性）对分类任务的影响，比较结构化数据和非结构化数据在不同算法策略下的表现差异，为实际应用提供算法选择框架。

Method: 采用双重方法：1）对结构化数值数据使用进化算法和贝叶斯超参数优化（遗传算法、Optuna）；2）对大规模文本数据使用Apache Spark分布式处理，结合Transformer嵌入（ROBERTa）和贝叶斯目标编码。

Result: 发现"复杂性悖论"：在高维数值数据中，优化后的线性模型（SVM、逻辑回归）优于深度架构和梯度提升；而在文本领域，复杂模型的分布式微调容易过拟合，而简单模型配合特征工程能更好泛化。

Conclusion: 提出了基于数据性质和基础设施约束的统一算法选择框架，强调根据数据类型选择合适策略的重要性，而非一味追求复杂模型。

Abstract: This study analyzes the impact of heterogeneity ("Variety") in Big Data by comparing classification strategies across structured (Epsilon) and unstructured (Rest-Mex, IMDB) domains. A dual methodology was implemented: evolutionary and Bayesian hyperparameter optimization (Genetic Algorithms, Optuna) in Python for numerical data, and distributed processing in Apache Spark for massive textual corpora. The results reveal a "complexity paradox": in high-dimensional spaces, optimized linear models (SVM, Logistic Regression) outperformed deep architectures and Gradient Boosting. Conversely, in text-based domains, the constraints of distributed fine-tuning led to overfitting in complex models, whereas robust feature engineering -- specifically Transformer-based embeddings (ROBERTa) and Bayesian Target Encoding -- enabled simpler models to generalize effectively. This work provides a unified framework for algorithm selection based on data nature and infrastructure constraints.

中文标题: 大数据异质性挑战：大规模结构化和非结构化领域分类比较研究

中文摘要: 本研究通过比较结构化（Epsilon）和非结构化（Rest-Mex、IMDB）领域的分类策略，分析大数据中异质性（"多样性"）的影响。采用双重方法：针对数值数据的进化算法和贝叶斯超参数优化（遗传算法、Optuna），以及针对大规模文本语料库的Apache Spark分布式处理。结果揭示了"复杂性悖论"：在高维空间中，优化的线性模型（SVM、逻辑回归）优于深度架构和梯度提升。相反，在基于文本的领域中，分布式微调的限制导致复杂模型过拟合，而鲁棒的特征工程——特别是基于Transformer的嵌入（ROBERTa）和贝叶斯目标编码——使简单模型能够有效泛化。这项工作为基于数据性质和基础设施约束的算法选择提供了一个统一框架。

</details>


### [346] [Dynamic Algorithm for Explainable k-medians Clustering under lp Norm](https://arxiv.org/abs/2512.01150)
*Konstantin Makarychev,Ilias Papanikolaou,Liren Shan*

Main category: cs.LG

TL;DR: 提出了第一个针对所有p≥1的lp范数下可解释k-medians聚类算法，并实现了动态维护，具有较好的近似比和效率。


<details>
  <summary>Details</summary>
Motivation: 可解释聚类对于用户理解和信任聚类结果至关重要。现有的可解释k-medians算法仅适用于p=1和p=2，缺乏通用性。同时，现实世界的数据集通常是动态变化的，需要能够高效更新的算法。

Method: 设计了一种阈值决策树构建算法，通过阈值化单个特征来划分数据。针对所有p≥1的lp范数，提出了新的近似算法。进一步实现了动态版本，支持插入和删除操作，通过摊销更新和有限调整代价来维护聚类。

Result: 算法对任何p≥1实现了O(p(log k)^{1 + 1/p - 1/p^2})的近似比。对于p=2，改进了现有的O(log^{3/2}k)界限；对于p=1，在乘法O(log log k)因子内匹配了log k + O(1)的紧界。动态算法具有O(d log^3 k)的摊销更新时间和O(log k)的调整代价。

Conclusion: 该研究首次为所有p≥1的lp范数提供了可解释k-medians聚类算法，填补了理论空白。动态实现使其适用于实际应用中的大规模动态数据集，平衡了可解释性、近似质量和计算效率。

Abstract: We study the problem of explainable k-medians clustering introduced by Dasgupta, Frost, Moshkovitz, and Rashtchian (2020). In this problem, the goal is to construct a threshold decision tree that partitions data into k clusters while minimizing the k-medians objective. These trees are interpretable because each internal node makes a simple decision by thresholding a single feature, allowing users to trace and understand how each point is assigned to a cluster. We present the first algorithm for explainable k-medians under lp norm for every finite p >= 1. Our algorithm achieves an O(p(log k)^{1 + 1/p - 1/p^2}) approximation to the optimal k-medians cost for any p >= 1. Previously, algorithms were known only for p = 1 and p = 2. For p = 2, our algorithm improves upon the existing bound of O(log^{3/2}k), and for p = 1, it matches the tight bound of log k + O(1) up to a multiplicative O(log log k) factor. We show how to implement our algorithm in a dynamic setting. The dynamic algorithm maintains an explainable clustering under a sequence of insertions and deletions, with amortized update time O(d log^3 k) and O(log k) recourse, making it suitable for large-scale and evolving datasets.

中文标题: 基于lp范数的可解释k-medians聚类动态算法

中文摘要: 我们研究了Dasgupta、Frost、Moshkovitz和Rashtchian（2020）提出的可解释k-medians聚类问题。在这个问题中，目标是构建一个阈值决策树，将数据划分为k个簇，同时最小化k-medians目标函数。这些树具有可解释性，因为每个内部节点通过阈值化单个特征做出简单决策，使用户能够追踪和理解每个点如何被分配到簇中。我们提出了第一个针对所有有限p≥1的lp范数下可解释k-medians算法。我们的算法对任何p≥1实现了O(p(log k)^{1 + 1/p - 1/p^2})的近似比。先前，算法仅针对p=1和p=2已知。对于p=2，我们的算法改进了现有的O(log^{3/2}k)界限，对于p=1，它在乘法O(log log k)因子内匹配了log k + O(1)的紧界。我们展示了如何在动态设置中实现我们的算法。该动态算法在插入和删除序列下维护可解释聚类，具有摊销更新时间O(d log^3 k)和O(log k)的调整代价，使其适用于大规模和不断演变的数据集。

</details>


### [347] [BioArc: Discovering Optimal Neural Architectures for Biological Foundation Models](https://arxiv.org/abs/2512.00283)
*Yi Fang,Haoran Xu,Jiaxin Han,Sirui Ding,Yizhi Wang,Yue Wang,Xuan Wang*

Main category: cs.LG

TL;DR: BioArc是一个利用神经架构搜索（NAS）为生物基础模型自动发现最优神经架构的框架，解决了现有方法直接采用通用AI架构而不考虑生物数据独特特性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生物基础模型主要直接采用通用AI领域的架构，没有考虑生物数据的独特物理化学和结构特性（如长程依赖、稀疏信息、复杂语法），导致性能次优。需要系统化、原则化的方法来自动发现适合生物数据的最优架构。

Method: 1. 使用神经架构搜索（NAS）系统探索广阔的架构设计空间；2. 评估跨多个生物模态的架构；3. 分析架构、标记化和训练策略之间的相互作用；4. 提出并比较多种架构预测方法，用于预测新生物任务的最优架构。

Result: 1. 通过大规模分析发现了新颖的高性能架构；2. 提炼出一套经验设计原则指导未来模型开发；3. 开发了有效且高效的架构预测方法；4. 为生物基础模型提供了系统化的架构发现框架。

Conclusion: BioArc提供了一个基础资源和原则化方法论，用于指导创建下一代面向特定任务和基础的生物学模型，超越了直觉驱动的架构设计，实现了系统化、自动化的最优架构发现。

Abstract: Foundation models have revolutionized various fields such as natural language processing (NLP) and computer vision (CV). While efforts have been made to transfer the success of the foundation models in general AI domains to biology, existing works focus on directly adopting the existing foundation model architectures from general machine learning domains without a systematic design considering the unique physicochemical and structural properties of each biological data modality. This leads to suboptimal performance, as these repurposed architectures struggle to capture the long-range dependencies, sparse information, and complex underlying ``grammars'' inherent to biological data. To address this gap, we introduce BioArc, a novel framework designed to move beyond intuition-driven architecture design towards principled, automated architecture discovery for biological foundation models. Leveraging Neural Architecture Search (NAS), BioArc systematically explores a vast architecture design space, evaluating architectures across multiple biological modalities while rigorously analyzing the interplay between architecture, tokenization, and training strategies. This large-scale analysis identifies novel, high-performance architectures, allowing us to distill a set of empirical design principles to guide future model development. Furthermore, to make the best of this set of discovered principled architectures, we propose and compare several architecture prediction methods that effectively and efficiently predict optimal architectures for new biological tasks. Overall, our work provides a foundational resource and a principled methodology to guide the creation of the next generation of task-specific and foundation models for biology.

中文标题: BioArc：为生物基础模型发现最优神经架构

中文摘要: 基础模型已经彻底改变了自然语言处理（NLP）和计算机视觉（CV）等多个领域。虽然已有努力将基础模型在通用人工智能领域的成功转移到生物学中，但现有工作主要关注直接采用通用机器学习领域现有的基础模型架构，而没有系统性地考虑每种生物数据模态独特的物理化学和结构特性。这导致了次优性能，因为这些重新利用的架构难以捕捉生物数据固有的长程依赖、稀疏信息和复杂底层"语法"。为填补这一空白，我们引入了BioArc，这是一个新颖的框架，旨在超越直觉驱动的架构设计，转向原则化、自动化的生物基础模型架构发现。利用神经架构搜索（NAS），BioArc系统地探索了广阔的架构设计空间，评估跨多个生物模态的架构，同时严格分析架构、标记化和训练策略之间的相互作用。这种大规模分析识别了新颖的高性能架构，使我们能够提炼出一套经验设计原则来指导未来的模型开发。此外，为了充分利用这组发现的原则化架构，我们提出并比较了几种架构预测方法，这些方法能够有效且高效地预测新生物任务的最优架构。总体而言，我们的工作提供了一个基础资源和一套原则化方法论，用于指导创建下一代面向特定任务和基础的生物学模型。

</details>


### [348] [Gradient Inversion in Federated Reinforcement Learning](https://arxiv.org/abs/2512.00303)
*Shenghong He*

Main category: cs.LG

TL;DR: 本文提出了一种针对联邦强化学习的新型攻击方法RGIA，通过引入基于先验知识的正则化来约束重建数据的转移分布，从而成功恢复本地私有数据。


<details>
  <summary>Details</summary>
Motivation: 联邦强化学习虽然通过梯度共享保护了数据隐私，但攻击者仍可能利用共享梯度重建本地训练数据。与传统监督学习不同，FRL中的成功重建需要生成的数据不仅匹配梯度，还要符合环境的真实转移动态。现有方法在这方面存在不足，需要新的攻击方法来有效重建符合真实动态的私有数据。

Method: 提出了正则化梯度反演攻击（RGIA）方法，在梯度反演优化过程中引入基于先验知识的正则化项，对状态、奖励和转移动态进行约束，确保重建的数据分布接近真实的转移分布。该方法将解空间从包含虚假解的广泛集合缩小到满足梯度匹配和真实转移动态的约束子集。

Result: 在控制任务和自动驾驶任务上的实验表明，RGIA能够有效约束重建的数据转移分布，成功重建本地私有数据。理论分析证明了正则化项能够缩小解空间，提高重建的准确性和有效性。

Conclusion: RGIA是一种有效的联邦强化学习隐私攻击方法，通过引入基于先验知识的正则化约束，成功解决了FRL中数据重建需要符合真实转移动态的挑战，揭示了FRL系统中存在的隐私泄露风险。

Abstract: Federated reinforcement learning (FRL) enables distributed learning of optimal policies while preserving local data privacy through gradient sharing.However, FRL faces the risk of data privacy leaks, where attackers exploit shared gradients to reconstruct local training data.Compared to traditional supervised federated learning, successful reconstruction in FRL requires the generated data not only to match the shared gradients but also to align with real transition dynamics of the environment (i.e., aligning with the real data transition distribution).To address this issue, we propose a novel attack method called Regularization Gradient Inversion Attack (RGIA), which enforces prior-knowledge-based regularization on states, rewards, and transition dynamics during the optimization process to ensure that the reconstructed data remain close to the true transition distribution.Theoretically, we prove that the prior-knowledge-based regularization term narrows the solution space from a broad set containing spurious solutions to a constrained subset that satisfies both gradient matching and true transition dynamics.Extensive experiments on control tasks and autonomous driving tasks demonstrate that RGIA can effectively constrain reconstructed data transition distributions and thus successfully reconstruct local private data.

中文标题: 联邦强化学习中的梯度反演攻击

中文摘要: 联邦强化学习（FRL）通过梯度共享实现了分布式学习最优策略，同时保护了本地数据隐私。然而，FRL面临着数据隐私泄露的风险，攻击者可以利用共享的梯度来重建本地训练数据。与传统的监督式联邦学习相比，FRL中的成功重建要求生成的数据不仅要匹配共享梯度，还要与环境真实的转移动态保持一致（即与真实数据转移分布对齐）。为了解决这个问题，我们提出了一种名为正则化梯度反演攻击（RGIA）的新型攻击方法，该方法在优化过程中对状态、奖励和转移动态施加基于先验知识的正则化，以确保重建的数据保持接近真实的转移分布。理论上，我们证明了基于先验知识的正则化项将解空间从包含虚假解的广泛集合缩小到满足梯度匹配和真实转移动态的约束子集。在控制任务和自动驾驶任务上的大量实验表明，RGIA能够有效约束重建的数据转移分布，从而成功重建本地私有数据。

</details>


### [349] [Tracing Mathematical Proficiency Through Problem-Solving Processes](https://arxiv.org/abs/2512.00311)
*Jungyang Park,Suho Kang,Jaewoo Park,Jaehong Kim,Jaewoo Shin,Seonjoon Park,Youngjae Yu*

Main category: cs.LG

TL;DR: 提出KT-PSP方法，利用学生解题过程追踪数学能力，并开发StatusKT框架，通过三阶段LLM流水线提取能力指标，提升预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪方法仅依赖回答正确性，缺乏可解释性，忽视了学生解题过程中蕴含的丰富信息。需要一种能够捕捉数学能力多维方面的方法。

Method: 提出KT-PSP方法，结合学生解题过程；引入KT-PSP-25数据集；开发StatusKT框架，采用教师-学生-教师三阶段LLM流水线：教师LLM提取能力指标，学生LLM基于解题过程生成回答，教师LLM评估掌握程度。

Result: 在KT-PSP-25数据集上的实验表明，StatusKT提高了现有知识追踪方法的预测性能，同时提供了可解释的预测解释。

Conclusion: 通过利用学生解题过程，KT-PSP和StatusKT框架能够更准确地追踪数学能力，提供更好的预测性能和可解释性，为个性化学习提供支持。

Abstract: Knowledge Tracing (KT) aims to model student's knowledge state and predict future performance to enable personalized learning in Intelligent Tutoring Systems. However, traditional KT methods face fundamental limitations in explainability, as they rely solely on the response correctness, neglecting the rich information embedded in students' problem-solving processes. To address this gap, we propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), which incorporates students' problem-solving processes to capture the multidimensional aspects of mathematical proficiency. We also introduce KT-PSP-25, a new dataset specifically designed for the KT-PSP. Building on this, we present StatusKT, a KT framework that employs a teacher-student-teacher three-stage LLM pipeline to extract students' MP as intermediate signals. In this pipeline, the teacher LLM first extracts problem-specific proficiency indicators, then a student LLM generates responses based on the student's solution process, and a teacher LLM evaluates these responses to determine mastery of each indicator. The experimental results on KT-PSP-25 demonstrate that StatusKT improves the prediction performance of existing KT methods. Moreover, StatusKT provides interpretable explanations for its predictions by explicitly modeling students' mathematical proficiency.

中文标题: 通过解题过程追踪数学能力

中文摘要: 知识追踪（KT）旨在建模学生的知识状态并预测未来表现，以实现智能辅导系统中的个性化学习。然而，传统的KT方法在可解释性方面存在根本性限制，因为它们仅依赖于回答的正确性，忽视了学生解题过程中蕴含的丰富信息。为解决这一差距，我们提出了利用解题过程的知识追踪（KT-PSP），该方法结合学生的解题过程来捕捉数学能力的多维方面。我们还引入了KT-PSP-25，这是一个专门为KT-PSP设计的新数据集。在此基础上，我们提出了StatusKT，这是一个采用教师-学生-教师三阶段LLM流水线的KT框架，用于提取学生的数学能力作为中间信号。在该流水线中，教师LLM首先提取问题特定的能力指标，然后学生LLM基于学生的解题过程生成回答，最后教师LLM评估这些回答以确定每个指标的掌握程度。在KT-PSP-25上的实验结果表明，StatusKT提高了现有KT方法的预测性能。此外，StatusKT通过显式建模学生的数学能力，为其预测提供了可解释的解释。

</details>


### [350] [Introducing AI-Driven IoT Energy Management Framework](https://arxiv.org/abs/2512.00321)
*Shivani Mruthyunjaya,Anandi Dutta,Kazi Sifatul Islam*

Main category: cs.LG

TL;DR: 提出AI驱动的物联网能源管理框架，通过情境决策、主动适应和可扩展结构来降低电力消耗、支持电网稳定，并在时间序列数据上验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 现代生活对技术的高度依赖使电力消耗成为关键问题，降低能耗或遵循预测能减少月度成本并提高电力可靠性，需要建立系统化的物联网能源管理解决方案。

Method: 提出整体框架，包括情境决策、主动适应和可扩展结构，支持长期/短期预测、异常检测和定性数据考虑，在电力消耗时间序列数据上评估性能。

Result: 通过应用框架各方面展示了可行性，系统能够有效处理电力消耗预测和管理决策，支持降低能耗和电网稳定性。

Conclusion: AI驱动的物联网能源管理框架为降低电力消耗、提高电网稳定性提供了可行的系统化解决方案，具有实际应用价值。

Abstract: Power consumption has become a critical aspect of modern life due to the consistent reliance on technological advancements. Reducing power consumption or following power usage predictions can lead to lower monthly costs and improved electrical reliability. The proposal of a holistic framework to establish a foundation for IoT systems with a focus on contextual decision making, proactive adaptation, and scalable structure. A structured process for IoT systems with accuracy and interconnected development would support reducing power consumption and support grid stability. This study presents the feasibility of this proposal through the application of each aspect of the framework. This system would have long term forecasting, short term forecasting, anomaly detection, and consideration of qualitative data with any energy management decisions taken. Performance was evaluated on Power Consumption Time Series data to display the direct application of the framework.

中文标题: 引入AI驱动的物联网能源管理框架

中文摘要: 由于对技术进步的一贯依赖，电力消耗已成为现代生活的关键方面。降低电力消耗或遵循电力使用预测可以降低月度成本并提高电力可靠性。本文提出了一个整体框架，为物联网系统建立基础，重点关注情境决策、主动适应和可扩展结构。具有准确性和互联开发的物联网系统结构化流程将有助于降低电力消耗并支持电网稳定性。本研究通过应用框架的各个方面展示了该提案的可行性。该系统将具有长期预测、短期预测、异常检测功能，并在任何能源管理决策中考虑定性数据。在电力消耗时间序列数据上评估了性能，以展示框架的直接应用。

</details>


### [351] [Adaptive prediction theory combining offline and online learning](https://arxiv.org/abs/2512.00342)
*Haizheng Li,Lei Guo*

Main category: cs.LG

TL;DR: 本文提出了一种结合离线学习和在线适应的两阶段预测理论框架，用于处理非线性随机动力系统中的相关非平稳数据，通过理论分析和实证研究验证了该框架相比纯离线或在线方法的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界智能系统通常需要结合离线学习和在线适应来处理高度相关且非平稳的系统数据或信号，但现有文献很少对此进行理论探讨。本文旨在从理论上研究这种两阶段学习框架的预测性能。

Method: 提出两阶段学习框架：1) 离线学习阶段，建立近似非线性最小二乘估计在强相关和分布偏移数据集下的泛化误差上界，利用KL散度量化分布差异；2) 在线适应阶段，基于离线训练模型，针对现实目标系统中可能的参数漂移，提出元LMS预测算法。

Result: 该两阶段框架在预测性能上优于纯离线或纯在线方法，提供了理论保证和实证研究支持，能够有效处理非线性随机动力系统中的相关非平稳数据。

Conclusion: 结合离线学习和在线适应的两阶段框架为处理现实世界智能系统中的相关非平稳数据提供了有效的理论解决方案，在理论和实证上都表现出优越的预测性能。

Abstract: Real-world intelligence systems usually operate by combining offline learning and online adaptation with highly correlated and non-stationary system data or signals, which, however, has rarely been investigated theoretically in the literature. This paper initiates a theoretical investigation on the prediction performance of a two-stage learning framework combining offline and online algorithms for a class of nonlinear stochastic dynamical systems. For the offline-learning phase, we establish an upper bound on the generalization error for approximate nonlinear-least-squares estimation under general datasets with strong correlation and distribution shift, leveraging the Kullback-Leibler divergence to quantify the distributional discrepancies. For the online-adaptation phase, we address, on the basis of the offline-trained model, the possible uncertain parameter drift in real-world target systems by proposing a meta-LMS prediction algorithm. This two-stage framework, integrating offline learning with online adaptation, demonstrates superior prediction performances compared with either purely offline or online methods. Both theoretical guarantees and empirical studies are provided.

中文标题: 结合离线与在线学习的自适应预测理论

中文摘要: 现实世界智能系统通常通过结合离线学习和在线适应来处理高度相关且非平稳的系统数据或信号，然而这在文献中很少得到理论研究。本文对一类非线性随机动力系统，结合离线算法和在线算法的两阶段学习框架的预测性能进行了理论探讨。在离线学习阶段，我们利用Kullback-Leibler散度量化分布差异，为强相关和分布偏移的一般数据集下的近似非线性最小二乘估计建立了泛化误差的上界。在线适应阶段，我们在离线训练模型的基础上，针对现实目标系统中可能的参数漂移，提出了元LMS预测算法。这种结合离线学习和在线适应的两阶段框架，相比纯离线或纯在线方法，展现出更优越的预测性能。本文提供了理论保证和实证研究。

</details>


### [352] [Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning](https://arxiv.org/abs/2512.00351)
*Na Li,Yuchen Jiao,Hangguan Shan,Shefeng Yan*

Main category: cs.LG

TL;DR: 提出了一种内存高效纳什Q学习算法，用于双人零和马尔可夫博弈，在空间复杂度、样本复杂度、计算复杂度和预热成本方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习理论研究存在多个障碍：内存效率低下、样本复杂度对长时域和大状态空间的依赖、高计算复杂度、非马尔可夫策略、非纳什策略以及高预热成本。需要设计一个能同时解决这些问题的算法。

Method: 设计了内存高效纳什Q学习算法，这是一种无模型自博弈算法，专门用于双人零和马尔可夫博弈。算法通过优化内存使用和计算效率，同时保持马尔可夫策略特性。

Result: ME-Nash-QL算法实现了：1）空间复杂度O(SABH)和样本复杂度Õ(H⁴SAB/ε²)；2）最低计算复杂度O(Tpoly(AB))；3）最佳预热成本O(SABpoly(H))。在表格情况下空间复杂度最优，长时域情况下样本复杂度最优。

Conclusion: ME-Nash-QL算法在多智能体强化学习的关键性能指标上全面超越了现有方法，为解决该领域长期存在的理论障碍提供了有效的解决方案，特别是在内存效率、计算效率和预热成本方面取得了显著改进。

Abstract: The thriving field of multi-agent reinforcement learning (MARL) studies how a group of interacting agents make decisions autonomously in a shared dynamic environment. Existing theoretical studies in this area suffer from at least two of the following obstacles: memory inefficiency, the heavy dependence of sample complexity on the long horizon and the large state space, the high computational complexity, non-Markov policy, non-Nash policy, and high burn-in cost. In this work, we take a step towards settling this problem by designing a model-free self-play algorithm \emph{Memory-Efficient Nash Q-Learning (ME-Nash-QL)} for two-player zero-sum Markov games, which is a specific setting of MARL. ME-Nash-QL is proven to enjoy the following merits. First, it can output an $\varepsilon$-approximate Nash policy with space complexity $O(SABH)$ and sample complexity $\widetilde{O}(H^4SAB/\varepsilon^2)$, where $S$ is the number of states, $\{A, B\}$ is the number of actions for two players, and $H$ is the horizon length. It outperforms existing algorithms in terms of space complexity for tabular cases, and in terms of sample complexity for long horizons, i.e., when $\min\{A, B\}\ll H^2$. Second, ME-Nash-QL achieves the lowest computational complexity $O(T\mathrm{poly}(AB))$ while preserving Markov policies, where $T$ is the number of samples. Third, ME-Nash-QL also achieves the best burn-in cost $O(SAB\,\mathrm{poly}(H))$, whereas previous algorithms have a burn-in cost of at least $O(S^3 AB\,\mathrm{poly}(H))$ to attain the same level of sample complexity with ours.

中文标题: 可证明内存高效的无模型强化学习自博弈算法

中文摘要: 多智能体强化学习（MARL）这一蓬勃发展的领域研究一组相互作用的智能体如何在共享的动态环境中自主做出决策。该领域现有的理论研究至少存在以下两个障碍：内存效率低下、样本复杂度对长时域和大状态空间的严重依赖、高计算复杂度、非马尔可夫策略、非纳什策略以及高预热成本。在本工作中，我们通过设计一种用于双人零和马尔可夫博弈（MARL的一个特定设置）的无模型自博弈算法——内存高效纳什Q学习（ME-Nash-QL），朝着解决这一问题迈出了一步。ME-Nash-QL被证明具有以下优点。首先，它能够以空间复杂度O(SABH)和样本复杂度Õ(H⁴SAB/ε²)输出ε近似纳什策略，其中S是状态数，{A, B}是两个玩家的动作数，H是时域长度。在表格情况下，它在空间复杂度方面优于现有算法；在长时域情况下（即当min{A, B} ≪ H²时），在样本复杂度方面也表现更优。其次，ME-Nash-QL在保持马尔可夫策略的同时实现了最低的计算复杂度O(Tpoly(AB))，其中T是样本数。第三，ME-Nash-QL还实现了最佳的预热成本O(SABpoly(H))，而先前算法要达到与我们相同水平的样本复杂度，预热成本至少为O(S³ABpoly(H))。

</details>


### [353] [Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning](https://arxiv.org/abs/2512.00352)
*Na Li,Zewu Zheng,Wei Ni,Hangguan Shan,Wenjie Zhang,Xinyu Li*

Main category: cs.LG

TL;DR: 提出RTZ-VI-LCB算法，首个在离线鲁棒双人零和马尔可夫博弈中实现最优样本复杂度的模型，结合乐观鲁棒值迭代与伯恩斯坦惩罚项，在部分覆盖下保证性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习面临环境不确定性导致的模拟到现实差距问题，需要鲁棒策略。离线设置下的鲁棒双人零和马尔可夫博弈缺乏样本高效的算法，现有方法样本复杂度不够理想。

Method: 提出RTZ-VI-LCB算法：基于模型的离线算法，结合乐观鲁棒值迭代（optimistic robust value iteration）与数据驱动的伯恩斯坦风格惩罚项（Bernstein-style penalty term）进行鲁棒值估计，处理历史数据中的分布偏移。

Result: 算法在部分覆盖和环境不确定性下建立了近乎最优的样本复杂度保证，开发了信息论下界证明其紧致性，在状态和动作空间维度上达到最优，实验验证了算法有效性。

Conclusion: RTZ-VI-LCB是首个在离线鲁棒双人零和马尔可夫博弈中达到最优样本复杂度的算法，为领域设定了新基准，通过理论分析和实验验证了其优越性能。

Abstract: Multi-agent reinforcement learning (MARL), as a thriving field, explores how multiple agents independently make decisions in a shared dynamic environment. Due to environmental uncertainties, policies in MARL must remain robust to tackle the sim-to-real gap. We focus on robust two-player zero-sum Markov games (TZMGs) in offline settings, specifically on tabular robust TZMGs (RTZMGs). We propose a model-based algorithm (\textit{RTZ-VI-LCB}) for offline RTZMGs, which is optimistic robust value iteration combined with a data-driven Bernstein-style penalty term for robust value estimation. By accounting for distribution shifts in the historical dataset, the proposed algorithm establishes near-optimal sample complexity guarantees under partial coverage and environmental uncertainty. An information-theoretic lower bound is developed to confirm the tightness of our algorithm's sample complexity, which is optimal regarding both state and action spaces. To the best of our knowledge, RTZ-VI-LCB is the first to attain this optimality, sets a new benchmark for offline RTZMGs, and is validated experimentally.

中文标题: 样本高效的表格自博弈用于离线鲁棒强化学习

中文摘要: 多智能体强化学习（MARL）作为一个蓬勃发展的领域，探索多个智能体如何在共享的动态环境中独立做出决策。由于环境的不确定性，MARL中的策略必须保持鲁棒性以应对模拟到现实的差距。我们专注于离线设置中的鲁棒双人零和马尔可夫博弈（TZMGs），特别是表格鲁棒TZMGs（RTZMGs）。我们提出了一种用于离线RTZMGs的基于模型的算法（RTZ-VI-LCB），该算法结合了乐观鲁棒值迭代和数据驱动的伯恩斯坦风格惩罚项用于鲁棒值估计。通过考虑历史数据集中的分布偏移，所提出的算法在部分覆盖和环境不确定性的情况下建立了近乎最优的样本复杂度保证。我们开发了一个信息论下界来确认我们算法样本复杂度的紧致性，该复杂度在状态和动作空间方面都是最优的。据我们所知，RTZ-VI-LCB是第一个达到这种最优性的算法，为离线RTZMGs设定了新的基准，并通过实验验证。

</details>


### [354] [Learning Causal States Under Partial Observability and Perturbation](https://arxiv.org/abs/2512.00357)
*Na Li,Hangguan Shan,Wei Ni,Wenjie Zhang,Xinyu Li,Yamin Wang*

Main category: cs.LG

TL;DR: CaDiff是一个结合异步扩散模型和双模拟度量的框架，用于在部分可观测和扰动环境中学习因果状态表示，提升强化学习算法性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时处理部分可观测性和环境扰动问题，特别是在P^2OMDPs（部分可观测且扰动的马尔可夫决策过程）中，这限制了强化学习在现实复杂环境中的应用。

Method: 提出CaDiff框架，包含异步扩散模型（ADM）和新双模拟度量。ADM允许前向和反向过程具有不同步数，将P^2OMDP的扰动解释为可通过扩散抑制的噪声。双模拟度量量化部分可观测环境与其因果对应物之间的相似性。

Result: 在Roboschool任务上的实验表明，CaDiff相比基线方法至少提升14.18%的回报。该框架首次使用扩散模型近似因果状态，兼具理论严谨性和实用性。

Conclusion: CaDiff是首个通过扩散模型近似因果状态的框架，能有效处理部分可观测性和环境扰动问题，为强化学习在复杂现实环境中的应用提供了理论保证和实践方案。

Abstract: A critical challenge for reinforcement learning (RL) is making decisions based on incomplete and noisy observations, especially in perturbed and partially observable Markov decision processes (P$^2$OMDPs). Existing methods fail to mitigate perturbations while addressing partial observability. We propose \textit{Causal State Representation under Asynchronous Diffusion Model (CaDiff)}, a framework that enhances any RL algorithm by uncovering the underlying causal structure of P$^2$OMDPs. This is achieved by incorporating a novel asynchronous diffusion model (ADM) and a new bisimulation metric. ADM enables forward and reverse processes with different numbers of steps, thus interpreting the perturbation of P$^2$OMDP as part of the noise suppressed through diffusion. The bisimulation metric quantifies the similarity between partially observable environments and their causal counterparts. Moreover, we establish the theoretical guarantee of CaDiff by deriving an upper bound for the value function approximation errors between perturbed observations and denoised causal states, reflecting a principled trade-off between approximation errors of reward and transition-model. Experiments on Roboschool tasks show that CaDiff enhances returns by at least 14.18\% compared to baselines. CaDiff is the first framework that approximates causal states using diffusion models with both theoretical rigor and practicality.

中文标题: 在部分可观测性和扰动下学习因果状态

中文摘要: 强化学习（RL）面临的一个关键挑战是基于不完整和噪声观测做出决策，特别是在扰动和部分可观测的马尔可夫决策过程（P^2OMDPs）中。现有方法无法在解决部分可观测性的同时缓解扰动。我们提出了《基于异步扩散模型的因果状态表示（CaDiff）》，这是一个通过揭示P^2OMDPs的底层因果结构来增强任何RL算法的框架。这是通过结合新颖的异步扩散模型（ADM）和新的双模拟度量实现的。ADM允许前向和反向过程具有不同步数，从而将P^2OMDP的扰动解释为可通过扩散抑制的噪声部分。双模拟度量量化了部分可观测环境与其因果对应物之间的相似性。此外，我们通过推导扰动观测与去噪因果状态之间价值函数近似误差的上界，建立了CaDiff的理论保证，反映了奖励和转移模型近似误差之间的原则性权衡。在Roboschool任务上的实验表明，CaDiff相比基线方法至少提升14.18%的回报。CaDiff是首个使用扩散模型近似因果状态的框架，兼具理论严谨性和实用性。

</details>


### [355] [S^2-KD: Semantic-Spectral Knowledge Distillation Spatiotemporal Forecasting](https://arxiv.org/abs/2512.00366)
*Wenshuo Wang,Yaomin Shen,Yingjie Tan,Yihao Chen*

Main category: cs.LG

TL;DR: S^2-KD是一个结合语义和频谱知识蒸馏的时空预测框架，通过多模态教师模型提取语义因果信息和频谱特征，蒸馏到轻量级视觉学生模型中，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法主要关注频谱特性（高频细节和低频趋势），但忽略了视觉模式背后的语义和因果上下文信息，限制了预测模型的准确性和鲁棒性。

Method: 1. 训练多模态教师模型，结合大型多模态模型的文本叙述进行语义推理，同时在潜在空间解耦频谱分量；2. 设计新的蒸馏目标，将统一的语义-频谱知识转移到仅视觉的学生模型中；3. 学生模型在推理时无需文本输入或额外架构。

Result: 在WeatherBench和TaxiBJ+基准测试中，S^2-KD显著提升了轻量级学生模型的性能，在长时域预测和复杂非平稳场景中超越了现有最先进方法。

Conclusion: S^2-KD成功将语义先验与频谱表示相结合，为时空预测提供了一种有效的知识蒸馏框架，使轻量级模型能够获得更准确和语义连贯的预测能力。

Abstract: Spatiotemporal forecasting often relies on computationally intensive models to capture complex dynamics. Knowledge distillation (KD) has emerged as a key technique for creating lightweight student models, with recent advances like frequency-aware KD successfully preserving spectral properties (i.e., high-frequency details and low-frequency trends). However, these methods are fundamentally constrained by operating on pixel-level signals, leaving them blind to the rich semantic and causal context behind the visual patterns. To overcome this limitation, we introduce S^2-KD, a novel framework that unifies Semantic priors with Spectral representations for distillation. Our approach begins by training a privileged, multimodal teacher model. This teacher leverages textual narratives from a Large Multimodal Model (LMM) to reason about the underlying causes of events, while its architecture simultaneously decouples spectral components in its latent space. The core of our framework is a new distillation objective that transfers this unified semantic-spectral knowledge into a lightweight, vision-only student. Consequently, the student learns to make predictions that are not only spectrally accurate but also semantically coherent, without requiring any textual input or architectural overhead at inference. Extensive experiments on benchmarks like WeatherBench and TaxiBJ+ show that S^2-KD significantly boosts the performance of simple student models, enabling them to outperform state-of-the-art methods, particularly in long-horizon and complex non-stationary scenarios.

中文标题: S^2-KD：语义-频谱知识蒸馏时空预测框架

中文摘要: 时空预测通常依赖计算密集型模型来捕捉复杂动态。知识蒸馏（KD）已成为创建轻量级学生模型的关键技术，最近如频率感知KD等进展成功保留了频谱特性（即高频细节和低频趋势）。然而，这些方法本质上受限于在像素级信号上操作，使其对视觉模式背后丰富的语义和因果上下文视而不见。为克服这一限制，我们引入了S^2-KD，一个将语义先验与频谱表示统一用于蒸馏的新框架。我们的方法首先训练一个特权多模态教师模型。该教师利用大型多模态模型（LMM）的文本叙述来推理事件的根本原因，同时其架构在潜在空间中解耦频谱分量。我们框架的核心是一个新的蒸馏目标，将这种统一的语义-频谱知识转移到轻量级、仅视觉的学生模型中。因此，学生学会做出不仅频谱准确而且语义连贯的预测，在推理时无需任何文本输入或架构开销。在WeatherBench和TaxiBJ+等基准上的大量实验表明，S^2-KD显著提升了简单学生模型的性能，使其能够超越最先进的方法，特别是在长时域和复杂非平稳场景中。

</details>


### [356] [An Empirical Study on the Effectiveness of Incorporating Offline RL As Online RL Subroutines](https://arxiv.org/abs/2512.00383)
*Jianhai Su,Jinzhu Luo,Qi Zhang*

Main category: cs.LG

TL;DR: 该研究探索将离线RL作为在线RL子程序的新方法，利用历史交互作为离线数据集，通过框架实现多种融合方式，实证表明任务性质影响效果，提出的技术能显著提升效率，但现有在线微调方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何将离线强化学习算法有效整合到在线强化学习过程中，利用智能体历史交互数据作为离线数据集，以提高在线学习效率并探索新的学习范式。

Method: 提出一个框架将离线RL作为在线RL的子程序，包括最终策略推荐和在线微调等变体，并引入技术改进其有效性。通过系统实证分析评估不同任务下的表现。

Result: 实证结果显示：1）框架有效性高度依赖任务特性；2）提出的技术能显著提升效果；3）现有在线微调方法整体效果不佳，需要进一步研究。

Conclusion: 将离线RL作为在线RL子程序是可行的新方向，但效果受任务性质影响，提出的改进技术有效，而现有在线微调方法需要更多研究来提升效果。

Abstract: We take the novel perspective of incorporating offline RL algorithms as subroutines of tabula rasa online RL. This is feasible because an online learning agent can repurpose its historical interactions as offline dataset. We formalize this idea into a framework that accommodates several variants of offline RL incorporation such as final policy recommendation and online fine-tuning. We further introduce convenient techniques to improve its effectiveness in enhancing online learning efficiency. Our extensive and systematic empirical analyses show that 1) the effectiveness of the proposed framework depends strongly on the nature of the task, 2) our proposed techniques greatly enhance its effectiveness, and 3) existing online fine-tuning methods are overall ineffective, calling for more research therein.

中文标题: 将离线强化学习作为在线强化学习子程序的实证研究

中文摘要: 我们采用了一个新颖的视角：将离线强化学习算法作为从零开始的在线强化学习的子程序。这是可行的，因为在线学习智能体可以将其历史交互重新用作离线数据集。我们将这一思想形式化为一个框架，该框架适应了离线RL融合的多种变体，如最终策略推荐和在线微调。我们进一步引入了便捷的技术来提高其在增强在线学习效率方面的有效性。我们广泛而系统的实证分析表明：1）所提出框架的有效性在很大程度上取决于任务的性质；2）我们提出的技术极大地提高了其有效性；3）现有的在线微调方法总体上效果不佳，需要更多相关研究。

</details>


### [357] [Efficient and Programmable Exploration of Synthesizable Chemical Space](https://arxiv.org/abs/2512.00384)
*Shitong Luo,Connor W. Coley*

Main category: cs.LG

TL;DR: PrexSyn是一个基于解码器Transformer的高效可编程模型，用于在可合成化学空间中探索分子。它通过十亿级可合成路径数据训练，能够完美重建可合成化学空间，并学习分子性质与合成路径的关联，支持逻辑运算符组合的复合性质查询和黑盒优化。


<details>
  <summary>Details</summary>
Motivation: 可合成化学空间的受限特性使得同时满足合成可行性和期望性质的分子采样面临重大挑战。现有方法难以高效探索这一空间，需要开发能够同时考虑合成可行性和多种性质需求的分子发现工具。

Method: 基于解码器Transformer架构，在十亿级可合成路径数据流上进行训练，这些数据由实时、高吞吐量的C++数据生成引擎提供。模型学习性质与可合成分子之间的关联，支持逻辑运算符组合的复合性质查询，并能通过迭代查询优化对黑盒函数进行分子优化。

Result: PrexSyn在可合成化学空间覆盖度、分子采样效率和推理速度方面达到新的最先进水平。它能够近乎完美地重建可合成化学空间，以高推理速度生成分子，并且相比不考虑合成可行性的基线方法具有更高的采样效率。

Conclusion: PrexSyn通过大规模数据训练和可编程性质查询能力，推动了可合成分子设计的前沿，成为强大的通用分子优化工具，在可合成化学空间探索方面实现了突破性进展。

Abstract: The constrained nature of synthesizable chemical space poses a significant challenge for sampling molecules that are both synthetically accessible and possess desired properties. In this work, we present PrexSyn, an efficient and programmable model for molecular discovery within synthesizable chemical space. PrexSyn is based on a decoder-only transformer trained on a billion-scale datastream of synthesizable pathways paired with molecular properties, enabled by a real-time, high-throughput C++-based data generation engine. The large-scale training data allows PrexSyn to reconstruct the synthesizable chemical space nearly perfectly at a high inference speed and learn the association between properties and synthesizable molecules. Based on its learned property-pathway mappings, PrexSyn can generate synthesizable molecules that satisfy not only single-property conditions but also composite property queries joined by logical operators, thereby allowing users to ``program'' generation objectives. Moreover, by exploiting this property-based querying capability, PrexSyn can efficiently optimize molecules against black-box oracle functions via iterative query refinement, achieving higher sampling efficiency than even synthesis-agnostic baselines, making PrexSyn a powerful general-purpose molecular optimization tool. Overall, PrexSyn pushes the frontier of synthesizable molecular design by setting a new state of the art in synthesizable chemical space coverage, molecular sampling efficiency, and inference speed.

中文标题: 可合成化学空间的高效可编程探索

中文摘要: 可合成化学空间的受限特性对同时满足合成可行性和期望性质的分子采样构成了重大挑战。在这项工作中，我们提出了PrexSyn，这是一个用于在可合成化学空间中进行分子发现的高效可编程模型。PrexSyn基于解码器Transformer架构，在十亿级可合成路径数据流上进行训练，这些数据流与分子性质配对，由实时、高吞吐量的C++数据生成引擎支持。大规模训练数据使PrexSyn能够以高推理速度近乎完美地重建可合成化学空间，并学习性质与可合成分子之间的关联。基于学习到的性质-路径映射，PrexSyn可以生成不仅满足单性质条件，还能满足由逻辑运算符连接的复合性质查询的可合成分子，从而允许用户"编程"生成目标。此外，通过利用这种基于性质的查询能力，PrexSyn可以通过迭代查询优化对黑盒函数进行分子优化，实现比不考虑合成可行性的基线方法更高的采样效率，使PrexSyn成为强大的通用分子优化工具。总体而言，PrexSyn通过在可合成化学空间覆盖度、分子采样效率和推理速度方面达到新的最先进水平，推动了可合成分子设计的前沿。

</details>


### [358] [From Coefficients to Directions: Rethinking Model Merging with Directional Alignment](https://arxiv.org/abs/2512.00391)
*Zhikang Chen,Sen Cui,Deheng Ye,Min Zhang,Gang Niu,Yu Zhang,Masashi Sugiyama,Tingting Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种新的模型合并框架，强调方向对齐的重要性，通过统一几何框架在参数和特征空间中对齐方向结构，解决了传统系数优化方法忽略方向信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的模型合并方法主要关注参数空间分解或融合系数优化，但忽视了参数和特征空间中方向信息的关键作用。朴素合并会导致主导参数方向不一致并破坏模型间的结构一致性，而基于系数的优化方法隐含假设模型间特征空间方向兼容，但神经崩溃现象表明独立训练模型的类别特征可能具有不同的方向模式。

Method: 提出了一个统一的几何框架——方向对齐合并（Merging with Directional Alignment），该方法在参数空间和特征空间中对齐方向结构，确保模型合并时保持结构一致性，而不是仅仅优化融合系数。

Result: 方向对齐提高了结构一致性，在多个基准测试、模型规模和任务配置上的广泛实验验证了该方法的有效性，相比传统方法表现出更好的性能。

Conclusion: 方向对齐是模型合并中的关键因素，提出的统一几何框架通过同时考虑参数和特征空间的方向结构，显著提升了合并模型的性能，为模型合并研究提供了新的视角。

Abstract: Model merging has emerged as a practical paradigm for integrating multiple independently trained models into a single model without joint retraining. Previous studies have demonstrated the effectiveness of combining parameters through strategies such as parameter decomposition, coefficient optimization, and subspace learning, significantly reducing the need for expensive joint training and achieving strong empirical performance across diverse tasks. However, these approaches predominantly treat merging as a problem of parameter space decomposition or fusion coefficient optimization, while overlooking the critical role of directional information in both parameter and feature spaces. In practice, naïve merging introduces inconsistencies in dominant parameter directions and disrupts structural coherence across models, which can degrade performance. Moreover, coefficient-based optimization methods implicitly assume compatible feature-space directions across models. However, Neural Collapse indicates that class features follow structured directional patterns, which may differ across independently trained models, making coefficient optimization alone insufficient. In this work, we emphasize the importance of \emph{directional alignment} and introduce a unified geometric framework, \emph{Merging with Directional Alignment} (\method{}), which aligns directional structures consistently in both the parameter and feature spaces. Our analysis shows that directional alignment improves structural coherence, and extensive experiments across benchmarks, model scales, and task configurations further validate the effectiveness of our approach.

中文标题: 从系数到方向：重新思考基于方向对齐的模型合并

中文摘要: 模型合并已成为将多个独立训练模型集成到单个模型中的实用范式，无需联合重新训练。先前研究通过参数分解、系数优化和子空间学习等策略证明了结合参数的有效性，显著减少了昂贵的联合训练需求，并在多样化任务中实现了强大的实证性能。然而，这些方法主要将合并视为参数空间分解或融合系数优化问题，而忽视了参数和特征空间中方向信息的关键作用。实际上，朴素合并会引入主导参数方向的不一致性，并破坏模型间的结构一致性，从而可能降低性能。此外，基于系数的优化方法隐含假设模型间特征空间方向兼容。然而，神经崩溃现象表明类别特征遵循结构化方向模式，这些模式在独立训练模型间可能不同，使得仅靠系数优化不足够。在这项工作中，我们强调方向对齐的重要性，并引入一个统一的几何框架——方向对齐合并，该方法在参数和特征空间中对齐方向结构。我们的分析表明方向对齐提高了结构一致性，在基准测试、模型规模和任务配置上的广泛实验进一步验证了我们方法的有效性。

</details>


### [359] [Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement](https://arxiv.org/abs/2512.00396)
*Andrea Procopio,Marco Esposito,Sara Raggiunto,Andrey Gizdov,Alberto Belli,Paola Pierleoni*

Main category: cs.LG

TL;DR: 该研究开发了超轻量可分离CNN模型用于帕金森病步态检测，在资源受限的可穿戴设备上实现了高精度检测，参数比基准模型少10倍，同时确定了胸部和腿部是最优传感器放置位置。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决资源受限的可穿戴设备和边缘节点上帕金森病步态检测的挑战，需要开发既准确又高效的时序分析模型，以替代传统的阈值方法，实现设备端的实时处理。

Method: 研究方法包括：1）比较幅度阈值法与三种1D CNN模型（基准可分离卷积、纯可分离CNN、残差可分离CNN）；2）使用BioStampRC21数据集，2秒窗口30Hz采样率；3）采用受试者独立的留一受试者交叉验证（LOSO）；4）分析不同传感器位置（胸部、大腿、前臂）的性能；5）在STM32级MCU上评估模型的内存和延迟性能。

Result: 研究结果显示：残差可分离模型（533个参数）达到PR-AUC = 94.5%，F1 = 91.2%，MCC = 89.4%，性能优于基准模型（5,552个参数）；最小模型（305个参数）也表现优异；阈值法召回率高但精度低；传感器位置分析表明胸部和腿部最可靠，前臂性能较差；所有模型都能在低功耗MCU上实现亚10毫秒延迟。

Conclusion: 超轻量可分离CNN为可穿戴帕金森病步态检测提供了优越的准确率-效率-泛化能力权衡，优于传统阈值方法，并确定了最优传感器放置位置，为边缘部署的定制时序模型开发提供了重要参考。

Abstract: We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.

中文标题: 边缘时序分析：用于可穿戴步态检测和最优传感器放置的微型可分离CNN

中文摘要: 我们研究了用于帕金森病（PD）步态检测的设备端时序分析，基于三轴加速度计的短时间窗口数据，针对资源受限的可穿戴设备和边缘节点。我们比较了幅度阈值法与三种1D CNN时序分析方法：一个文献基准（可分离卷积）和两个超轻量模型——一个纯可分离模型和一个带残差连接的模型。使用BioStampRC21数据集，2秒窗口30Hz采样率，在16名PD患者佩戴胸部IMU的情况下进行受试者独立的留一受试者交叉验证（LOSO），我们的残差可分离模型（模型2，533个参数）达到了PR-AUC = 94.5%，F1 = 91.2%，MCC = 89.4%，与基准模型（5,552个参数；PR-AUC = 93.7%，F1 = 90.5%，MCC = 88.5%）相当或更好，但参数减少了约10倍。最小模型（模型1，305个参数）达到PR-AUC = 94.0%，F1 = 91.0%，MCC = 89.1%。阈值法获得高召回率（89.0%）但低精度（76.5%），产生大量误报和高受试者间方差。传感器位置分析（在所有位置训练）显示胸部和腿部最可靠；前臂由于非步态手臂运动降低了精度/召回率；所有位置的简单融合并不优于最佳单位置。两种紧凑CNN在STM32级MCU上都能在严格的内存/延迟预算内执行（低功耗板上低于10毫秒），实现了传感器端的传输/存储门控。总体而言，超轻量可分离CNN为可穿戴PD步态检测提供了优于固定阈值法的准确率-效率-泛化能力权衡，并强调了为边缘部署定制时序模型的价值。

</details>


### [360] [ESPO: Entropy Importance Sampling Policy Optimization](https://arxiv.org/abs/2512.00499)
*Yuepeng Sheng,Yuwei Huang,Shuman Liu,Haibo Zhang,Anxiang Zeng*

Main category: cs.LG

TL;DR: ESPO提出了一种基于熵的重要性采样策略优化框架，通过熵驱动的分组机制解决现有方法在优化粒度与训练稳定性之间的权衡问题，显著提升数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的策略优化方法（如GRPO和GSPO）存在优化粒度与训练稳定性之间的根本权衡。GSPO虽然通过序列级优化提高了鲁棒性，但其整体处理序列的方式引入了严重低效性：保守的裁剪机制不加区分地丢弃有效训练样本（梯度未充分利用现象），且统一的信用分配无法捕捉关键推理步骤的异质性贡献。

Method: ESPO基于预测熵将序列分解为不同组别，实现两个核心机制：1）熵驱动的重要性采样，捕捉序列内异质性；2）熵自适应裁剪，基于模型不确定性动态分配信任区域。

Result: 在数学推理基准测试上的广泛实验表明，ESPO不仅加速了收敛，还实现了最先进的性能，特别是在具有挑战性的HMMT基准上，准确率从4.4%显著提升至13.13%。

Conclusion: ESPO成功调和了细粒度控制与训练稳定性之间的矛盾，通过熵驱动的分组机制有效解决了现有方法的梯度未充分利用问题，为大规模语言模型强化学习提供了更高效的优化框架。

Abstract: Large language model (LLM) reinforcement learning has increasingly relied on group-based policy optimization frameworks, such as GRPO and GSPO, to achieve stable fine-tuning at scale. However, a fundamental trade-off persists between optimization granularity and training stability. While GSPO improves robustness via sequence-level optimization, its monolithic treatment of sequences introduces severe inefficiencies: its conservative clipping mechanism indiscriminately discards valid training samples-a phenomenon we term gradient underutilization-and its uniform credit assignment fails to capture the heterogeneous contributions of critical reasoning steps. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that reconciles fine-grained control with training stability. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy-driven Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy-adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging HMMT benchmark from 4.4% to 13.13%.

中文标题: ESPO：基于熵的重要性采样策略优化

中文摘要: 大规模语言模型（LLM）强化学习越来越依赖基于群体的策略优化框架，如GRPO和GSPO，以实现大规模稳定微调。然而，优化粒度与训练稳定性之间的根本权衡仍然存在。虽然GSPO通过序列级优化提高了鲁棒性，但其整体处理序列的方式引入了严重低效性：其保守的裁剪机制不加区分地丢弃有效训练样本——我们称之为梯度未充分利用现象——且其统一的信用分配无法捕捉关键推理步骤的异质性贡献。在这项工作中，我们提出了基于熵的重要性采样策略优化（ESPO），这是一个新颖的框架，调和了细粒度控制与训练稳定性。ESPO基于预测熵将序列分解为组别，实现了（1）熵驱动的重要性采样以捕捉序列内异质性，以及（2）熵自适应裁剪以基于模型不确定性动态分配信任区域。在数学推理基准测试上的广泛实验表明，ESPO不仅加速了收敛，还实现了最先进的性能，特别是在具有挑战性的HMMT基准上，准确率从4.4%显著提升至13.13%。

</details>


### [361] [Pushing the Boundaries of Interpretability: Incremental Enhancements to the Explainable Boosting Machine](https://arxiv.org/abs/2512.00528)
*Isara Liyanage,Uthayasanker Thayasivam*

Main category: cs.LG

TL;DR: 本文通过贝叶斯超参数优化、自定义多目标公平性函数和自监督预训练三种方法增强可解释提升机(EBM)，在多个基准数据集上评估，结果显示性能指标提升有限但模型决策行为发生重要变化，强调超越单一性能分数的多维度评估价值。


<details>
  <summary>Details</summary>
Motivation: 随着复杂机器学习模型在高风险领域的广泛应用，"黑箱"问题已成为负责任AI研究的核心挑战。本文旨在通过改进可解释提升机(EBM)这一既保持高准确性又提供完全透明性的玻璃盒模型，解决模型可解释性问题。

Method: 提出三种增强方法：1) 使用贝叶斯方法进行针对性超参数优化；2) 为超参数优化实现自定义多目标公平性函数；3) 针对冷启动场景设计新颖的自监督预训练流程。所有方法在Adult Income、信用卡欺诈检测和UCI心脏病数据集上进行评估。

Result: 分析表明，虽然调优过程在主要ROC AUC指标上仅带来边际改进，但导致了模型决策行为的微妙但重要变化，这证明了超越单一性能分数的多维度评估的价值。

Conclusion: 这项工作被认为是开发不仅准确而且稳健、公平、透明的机器学习系统的关键一步，满足日益增长的监管和伦理合规要求。

Abstract: The widespread adoption of complex machine learning models in high-stakes domains has brought the "black-box" problem to the forefront of responsible AI research. This paper aims at addressing this issue by improving the Explainable Boosting Machine (EBM), a state-of-the-art glassbox model that delivers both high accuracy and complete transparency. The paper outlines three distinct enhancement methodologies: targeted hyperparameter optimization with Bayesian methods, the implementation of a custom multi-objective function for fairness for hyperparameter optimization, and a novel self-supervised pre-training pipeline for cold-start scenarios. All three methodologies are evaluated across standard benchmark datasets, including the Adult Income, Credit Card Fraud Detection, and UCI Heart Disease datasets. The analysis indicates that while the tuning process yielded marginal improvements in the primary ROC AUC metric, it led to a subtle but important shift in the model's decision-making behavior, demonstrating the value of a multi-faceted evaluation beyond a single performance score. This work is positioned as a critical step toward developing machine learning systems that are not only accurate but also robust, equitable, and transparent, meeting the growing demands of regulatory and ethical compliance.

中文标题: 推动可解释性边界：可解释提升机的增量增强

中文摘要: 复杂机器学习模型在高风险领域的广泛采用，使得"黑箱"问题成为负责任AI研究的前沿课题。本文旨在通过改进可解释提升机(EBM)来解决这一问题，EBM是一种既提供高准确性又保持完全透明性的先进玻璃盒模型。本文概述了三种不同的增强方法：使用贝叶斯方法进行针对性超参数优化、为超参数优化实现自定义多目标公平性函数，以及针对冷启动场景设计新颖的自监督预训练流程。所有三种方法均在标准基准数据集上进行评估，包括Adult Income、信用卡欺诈检测和UCI心脏病数据集。分析表明，虽然调优过程在主要ROC AUC指标上仅带来边际改进，但导致了模型决策行为的微妙但重要变化，这证明了超越单一性能分数的多维度评估的价值。这项工作被认为是开发不仅准确而且稳健、公平、透明的机器学习系统的关键一步，满足日益增长的监管和伦理合规要求。

</details>


### [362] [Algorithmic Guarantees for Distilling Supervised and Offline RL Datasets](https://arxiv.org/abs/2512.00536)
*Aaryan Gupta,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 本文提出了一种高效的数据集蒸馏算法，通过匹配随机采样回归器上的损失，在监督学习和离线强化学习中实现性能保证，仅需$\tilde{O}(d^2)$个采样回归器。


<details>
  <summary>Details</summary>
Motivation: 数据集蒸馏旨在从原始训练数据中提取关键信息生成合成数据集，使得在合成数据上训练的模型性能接近在原始数据上训练的模型。现有方法通常需要模型训练或缺乏理论保证，本文旨在开发具有严格理论保证的高效蒸馏算法。

Method: 1. 监督学习：通过匹配固定随机采样回归器集合上的MSE损失来蒸馏数据集，无需模型训练
2. 离线强化学习：扩展算法通过匹配贝尔曼损失（而非行为克隆损失），利用奖励和下一个状态信息

Result: 1. 理论保证：证明仅需$\tilde{O}(d^2)$个采样回归器即可保证合成数据集上线性模型的MSE损失接近原始数据损失
2. 下界证明：证明$Ω(d^2)$采样回归器的必要性，显示理论紧致性
3. 实验验证：在监督学习和离线强化学习任务中验证理论保证，观察到性能提升

Conclusion: 本文提出了一种高效的数据集蒸馏算法，具有严格的理论保证，适用于监督学习和离线强化学习。算法通过匹配损失函数而非模型训练，显著降低了计算复杂度，为数据集压缩和高效学习提供了新思路。

Abstract: Given a training dataset, the goal of dataset distillation is to derive a synthetic dataset such that models trained on the latter perform as well as those trained on the training dataset. In this work, we develop and analyze an efficient dataset distillation algorithm for supervised learning, specifically regression in $\mathbb{R}^d$, based on matching the losses on the training and synthetic datasets with respect to a fixed set of randomly sampled regressors without any model training. Our first key contribution is a novel performance guarantee proving that our algorithm needs only $\tilde{O}(d^2)$ sampled regressors to derive a synthetic dataset on which the MSE loss of any bounded linear model is nearly the same as its MSE loss on the given training data. In particular, the model optimized on the synthetic data has close to minimum loss on the training data, thus performing nearly as well as the model optimized on the latter. Complementing this, we also prove a matching lower bound of $Ω(d^2)$ for the number of sampled regressors showing the tightness of our analysis.
  Our second contribution is to extend our algorithm to offline RL dataset distillation by matching the Bellman loss, unlike previous works which used a behavioral cloning objective. This is the first such method which leverages both, the rewards and the next state information, available in offline RL datasets, without any policy model optimization. Our algorithm generates a synthetic dataset whose Bellman loss with respect to any linear action-value predictor is close to the latter's Bellman loss on the offline RL training dataset. Therefore, a policy associated with an action-value predictor optimized on the synthetic dataset performs nearly as well as that derived from the one optimized on the training data. We conduct experiments to validate our theoretical guarantees and observe performance gains.

中文标题: 监督学习和离线强化学习数据集蒸馏的算法保证

中文摘要: 给定一个训练数据集，数据集蒸馏的目标是生成一个合成数据集，使得在后者上训练的模型性能与在训练数据集上训练的模型相当。在这项工作中，我们为监督学习（特别是$\mathbb{R}^d$中的回归问题）开发并分析了一种高效的数据集蒸馏算法，该算法基于在固定随机采样的回归器集合上匹配训练数据集和合成数据集的损失，无需任何模型训练。我们的第一个关键贡献是一个新颖的性能保证，证明我们的算法只需要$\tilde{O}(d^2)$个采样回归器就能生成一个合成数据集，在该数据集上任何有界线性模型的MSE损失与在给定训练数据上的MSE损失几乎相同。具体来说，在合成数据上优化的模型在训练数据上的损失接近最小损失，因此性能几乎与在训练数据上优化的模型相当。作为补充，我们还证明了采样回归器数量的匹配下界$Ω(d^2)$，显示了我们的分析的紧致性。

我们的第二个贡献是将算法扩展到离线强化学习数据集蒸馏，通过匹配贝尔曼损失（与之前使用行为克隆目标的工作不同）。这是第一种利用离线强化学习数据集中可用的奖励和下一个状态信息的方法，无需任何策略模型优化。我们的算法生成一个合成数据集，其中任何线性动作值预测器的贝尔曼损失都接近其在离线强化学习训练数据集上的贝尔曼损失。因此，在合成数据集上优化的动作值预测器关联的策略性能几乎与在训练数据上优化的策略相当。我们进行了实验来验证我们的理论保证，并观察到性能提升。

</details>


### [363] [List Replicable Reinforcement Learning](https://arxiv.org/abs/2512.00553)
*Bohan Zhang,Michael Chen,A. Pavan,N. V. Vinodchandran,Lin F. Yang,Ruosong Wang*

Main category: cs.LG

TL;DR: 本文提出了列表可复现强化学习框架，通过创新规划策略和可达性测试机制，实现了多项式列表复杂度的可复现RL算法，解决了传统RL算法的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法在实践中表现出显著的不稳定性和对训练条件变化的敏感性，这严重影响了其可复现性。现有的RL算法在不同运行中可能产生完全不同的策略，导致结果难以复现和比较。本文旨在通过形式化定义列表可复现性来解决这一根本问题。

Method: 1. 在PAC RL框架中定义列表可复现性（弱形式和强形式）；2. 提出基于字典序动作选择的创新规划策略；3. 开发在随机环境中测试状态可达性同时保持可复现性的机制；4. 设计可证明高效的表格式RL算法，确保多项式列表复杂度。

Result: 1. 理论证明了提出的算法能够实现多项式列表复杂度；2. 弱列表可复现性：最终策略属于小列表；3. 强列表可复现性：整个策略执行轨迹数量多项式有界；4. 实验表明新规划策略可以增强实际RL框架的稳定性。

Conclusion: 本文首次在RL中形式化定义了列表可复现性问题，并提出了可证明高效的解决方案。通过创新的规划策略和可达性测试机制，实现了多项式复杂度的可复现RL算法，为解决RL算法的不稳定性问题提供了理论基础和实践指导。

Abstract: Replicability is a fundamental challenge in reinforcement learning (RL), as RL algorithms are empirically observed to be unstable and sensitive to variations in training conditions. To formally address this issue, we study \emph{list replicability} in the Probably Approximately Correct (PAC) RL framework, where an algorithm must return a near-optimal policy that lies in a \emph{small list} of policies across different runs, with high probability. The size of this list defines the \emph{list complexity}. We introduce both weak and strong forms of list replicability: the weak form ensures that the final learned policy belongs to a small list, while the strong form further requires that the entire sequence of executed policies remains constrained. These objectives are challenging, as existing RL algorithms exhibit exponential list complexity due to their instability. Our main theoretical contribution is a provably efficient tabular RL algorithm that guarantees list replicability by ensuring the list complexity remains polynomial in the number of states, actions, and the horizon length. We further extend our techniques to achieve strong list replicability, bounding the number of possible policy execution traces polynomially with high probability. Our theoretical result is made possible by key innovations including (i) a novel planning strategy that selects actions based on lexicographic order among near-optimal choices within a randomly chosen tolerance threshold, and (ii) a mechanism for testing state reachability in stochastic environments while preserving replicability. Finally, we demonstrate that our theoretical investigation sheds light on resolving the \emph{instability} issue of RL algorithms used in practice. In particular, we show that empirically, our new planning strategy can be incorporated into practical RL frameworks to enhance their stability.

中文标题: 列表可复现强化学习

中文摘要: 可复现性是强化学习（RL）中的一个基本挑战，因为经验观察表明RL算法不稳定且对训练条件的变化敏感。为了正式解决这个问题，我们在可能近似正确（PAC）RL框架中研究\emph{列表可复现性}，其中算法必须在不同运行中以高概率返回一个位于\emph{小列表}中的接近最优策略。这个列表的大小定义了\emph{列表复杂度}。我们引入了列表可复现性的弱形式和强形式：弱形式确保最终学习到的策略属于一个小列表，而强形式进一步要求整个执行的策略序列保持受限。这些目标具有挑战性，因为现有的RL算法由于其不稳定性而表现出指数级的列表复杂度。我们的主要理论贡献是一个可证明高效的表格式RL算法，通过确保列表复杂度在状态数、动作数和时间范围长度上保持多项式来保证列表可复现性。我们进一步扩展了我们的技术以实现强列表可复现性，以高概率将可能的策略执行轨迹数量限制在多项式范围内。我们的理论结果得益于关键创新，包括（i）一种新颖的规划策略，该策略基于在随机选择的容差阈值内接近最优选择之间的字典序来选择动作，以及（ii）一种在保持可复现性的同时测试随机环境中状态可达性的机制。最后，我们证明了我们的理论研究为解决实践中使用的RL算法的\emph{不稳定性}问题提供了启示。特别是，我们展示了在经验上，我们的新规划策略可以纳入实际的RL框架中以增强其稳定性。

</details>


### [364] [Pre-Generating Multi-Difficulty PDE Data for Few-Shot Neural PDE Solvers](https://arxiv.org/abs/2512.00564)
*Naman Choudhary,Vedant Singh,Ameet Talwalkar,Nicholas Matthew Boffi,Mikhail Khodak,Tanya Marwah*

Main category: cs.LG

TL;DR: 通过预生成多难度PDE训练数据，可以显著减少高难度问题求解所需的计算成本，实现8.9倍的效率提升。


<details>
  <summary>Details</summary>
Motivation: 神经PDE求解器的主要成本在于用经典求解器生成训练数据，而高难度问题（复杂几何、高雷诺数）对经典求解器计算成本高，但可能从神经加速中获益最大。这形成了"鸡与蛋"的挑战：需要高难度数据训练神经求解器，但生成这些数据本身就很昂贵。

Method: 在2D不可压缩Navier-Stokes方程上系统研究难度迁移，通过变化几何复杂度（障碍物数量与位置）和物理复杂度（雷诺数）创建不同难度级别的任务。预生成大量低和中等难度示例作为训练数据，然后评估这些数据对学习高难度物理的促进作用。

Result: 研究发现：1）通过包含预生成的低和中等难度示例，可以从少得多的样本中学习高难度物理；2）结合低和高难度数据，可以比仅使用高难度示例少花费8.9倍的计算来达到相同的误差水平。

Conclusion: 如何在不同难度级别之间分配经典求解器计算与分配多少计算同样重要。原则性地策划预生成的PDE数据可以为神经求解器带来实质性收益，这类似于预训练基础模型的理念。

Abstract: A key aspect of learned partial differential equation (PDE) solvers is that the main cost often comes from generating training data with classical solvers rather than learning the model itself. Another is that there are clear axes of difficulty--e.g., more complex geometries and higher Reynolds numbers--along which problems become (1) harder for classical solvers and thus (2) more likely to benefit from neural speedups. Towards addressing this chicken-and-egg challenge, we study difficulty transfer on 2D incompressible Navier-Stokes, systematically varying task complexity along geometry (number and placement of obstacles), physics (Reynolds number), and their combination. Similar to how it is possible to spend compute to pre-train foundation models and improve their performance on downstream tasks, we find that by classically solving (analogously pre-generating) many low and medium difficulty examples and including them in the training set, it is possible to learn high-difficulty physics from far fewer samples. Furthermore, we show that by combining low and high difficulty data, we can spend 8.9x less compute on pre-generating a dataset to achieve the same error as using only high difficulty examples. Our results highlight that how we allocate classical-solver compute across difficulty levels is as important as how much we allocate overall, and suggest substantial gains from principled curation of pre-generated PDE data for neural solvers. Our code is available at https://github.com/Naman-Choudhary-AI-ML/pregenerating-pde

中文标题: 为少样本神经PDE求解器预生成多难度PDE数据

中文摘要: 学习型偏微分方程（PDE）求解器的一个关键方面是，主要成本通常来自使用经典求解器生成训练数据，而不是学习模型本身。另一个方面是存在明显的难度轴——例如，更复杂的几何形状和更高的雷诺数——沿着这些轴，问题变得（1）对经典求解器更难，因此（2）更可能从神经加速中受益。为了解决这个鸡与蛋的挑战，我们研究了2D不可压缩Navier-Stokes方程上的难度迁移，系统地沿着几何（障碍物的数量和位置）、物理（雷诺数）及其组合变化任务复杂性。类似于可以花费计算资源预训练基础模型并提高其在下游任务上的性能，我们发现通过经典求解（类似预生成）许多低和中等难度的示例并将其包含在训练集中，可以从少得多的样本中学习高难度物理。此外，我们表明，通过结合低和高难度数据，我们可以花费比仅使用高难度示例少8.9倍的计算来预生成数据集，以达到相同的误差。我们的结果强调，我们如何在难度级别之间分配经典求解器计算与分配多少计算同样重要，并表明为神经求解器原则性地策划预生成的PDE数据可以带来实质性收益。我们的代码可在https://github.com/Naman-Choudhary-AI-ML/pregenerating-pde获取。

</details>


### [365] [Non-Asymptotic Convergence of Discrete Diffusion Models: Masked and Random Walk dynamics](https://arxiv.org/abs/2512.00580)
*Giovanni Conforti,Alain Durmus,Le-Tuyet-Nhi Pham*

Main category: cs.LG

TL;DR: 该论文研究了离散扩散模型在离散状态空间上的理论收敛性，针对有限空间ℤᵐᵈ和可数无限空间ℕᵈ，建立了掩码和随机游走动态下的非渐近收敛保证，模型复杂度随维度线性增长而非指数增长。


<details>
  <summary>Details</summary>
Motivation: 连续空间中的扩散模型已有充分的理论和实证研究，但离散情况由于组合结构和缺乏严格分析而面临重大挑战。本文旨在填补离散扩散模型理论分析的空白。

Method: 研究离散扩散模型在有限空间ℤᵐᵈ和可数无限空间ℕᵈ上的收敛性，重点关注前向掩码和随机游走动态。通过离散得分函数的单调性推导生成数据的误差界。

Result: 建立了离散扩散模型的非渐近收敛保证，模型复杂度随维度线性增长（最多对数因子），而非指数增长，使其能高效扩展到高维数据。这是首个不依赖估计得分有界性的收敛保证。

Conclusion: 该研究为离散扩散模型提供了坚实的理论基础，证明了其在离散状态空间上的收敛性和可扩展性，为高维离散数据的生成建模开辟了新途径。

Abstract: We investigate the theoretical underpinnings of Discrete Diffusion Models (DDMs) on discrete state spaces. Unlike in the continuous setting-where diffusion models are well understood both theoretically and empirically-the discrete case poses significant challenges due to its combinatorial structure and the lack of rigorous analysis. In this work, we establish convergence guarantees for DDMs on both the finite space $\mathbb{Z}^d_m=\{0,...,m-1\}^d$ and the countably infinite space $\mathbb{N}^d$ under mild assumptions, focusing on forward masked and random walk dynamics. Similar to the continuous case, the backward process can be characterized by a discrete score function, whose monotonicity plays a central role in deriving the error bounds of the generated data. Notably, the complexity of our model scales linearly up to logarithmic factors, rather than exponentially, with the dimension, making it efficiently scalable to high-dimensional data. To the best of our knowledge, this study provides the first non-asymptotic convergence guarantees that do not rely on the boundedness of the estimated score-covering not only uniform noising processes on $\mathbb{Z}^d_m$ and on $\mathbb{N}^d$, but also masking-based noising dynamics.

中文标题: 离散扩散模型的非渐近收敛性：掩码与随机游走动态

中文摘要: 我们研究了离散状态空间上离散扩散模型（DDMs）的理论基础。与连续设置不同——在连续设置中扩散模型在理论和实证上都得到了充分理解——离散情况由于其组合结构和缺乏严格分析而面临重大挑战。在这项工作中，我们在温和假设下建立了离散扩散模型在有限空间ℤᵐᵈ={0,...,m-1}ᵈ和可数无限空间ℕᵈ上的收敛保证，重点关注前向掩码和随机游走动态。与连续情况类似，反向过程可以通过离散得分函数来表征，其单调性在推导生成数据的误差界中起着核心作用。值得注意的是，我们模型的复杂度随维度线性增长（最多对数因子），而非指数增长，使其能够高效扩展到高维数据。据我们所知，这项研究提供了首个不依赖估计得分有界性的非渐近收敛保证——不仅涵盖了ℤᵐᵈ和ℕᵈ上的均匀噪声过程，还包括基于掩码的噪声动态。

</details>


### [366] [Statistical NLP for Optimization of Clinical Trial Success Prediction in Pharmaceutical R&D](https://arxiv.org/abs/2512.00586)
*Michael R. Doane*

Main category: cs.LG

TL;DR: 开发NLP概率分类器预测神经科学临床试验成功率，BioBERT模型表现最佳（ROC-AUC 0.74），比基准误差低40%，可优化制药研发决策。


<details>
  <summary>Details</summary>
Motivation: 神经科学领域制药研发成功率低于10%，面临高失败率和巨大成本。需要及时识别有前景的临床试验项目以优化资源分配和降低财务风险。

Method: 1. 使用ClinicalTrials.gov数据和临床试验结果数据集；2. 采用统计NLP技术提取文本特征；3. 整合到非LLM框架（逻辑回归、梯度提升、随机森林）；4. 构建基于BioBERT的LLM预测模型；5. 在101,145个临床试验的回顾性数据集上评估。

Result: 1. 非LLM模型总体ROC-AUC为0.64；2. BioBERT模型总体ROC-AUC为0.74，Brier得分为0.185；3. BioBERT预测比行业基准平均平方误差低40%；4. 70%的情况下预测优于基准值。

Conclusion: 基于BioBERT的NLP模型显著提升了临床试验成功率预测准确性，能够为神经科学药物开发提供更好的决策支持，优化投资分配和战略规划。

Abstract: This work presents the development and evaluation of an NLP-enabled probabilistic classifier designed to estimate the probability of technical and regulatory success (pTRS) for clinical trials in the field of neuroscience. While pharmaceutical R&D is plagued by high attrition rates and enormous costs, particularly within neuroscience, where success rates are below 10%, timely identification of promising programs can streamline resource allocation and reduce financial risk. Leveraging data from the ClinicalTrials.gov database and success labels from the recently developed Clinical Trial Outcome dataset, the classifier extracts text-based clinical trial features using statistical NLP techniques. These features were integrated into several non-LLM frameworks (logistic regression, gradient boosting, and random forest) to generate calibrated probability scores. Model performance was assessed on a retrospective dataset of 101,145 completed clinical trials spanning 1976-2024, achieving an overall ROC-AUC of 0.64. An LLM-based predictive model was then built using BioBERT, a domain-specific language representation encoder. The BioBERT-based model achieved an overall ROC-AUC of 0.74 and a Brier Score of 0.185, indicating its predictions had, on average, 40% less squared error than would be observed using industry benchmarks. The BioBERT-based model also made trial outcome predictions that were superior to benchmark values 70% of the time overall. By integrating NLP-driven insights into drug development decision-making, this work aims to enhance strategic planning and optimize investment allocation in neuroscience programs.

中文标题: 基于统计NLP的制药研发临床试验成功率预测优化

中文摘要: 本研究开发并评估了一种基于自然语言处理的概率分类器，用于估计神经科学领域临床试验的技术和监管成功率(pTRS)。尽管制药研发面临高失败率和巨大成本，特别是在神经科学领域成功率低于10%，但及时识别有前景的项目可以优化资源分配并降低财务风险。该分类器利用ClinicalTrials.gov数据库的数据和最近开发的临床试验结果数据集的成功标签，采用统计NLP技术提取基于文本的临床试验特征。这些特征被整合到多个非LLM框架（逻辑回归、梯度提升和随机森林）中，生成校准的概率分数。模型性能在1976-2024年间完成的101,145个临床试验的回顾性数据集上进行评估，总体ROC-AUC达到0.64。随后使用领域特定的语言表示编码器BioBERT构建了基于LLM的预测模型。基于BioBERT的模型实现了总体ROC-AUC为0.74，Brier得分为0.185，表明其预测的平均平方误差比行业基准低40%。基于BioBERT的模型在70%的情况下做出的试验结果预测优于基准值。通过将NLP驱动的洞察整合到药物开发决策中，本研究旨在增强神经科学项目的战略规划并优化投资分配。

</details>


### [367] [Developing Fairness-Aware Task Decomposition to Improve Equity in Post-Spinal Fusion Complication Prediction](https://arxiv.org/abs/2512.00598)
*Yining Yuan,J. Ben Tamo,Wenqi Shi,Yishan Zhong,Micky C. Nnamdi,B. Randall Brenn,Steven W. Hwang,May D. Wang*

Main category: cs.LG

TL;DR: FAIR-MTL是一个公平感知的多任务学习框架，通过数据驱动的子组发现和任务分解，在脊柱融合术后并发症预测中实现更好的公平性和性能。


<details>
  <summary>Details</summary>
Motivation: 临床预测模型中的公平性问题在高风险医疗应用中尤为突出，现有方法依赖粗粒度的人口统计调整或事后修正，无法捕捉临床人群的潜在结构，可能无意中强化偏见。

Method: FAIR-MTL采用数据驱动的子组推断机制，通过人口统计嵌入和k-means聚类发现潜在患者子组，构建多任务学习框架，使用逆频率加权缓解子组不平衡，正则化防止过拟合。

Result: 在术后并发症预测中，FAIR-MTL达到AUC 0.86和准确率75%，显著减少偏见：性别的人口统计奇偶差异降至0.055，均衡几率降至0.094；年龄分别降至0.056和0.148。

Conclusion: 将无监督子组发现纳入多任务框架能够为手术风险分层提供更公平、可解释和临床可操作的预测，解决了传统公平性方法的局限性。

Abstract: Fairness in clinical prediction models remains a persistent challenge, particularly in high-stakes applications such as spinal fusion surgery for scoliosis, where patient outcomes exhibit substantial heterogeneity. Many existing fairness approaches rely on coarse demographic adjustments or post-hoc corrections, which fail to capture the latent structure of clinical populations and may unintentionally reinforce bias. We propose FAIR-MTL, a fairness-aware multitask learning framework designed to provide equitable and fine-grained prediction of postoperative complication severity.
  Instead of relying on explicit sensitive attributes during model training, FAIR-MTL employs a data-driven subgroup inference mechanism. We extract a compact demographic embedding, and apply k-means clustering to uncover latent patient subgroups that may be differentially affected by traditional models. These inferred subgroup labels determine task routing within a shared multitask architecture. During training, subgroup imbalance is mitigated through inverse-frequency weighting, and regularization prevents overfitting to smaller groups.
  Applied to postoperative complication prediction with four severity levels, FAIR-MTL achieves an AUC of 0.86 and an accuracy of 75%, outperforming single-task baselines while substantially reducing bias. For gender, the demographic parity difference decreases to 0.055 and equalized odds to 0.094; for age, these values reduce to 0.056 and 0.148, respectively. Model interpretability is ensured through SHAP and Gini importance analyses, which consistently highlight clinically meaningful predictors such as hemoglobin, hematocrit, and patient weight. Our findings show that incorporating unsupervised subgroup discovery into a multitask framework enables more equitable, interpretable, and clinically actionable predictions for surgical risk stratification.

中文标题: 开发公平感知的任务分解方法以改善脊柱融合术后并发症预测的公平性

中文摘要: 临床预测模型中的公平性仍然是一个持续的挑战，特别是在高风险应用中，如脊柱侧弯的脊柱融合手术，其中患者结果表现出显著的异质性。许多现有的公平性方法依赖于粗略的人口统计调整或事后修正，这些方法未能捕捉临床人群的潜在结构，并可能无意中强化偏见。我们提出了FAIR-MTL，一个公平感知的多任务学习框架，旨在提供术后并发症严重程度的公平和细粒度预测。

FAIR-MTL不依赖模型训练期间的显式敏感属性，而是采用数据驱动的子组推断机制。我们提取紧凑的人口统计嵌入，并应用k-means聚类来揭示可能受到传统模型不同影响的潜在患者子组。这些推断的子组标签决定了共享多任务架构内的任务路由。在训练期间，通过逆频率加权缓解子组不平衡，正则化防止对小群体的过拟合。

应用于具有四个严重程度的术后并发症预测，FAIR-MTL实现了0.86的AUC和75%的准确率，优于单任务基线，同时显著减少了偏见。对于性别，人口统计奇偶差异降至0.055，均衡几率降至0.094；对于年龄，这些值分别降至0.056和0.148。通过SHAP和基尼重要性分析确保模型可解释性，这些分析一致地突出了具有临床意义的预测因子，如血红蛋白、血细胞比容和患者体重。我们的研究结果表明，将无监督子组发现纳入多任务框架能够为手术风险分层提供更公平、可解释和临床可操作的预测。

</details>


### [368] [Efficient Matroid Bandit Linear Optimization Leveraging Unimodality](https://arxiv.org/abs/2512.00605)
*Aurélien Delage,Romaric Gaudel*

Main category: cs.LG

TL;DR: 该论文提出了一种利用单峰结构优化拟阵半赌博机问题的方法，显著减少了成员资格查询的次数，在保持最优遗憾的同时大幅降低了时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有拟阵半赌博机算法虽然达到了最优遗憾界，但时间复杂度过高，特别是在大规模拟阵或成员资格查询成本较高（如在线推荐系统）的场景下，这限制了实际应用。

Method: 通过利用拟阵半赌博机问题的底层单峰结构，将涉及成员资格查询的迭代次数限制在O(log log T)级别，从而显著减少计算开销。

Result: 实验表明：(1) 与最先进方法相比，没有遗憾损失；(2) 时间复杂度和成员资格查询调用次数显著减少。

Conclusion: 通过利用单峰结构，可以在几乎不损失遗憾的情况下大幅降低拟阵半赌博机问题的时间复杂度，为大规模实际应用提供了可行的解决方案。

Abstract: We study the combinatorial semi-bandit problem under matroid constraints. The regret achieved by recent approaches is optimal, in the sense that it matches the lower bound. Yet, time complexity remains an issue for large matroids or for matroids with costly membership oracles (e.g. online recommendation that ensures diversity). This paper sheds a new light on the matroid semi-bandit problem by exploiting its underlying unimodal structure. We demonstrate that, with negligible loss in regret, the number of iterations involving the membership oracle can be limited to \mathcal{O}(\log \log T)$. This results in an overall improved time complexity of the learning process. Experiments conducted on various matroid benchmarks show (i) no loss in regret compared to state-of-the-art approaches; and (ii) reduced time complexity and number of calls to the membership oracle.

中文标题: 利用单峰性实现高效的拟阵赌博机线性优化

中文摘要: 我们研究了拟阵约束下的组合半赌博机问题。最近方法实现的遗憾是最优的，因为它匹配了下界。然而，对于大型拟阵或具有昂贵成员资格查询的拟阵（例如确保多样性的在线推荐），时间复杂度仍然是一个问题。本文通过利用其底层单峰结构，为拟阵半赌博机问题提供了新的视角。我们证明，在可忽略的遗憾损失下，涉及成员资格查询的迭代次数可以限制在O(log log T)。这导致学习过程的整体时间复杂度得到改善。在各种拟阵基准上进行的实验显示：(i) 与最先进方法相比没有遗憾损失；(ii) 时间复杂度和成员资格查询调用次数减少。

</details>


### [369] [Financial Text Classification Based On rLoRA Finetuning On Qwen3-8B model](https://arxiv.org/abs/2512.00630)
*Zhiming Lian*

Main category: cs.LG

TL;DR: 本文研究了基于Qwen3-8B大语言模型的金融文本分类方法，采用噪声嵌入指令微调和秩稳定低秩适应优化技术，在金融情感分析和新闻分类任务上超越了多种基线模型，展示了高效、经济的金融NLP应用潜力。


<details>
  <summary>Details</summary>
Motivation: 金融文本分类在量化交易系统中越来越重要，但现有模型在金融领域的适应性和效率存在不足。需要探索更适合金融应用的大语言模型优化方法，以提高分类准确率和训练效率。

Method: 采用Qwen3-8B大语言模型，结合噪声嵌入指令微调技术提高模型鲁棒性，使用秩稳定低秩适应优化方法和FlashAttention降低计算资源需求，在金融情感分析和新闻分类任务上进行实验。

Result: Qwen3-8B在金融文本分类任务上显著优于T5、BERT、RoBERTa等经典Transformer模型，以及LLaMA1-7B、LLaMA2-7B、Baichuan2-7B等大模型，获得更高的分类准确率且需要更少的训练轮次。

Conclusion: Qwen3-8B结合指令微调和内存高效优化方法，为实时金融NLP应用提供了可扩展、经济高效的解决方案，有望推动动态量化交易系统的发展。

Abstract: Financial text classification has increasingly become an important aspect in quantitative trading systems and related tasks, such as financial sentiment analysis and the classification of financial news. In this paper, we assess the performance of the large language model Qwen3-8B on both tasks. Qwen3-8B is a state-of-the-art model that exhibits strong instruction-following and multilingual capabilities, and is distinct from standard models, primarily because it is specifically optimized for efficient fine tuning and high performance on reasoning-based benchmarks, making it suitable for financial applications. To adapt this model, we apply Noisy Embedding Instruction Finetuning and based on our previous work, this method increases robustness by injecting controlled noise into the embedding layers during supervised adaptation. We improve efficiency further with Rank-stabilized Low-Rank Adaptation low-rank optimization approach, and FlashAttention, which allow for faster training with lower GPU memory. For both tasks, we benchmark Qwen3-8B against standard classical transformer models, such as T5, BERT, and RoBERTa, and large models at scale, such as LLaMA1-7B, LLaMA2-7B, and Baichuan2-7B. The findings reveal that Qwen3-8B consistently surpasses these baselines by obtaining better classification accuracy and needing fewer training epochs. The synergy of instruction-based fine-tuning and memory-efficient optimization methods suggests Qwen3-8B can potentially serve as a scalable, economical option for real-time financial NLP applications. Qwen3-8B provides a very promising base for advancing dynamic quantitative trading systems in the future.

中文标题: 基于Qwen3-8B模型的rLoRA微调金融文本分类研究

中文摘要: 金融文本分类在量化交易系统及相关任务中日益重要，如金融情感分析和金融新闻分类。本文评估了大语言模型Qwen3-8B在这两项任务上的性能。Qwen3-8B是一个最先进的模型，具有强大的指令跟随和多语言能力，与标准模型不同，它专门针对高效微调和基于推理的基准测试进行了优化，适合金融应用。为适应该模型，我们应用了噪声嵌入指令微调方法，基于我们之前的工作，该方法通过在监督适应期间向嵌入层注入受控噪声来提高鲁棒性。我们进一步采用秩稳定低秩适应优化方法和FlashAttention来提高效率，实现更快的训练和更低的GPU内存占用。在这两项任务中，我们将Qwen3-8B与标准经典Transformer模型（如T5、BERT和RoBERTa）以及大规模模型（如LLaMA1-7B、LLaMA2-7B和Baichuan2-7B）进行了基准测试。结果表明，Qwen3-8B始终优于这些基线模型，获得了更好的分类准确率且需要更少的训练轮次。基于指令的微调和内存高效优化方法的协同作用表明，Qwen3-8B有潜力成为实时金融NLP应用的可扩展、经济高效的选择。Qwen3-8B为未来推进动态量化交易系统提供了非常有前景的基础。

</details>


### [370] [Privacy Preserving Diffusion Models for Mixed-Type Tabular Data Generation](https://arxiv.org/abs/2512.00638)
*Timur Sattarov,Marco Schreyer,Damian Borth*

Main category: cs.LG

TL;DR: DP-FinDiff是一种用于混合类型表格数据生成的差分隐私扩散模型，通过嵌入表示、自适应时间步采样和特征聚合损失来提高数据效用，在金融和医疗数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在金融、医疗等敏感领域，需要合成高质量的表格数据用于分析和共享，同时必须保护原始数据的隐私。现有方法在处理混合类型（数值和分类）表格数据时存在编码复杂、隐私保护效果与数据质量难以平衡的问题。

Method: 1. 采用基于嵌入的表示方法处理分类特征，减少编码开销；2. 提出自适应时间步采样器，使训练更新与扩散动态对齐；3. 设计特征聚合损失函数，减轻差分隐私训练中梯度裁剪引起的偏差。

Result: 在金融和医疗数据集上，DP-FinDiff在可比隐私水平下实现了比差分隐私基线高16-42%的效用提升，同时保持了相同的隐私保证水平。

Conclusion: DP-FinDiff框架为混合类型表格数据的隐私保护合成提供了一种有效的解决方案，在保持差分隐私保证的同时显著提高了生成数据的质量，有望促进敏感领域的安全数据共享。

Abstract: We introduce DP-FinDiff, a differentially private diffusion framework for synthesizing mixed-type tabular data. DP-FinDiff employs embedding-based representations for categorical features, reducing encoding overhead and scaling to high-dimensional datasets. To adapt DP-training to the diffusion process, we propose two privacy-aware training strategies: an adaptive timestep sampler that aligns updates with diffusion dynamics, and a feature-aggregated loss that mitigates clipping-induced bias. Together, these enhancements improve fidelity and downstream utility without weakening privacy guarantees. On financial and medical datasets, DP-FinDiff achieves 16-42% higher utility than DP baselines at comparable privacy levels, demonstrating its promise for safe and effective data sharing in sensitive domains.

中文标题: 面向混合类型表格数据的隐私保护扩散模型

中文摘要: 我们提出了DP-FinDiff，一种用于合成混合类型表格数据的差分隐私扩散框架。DP-FinDiff采用基于嵌入的表示方法来处理分类特征，减少了编码开销并能够扩展到高维数据集。为了将差分隐私训练适应扩散过程，我们提出了两种隐私感知的训练策略：一种与扩散动态对齐的自适应时间步采样器，以及一种减轻裁剪诱导偏差的特征聚合损失。这些增强功能共同提高了保真度和下游效用，同时不削弱隐私保证。在金融和医疗数据集上，DP-FinDiff在可比隐私水平下实现了比差分隐私基线高16-42%的效用，展示了其在敏感领域安全有效数据共享方面的潜力。

</details>


### [371] [Flow Matching for Tabular Data Synthesis](https://arxiv.org/abs/2512.00698)
*Bahrul Ilmi Nasution,Floor Eijkelboom,Mark Elliot,Richard Allmendinger,Christian A. Naesseth*

Main category: cs.LG

TL;DR: 本文提出TabbyFlow方法，将流匹配技术应用于表格数据合成，相比扩散模型在性能、计算效率和隐私保护方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 表格数据合成是隐私保护数据共享的重要工具，现有扩散模型虽取得进展但计算成本高。流匹配技术作为有前景的替代方案，需要系统研究其在表格数据合成中的应用效果。

Method: 提出了多种流匹配实现方式，包括标准流匹配和变分流匹配，比较了最优传输和方差保持两种概率路径，并评估了确定性和随机性采样器。与TabDDPM和TabSyn等扩散方法进行对比。

Result: 流匹配方法（特别是TabbyFlow）在性能上超越扩散基线，仅需≤100步函数评估即可获得优异结果，计算优势显著。最优传输路径表现最佳，方差保持路径在降低披露风险方面有潜力。随机性流匹配能保持边缘分布并生成高效用、低风险的合成数据。

Conclusion: 流匹配是表格数据合成的有效方法，在性能、效率和隐私保护方面均优于扩散模型。最优传输路径提供最佳性能，而随机性采样器在效用与风险平衡方面表现出色。

Abstract: Synthetic data generation is an important tool for privacy-preserving data sharing. While diffusion models have set recent benchmarks, flow matching (FM) offers a promising alternative. This paper presents different ways to implement flow matching for tabular data synthesis. We provide a comprehensive empirical study that compares flow matching (FM and variational FM) with a state-of-the-art diffusion method (TabDDPM and TabSyn) in tabular data synthesis. We evaluate both the standard Optimal Transport (OT) and the Variance Preserving (VP) probability paths, and also compare deterministic and stochastic samplers -- something possible when learning to generate using \textit{variational} flow matching -- characterising the empirical relationship between data utility and privacy risk. Our key findings reveal that flow matching, particularly TabbyFlow, outperforms diffusion baselines. Flow matching methods also achieves better performance with remarkably low function evaluations ($\leq$ 100 steps), offering a substantial computational advantage. The choice of probability path is also crucial, as using the OT path demonstrates superior performance, while VP has potential for producing synthetic data with lower disclosure risk. Lastly, our results show that making flows stochastic not only preserves marginal distributions but, in some instances, enables the generation of high utility synthetic data with reduced disclosure risk.

中文标题: 表格数据合成的流匹配方法

中文摘要: 合成数据生成是隐私保护数据共享的重要工具。虽然扩散模型最近设定了基准，但流匹配提供了一个有前景的替代方案。本文提出了实现表格数据合成流匹配的不同方法。我们进行了全面的实证研究，将流匹配（FM和变分FM）与最先进的扩散方法（TabDDPM和TabSyn）在表格数据合成方面进行比较。我们评估了标准最优传输和方差保持两种概率路径，并比较了确定性和随机性采样器——这在学习使用变分流匹配生成时是可能的——描述了数据效用和隐私风险之间的经验关系。我们的关键发现表明，流匹配（特别是TabbyFlow）优于扩散基线。流匹配方法在极低的函数评估次数（≤100步）下也能实现更好的性能，提供了显著的计算优势。概率路径的选择也至关重要，使用最优传输路径表现出卓越性能，而方差保持路径在产生低披露风险的合成数据方面具有潜力。最后，我们的结果表明，使流具有随机性不仅能保持边缘分布，在某些情况下还能生成高效用、低披露风险的合成数据。

</details>


### [372] [Towards Precision Protein-Ligand Affinity Prediction Benchmark: A Complete and Modification-Aware DAVIS Dataset](https://arxiv.org/abs/2512.00708)
*Ming-Hsiu Wu,Ziqian Xie,Shuiwang Ji,Degui Zhi*

Main category: cs.LG

TL;DR: 研究者创建了一个包含4,032个激酶-配体对的完整且修饰感知的DAVIS数据集版本，用于在生物学现实条件下评估蛋白质-配体亲和力预测模型，并提出了三个基准测试来评估模型对蛋白质修饰的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在蛋白质-配体结合亲和力预测任务中存在过度拟合问题，主要原因是现有数据集过于简化，未能包含自然发生且具有生物学相关性的蛋白质修饰，这限制了模型在真实生物条件下的应用。

Method: 通过整合涉及替代、插入、缺失和磷酸化事件的4,032个激酶-配体对，构建了一个完整且修饰感知的DAVIS数据集。基于此数据集设计了三个基准设置：增强数据集预测、野生型到修饰泛化、少样本修饰泛化，用于评估不同预测方法在蛋白质修饰条件下的表现。

Result: 基于对接的模型在零样本设置中表现出更好的泛化能力，而无对接模型倾向于过度拟合野生型蛋白质，难以处理未见过的修饰。然而，当在少量修饰示例上进行微调时，无对接模型显示出显著改进。

Conclusion: 该研究提供了一个有价值的基准数据集和评估框架，有助于开发能更好泛化到蛋白质修饰的预测模型，推动药物发现中的精准医学发展。

Abstract: Advancements in AI for science unlocks capabilities for critical drug discovery tasks such as protein-ligand binding affinity prediction. However, current models overfit to existing oversimplified datasets that does not represent naturally occurring and biologically relevant proteins with modifications. In this work, we curate a complete and modification-aware version of the widely used DAVIS dataset by incorporating 4,032 kinase-ligand pairs involving substitutions, insertions, deletions, and phosphorylation events. This enriched dataset enables benchmarking of predictive models under biologically realistic conditions. Based on this new dataset, we propose three benchmark settings-Augmented Dataset Prediction, Wild-Type to Modification Generalization, and Few-Shot Modification Generalization-designed to assess model robustness in the presence of protein modifications. Through extensive evaluation of both docking-free and docking-based methods, we find that docking-based model generalize better in zero-shot settings. In contrast, docking-free models tend to overfit to wild-type proteins and struggle with unseen modifications but show notable improvement when fine-tuned on a small set of modified examples. We anticipate that the curated dataset and benchmarks offer a valuable foundation for developing models that better generalize to protein modifications, ultimately advancing precision medicine in drug discovery. The benchmark is available at: https://github.com/ZhiGroup/DAVIS-complete

中文标题: 迈向精准蛋白质-配体亲和力预测基准：一个完整且修饰感知的DAVIS数据集

中文摘要: 人工智能在科学领域的进展解锁了关键药物发现任务的能力，如蛋白质-配体结合亲和力预测。然而，当前模型过度拟合于现有的过于简化的数据集，这些数据集不能代表自然发生且具有生物学相关性的修饰蛋白质。在这项工作中，我们通过整合涉及替代、插入、缺失和磷酸化事件的4,032个激酶-配体对，策划了一个完整且修饰感知的广泛使用的DAVIS数据集版本。这个丰富的数据集能够在生物学现实条件下对预测模型进行基准测试。基于这个新数据集，我们提出了三个基准设置——增强数据集预测、野生型到修饰泛化以及少样本修饰泛化——旨在评估模型在存在蛋白质修饰情况下的鲁棒性。通过对无对接和基于对接方法的广泛评估，我们发现基于对接的模型在零样本设置中泛化能力更好。相比之下，无对接模型倾向于过度拟合野生型蛋白质，难以处理未见过的修饰，但在少量修饰示例上进行微调时显示出显著改进。我们预计，策划的数据集和基准为开发能更好泛化到蛋白质修饰的模型提供了宝贵基础，最终推动药物发现中的精准医学发展。基准可在以下网址获取：https://github.com/ZhiGroup/DAVIS-complete

</details>


### [373] [Deep Learning for Modeling and Dispatching Hybrid Wind Farm Power Generation](https://arxiv.org/abs/2512.00728)
*Zach Lawrence,Jessica Yao,Chris Qin*

Main category: cs.LG

TL;DR: 本文开发了两个深度学习框架：COVE-NN（基于LSTM的调度策略）和功率生成建模框架，用于提升混合风电场的调度效率和功率预测精度。


<details>
  <summary>Details</summary>
Motivation: 混合风电场（集成储能系统）需要更智能的调度策略来最大化风能价值。现有方法在应对局部电网需求和市场条件变化方面存在不足，需要数据驱动的解决方案来提高调度鲁棒性。

Method: 1. COVE-NN：基于LSTM的调度策略，针对单个风电场设计，使用局部电网需求和市场条件作为输入参数；2. 功率生成建模框架：基于大气条件合成功率生成数据，提高数据驱动调度策略的鲁棒性。

Result: 1. COVE-NN在Pyron站点的43年模拟运行中将年度COVE降低了32.3%；2. 功率生成建模框架在Palouse风电场验证中，RMSE降低了9.5%，功率曲线相似性提高了18.9%。

Conclusion: 这两个模型为更鲁棒的数据驱动调度策略铺平了道路，并可扩展到其他可再生能源系统，显著提升了混合风电场的运营效率和经济效益。

Abstract: Wind farms with integrated energy storage, or hybrid wind farms, are able to store energy and dispatch it to the grid following an operational strategy. For individual wind farms with integrated energy storage capacity, data-driven dispatch strategies using localized grid demand and market conditions as input parameters stand to maximize wind energy value. Synthetic power generation data modeled on atmospheric conditions provide another avenue for improving the robustness of data-driven dispatch strategies. To these ends, the present work develops two deep learning frameworks: COVE-NN, an LSTM-based dispatch strategy tailored to individual wind farms, which reduced annual COVE by 32.3% over 43 years of simulated operations in a case study at the Pyron site; and a power generation modeling framework that reduced RMSE by 9.5% and improved power curve similarity by 18.9% when validated on the Palouse wind farm. Together, these models pave the way for more robust, data-driven dispatch strategies and potential extensions to other renewable energy systems.

中文标题: 深度学习在混合风电场功率生成建模与调度中的应用

中文摘要: 集成储能的混合风电场能够存储能量并根据运营策略向电网调度。对于具有集成储能容量的单个风电场，使用局部电网需求和市场条件作为输入参数的数据驱动调度策略有望最大化风能价值。基于大气条件建模的合成功率生成数据为提升数据驱动调度策略的鲁棒性提供了另一途径。为此，本研究开发了两个深度学习框架：COVE-NN，一种基于LSTM的调度策略，针对单个风电场定制，在Pyron站点的案例研究中，43年模拟运行中将年度COVE降低了32.3%；以及一个功率生成建模框架，在Palouse风电场验证中，RMSE降低了9.5%，功率曲线相似性提高了18.9%。这些模型共同为更鲁棒的数据驱动调度策略铺平了道路，并可扩展到其他可再生能源系统。

</details>


### [374] [REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories](https://arxiv.org/abs/2512.00736)
*Jacob Thompson,Emiliano Garcia-Lopez,Yonatan Bisk*

Main category: cs.LG

TL;DR: REM是一个评估多模态大语言模型具身空间推理能力的基准测试，发现现有模型在物体恒存性、空间关系和数量追踪等基本空间推理任务上表现有限，尤其是在中等复杂度水平上。


<details>
  <summary>Details</summary>
Motivation: 人类通过导航建立视角独立的认知地图，能够进行直觉性的空间推理，但多模态大语言模型尽管经过大量视频训练，却缺乏这种基本的空间推理能力，这对具身应用构成了关键限制。

Method: 开发了REM基准测试，使用可控3D环境创建长时程具身空间推理任务，系统评估物体恒存性/区分、空间关系和动态视角下的数量追踪等关键方面。

Result: 当前最佳模型展现出有希望的整体性能，但在中等复杂度水平上变得不可靠，而这些任务对人类来说很容易处理，突显了模型从序列视觉输入中发展稳健空间表示的挑战。

Conclusion: 多模态大语言模型缺乏人类般的空间推理能力，REM基准提供了有针对性的指标和诊断工具，以促进未来模型在具身空间理解方面的改进。

Abstract: Humans build viewpoint-independent cognitive maps through navigation, enabling intuitive reasoning about object permanence and spatial relations. We argue that multimodal large language models (MLLMs), despite extensive video training, lack this fundamental spatial reasoning capability, a critical limitation for embodied applications. To demonstrate these limitations and drive research, we introduce REM (Reasoning over Embodied Multi-Frame Trajectories), a benchmark using controllable 3D environments for long-horizon embodied spatial reasoning. REM systematically evaluates key aspects like object permanence/distinction, spatial relationships, and numerical tracking across dynamic embodied viewpoints. Our evaluation shows that the best-performing current models exhibit promising overall performance, but become increasingly unreliable at even moderate complexity levels easily handled by humans. These findings highlight challenges MLLMs face in developing robust spatial representations from sequential visual input. Consequently, REM provides targeted metrics and diagnostics to foster improved spatial understanding in future models.

中文标题: REM：通过多帧轨迹评估LLM具身空间推理能力

中文摘要: 人类通过导航建立视角独立的认知地图，从而能够直观地推理物体恒存性和空间关系。我们认为，尽管多模态大语言模型（MLLMs）经过大量视频训练，但缺乏这种基本的空间推理能力，这对具身应用来说是一个关键限制。为了展示这些限制并推动研究，我们引入了REM（具身多帧轨迹推理），这是一个使用可控3D环境进行长时程具身空间推理的基准测试。REM系统地评估了关键方面，如物体恒存性/区分、空间关系和动态具身视角下的数量追踪。我们的评估显示，当前表现最佳的模型展现出有希望的整体性能，但在中等复杂程度上变得日益不可靠，而这些复杂程度对人类来说很容易处理。这些发现突显了MLLMs从序列视觉输入中发展稳健空间表示所面临的挑战。因此，REM提供了有针对性的指标和诊断工具，以促进未来模型的空间理解能力提升。

</details>


### [375] [Provable Benefit of Sign Descent: A Minimal Model Under Heavy-Tailed Class Imbalance](https://arxiv.org/abs/2512.00763)
*Robin Yadav,Shuo Xie,Tianhao Wang,Zhiyuan Li*

Main category: cs.LG

TL;DR: 该论文证明了在重尾类别不平衡的数据分布下，符号下降（ℓ∞范数下降）比归一化梯度下降（ℓ2范数下降）具有更快的收敛速度，为自适应优化方法在语言建模中的优势提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 自适应优化方法（如Adam）在LLM预训练中表现出色，但现有理论解释主要基于损失函数的平滑性假设，这些假设在语言建模任务中难以验证。本文希望直接从数据分布特性（重尾类别不平衡）来解释符号下降的优势。

Method: 提出了一个最小但具有代表性的下一个token预测模型设置，在该设置中可证明地分析坐标级算法（符号下降）与归一化梯度下降的收敛性能差异。通过理论分析证明在重尾类别不平衡条件下，符号下降具有更快的收敛速度。

Result: 理论证明显示，在存在重尾类别不平衡的情况下，符号下降（ℓ∞范数下降）比归一化梯度下降（ℓ2范数下降）具有更快的收敛速度，为自适应优化方法在语言建模中的实际优势提供了理论依据。

Conclusion: 该研究从数据分布特性（重尾类别不平衡）而非损失函数平滑性假设的角度，为符号下降在语言建模任务中的优势提供了可证明的理论解释，填补了现有理论框架与实际应用之间的差距。

Abstract: Adaptive optimization methods (such as Adam) play a major role in LLM pretraining, significantly outperforming Gradient Descent (GD). Recent studies have proposed new smoothness assumptions on the loss function to explain the advantages of adaptive algorithms with structured preconditioners, e.g., coordinate-wise or layer-wise, and steepest descent methods w.r.t. non-euclidean norms, e.g., $\ell_\infty$ norm or spectral norm, over GD. However, it remains unclear how these smoothness assumptions manifest in language modelling tasks. In this work, we aim to analyze the benefit of $\ell_\infty$-norm descent (a.k.a. sign descent) directly from properties of the data distribution, namely, heavy-tailed class imbalance. We propose a minimal yet representative setting of next-token prediction, where we can provably show faster convergence of coordinate-wise algorithms such as Sign descent (steepest descent w.r.t. $\ell_\infty$ norm) over normalized GD (steepest descent w.r.t. to $\ell_2$ norm) in the presence of heavy tail class imbalance.

中文标题: 符号下降的可证明优势：重尾类别不平衡下的最小模型

中文摘要: 自适应优化方法（如Adam）在LLM预训练中发挥着重要作用，显著优于梯度下降（GD）。最近的研究提出了损失函数上的新平滑性假设，以解释具有结构化预条件子的自适应算法（例如坐标级或层级）以及相对于非欧几里得范数（例如ℓ∞范数或谱范数）的最速下降方法相对于GD的优势。然而，这些平滑性假设如何在语言建模任务中体现仍不清楚。在这项工作中，我们旨在直接从数据分布的特性（即重尾类别不平衡）分析ℓ∞范数下降（又称符号下降）的优势。我们提出了一个最小但具有代表性的下一个token预测设置，在该设置中，我们可以可证明地展示在存在重尾类别不平衡的情况下，坐标级算法（如符号下降，相对于ℓ∞范数的最速下降）相对于归一化GD（相对于ℓ2范数的最速下降）具有更快的收敛速度。

</details>


### [376] [Text Mining Analysis of Symptom Patterns in Medical Chatbot Conversations](https://arxiv.org/abs/2512.00768)
*Hamed Razavi*

Main category: cs.LG

TL;DR: 该研究使用多种NLP方法分析医疗聊天机器人对话中的症状模式，发现对话数据可作为有价值的诊断信号，有助于早期症状解释和决策支持。


<details>
  <summary>Details</summary>
Motivation: 随着数字健康系统的快速发展，需要更好地理解这些系统如何解释和表示患者报告的症状。医疗聊天机器人被用于提供临床支持和改善用户体验，但需要系统分析这些对话中出现的症状模式。

Method: 研究采用多方法NLP方法：使用LDA识别潜在症状主题，K-Means聚类相似症状描述，基于Transformer的NER提取医学概念，以及Apriori算法发现频繁症状对。使用包含960个多轮对话、涵盖24种临床状况的医疗对话数据集。

Result: 分析发现：临床相关主题具有连贯结构，聚类凝聚力处于中等水平，症状对（如发热-头痛、皮疹-瘙痒）之间存在高置信度的关联关系。这些发现表明对话医疗数据可作为有价值的诊断信号。

Conclusion: 该研究证明可以将非结构化的自由对话转化为可操作的症状知识，为增强医疗聊天机器人的性能、可靠性和临床实用性提供了可扩展的框架。

Abstract: The fast growth of digital health systems has led to a need to better comprehend how they interpret and represent patient-reported symptoms. Chatbots have been used in healthcare to provide clinical support and enhance the user experience, making it possible to provide meaningful clinical patterns from text-based data through chatbots. The proposed research utilises several different natural language processing methods to study the occurrences of symptom descriptions in medicine as well as analyse the patterns that emerge through these conversations within medical bots. Through the use of the Medical Conversations to Disease Dataset which contains 960 multi-turn dialogues divided into 24 Clinical Conditions, a standardised representation of conversations between patient and bot is created for further analysis by computational means. The multi-method approach uses a variety of tools, including Latent Dirichlet Allocation (LDA) to identify latent symptom themes, K-Means to group symptom descriptions by similarity, Transformer-based Named Entity Recognition (NER) to extract medical concepts, and the Apriori algorithm to discover frequent symptom pairs. Findings from the analysis indicate a coherent structure of clinically relevant topics, moderate levels of clustering cohesiveness and several high confidence rates on the relationships between symptoms like fever headache and rash itchiness. The results support the notion that conversational medical data can be a valuable diagnostic signal for early symptom interpretation, assist in strengthening decision support and improve how users interact with tele-health technology. By demonstrating a method for converting unstructured free-flowing dialogue into actionable knowledge regarding symptoms this work provides an extensible framework to further enhance future performance, dependability and clinical utility of selecting medical chatbots.

中文标题: 医疗聊天机器人对话中症状模式的文本挖掘分析

中文摘要: 数字健康系统的快速发展导致需要更好地理解它们如何解释和表示患者报告的症状。聊天机器人已在医疗保健领域用于提供临床支持和增强用户体验，使得通过聊天机器人从文本数据中提取有意义的临床模式成为可能。本研究利用多种自然语言处理方法研究医学中症状描述的出现情况，并分析医疗机器人对话中出现的模式。通过使用包含960个多轮对话、分为24种临床状况的医疗对话数据集，创建了患者与机器人对话的标准化表示，以便进行进一步的计算分析。多方法途径使用了多种工具，包括潜在狄利克雷分配（LDA）识别潜在症状主题、K-Means聚类相似症状描述、基于Transformer的命名实体识别（NER）提取医学概念，以及Apriori算法发现频繁症状对。分析结果表明：临床相关主题具有连贯结构，聚类凝聚力处于中等水平，症状对（如发热-头痛、皮疹-瘙痒）之间存在高置信度的关联关系。这些结果支持了对话医疗数据可作为有价值的诊断信号用于早期症状解释、有助于加强决策支持并改善用户与远程医疗技术交互的观点。通过展示将非结构化的自由对话转化为可操作症状知识的方法，这项工作为未来进一步提升医疗聊天机器人的性能、可靠性和临床实用性提供了可扩展的框架。

</details>


### [377] [What Is Preference Optimization Doing, How and Why?](https://arxiv.org/abs/2512.00778)
*Yue Wang,Qizhou Wang,Zizhuo Zhang,Ang Li,Gang Niu,Bo Han,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 本文深入分析了偏好优化（PO）方法，特别是DPO和PPO的优化动态差异，揭示了它们在目标方向、正负学习、损失重加权等方面的不同作用机制。


<details>
  <summary>Details</summary>
Motivation: 尽管DPO和PPO等偏好优化方法在大型语言模型中取得了巨大成功，但普遍认为DPO是监督学习而PPO是强化学习，这种差异背后的深层原因缺乏系统分析。本文旨在填补这一空白，深入理解这些方法的优化动态和内在机制。

Method: 通过分析梯度更新的目标方向，研究正学习、负学习和损失重加权三个关键组件的作用，并进行精心设计的消融实验来验证分析结果。

Result: 发现DPO遵循稳定目标，而PPO遵循动态目标以平衡探索与利用；在DPO中正负学习共同塑造学习目标并相互抵消，损失重加权主要起正则化作用；在PPO中负学习主要支持探索，损失重加权与token级优势的绝对值相关，指示不同token组在更新目标中的不同作用。

Conclusion: 对PO方法优化动态的深入分析不仅加深了对这些方法的理解，还为开发更偏好对齐的LLM提供了启发。研究揭示了看似相似的方法在底层机制上的重要差异。

Abstract: Preference optimization (PO) is indispensable for large language models (LLMs), with methods such as direct preference optimization (DPO) and proximal policy optimization (PPO) achieving great success. A common belief is that DPO is supervised learning while PPO is reinforcement learning, yet deeper analyses for the reasons underlying these differences remain lacking. To fill this gap, we analyze their optimization dynamics, revealing distinct algorithmic behaviors and comprehending their underlying causes. First, we examine the target directions of gradient-based updates and find that DPO follows stable targets, whereas PPO follows dynamic targets that balance exploration and exploitation, thus validating the common belief from a new perspective. Second, we examine the roles of positive learning, negative learning, and loss reweighting, which are three key components in PO methods. Our analyses reveal that these components play fairly different roles. In DPO, positive and negative learning jointly shape the learning targets meanwhile mutually offset each other. However, loss reweighting in DPO acts less as a reward signal but more as a regularizer to mitigate overfitting. In PPO, negative learning primarily supports exploration rather than determining the targets. Meanwhile, loss reweighting, related to absolute values of token-level advantages, indicates the distinct roles of token groups in updating targets. Given these findings, we conduct carefully designed ablation studies to further examine how controlling these dynamics impacts optimization efficiency and practical performance. The insights gained from our analyses not only deepen the understanding of PO methods but also inspire the development of more preference-aligned LLMs.

中文标题: 偏好优化在做什么、如何做以及为什么？

中文摘要: 偏好优化（PO）对于大型语言模型（LLM）来说是不可或缺的，直接偏好优化（DPO）和近端策略优化（PPO）等方法取得了巨大成功。普遍认为DPO是监督学习而PPO是强化学习，但这些差异背后的深层原因仍然缺乏分析。为了填补这一空白，我们分析了它们的优化动态，揭示了不同的算法行为并理解其根本原因。首先，我们检查基于梯度的更新目标方向，发现DPO遵循稳定目标，而PPO遵循动态目标以平衡探索与利用，从而从新角度验证了普遍观点。其次，我们研究了正学习、负学习和损失重加权这三个PO方法中的关键组件。我们的分析显示这些组件扮演着相当不同的角色。在DPO中，正学习和负学习共同塑造学习目标，同时相互抵消。然而，DPO中的损失重加权较少作为奖励信号，更多作为正则化器来缓解过拟合。在PPO中，负学习主要支持探索而非确定目标。同时，与token级优势绝对值相关的损失重加权指示了不同token组在更新目标中的不同作用。基于这些发现，我们进行了精心设计的消融研究，进一步检验控制这些动态如何影响优化效率和实际性能。从分析中获得的见解不仅加深了对PO方法的理解，还启发了开发更偏好对齐的LLM。

</details>


### [378] [Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment](https://arxiv.org/abs/2512.00783)
*Libo Wang*

Main category: cs.LG

TL;DR: Sigma是一个在单张RTX 4090上训练的VLA模型，通过结合深度语义理解和关联实现"心灵感应"对齐，无需重新训练基础模型就能实现意图驱动的机器人控制。


<details>
  <summary>Details</summary>
Motivation: 为了解决人形机器人认知系统中缺乏可随时间更新的语义与连续控制之间的中介思维空间的问题，构建一个能够实现"心灵感应"通信的视觉-语言-动作模型。

Method: 以开源pi05_base模型为基础，预处理svla_so101_pickplace数据集，设计结合深度语义理解和关联的VLA架构，通过数据预处理、LoRA微调和推理阶段适配器的重复优化进行训练。

Result: Sigma在向量、片段和整个轨迹时间尺度上均表现出控制MSE的稳定下降，同时保持"心灵感应"范式和语义-文本对齐质量不变，优于未调优的纯pi05_base模型。

Conclusion: 通过结合深度语义理解和关联的架构，无需重新训练基础模型就能实现心灵响应式对齐控制，为人形机器人的语义对齐和意图驱动行为提供了可复现的经验。

Abstract: To address the gap in humanoid robot cognitive systems regarding the lack of a time-updable mediating thought space between semantics and continuous control, this study constructs and trains a VLA model named "Sigma" that runs on a single RTX 4090. It uses the open-source pi05_base model as a foundation and preprocesses svla_so101_pickplace into a training dataset. The researcher independently designed an architecture for a vision-language-action model that combines deep semantic understanding and association to achieve telepathic communication. The training process involved repeated optimizations of data preprocessing, LoRA fine-tuning, and the inference-stage adapter. The experiment employed offline closed-loop replay, comparing Sigma with the untuned pure pi05_base_base model under data conditions. Results showed that Sigma exhibited a stable decrease in control MSE across vector, fragment, and entire trajectory timescales, while maintaining the telepathy norm and semantic-text alignment quality unchanged. It demonstrates that mind-responsive alignment control is quantified through an architecture that combines deep understanding of semantics and association without retraining the base model, which provides reproducible experience for semantic alignment and intention-driven behavior in humanoid robots.

中文标题: Sigma：视觉-语言-动作模型实现心灵感应对齐的关键

中文摘要: 为解决人形机器人认知系统中缺乏可随时间更新的语义与连续控制之间的中介思维空间的问题，本研究构建并训练了一个名为"Sigma"的VLA模型，该模型在单张RTX 4090上运行。它以开源pi05_base模型为基础，将svla_so101_pickplace预处理为训练数据集。研究者独立设计了一个结合深度语义理解和关联以实现心灵感应通信的视觉-语言-动作模型架构。训练过程涉及数据预处理、LoRA微调和推理阶段适配器的重复优化。实验采用离线闭环重放，在数据条件下比较Sigma与未调优的纯pi05_base模型。结果显示，Sigma在向量、片段和整个轨迹时间尺度上均表现出控制MSE的稳定下降，同时保持心灵感应范式和语义-文本对齐质量不变。这表明通过结合深度语义理解和关联的架构，无需重新训练基础模型就能实现心灵响应式对齐控制，为人形机器人的语义对齐和意图驱动行为提供了可复现的经验。

</details>


### [379] [Limitations of Using Identical Distributions for Training and Testing When Learning Boolean Functions](https://arxiv.org/abs/2512.00791)
*Jordi Pérez-Guijarro*

Main category: cs.LG

TL;DR: 研究发现训练和测试数据分布相同时并不总是最优选择，存在单向函数时，匹配分布可能不是最佳方案，但目标函数具有特定规律时，均匀分布下标准结论仍成立。


<details>
  <summary>Details</summary>
Motivation: 当训练和测试数据分布不一致时，理解泛化问题变得更加复杂。本研究探讨一个基本问题：训练分布是否总是应该与测试分布相同？这挑战了机器学习中常见的假设。

Method: 通过理论分析布尔函数学习问题，假设单向函数存在，研究训练分布与测试分布匹配的最优性。分析不同分布设置下的学习性能。

Result: 研究发现，在存在单向函数的假设下，训练分布与测试分布相同并不总是最优选择。然而，当目标函数具有特定规律时，在均匀分布情况下，标准结论（匹配分布最优）仍然成立。

Conclusion: 训练分布与测试分布相同并不总是最优策略，这挑战了机器学习中的常见假设。但在特定条件下（如目标函数具有规律性、均匀分布），匹配分布仍是最佳选择。

Abstract: When the distributions of the training and test data do not coincide, the problem of understanding generalization becomes considerably more complex, prompting a variety of questions. In this work, we focus on a fundamental one: Is it always optimal for the training distribution to be identical to the test distribution? Surprisingly, assuming the existence of one-way functions, we find that the answer is no. That is, matching distributions is not always the best scenario, which contrasts with the behavior of most learning methods. Nonetheless, we also show that when certain regularities are imposed on the target functions, the standard conclusion is recovered in the case of the uniform distribution.

中文标题: 学习布尔函数时训练和测试使用相同分布的局限性

中文摘要: 当训练数据和测试数据的分布不一致时，理解泛化问题变得更加复杂，引发了各种疑问。在这项工作中，我们关注一个基本问题：训练分布是否总是应该与测试分布相同？令人惊讶的是，假设单向函数存在，我们发现答案是否定的。也就是说，匹配分布并不总是最佳情况，这与大多数学习方法的行为形成对比。然而，我们也表明，当对目标函数施加某些规律性约束时，在均匀分布的情况下，标准结论仍然成立。

</details>


### [380] [Causal Invariance and Counterfactual Learning Driven Cooperative Game for Multi-Label Classification](https://arxiv.org/abs/2512.00812)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: 本文提出了因果合作博弈（CCG）框架，将多标签分类视为合作多玩家交互，通过神经结构方程模型进行显式因果发现，结合反事实好奇心奖励驱动鲁棒特征学习，并引入因果不变性损失确保跨环境泛化能力，特别针对稀有标签设计了增强策略。


<details>
  <summary>Details</summary>
Motivation: 多标签分类面临标签不平衡、虚假相关性和分布偏移等挑战，这些问题对稀有标签预测尤为不利。现有方法在这些问题上表现不足，需要一种能够处理因果结构、增强泛化能力并特别关注稀有标签的框架。

Method: 1. 将多标签分类建模为合作博弈，每个标签视为一个玩家；2. 使用神经结构方程模型进行显式因果发现；3. 引入反事实好奇心奖励机制驱动鲁棒特征学习；4. 设计因果不变性损失确保跨环境泛化；5. 针对稀有标签开发专门的增强策略。

Result: 在多个基准测试中，CCG框架在稀有标签预测和整体鲁棒性方面显著优于强基线方法。消融研究和定性分析验证了各组件（因果发现、反事实奖励、不变性损失、稀有标签增强）的有效性和可解释性。

Conclusion: CCG框架成功地将因果推理与合作博弈理论相结合，为多标签学习提供了新的解决方案。该方法不仅提升了稀有标签预测性能，还增强了模型对分布偏移和虚假相关性的鲁棒性，展示了因果方法与博弈理论协同的潜力。

Abstract: Multi-label classification (MLC) remains vulnerable to label imbalance, spurious correlations, and distribution shifts, challenges that are particularly detrimental to rare label prediction. To address these limitations, we introduce the Causal Cooperative Game (CCG) framework, which conceptualizes MLC as a cooperative multi-player interaction. CCG unifies explicit causal discovery via Neural Structural Equation Models with a counterfactual curiosity reward to drive robust feature learning. Furthermore, it incorporates a causal invariance loss to ensure generalization across diverse environments, complemented by a specialized enhancement strategy for rare labels. Extensive benchmarking demonstrates that CCG substantially outperforms strong baselines in both rare label prediction and overall robustness. Through rigorous ablation studies and qualitative analysis, we validate the efficacy and interpretability of our components, underscoring the potential of synergizing causal inference with cooperative game theory for advancing multi-label learning.

中文标题: 因果不变性与反事实学习驱动的合作博弈用于多标签分类

中文摘要: 多标签分类（MLC）仍然容易受到标签不平衡、虚假相关性和分布偏移的影响，这些挑战对稀有标签预测尤为不利。为了解决这些限制，我们引入了因果合作博弈（CCG）框架，该框架将MLC概念化为合作多玩家交互。CCG通过神经结构方程模型统一了显式因果发现，并结合反事实好奇心奖励来驱动鲁棒特征学习。此外，它引入了因果不变性损失以确保跨不同环境的泛化能力，并辅以针对稀有标签的专门增强策略。广泛的基准测试表明，CCG在稀有标签预测和整体鲁棒性方面都显著优于强基线方法。通过严格的消融研究和定性分析，我们验证了各组件（因果发现、反事实奖励、不变性损失、稀有标签增强）的有效性和可解释性，强调了将因果推理与合作博弈理论协同用于推进多标签学习的潜力。

</details>


### [381] [Uncertainty Quantification for Deep Regression using Contextualised Normalizing Flows](https://arxiv.org/abs/2512.00835)
*Adriel Sosa Marco,John Daniel Kirwan,Alexia Toumpa,Simos Gerasimou*

Main category: cs.LG

TL;DR: MCNF是一种基于情境化归一化流的后处理不确定性量化方法，无需重新训练模型即可提供预测区间和完整的条件预测分布。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法存在两个主要问题：1）预测区间方法忽略了分布信息，无法处理多模态或非对称分布；2）贝叶斯方法需要修改模型架构并重新训练，成本高昂。需要一种既能提供完整分布信息又无需重新训练的方法。

Method: MCNF（基于情境化归一化流的方法）是一种后处理不确定性量化方法，在已训练好的预测模型之上运行。它使用归一化流来建模条件预测分布，能够捕捉多模态和不对称性，而无需修改原始模型架构或重新训练。

Result: 实验表明MCNF具有良好校准性，与最先进的不确定性量化方法竞争力相当。相比仅提供预测区间的方法，MCNF能提供更丰富的分布信息，有助于下游决策任务。

Conclusion: MCNF是一种有效的不确定性量化方法，既能提供预测区间又能提供完整条件分布，无需重新训练模型，为高风险领域的决策提供了更丰富的信息支持。

Abstract: Quantifying uncertainty in deep regression models is important both for understanding the confidence of the model and for safe decision-making in high-risk domains. Existing approaches that yield prediction intervals overlook distributional information, neglecting the effect of multimodal or asymmetric distributions on decision-making. Similarly, full or approximated Bayesian methods, while yielding the predictive posterior density, demand major modifications to the model architecture and retraining. We introduce MCNF, a novel post hoc uncertainty quantification method that produces both prediction intervals and the full conditioned predictive distribution. MCNF operates on top of the underlying trained predictive model; thus, no predictive model retraining is needed. We provide experimental evidence that the MCNF-based uncertainty estimate is well calibrated, is competitive with state-of-the-art uncertainty quantification methods, and provides richer information for downstream decision-making tasks.

中文标题: 基于情境化归一化流的深度回归不确定性量化

中文摘要: 量化深度回归模型的不确定性对于理解模型置信度和在高风险领域进行安全决策至关重要。现有的预测区间方法忽略了分布信息，忽视了多模态或非对称分布对决策的影响。同样，完全或近似的贝叶斯方法虽然能产生预测后验密度，但需要对模型架构进行重大修改并重新训练。我们提出了MCNF，一种新颖的后处理不确定性量化方法，既能产生预测区间，又能提供完整的条件预测分布。MCNF在底层训练好的预测模型之上运行；因此不需要重新训练预测模型。我们提供的实验证据表明，基于MCNF的不确定性估计具有良好的校准性，与最先进的不确定性量化方法具有竞争力，并为下游决策任务提供了更丰富的信息。

</details>


### [382] [Prediction-space knowledge markets for communication-efficient federated learning on multimedia tasks](https://arxiv.org/abs/2512.00841)
*Wenzhang Du*

Main category: cs.LG

TL;DR: KTA v2是一种联邦学习新方法，通过预测空间知识交易市场解决通信效率和统计异构性问题。客户端仅共享小公共参考集上的logits，服务器构建相似度图并生成个性化软目标进行蒸馏更新，大幅降低通信成本同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在多媒体任务中面临统计异构性和通信约束的挑战，特别是当客户端部署大模型时。传统的参数平均方法如FedAvg传输完整模型权重，在非独立同分布数据下可能发散，且通信开销巨大。

Method: 提出KTA v2预测空间知识交易市场：1) 客户端在私有数据上本地训练，仅共享小公共参考集上的logits；2) 服务器在预测空间构建客户端相似度图，结合参考集准确率形成个性化教师集成；3) 服务器发送个性化软目标进行第二阶段蒸馏更新；4) 该方法可解释为具有预测空间正则化的统一目标上的近似块坐标下降。

Result: 在FEMNIST、CIFAR-10和AG News数据集上的实验表明，在可比或更低通信预算下，KTA v2始终优于本地基线、参数方法（FedAvg、FedProx），并显著超越FedMD风格的全局教师。在CIFAR-10上使用ResNet-18达到57.7%测试准确率，通信量约为FedAvg的1/1100；在AG News上达到89.3%准确率，通信量约为FedAvg的1/300。

Conclusion: KTA v2通过预测空间知识交易有效解决了联邦学习中的通信效率和统计异构性问题，在保持高性能的同时大幅降低通信开销，为多媒体任务的联邦学习提供了高效解决方案。

Abstract: Federated learning (FL) enables collaborative training over distributed multimedia data but suffers acutely from statistical heterogeneity and communication constraints, especially when clients deploy large models. Classic parameter-averaging methods such as FedAvg transmit full model weights and can diverge under nonindependent and identically distributed (non-IID) data. We propose KTA v2, a prediction-space knowledge trading market for FL. Each round, clients locally train on their private data, then share only logits on a small public reference set. The server constructs a client-client similarity graph in prediction space, combines it with reference-set accuracy to form per-client teacher ensembles, and sends back personalized soft targets for a second-stage distillation update. This two-stage procedure can be interpreted as approximate block-coordinate descent on a unified objective with prediction-space regularization. Experiments on FEMNIST, CIFAR-10 and AG News show that, under comparable or much lower communication budgets, KTA v2 consistently outperforms a local-only baseline and strong parameter-based methods (FedAvg, FedProx), and substantially improves over a FedMD-style global teacher. On CIFAR-10 with ResNet-18, KTA v2 reaches 57.7% test accuracy using approximately 1/1100 of FedAvg's communication, while on AG News it attains 89.3% accuracy with approximately 1/300 of FedAvg's traffic.

中文标题: 面向多媒体任务通信高效联邦学习的预测空间知识市场

中文摘要: 联邦学习（FL）支持在分布式多媒体数据上进行协作训练，但面临严重的统计异构性和通信约束，特别是当客户端部署大模型时。经典的参数平均方法如FedAvg传输完整模型权重，在非独立同分布（non-IID）数据下可能发散。我们提出KTA v2，一种用于联邦学习的预测空间知识交易市场。每轮中，客户端在私有数据上本地训练，然后仅共享小公共参考集上的logits。服务器在预测空间构建客户端相似度图，结合参考集准确率形成每个客户端的教师集成，并发送个性化软目标进行第二阶段蒸馏更新。这种两阶段过程可解释为具有预测空间正则化的统一目标上的近似块坐标下降。在FEMNIST、CIFAR-10和AG News上的实验表明，在可比或更低的通信预算下，KTA v2始终优于仅本地基线和强参数方法（FedAvg、FedProx），并显著超越FedMD风格的全局教师。在CIFAR-10上使用ResNet-18，KTA v2达到57.7%测试准确率，通信量约为FedAvg的1/1100；在AG News上达到89.3%准确率，通信量约为FedAvg的1/300。

</details>


### [383] [Topological Federated Clustering via Gravitational Potential Fields under Local Differential Privacy](https://arxiv.org/abs/2512.00849)
*Yunbo Long,Jiaquan Zhang,Xi Chen,Alexandra Brintrup*

Main category: cs.LG

TL;DR: GFC是一种新颖的联邦聚类方法，通过引力势场和拓扑持久性分析，在强局部差分隐私约束下实现非IID数据的高效隐私保护聚类，无需迭代通信。


<details>
  <summary>Details</summary>
Motivation: 现有联邦聚类方法在强LDP噪声和非IID数据下性能严重下降，特别是基于距离的方法不稳定。需要一种能同时处理隐私保护、数据异构性和通信效率的新方法。

Method: 1. 客户端紧凑性感知扰动：将局部聚类几何编码为"质量"值进行隐私保护；2. 服务器端拓扑聚合：构建全局引力势场，通过持久同调分析超水平集提取稳定聚类中心。

Result: 理论证明：建立了隐私预算与估计误差的闭式界限，势场特性指数级抑制噪声。实证结果：在10个基准测试中优于现有方法，特别是在强LDP约束下（ε<1）。

Conclusion: GFC通过将联邦聚类重新表述为拓扑持久性问题，在合成物理空间中实现了前所未有的隐私-准确性权衡，为隐私保护分布式学习提供了新范式。

Abstract: Clustering non-independent and identically distributed (non-IID) data under local differential privacy (LDP) in federated settings presents a critical challenge: preserving privacy while maintaining accuracy without iterative communication. Existing one-shot methods rely on unstable pairwise centroid distances or neighborhood rankings, degrading severely under strong LDP noise and data heterogeneity. We present Gravitational Federated Clustering (GFC), a novel approach to privacy-preserving federated clustering that overcomes the limitations of distance-based methods under varying LDP. Addressing the critical challenge of clustering non-IID data with diverse privacy guarantees, GFC transforms privatized client centroids into a global gravitational potential field where true cluster centers emerge as topologically persistent singularities. Our framework introduces two key innovations: (1) a client-side compactness-aware perturbation mechanism that encodes local cluster geometry as "mass" values, and (2) a server-side topological aggregation phase that extracts stable centroids through persistent homology analysis of the potential field's superlevel sets. Theoretically, we establish a closed-form bound between the privacy budget $ε$ and centroid estimation error, proving the potential field's Lipschitz smoothing properties exponentially suppress noise in high-density regions. Empirically, GFC outperforms state-of-the-art methods on ten benchmarks, especially under strong LDP constraints ($ε< 1$), while maintaining comparable performance at lower privacy budgets. By reformulating federated clustering as a topological persistence problem in a synthetic physics-inspired space, GFC achieves unprecedented privacy-accuracy trade-offs without iterative communication, providing a new perspective for privacy-preserving distributed learning.

中文标题: 基于局部差分隐私下引力势场的拓扑联邦聚类

中文摘要: 在联邦学习环境中，对非独立同分布（non-IID）数据进行局部差分隐私（LDP）下的聚类面临关键挑战：在保护隐私的同时保持准确性，且无需迭代通信。现有的一轮方法依赖于不稳定的成对质心距离或邻域排名，在强LDP噪声和数据异构性下性能严重下降。我们提出了引力联邦聚类（GFC），这是一种新颖的隐私保护联邦聚类方法，克服了不同LDP下基于距离方法的局限性。针对具有不同隐私保证的非IID数据聚类的关键挑战，GFC将私有化的客户端质心转换为全局引力势场，其中真实的聚类中心作为拓扑持久的奇点出现。我们的框架引入了两个关键创新：（1）客户端紧凑性感知扰动机制，将局部聚类几何编码为"质量"值；（2）服务器端拓扑聚合阶段，通过对势场超水平集的持久同调分析提取稳定的质心。理论上，我们建立了隐私预算$ε$和质心估计误差之间的闭式界限，证明了势场的Lipschitz平滑特性在高密度区域指数级抑制噪声。实证上，GFC在十个基准测试中优于最先进的方法，特别是在强LDP约束下（$ε< 1$），同时在较低隐私预算下保持可比性能。通过将联邦聚类重新表述为合成物理启发空间中的拓扑持久性问题，GFC实现了前所未有的隐私-准确性权衡，无需迭代通信，为隐私保护分布式学习提供了新视角。

</details>


### [384] [Robust Probabilistic Load Forecasting for a Single Household: A Comparative Study from SARIMA to Transformers on the REFIT Dataset](https://arxiv.org/abs/2512.00856)
*Midhun Manoj*

Main category: cs.LG

TL;DR: 本文在REFIT家庭用电数据集上系统比较了从传统时序模型到深度学习模型的概率负荷预测性能，发现TFT模型在点预测精度和区间预测安全性方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 概率预测对于现代风险管理至关重要，但家庭用电数据具有高度波动性和结构性缺失，需要开发能够有效处理这些挑战的预测方法。

Method: 首先通过比较实验选择季节性插补方法处理数据缺失，然后系统评估SARIMA、Prophet、XGBoost、LSTM和TFT等层次化模型，从传统基线到深度学习架构。

Result: 传统模型无法捕捉数据的非线性、状态切换行为；LSTM提供最校准的概率预测；TFT成为最佳全能模型，获得最佳点预测精度（RMSE 481.94）并产生更安全、谨慎的预测区间，能有效捕捉极端波动。

Conclusion: TFT模型在家庭负荷概率预测中表现最优，既能提供准确的点预测，又能生成安全的预测区间，特别适合处理具有波动性和结构性缺失的用电数据。

Abstract: Probabilistic forecasting is essential for modern risk management, allowing decision-makers to quantify uncertainty in critical systems. This paper tackles this challenge using the volatile REFIT household dataset, which is complicated by a large structural data gap. We first address this by conducting a rigorous comparative experiment to select a Seasonal Imputation method, demonstrating its superiority over linear interpolation in preserving the data's underlying distribution. We then systematically evaluate a hierarchy of models, progressing from classical baselines (SARIMA, Prophet) to machine learning (XGBoost) and advanced deep learning architectures (LSTM). Our findings reveal that classical models fail to capture the data's non-linear, regime-switching behavior. While the LSTM provided the most well-calibrated probabilistic forecast, the Temporal Fusion Transformer (TFT) emerged as the superior all-round model, achieving the best point forecast accuracy (RMSE 481.94) and producing safer, more cautious prediction intervals that effectively capture extreme volatility.

中文标题: 单户家庭稳健概率负荷预测：基于REFIT数据集的从SARIMA到Transformer的比较研究

中文摘要: 概率预测对于现代风险管理至关重要，使决策者能够量化关键系统中的不确定性。本文使用波动性强的REFIT家庭数据集应对这一挑战，该数据集因存在大量结构性数据缺失而变得复杂。我们首先通过严格的比较实验选择季节性插补方法，证明其在保留数据底层分布方面优于线性插值。然后我们系统地评估了一系列层次化模型，从经典基线（SARIMA、Prophet）到机器学习（XGBoost）和先进的深度学习架构（LSTM）。我们的研究结果表明，经典模型无法捕捉数据的非线性、状态切换行为。虽然LSTM提供了最校准的概率预测，但时序融合变换器（TFT）成为最佳全能模型，实现了最佳点预测精度（RMSE 481.94），并产生了更安全、更谨慎的预测区间，能有效捕捉极端波动。

</details>


### [385] [HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs](https://arxiv.org/abs/2512.00862)
*Ningning Chen,Weicai Ye,Ying Jiang*

Main category: cs.LG

TL;DR: HBLLM提出了一种基于小波增强的1比特后训练量化方法，通过Haar小波变换进行频率分解增强表达能力，采用两种结构感知分组策略，在保持最小开销的同时显著提升量化保真度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的部署面临巨大的存储和计算开销挑战。现有的1比特量化方法在保持模型性能方面存在局限性，需要一种既能大幅压缩模型大小又能保持高保真度的量化方法。

Method: HBLLM采用Haar小波变换进行频率分解，增强表达能力。提出两种创新结构感知分组策略：1）频率感知的多参数行内分组；2）基于ℓ₂范数的显著性驱动列选择。对于非显著权重，在每个频带内的量化组中使用共享均值来优化存储效率。

Result: 在OPT和LLaMA模型上的实验表明，HBLLM在1比特量化中达到最先进性能。在LLaMA2-13B模型上实现了6.71的困惑度，平均权重存储仅为1.08比特。

Conclusion: HBLLM通过小波增强和结构感知分组策略，成功实现了高保真度的1比特量化，在显著压缩模型大小的同时保持了优异的性能表现。

Abstract: We introduce HBLLM, a wavelet-enhanced high-fidelity $1$-bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) $\ell_2$-norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in $1$-bit quantization, attaining a perplexity of $6.71$ on LLaMA$2$-$13$B with an average weight storage of only $1.08$ bits. Code available at: https://github.com/Yeyke/HBLLM.

中文标题: HBLLM：面向大语言模型的小波增强高保真1比特量化方法

中文摘要: 我们提出了HBLLM，一种面向大语言模型（LLMs）的小波增强高保真1比特后训练量化方法。通过利用Haar小波变换进行频率分解以增强表达能力，HBLLM在保持最小开销的同时显著提高了量化保真度。该方法具有两种创新的结构感知分组策略：（1）频率感知的多参数行内分组；（2）基于ℓ₂范数的显著性驱动列选择。对于非显著权重，在每个频带内的量化组中采用共享均值来优化存储效率。在OPT和LLaMA模型上进行的实验表明，HBLLM在1比特量化中实现了最先进的性能，在LLaMA2-13B模型上达到了6.71的困惑度，平均权重存储仅为1.08比特。代码可在https://github.com/Yeyke/HBLLM获取。

</details>


### [386] [Towards Active Synthetic Data Generation for Finetuning Language Models](https://arxiv.org/abs/2512.00884)
*Samuel Kessler,Menglin Xia,Daniel Madrigal Diaz,Dongge Han,Helia Heshemi,Saravan Rajmohan,Victor Ruehle,Jordan T. Ash*

Main category: cs.LG

TL;DR: 本文研究了在语言模型微调过程中采用主动、迭代方式生成合成数据的方法，相比静态生成能更有效地提升学生模型性能，且发现简单的主动学习选择标准效果最好。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型能力提升的常见方法是使用更熟练的"教师"模型生成合成数据来微调"学生"模型。大多数研究在微调前一次性生成所有合成数据，但本文关注在训练过程中迭代生成新样本的方法，探索这种主动、闭环的数据生成方式是否能更有效地利用有限的生成预算。

Method: 采用迭代、闭环的合成数据生成方法，在训练过程中根据学生模型的当前状态指导新数据的生成。研究了多种选择标准，包括来自主动学习文献的简单、低成本方法，以及一些专门为LLM设计的方法。在四个数学和逻辑推理数据集上使用四种不同的小型语言模型进行验证。

Result: 研究发现：1）在固定的生成样本预算或教师模型查询计算预算下，迭代生成合成数据相比静态生成能显著提升学生模型性能；2）尽管存在专门为LLM设计的复杂方法，但来自主动学习文献的简单、低成本选择标准往往表现最佳。

Conclusion: 主动、迭代的合成数据生成方法能更有效地利用有限的生成资源提升语言模型性能，且简单的主动学习选择标准优于复杂的LLM专用方法，为语言模型微调提供了实用且高效的策略。

Abstract: A common and effective means for improving language model capabilities involves finetuning a ``student'' language model's parameters on generations from a more proficient ``teacher'' model. Termed ``synthetic data'', these generations are often produced before any student finetuning, but some work has considered generating new synthetic samples as training progresses. This paper studies and advocates for the latter case, where data are generated in an iterative, closed-loop fashion that is guided by the current state of the student model. For a fixed budget of generated samples, or a budget in terms of compute spent querying a teacher, we show that this curation of finetuning data affords improved student performance over static generation. Further, while there have been several LLM-specific methods proposed that operate in this regime, we find that simple, inexpensive selection criteria from the active learning literature tend to be most performant. We validate these claims across four mathematical and logical reasoning datasets using four different small language models.

中文标题: 面向语言模型微调的主动合成数据生成研究

中文摘要: 提升语言模型能力的常见有效方法涉及使用更熟练的"教师"模型生成的数据来微调"学生"语言模型的参数。这些被称为"合成数据"的生成通常在学生微调之前进行，但一些研究考虑了在训练过程中生成新的合成样本。本文研究并提倡后一种情况，即数据以迭代、闭环的方式生成，并由学生模型的当前状态指导。对于固定的生成样本预算或教师模型查询计算预算，我们表明这种微调数据的筛选比静态生成能提供更好的学生性能。此外，尽管已有几种专门为LLM设计的在这种机制下运行的方法，但我们发现来自主动学习文献的简单、低成本选择标准往往表现最佳。我们在四个数学和逻辑推理数据集上使用四种不同的小型语言模型验证了这些主张。

</details>


### [387] [Light-Weight Benchmarks Reveal the Hidden Hardware Cost of Zero-Shot Tabular Foundation Models](https://arxiv.org/abs/2512.00888)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.LG

TL;DR: 零样本表格基础模型（TabPFN和TabICL）在准确率上相比传统树模型（XGBoost、LightGBM、Random Forest）提升有限，但硬件成本（延迟、GPU内存）高出数千倍，揭示了显著的效率-准确率权衡。


<details>
  <summary>Details</summary>
Motivation: 零样本基础模型在表格数据预测中承诺无需训练即可使用，但其实际硬件资源消耗（延迟、内存占用）尚未得到充分量化。研究者希望揭示这些模型的隐藏硬件成本，为实际部署提供参考。

Method: 开发了完全可复现的轻量级基准测试框架，在四个公共数据集上评估两个开源表格基础模型（TabPFN-1.0和TabICL-base）与三个调优树模型（XGBoost、LightGBM、Random Forest）。在单个NVIDIA T4 GPU上测量测试准确率、实时延迟、峰值CPU RAM和峰值GPU VRAM。

Result: 树集成模型在三个数据集上准确率等于或超过基础模型，且延迟极低（≤0.40秒）、内存占用小（≤150 MB RAM，0 VRAM）。TabICL在Higgs数据集上准确率提升0.8个百分点，但延迟增加约40,000倍（960秒）且需要9 GB VRAM。TabPFN无法处理大型Higgs表（100k行），峰值VRAM达4 GB。

Conclusion: 当前表格基础模型在准确率提升有限的情况下，硬件成本（特别是GPU内存和延迟）显著高于传统树模型。这揭示了零样本表格FMs在实际部署中面临严重的效率-准确率权衡，为未来效率优化研究提供了重要基准。

Abstract: Zero-shot foundation models (FMs) promise training-free prediction on tabular data, yet their hardware footprint remains poorly characterized. We present a fully reproducible benchmark that reports test accuracy together with wall-clock latency, peak CPU RAM, and peak GPU VRAM on four public datasets: Adult-Income, Higgs-100k, Wine-Quality, and California-Housing. Two open FMs (TabPFN-1.0 and TabICL-base) are compared against tuned XGBoost, LightGBM, and Random Forest baselines on a single NVIDIA T4 GPU. The tree ensembles equal or surpass FM accuracy on three datasets while completing full-test batches in <= 0.40 s and <= 150 MB RAM, using zero VRAM. TabICL achieves a 0.8 percentage-point gain on Higgs but requires roughly 40,000 times more latency (960 s) and 9 GB VRAM. TabPFN matches tree-model accuracy on Wine and Housing but peaks at 4 GB VRAM and cannot process the full 100k-row Higgs table. These results quantify the substantial hardware-versus-accuracy trade-offs in current tabular FMs and provide an open baseline for future efficiency-oriented research.

中文标题: 轻量级基准测试揭示零样本表格基础模型的隐藏硬件成本

中文摘要: 零样本基础模型（FMs）承诺在表格数据上实现无需训练即可预测，但其硬件占用情况仍缺乏充分表征。我们提出了一个完全可复现的基准测试，报告了在四个公共数据集（Adult-Income、Higgs-100k、Wine-Quality和California-Housing）上的测试准确率、实时延迟、峰值CPU RAM和峰值GPU VRAM。在单个NVIDIA T4 GPU上，比较了两个开源FMs（TabPFN-1.0和TabICL-base）与调优后的XGBoost、LightGBM和Random Forest基线模型。在三个数据集上，树集成模型的准确率等于或超过FM，同时完成完整测试批次的延迟≤0.40秒，RAM≤150 MB，且不使用任何VRAM。TabICL在Higgs数据集上实现了0.8个百分点的提升，但需要约40,000倍的延迟（960秒）和9 GB VRAM。TabPFN在Wine和Housing数据集上与树模型准确率相当，但峰值VRAM达到4 GB，且无法处理完整的100k行Higgs表。这些结果量化了当前表格FMs中显著的硬件与准确率权衡，并为未来面向效率的研究提供了开放的基准。

</details>


### [388] [Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs](https://arxiv.org/abs/2512.00908)
*Xinzhu Chen,Xuesheng Li,Zhongxiang Sun,Weijie Yu*

Main category: cs.LG

TL;DR: LESS框架通过细粒度优势调制低熵片段来改进推理LLMs的强化学习，强调正确响应特有的低熵片段，抑制错误响应特有的片段，同时保持高熵探索。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR研究主要关注高熵token的探索作用，但忽略了推理轨迹中大部分由低熵片段组成，这些片段编码了稳定可复用的结构模式。研究发现正确响应间的低熵片段重叠与模型准确性高度相关，而涉及错误响应的重叠则表现出稳定但无生产力的模式。

Method: 提出LESS框架，基于正确性感知对低熵片段进行细粒度优势调制：放大正确响应特有的片段，抑制错误响应特有的片段，中和两者共享的片段，同时保持底层RL算法的高熵探索能力。在GRPO基础上实现。

Result: 在三个骨干模型和六个数学基准测试中，LESS持续优于强RL基线，提高了准确性，并实现了更强的性能下限鲁棒性。

Conclusion: 低熵片段在推理LLMs中编码了重要的结构模式，正确性感知的优势调制能够有效利用这些模式，在保持探索能力的同时显著提升模型性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a central approach for improving the reasoning ability of large language models. Recent work studies RLVR through token entropy, arguing that high-entropy tokens drive exploration and should receive stronger updates. However, they overlook the fact that most of a reasoning trajectory consists of low-entropy segments that encode stable and reusable structural patterns. Through qualitative and quantitative analyses, we find that the overlap of low-entropy segments across correct responses strongly correlates with model accuracy, while overlaps involving incorrect responses exhibit stable but unproductive patterns. Motivated by these findings, we propose LESS, a correctness-aware reinforcement framework that performs fine-grained advantage modulation over low-entropy segments. LESS amplifies segments unique to correct responses, suppresses those unique to incorrect ones, and neutralizes segments shared by both, while preserving high-entropy exploration in the underlying RL algorithm. Instantiated on top of the popular GRPO, LESS consistently improves accuracy over strong RL baselines across three backbones and six math benchmarks, achieves stronger robustness of the performance floor.

中文标题: 超越高熵探索：面向推理大语言模型的正确性感知低熵片段优势塑造

中文摘要: 基于可验证奖励的强化学习已成为提升大语言模型推理能力的核心方法。近期研究通过token熵来探讨RLVR，认为高熵token驱动探索并应获得更强的更新。然而，他们忽略了推理轨迹主要由低熵片段组成，这些片段编码了稳定且可复用的结构模式。通过定性和定量分析，我们发现正确响应间低熵片段的重叠与模型准确性高度相关，而涉及错误响应的重叠则表现出稳定但无生产力的模式。基于这些发现，我们提出了LESS，一个正确性感知的强化学习框架，对低熵片段执行细粒度优势调制。LESS放大正确响应特有的片段，抑制错误响应特有的片段，中和两者共享的片段，同时保持底层RL算法的高熵探索。在流行的GRPO基础上实现，LESS在三个骨干模型和六个数学基准测试中持续优于强RL基线，提高了准确性，并实现了更强的性能下限鲁棒性。

</details>


### [389] [Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments](https://arxiv.org/abs/2512.00915)
*Junwoo Chang,Minwoo Park,Joohwan Seo,Roberto Horowitz,Jongmin Lee,Jongeun Choi*

Main category: cs.LG

TL;DR: 本文提出部分群不变MDP（PI-MDP）框架和部分等变RL算法（PE-DQN/PE-SAC），在对称性被局部破坏的环境中，通过选择性应用群不变Bellman备份来平衡等变性的优势与对称性破坏的鲁棒性，显著提升样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实环境几乎从不实现完全群不变的MDP，动力学、执行器限制和奖励设计通常会破坏对称性，且往往只是局部破坏。在群不变Bellman备份下，局部对称性破坏会引入误差并传播到整个状态-动作空间，导致全局价值估计错误。需要一种能够处理局部对称性破坏的框架。

Method: 提出部分群不变MDP（PI-MDP）框架，根据对称性是否成立，选择性应用群不变或标准Bellman备份。基于此框架开发了部分等变RL算法：PE-DQN用于离散控制，PE-SAC用于连续控制，结合了等变性的优势与对对称性破坏的鲁棒性。

Result: 在Grid-World、运动控制和操作基准测试中，PE-DQN和PE-SAC显著优于基线方法，证明了选择性利用对称性对于鲁棒且样本高效的RL的重要性。

Conclusion: 部分等变RL框架通过选择性应用对称性，在保持等变性优势的同时，有效处理局部对称性破坏，提高了RL的样本效率和泛化能力，为在现实对称性破坏环境中应用等变RL提供了实用解决方案。

Abstract: Group symmetries provide a powerful inductive bias for reinforcement learning (RL), enabling efficient generalization across symmetric states and actions via group-invariant Markov Decision Processes (MDPs). However, real-world environments almost never realize fully group-invariant MDPs; dynamics, actuation limits, and reward design usually break symmetries, often only locally. Under group-invariant Bellman backups for such cases, local symmetry-breaking introduces errors that propagate across the entire state-action space, resulting in global value estimation errors. To address this, we introduce Partially group-Invariant MDP (PI-MDP), which selectively applies group-invariant or standard Bellman backups depending on where symmetry holds. This framework mitigates error propagation from locally broken symmetries while maintaining the benefits of equivariance, thereby enhancing sample efficiency and generalizability. Building on this framework, we present practical RL algorithms -- Partially Equivariant (PE)-DQN for discrete control and PE-SAC for continuous control -- that combine the benefits of equivariance with robustness to symmetry-breaking. Experiments across Grid-World, locomotion, and manipulation benchmarks demonstrate that PE-DQN and PE-SAC significantly outperform baselines, highlighting the importance of selective symmetry exploitation for robust and sample-efficient RL.

中文标题: 对称性破坏环境中的部分等变强化学习

中文摘要: 群对称性为强化学习（RL）提供了强大的归纳偏置，通过群不变马尔可夫决策过程（MDP）实现跨对称状态和动作的高效泛化。然而，现实环境几乎从不实现完全群不变的MDP；动力学、执行器限制和奖励设计通常会破坏对称性，且往往只是局部破坏。在此类情况下，使用群不变Bellman备份时，局部对称性破坏会引入误差并传播到整个状态-动作空间，导致全局价值估计错误。为解决此问题，我们引入了部分群不变MDP（PI-MDP），根据对称性成立的位置选择性应用群不变或标准Bellman备份。该框架在保持等变性优势的同时，减轻了局部对称性破坏引起的误差传播，从而提高了样本效率和泛化能力。基于此框架，我们提出了实用的RL算法——用于离散控制的部分等变（PE）-DQN和用于连续控制的PE-SAC，结合了等变性的优势与对对称性破坏的鲁棒性。在Grid-World、运动控制和操作基准测试中的实验表明，PE-DQN和PE-SAC显著优于基线方法，突显了选择性利用对称性对于鲁棒且样本高效RL的重要性。

</details>


### [390] [Mode-Conditioning Unlocks Superior Test-Time Scaling](https://arxiv.org/abs/2512.01127)
*Chen Henry Wu,Sachin Goyal,Aditi Raghunathan*

Main category: cs.LG

TL;DR: ModC框架通过显式分配测试时计算到不同推理模式，解决了并行采样中的多样性崩溃问题，显著提升了测试时扩展效率，在多个基准测试中实现了4倍效率提升和10%性能增益。


<details>
  <summary>Details</summary>
Motivation: 并行采样在测试时扩展中具有巨大潜力，但其效果受到多样性崩溃问题的严重限制——模型集中在少数模式上，重复采样产生相同错误。标准训练未能充分利用数据多样性。

Method: 提出模式条件化（ModC）框架，通过专家模型或模式特定前缀显式分配测试时计算到不同推理模式。还展示了梯度聚类方法可以在没有显式模式标签的情况下实现ModC。

Result: 在控制图搜索任务和大规模推理基准测试中，ModC一致改善了扩展效果，涵盖0.5B到7B的模型系列和规模。在OpenThoughts上，使用ModC微调Qwen2.5-7B实现了4倍效率提升，同时提高了最大可达的Pass@k。梯度聚类方法在NuminaMath等数据集上实现了高达10%的性能增益。ModC还改善了强化学习，并能进一步提升多样性诱导的RL方法。

Conclusion: 标准训练未能充分利用数据多样性，而ModC提供了一个简单有效的解决方案，能够解锁测试时扩展中多样性的全部益处。该框架为提升并行采样效率提供了新途径。

Abstract: Parallel sampling promises substantial gains in test-time scaling, but its effectiveness is sharply limited by diversity collapse, where models concentrate on a few modes and repeated samples produce the same mistakes. We propose the mode-conditioning (ModC) framework, which explicitly allocates test-time compute across reasoning modes using either specialist models or mode-specific prefixes. ModC consistently improves scaling across controlled graph-search tasks and large-scale reasoning benchmarks, spanning model families and sizes from 0.5B to 7B. On OpenThoughts, fine-tuning Qwen2.5-7B with ModC achieves a 4x efficiency gain over standard training while also improving the maximum attainable Pass@k. We further show that gradient clustering enables ModC without explicit mode labels, yielding up to 10% gains on datasets such as NuminaMath. Finally, we show that ModC improves reinforcement learning (RL) and can further boost diversity-inducing RL methods. These results demonstrate that standard training underutilizes the diversity in data, and that ModC provides a simple, effective remedy for unlocking the full benefits of diversity in test-time scaling.

中文标题: 模式条件化解锁卓越的测试时扩展能力

中文摘要: 并行采样在测试时扩展中承诺了显著的增益，但其效果受到多样性崩溃的严重限制，即模型集中在少数模式上，重复采样产生相同的错误。我们提出了模式条件化（ModC）框架，该框架通过专家模型或模式特定前缀显式分配测试时计算到不同的推理模式。ModC在控制图搜索任务和大规模推理基准测试中一致改善了扩展效果，涵盖从0.5B到7B的模型系列和规模。在OpenThoughts上，使用ModC微调Qwen2.5-7B实现了相对于标准训练的4倍效率提升，同时提高了最大可达的Pass@k。我们进一步展示了梯度聚类能够在没有显式模式标签的情况下实现ModC，在NuminaMath等数据集上实现了高达10%的性能增益。最后，我们展示了ModC改善了强化学习，并能进一步提升多样性诱导的RL方法。这些结果表明，标准训练未能充分利用数据多样性，而ModC提供了一个简单有效的解决方案，能够解锁测试时扩展中多样性的全部益处。

</details>


### [391] [Multi-Modal AI for Remote Patient Monitoring in Cancer Care](https://arxiv.org/abs/2512.00949)
*Yansong Liu,Ronnie Stafford,Pramit Khetrapal,Huriye Kocadag,Graça Carvalho,Patricia de Winter,Maryam Imran,Amelia Snook,Adamos Hadjivasiliou,D. Vijay Anand,Weining Lin,John Kelly,Yukun Zhou,Ivana Drobnjak*

Main category: cs.LG

TL;DR: 开发了一个用于癌症护理的多模态AI远程患者监测系统，整合了人口统计、可穿戴设备、每日调查和临床事件等多模态数据，能够预测未来不良事件风险，准确率达83.9%。


<details>
  <summary>Details</summary>
Motivation: 接受系统性癌症治疗的患者在门诊就诊间隔期间面临未监测副作用的风险和不确定性，需要填补这一护理空白。

Method: 开发并前瞻性试验了一个多模态AI远程患者监测框架，整合HALO-X平台的多模态数据，包括人口统计、可穿戴传感器、每日调查和临床事件。采用多模态AI模型处理现实世界RPM数据的异步和不完整特性，预测未来不良事件的连续风险。

Result: 观察性试验收集了84名患者超过210万个数据点（6,080个患者日）。模型准确率达到83.9%（AUROC=0.70），识别出既往治疗、健康检查签到和每日最大心率作为关键预测特征。案例研究表明模型能够在事件发生前提供早期预警。

Conclusion: 这项工作确立了多模态AI远程患者监测在癌症护理中的可行性，为更主动的患者支持提供了路径。

Abstract: For patients undergoing systemic cancer therapy, the time between clinic visits is full of uncertainties and risks of unmonitored side effects. To bridge this gap in care, we developed and prospectively trialed a multi-modal AI framework for remote patient monitoring (RPM). This system integrates multi-modal data from the HALO-X platform, such as demographics, wearable sensors, daily surveys, and clinical events. Our observational trial is one of the largest of its kind and has collected over 2.1 million data points (6,080 patient-days) of monitoring from 84 patients. We developed and adapted a multi-modal AI model to handle the asynchronous and incomplete nature of real-world RPM data, forecasting a continuous risk of future adverse events. The model achieved an accuracy of 83.9% (AUROC=0.70). Notably, the model identified previous treatments, wellness check-ins, and daily maximum heart rate as key predictive features. A case study demonstrated the model's ability to provide early warnings by outputting escalating risk profiles prior to the event. This work establishes the feasibility of multi-modal AI RPM for cancer care and offers a path toward more proactive patient support.(Accepted at Europe NeurIPS 2025 Multimodal Representation Learning for Healthcare Workshop)

中文标题: 用于癌症护理的多模态人工智能远程患者监测

中文摘要: 对于接受系统性癌症治疗的患者来说，门诊就诊之间的时间充满了不确定性和未监测副作用的风险。为了填补这一护理空白，我们开发并前瞻性试验了一个用于远程患者监测（RPM）的多模态人工智能框架。该系统整合了来自HALO-X平台的多模态数据，如人口统计、可穿戴传感器、每日调查和临床事件。我们的观察性试验是同类研究中规模最大的之一，收集了84名患者超过210万个数据点（6,080个患者日）的监测数据。我们开发并调整了一个多模态人工智能模型来处理现实世界RPM数据的异步和不完整特性，预测未来不良事件的连续风险。该模型达到了83.9%的准确率（AUROC=0.70）。值得注意的是，该模型识别出既往治疗、健康检查签到和每日最大心率作为关键预测特征。一个案例研究展示了模型通过输出事件前不断升级的风险概况来提供早期预警的能力。这项工作确立了多模态人工智能远程患者监测在癌症护理中的可行性，并为更主动的患者支持提供了路径。（已被欧洲NeurIPS 2025医疗保健多模态表示学习研讨会接受）

</details>


### [392] [WUSH: Near-Optimal Adaptive Transforms for LLM Quantization](https://arxiv.org/abs/2512.00956)
*Jiale Chen,Vage Egiazarian,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: WUSH提出了一种用于LLM量化的自适应变换方法，通过结合Hadamard基干和数据相关的二阶矩信息，为常见的数值格式推导出闭式最优线性分块变换，相比固定变换能更好地减少动态范围。


<details>
  <summary>Details</summary>
Motivation: LLM量化到低比特位宽时，少数极端权重和激活值会拉伸动态范围，降低量化器的有效分辨率。现有的固定正交变换（如Hadamard矩阵）虽然能减少动态范围，但忽略了数据统计特性，且其最优性尚未得到理论理解。

Method: 为常见的数值格式（整数和浮点数）推导闭式最优线性分块变换，用于联合权重-激活量化。具体推导了RTN（最近舍入）和AbsMax缩放分块量化器的最优自适应（数据感知）变换。WUSH构造结合了Hadamard基干和数据相关的二阶矩分量，产生一个在温和假设下可证明最优的非正交变换，同时保持结构化以实现高效计算。

Result: 初步实验结果表明，WUSH方法在常见数值格式上始终优于Hadamard变换，能够更有效地减少量化过程中的动态范围问题。

Conclusion: WUSH首次为LLM量化提供了理论最优的自适应变换方法，通过数据感知的变换设计，在保持计算效率的同时，显著改善了量化性能，为低比特位宽部署提供了更优的解决方案。

Abstract: Quantization to low bitwidth is a standard approach for deploying large language models, however, a few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. A common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines a Hadamard backbone with a data-dependent component based on second-order moments, yielding a non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats.

中文标题: WUSH：LLM量化的近最优自适应变换

中文摘要: 将大语言模型量化到低比特位宽是一种标准的部署方法，然而，少数极端权重和激活值会拉伸动态范围，降低量化器的有效分辨率。常见的缓解方法是应用一些固定的正交变换，如Hadamard矩阵，这通常能减少动态范围。但这些变换忽略了数据的统计特性，且其最优性目前尚未得到理解。在本工作中，我们首次为常见数值格式的联合权重-激活量化推导出了闭式最优线性分块变换，使用标准无数据量化器。具体来说，我们推导了整数和浮点格式的RTN（最近舍入）和AbsMax缩放分块量化器的最优自适应（数据感知）变换。所得构造称为WUSH，它将Hadamard基干与基于二阶矩的数据相关分量相结合，产生一个在温和假设下可证明最优的非正交变换，同时保持结构化以实现高效计算。初步实验结果表明，我们的方法在常见格式上始终优于Hadamard变换。

</details>


### [393] [ZIP-RC: Zero-overhead Inference-time Prediction of Reward and Cost for Adaptive and Interpretable Generation](https://arxiv.org/abs/2512.01457)
*Rohin Manvi,Joey Hong,Tim Seyde,Maxime Labonne,Mathias Lechner,Sergey Levine*

Main category: cs.LG

TL;DR: ZIP-RC是一种零开销自适应推理方法，通过重用现有logits在推理时预测奖励和成本，实现智能的元认知决策，在数学基准测试中比多数投票提高12%准确率且成本更低。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型缺乏人类的自省能力，无法预测自身成功概率和所需计算量，导致无法做出智能的元认知决策。现有方法如Best-of-N使用固定样本预算增加成本，而验证器模型又带来额外开销。需要一种零开销的自适应推理方法。

Method: ZIP-RC在推理时重用相同前向传递中保留或未使用的logits，在每个token处输出最终奖励和剩余长度的联合分布。通过计算采样效用（期望最大奖励、总计算量和延迟的线性组合），并使用元操作最大化该效用来决定继续或开始采样的token前缀。

Result: 在混合难度数学基准测试中，ZIP-RC在相等或更低的平均成本下，比多数投票提高了高达12%的准确率，并在质量、计算量和延迟之间绘制了平滑的帕累托前沿。

Conclusion: ZIP-RC通过零开销推理时奖励-成本预测，为LLM提供了实时自省能力，实现了自适应、高效的推理，解决了现有方法在成本、延迟和自适应决策方面的局限性。

Abstract: Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, an adaptive inference method that equips models with zero-overhead inference-time predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.

中文标题: ZIP-RC：零开销推理时奖励与成本预测，实现自适应可解释生成

中文摘要: 大型语言模型在推理方面表现出色，但缺乏关键的自省能力，包括预测自身成功概率和实现成功所需的计算量。人类利用实时自省来决定投入多少努力、何时进行多次尝试、何时停止以及何时发出成功或失败信号。没有这种能力，LLM难以做出智能的元认知决策。测试时缩放方法（如Best-of-N）通过使用固定预算的样本来增加成本和延迟，而不考虑每个样本在任何生成点的边际效益，并且缺乏置信度信号可能误导人们、阻碍适当升级到更好的工具并破坏可信度。学习的验证器或奖励模型可以提供置信度估计，但无法实现自适应推理，并且需要额外模型或前向传递而增加大量成本。我们提出了ZIP-RC，一种自适应推理方法，使模型能够在推理时零开销地预测奖励和成本。在每个token处，ZIP-RC重用相同前向传递中保留或未使用的logits，作为下一个token预测的一部分，输出最终奖励和剩余长度的联合分布——无需额外模型、架构更改或推理开销。这个完整的联合分布用于计算采样效用，该效用是如果生成到完成时一组样本的期望最大奖励、总计算量和延迟的线性组合。在推理过程中，我们通过元操作最大化这个效用，元操作决定继续或从哪些token前缀开始采样。在混合难度数学基准测试中，ZIP-RC在相等或更低的平均成本下，比多数投票提高了高达12%的准确率，并在质量、计算量和延迟之间绘制了平滑的帕累托前沿。通过提供实时奖励-成本自省，ZIP-RC实现了自适应、高效的推理。

</details>


### [394] [Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning](https://arxiv.org/abs/2512.00961)
*Qi Wang,Mian Wu,Yuyang Zhang,Mingqi Yuan,Wenyao Zhang,Haoxiang You,Yunbo Wang,Xin Jin,Xiaokang Yang,Wenjun Zeng*

Main category: cs.LG

TL;DR: 利用预训练视频扩散模型为强化学习提供目标驱动的奖励信号，无需人工设计奖励函数。通过视频级和帧级目标评估智能体轨迹与生成目标视频的对齐度，在Meta-World任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习通常依赖精心设计的程序化奖励函数来指导智能体行为，但这类奖励函数设计困难且难以跨任务泛化。为解决这一限制，作者希望利用预训练视频扩散模型中丰富的世界知识，为RL智能体提供目标驱动的奖励信号，避免特设的奖励设计。

Method: 方法分为两个层面：1) 视频级奖励：首先在领域特定数据集上微调预训练视频扩散模型，然后使用其视频编码器评估智能体轨迹的潜在表示与生成目标视频的对齐度；2) 帧级奖励：使用CLIP从生成视频中识别最相关的帧作为目标状态，然后采用学习的前向-后向表示来评估从给定状态-动作对访问目标状态的概率作为帧级奖励。

Result: 在多种Meta-World任务上的实验证明了该方法的有效性，表明利用视频扩散模型的世界知识可以为强化学习提供有效的目标驱动奖励信号。

Conclusion: 该方法成功展示了如何利用大规模预训练视频扩散模型中的丰富世界知识，为强化学习提供无需人工设计的、目标驱动的奖励信号，解决了传统RL中奖励函数设计的挑战。

Abstract: Reinforcement Learning (RL) has achieved remarkable success in various domains, yet it often relies on carefully designed programmatic reward functions to guide agent behavior. Designing such reward functions can be challenging and may not generalize well across different tasks. To address this limitation, we leverage the rich world knowledge contained in pretrained video diffusion models to provide goal-driven reward signals for RL agents without ad-hoc design of reward. Our key idea is to exploit off-the-shelf video diffusion models pretrained on large-scale video datasets as informative reward functions in terms of video-level and frame-level goals. For video-level rewards, we first finetune a pretrained video diffusion model on domain-specific datasets and then employ its video encoder to evaluate the alignment between the latent representations of agent's trajectories and the generated goal videos. To enable more fine-grained goal-achievement, we derive a frame-level goal by identifying the most relevant frame from the generated video using CLIP, which serves as the goal state. We then employ a learned forward-backward representation that represents the probability of visiting the goal state from a given state-action pair as frame-level reward, promoting more coherent and goal-driven trajectories. Experiments on various Meta-World tasks demonstrate the effectiveness of our approach.

中文标题: 基于视频扩散模型的目标驱动奖励用于强化学习

中文摘要: 强化学习在各个领域取得了显著成功，但它通常依赖于精心设计的程序化奖励函数来指导智能体行为。设计这样的奖励函数可能具有挑战性，并且可能无法很好地泛化到不同的任务。为了解决这一限制，我们利用预训练视频扩散模型中包含的丰富世界知识，为RL智能体提供目标驱动的奖励信号，而无需特设的奖励设计。我们的核心思想是利用在大规模视频数据集上预训练的现成视频扩散模型，作为视频级和帧级目标的信息化奖励函数。对于视频级奖励，我们首先在领域特定数据集上微调预训练视频扩散模型，然后使用其视频编码器评估智能体轨迹的潜在表示与生成目标视频的对齐度。为了实现更细粒度的目标达成，我们使用CLIP从生成视频中识别最相关的帧作为目标状态。然后，我们采用学习的前向-后向表示，该表示表示从给定状态-动作对访问目标状态的概率作为帧级奖励，促进更连贯和目标驱动的轨迹。在各种Meta-World任务上的实验证明了我们方法的有效性。

</details>


### [395] [HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment](https://arxiv.org/abs/2512.01659)
*Valentin Noël,Elimane Yassine Seidou,Charly Ken Capo-Chichi,Ghanem Amari*

Main category: cs.LG

TL;DR: HalluGraph是一个用于法律RAG系统的可审计幻觉检测框架，通过知识图谱对齐量化幻觉，提供可解释的实体接地和关系保持指标，在结构化文档上达到AUC 0.979的高性能。


<details>
  <summary>Details</summary>
Motivation: 法律AI系统需要可验证的保证，确保生成的文本忠实地代表源文档。现有的幻觉检测器依赖语义相似度指标，容易容忍实体替换，这在混淆当事人、日期或法律条款时可能产生严重后果。

Method: 引入基于图论的框架，通过从上下文、查询和响应中提取的知识图谱之间的结构对齐来量化幻觉。该方法产生有界、可解释的指标，分解为实体接地（EG）和关系保持（RP）两个部分。

Result: 在结构化控制文档上（>400词，>20实体），HalluGraph达到AUC = 0.979；在具有挑战性的生成式法律任务上保持稳健性能（AUC ≈ 0.89），始终优于语义相似度基线方法。

Conclusion: HalluGraph为高风险法律应用提供了所需的透明度和可追溯性，能够从生成的断言追溯到源段落，实现完整的审计跟踪。

Abstract: Legal AI systems powered by retrieval-augmented generation (RAG) face a critical accountability challenge: when an AI assistant cites case law, statutes, or contractual clauses, practitioners need verifiable guarantees that generated text faithfully represents source documents. Existing hallucination detectors rely on semantic similarity metrics that tolerate entity substitutions, a dangerous failure mode when confusing parties, dates, or legal provisions can have material consequences. We introduce HalluGraph, a graph-theoretic framework that quantifies hallucinations through structural alignment between knowledge graphs extracted from context, query, and response. Our approach produces bounded, interpretable metrics decomposed into \textit{Entity Grounding} (EG), measuring whether entities in the response appear in source documents, and \textit{Relation Preservation} (RP), verifying that asserted relationships are supported by context. On structured control documents, HalluGraph achieves near-perfect discrimination ($>$400 words, $>$20 entities), HalluGraph achieves $AUC = 0.979$, while maintaining robust performance ($AUC \approx 0.89$) on challenging generative legal task, consistently outperforming semantic similarity baselines. The framework provides the transparency and traceability required for high-stakes legal applications, enabling full audit trails from generated assertions back to source passages.

中文标题: HalluGraph：基于知识图谱对齐的法律RAG系统可审计幻觉检测

中文摘要: 基于检索增强生成（RAG）的法律AI系统面临关键的问责挑战：当AI助手引用判例法、法规或合同条款时，从业者需要可验证的保证，确保生成的文本忠实地代表源文档。现有的幻觉检测器依赖容忍实体替换的语义相似度指标，这在混淆当事人、日期或法律条款时可能产生严重后果。我们引入了HalluGraph，这是一个基于图论的框架，通过从上下文、查询和响应中提取的知识图谱之间的结构对齐来量化幻觉。我们的方法产生有界、可解释的指标，分解为实体接地（EG）（衡量响应中的实体是否出现在源文档中）和关系保持（RP）（验证断言的关系是否得到上下文支持）。在结构化控制文档上（>400词，>20实体），HalluGraph达到AUC = 0.979，同时在具有挑战性的生成式法律任务上保持稳健性能（AUC ≈ 0.89），始终优于语义相似度基线方法。该框架为高风险法律应用提供了所需的透明度和可追溯性，能够从生成的断言追溯到源段落，实现完整的审计跟踪。

</details>


### [396] [Subgroup Validity in Machine Learning for Echocardiogram Data](https://arxiv.org/abs/2512.00976)
*Cynthia Feeney,Shane Williams,Benjamin S. Wessler,Michael C. Hughes*

Main category: cs.LG

TL;DR: 当前公开超声心动图数据集存在人口统计学信息报告不足问题，无法验证机器学习模型在不同亚组（性别、种族、民族）中的有效性，需要更多代表性数据和亚组分析来确保公平性。


<details>
  <summary>Details</summary>
Motivation: 超声心动图数据集可用于训练深度学习模型自动化解读心脏超声图像，但现有数据集存在人口统计学信息报告不足问题，包括性别、种族、民族等信息缺失，导致无法评估模型在不同亚组中的预测性能，这引发了亚组有效性的担忧。

Method: 1. 改善两个数据集（TMED-2和MIMIC-IV-ECHO）的社会人口统计学报告；2. 分析六个公开数据集的人口统计学覆盖情况；3. 对TMED-2数据集上已发表的主动脉瓣狭窄检测模型进行探索性亚组分析。

Result: 1. 当前公开数据集无法缓解亚组有效性担忧；2. 所有数据集均未考虑性别多样化患者；3. 许多种族和民族群体患者数量不足；4. 对主动脉瓣狭窄检测模型的亚组分析显示，在性别、种族和民族亚组中缺乏足够的亚组有效性证据。

Conclusion: 需要为代表性不足的亚组收集更多数据，改进人口统计学报告，并进行亚组聚焦分析，以证明未来工作中的亚组有效性。

Abstract: Echocardiogram datasets enable training deep learning models to automate interpretation of cardiac ultrasound, thereby expanding access to accurate readings of diagnostically-useful images. However, the gender, sex, race, and ethnicity of the patients in these datasets are underreported and subgroup-specific predictive performance is unevaluated. These reporting deficiencies raise concerns about subgroup validity that must be studied and addressed before model deployment. In this paper, we show that current open echocardiogram datasets are unable to assuage subgroup validity concerns. We improve sociodemographic reporting for two datasets: TMED-2 and MIMIC-IV-ECHO. Analysis of six open datasets reveals no consideration of gender-diverse patients and insufficient patient counts for many racial and ethnic groups. We further perform an exploratory subgroup analysis of two published aortic stenosis detection models on TMED-2. We find insufficient evidence for subgroup validity for sex, racial, and ethnic subgroups. Our findings highlight that more data for underrepresented subgroups, improved demographic reporting, and subgroup-focused analyses are needed to prove subgroup validity in future work.

中文标题: 机器学习在超声心动图数据中的亚组有效性

中文摘要: 超声心动图数据集能够训练深度学习模型来自动化解读心脏超声图像，从而扩大对诊断有用图像的准确解读的获取。然而，这些数据集中患者的性别、种族和民族信息报告不足，且亚组特异性预测性能未得到评估。这些报告缺陷引发了亚组有效性的担忧，必须在模型部署前加以研究和解决。在本文中，我们表明当前的公开超声心动图数据集无法缓解亚组有效性担忧。我们改进了两个数据集（TMED-2和MIMIC-IV-ECHO）的社会人口统计学报告。对六个公开数据集的分析显示，没有考虑性别多样化患者，且许多种族和民族群体的患者数量不足。我们进一步对TMED-2上两个已发表的主动脉瓣狭窄检测模型进行了探索性亚组分析。我们发现对于性别、种族和民族亚组缺乏足够的亚组有效性证据。我们的研究结果强调，需要为代表性不足的亚组收集更多数据、改进人口统计学报告并进行亚组聚焦分析，以证明未来工作中的亚组有效性。

</details>


### [397] [Agentic Policy Optimization via Instruction-Policy Co-Evolution](https://arxiv.org/abs/2512.01945)
*Han Zhou,Xingchen Wan,Ivan Vulić,Anna Korhonen*

Main category: cs.LG

TL;DR: INSPO是一个指令-策略协同进化框架，通过动态优化指令来提升LLM智能体的强化学习性能，避免静态指令的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法依赖静态手动设计的指令，但这些指令可能不是最优的，且随着智能体策略改进，最优指令会发生变化。需要动态指令优化机制来提升智能体性能。

Method: INSPO框架维护动态指令候选群体，通过RL循环中的奖励信号评估指令性能，定期修剪低性能指令，使用基于LLM的优化器分析回放缓冲区经验来生成新指令。

Result: INSPO在多轮检索和推理任务上显著优于依赖静态指令的基线方法，发现了创新的指令策略，以边际计算开销实现了显著性能提升。

Conclusion: 指令-策略协同进化是提升LLM智能体性能的有效方法，动态指令优化能够引导智能体走向更优的推理路径，INSPO框架为此提供了可行的解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.

中文标题: 基于指令-策略协同进化的智能体策略优化

中文摘要: 具有可验证奖励的强化学习（RLVR）提升了大型语言模型（LLMs）的推理能力，使得自主智能体能够进行有效的多轮推理和工具集成推理。虽然指令是定义智能体的主要协议，但RLVR通常依赖于静态和手动设计的指令。然而，这些指令对于基础模型可能不是最优的，并且随着智能体策略的改进和与环境交互的探索，最优指令可能会发生变化。为了弥合这一差距，我们引入了INSPO，一种新颖的指令-策略协同进化框架，将指令优化作为强化学习（RL）循环的动态组成部分。INSPO维护一个动态的指令候选群体，这些指令与问题一起采样，其中RL循环中的奖励信号会自动归因于每个指令，并定期修剪低性能指令。新指令通过一种基于策略的反思机制生成和验证，其中基于LLM的优化器分析来自回放缓冲区的过往经验，并根据当前策略演化出更有效的策略。我们在多轮检索和推理任务上进行了广泛的实验，证明INSPO显著优于依赖静态指令的强基线。INSPO发现了创新的指令，引导智能体走向更具战略性的推理路径，仅以边际计算开销实现了显著的性能提升。

</details>


### [398] [Operator-Theoretic Framework for Gradient-Free Federated Learning](https://arxiv.org/abs/2512.01025)
*Mohit Kumar,Mathias Brucker,Alexander Valentinitsch,Adnan Husakovic,Ali Abbas,Manuela Geiß,Bernhard A. Moser*

Main category: cs.LG

TL;DR: 提出基于算子理论的联邦学习框架，将L²最优解映射到再生核希尔伯特空间，实现无梯度学习。通过算子范数推导有限样本界，设计高效核机器，客户端通过标量空间折叠度量传输知识，减少通信并支持差分隐私。全局预测规则仅需整数最小值和相等比较操作，兼容全同态加密。在四个基准测试中，该方法匹配或优于基于梯度的微调，差分隐私下核平滑缓解精度损失。


<details>
  <summary>Details</summary>
Motivation: 联邦学习需要解决数据异质性、严格的通信计算限制和隐私保护问题，同时确保性能。现有基于梯度的方法面临通信开销大、隐私保护复杂、与全同态加密兼容性差等挑战，需要一种数学基础坚实、通信高效、隐私友好且性能可保证的替代方案。

Method: 提出算子理论框架：1) 通过前向算子将L²最优解映射到再生核希尔伯特空间；2) 用可用数据近似该解；3) 通过逆算子映射回原空间，实现无梯度方案。设计核仿射壳机器利用空间折叠特性，客户端通过标量空间折叠度量传输知识。差分隐私协议：从噪声扰动数据矩阵一步计算摘要，避免每轮裁剪和隐私计算。全局预测规则仅需整数最小值和相等比较操作。

Result: 在四个基准测试中，无梯度联邦学习方法匹配或优于强梯度微调方法，最高提升23.7分。差分隐私实验中，核平滑在高隐私机制下缓解精度损失。全局规则可实现全同态加密，每个测试点需要Q×C次加密最小值和C次相等比较操作，操作级基准显示实际延迟可行。

Conclusion: 该算子理论框架为联邦学习提供了可证明的保证、低通信开销、通过标量摘要支持隐私知识传输，并产生兼容全同态加密的预测规则，为异质性下的联邦学习提供了数学基础坚实的梯度替代方案。

Abstract: Federated learning must address heterogeneity, strict communication and computation limits, and privacy while ensuring performance. We propose an operator-theoretic framework that maps the $L^2$-optimal solution into a reproducing kernel Hilbert space (RKHS) via a forward operator, approximates it using available data, and maps back with the inverse operator, yielding a gradient-free scheme. Finite-sample bounds are derived using concentration inequalities over operator norms, and the framework identifies a data-dependent hypothesis space with guarantees on risk, error, robustness, and approximation. Within this space we design efficient kernel machines leveraging the space folding property of Kernel Affine Hull Machines. Clients transfer knowledge via a scalar space folding measure, reducing communication and enabling a simple differentially private protocol: summaries are computed from noise-perturbed data matrices in one step, avoiding per-round clipping and privacy accounting. The induced global rule requires only integer minimum and equality-comparison operations per test point, making it compatible with fully homomorphic encryption (FHE). Across four benchmarks, the gradient-free FL method with fixed encoder embeddings matches or outperforms strong gradient-based fine-tuning, with gains up to 23.7 points. In differentially private experiments, kernel smoothing mitigates accuracy loss in high-privacy regimes. The global rule admits an FHE realization using $Q \times C$ encrypted minimum and $C$ equality-comparison operations per test point, with operation-level benchmarks showing practical latencies. Overall, the framework provides provable guarantees with low communication, supports private knowledge transfer via scalar summaries, and yields an FHE-compatible prediction rule offering a mathematically grounded alternative to gradient-based federated learning under heterogeneity.

中文标题: 无梯度联邦学习的算子理论框架

中文摘要: 联邦学习必须解决异质性、严格的通信和计算限制以及隐私问题，同时确保性能。我们提出了一个算子理论框架，通过前向算子将L²最优解映射到再生核希尔伯特空间，利用可用数据近似它，并通过逆算子映射回来，产生无梯度方案。使用算子范数的集中不等式推导有限样本界，该框架识别出具有风险、误差、鲁棒性和近似保证的数据依赖假设空间。在此空间内，我们设计利用核仿射壳机器空间折叠特性的高效核机器。客户端通过标量空间折叠度量传输知识，减少通信并实现简单的差分隐私协议：摘要从噪声扰动数据矩阵一步计算，避免每轮裁剪和隐私计算。诱导的全局规则每个测试点仅需整数最小值和相等比较操作，使其与全同态加密兼容。在四个基准测试中，具有固定编码器嵌入的无梯度联邦学习方法匹配或优于强梯度微调，增益高达23.7分。在差分隐私实验中，核平滑在高隐私机制下缓解精度损失。全局规则允许使用每个测试点Q×C次加密最小值和C次相等比较操作实现全同态加密，操作级基准显示实际延迟可行。总体而言，该框架提供可证明的保证、低通信开销，通过标量摘要支持隐私知识传输，并产生兼容全同态加密的预测规则，为异质性下的联邦学习提供了数学基础坚实的梯度替代方案。

</details>


### [399] [Forecasting in Offline Reinforcement Learning for Non-stationary Environments](https://arxiv.org/abs/2512.01987)
*Suzan Ece Ada,Georg Martius,Emre Ugur,Erhan Oztop*

Main category: cs.LG

TL;DR: FORL框架结合条件扩散模型和零样本时间序列基础模型，解决离线强化学习在非平稳环境中的性能退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法假设环境平稳或仅考虑测试时的合成扰动，但现实世界环境存在突发、时变的偏移，导致部分可观测性和性能下降。

Method: 提出FORL框架，包含：1）基于条件扩散的候选状态生成（不预设未来非平稳模式）；2）零样本时间序列基础模型，针对非马尔可夫偏移环境。

Result: 在增强现实非平稳性的离线RL基准测试中，FORL相比竞争基线始终表现出性能提升。

Conclusion: FORL通过整合零样本预测与智能体经验，有效弥合了离线RL与现实世界非平稳环境复杂性之间的差距。

Abstract: Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time, assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific pattern of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience, we aim to bridge the gap between offline RL and the complexities of real-world, non-stationary environments.

中文标题: 非平稳环境下离线强化学习的预测方法

中文摘要: 离线强化学习（RL）为从预收集的数据集中训练策略提供了一条有前景的途径，特别是在收集额外交互数据不可行的情况下。然而，现有的离线RL方法通常假设环境是平稳的，或者仅在测试时考虑合成扰动，这些假设在现实世界中经常失效，因为现实环境通常以突发的、时变的偏移为特征。这些偏移可能导致部分可观测性，使智能体错误感知其真实状态并降低性能。为了克服这一挑战，我们引入了非平稳离线RL中的预测框架（FORL），该框架统一了（i）基于条件扩散的候选状态生成（训练时不预设任何特定的未来非平稳模式），以及（ii）零样本时间序列基础模型。FORL针对容易出现意外、可能非马尔可夫偏移的环境，要求智能体从每个episode开始就表现出鲁棒性能。在离线RL基准测试上的实证评估（使用真实世界时间序列数据增强以模拟现实非平稳性）表明，与竞争基线相比，FORL始终能提高性能。通过将零样本预测与智能体经验相结合，我们旨在弥合离线RL与现实世界非平稳环境复杂性之间的差距。

</details>


### [400] [AlignSAE: Concept-Aligned Sparse Autoencoders](https://arxiv.org/abs/2512.02004)
*Minglai Yang,Xinyu Guo,Mihai Surdeanu,Liangming Pan*

Main category: cs.LG

TL;DR: AlignSAE提出了一种通过"预训练-后训练"课程来对齐稀疏自编码器特征与人类定义概念的方法，解决了传统SAE特征与概念对齐不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型将事实知识编码在难以检查或控制的隐藏参数空间中。虽然稀疏自编码器可以将隐藏激活分解为更细粒度的可解释特征，但它们往往难以可靠地将这些特征与人类定义的概念对齐，导致纠缠和分布式的特征表示。

Method: AlignSAE采用"预训练-后训练"课程方法：首先进行无监督训练阶段，然后应用监督后训练，将特定概念绑定到专用的潜在槽位，同时保留其余容量用于一般重构。这种分离创建了一个可解释的界面，可以在其中检查和控制特定关系，而不受无关特征的干扰。

Result: 实证结果表明，AlignSAE能够实现精确的因果干预，例如通过针对单个语义对齐的槽位进行可靠的"概念交换"。

Conclusion: AlignSAE通过概念对齐的稀疏自编码器方法，为大型语言模型的可解释性和可控性提供了有效解决方案，实现了特征与人类定义概念的可靠对齐。

Abstract: Large Language Models (LLMs) encode factual knowledge within hidden parametric spaces that are difficult to inspect or control. While Sparse Autoencoders (SAEs) can decompose hidden activations into more fine-grained, interpretable features, they often struggle to reliably align these features with human-defined concepts, resulting in entangled and distributed feature representations. To address this, we introduce AlignSAE, a method that aligns SAE features with a defined ontology through a "pre-train, then post-train" curriculum. After an initial unsupervised training phase, we apply supervised post-training to bind specific concepts to dedicated latent slots while preserving the remaining capacity for general reconstruction. This separation creates an interpretable interface where specific relations can be inspected and controlled without interference from unrelated features. Empirical results demonstrate that AlignSAE enables precise causal interventions, such as reliable "concept swaps", by targeting single, semantically aligned slots.

中文标题: AlignSAE：概念对齐的稀疏自编码器

中文摘要: 大型语言模型（LLMs）将事实知识编码在难以检查或控制的隐藏参数空间中。虽然稀疏自编码器（SAEs）可以将隐藏激活分解为更细粒度、可解释的特征，但它们往往难以可靠地将这些特征与人类定义的概念对齐，导致纠缠和分布式的特征表示。为了解决这个问题，我们引入了AlignSAE，这是一种通过"预训练，然后后训练"课程来将SAE特征与定义的本体对齐的方法。在初始的无监督训练阶段之后，我们应用监督后训练将特定概念绑定到专用的潜在槽位，同时保留其余容量用于一般重构。这种分离创建了一个可解释的界面，可以在其中检查和控制特定关系，而不受无关特征的干扰。实证结果表明，AlignSAE能够实现精确的因果干预，例如通过针对单个语义对齐的槽位进行可靠的"概念交换"。

</details>


### [401] [PIANO: Physics-informed Dual Neural Operator for Precipitation Nowcasting](https://arxiv.org/abs/2512.01062)
*Seokhyun Chin,Junghwan Park,Woojin Cho*

Main category: cs.LG

TL;DR: PIANO是一种物理信息双神经算子模型，利用卫星图像进行降水临近预报，通过物理约束提高预测精度和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 当前降水临近预报方法计算成本高且限制多，许多国家难以获得。需要开发更准确、物理一致且可访问的降水预报方法。

Method: 提出物理信息双神经算子(PIANO)结构，在训练中通过PINN损失强制执行平流-扩散基本方程来预测卫星图像，然后使用生成模型将卫星图像转换为雷达图像用于降水预报。

Result: 与基线模型相比，PIANO在中度降水(4mm/h)和短期强降水(8mm/h)事件预测方面有显著改进，预测的季节变异性低，表明具有良好的泛化鲁棒性。

Conclusion: PIANO在物理信息降水临近预报方面具有潜力，为相关研究提供了良好的基准。

Abstract: Precipitation nowcasting, key for early warning of disasters, currently relies on computationally expensive and restrictive methods that limit access to many countries. To overcome this challenge, we propose precipitation nowcasting using satellite imagery with physics constraints for improved accuracy and physical consistency. We use a novel physics-informed dual neural operator (PIANO) structure to enforce the fundamental equation of advection-diffusion during training to predict satellite imagery using a PINN loss. Then, we use a generative model to convert satellite images to radar images, which are used for precipitation nowcasting. Compared to baseline models, our proposed model shows a notable improvement in moderate (4mm/h) precipitation event prediction alongside short-term heavy (8mm/h) precipitation event prediction. It also demonstrates low seasonal variability in predictions, indicating robustness for generalization. This study suggests the potential of the PIANO and serves as a good baseline for physics-informed precipitation nowcasting.

中文标题: PIANO：用于降水临近预报的物理信息双神经算子

中文摘要: 降水临近预报是灾害早期预警的关键，目前依赖于计算成本高且限制性强的传统方法，这限制了许多国家的获取。为了克服这一挑战，我们提出使用卫星图像进行降水临近预报，并加入物理约束以提高准确性和物理一致性。我们采用一种新颖的物理信息双神经算子(PIANO)结构，在训练过程中通过PINN损失强制执行平流-扩散基本方程来预测卫星图像。然后，我们使用生成模型将卫星图像转换为雷达图像，用于降水临近预报。与基线模型相比，我们提出的模型在中度降水(4mm/h)事件预测和短期强降水(8mm/h)事件预测方面都显示出显著改进。它还表现出预测的季节变异性低，表明具有良好的泛化鲁棒性。这项研究表明PIANO具有潜力，并为物理信息降水临近预报提供了良好的基准。

</details>


### [402] [World Model Robustness via Surprise Recognition](https://arxiv.org/abs/2512.01119)
*Geigh Zollicoffer,Tanush Chopra,Mingkuan Yan,Xiaoxu Ma,Kenneth Eaton,Mark Riedl*

Main category: cs.LG

TL;DR: 该论文提出了一种利用世界模型固有"惊讶度"来减少噪声影响的算法，通过多表示和单表示拒绝采样技术，在自动驾驶模拟环境中保持智能体性能，增强了不同架构世界模型的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中部署的AI系统面临各种干扰和分布外噪声，这些噪声会破坏策略稳定性并导致不安全行为。虽然鲁棒训练可以减少对某些噪声的敏感性，但无法预测所有可能的OOD条件。因此需要开发一种方法来减轻噪声对基于世界模型的强化学习智能体的影响。

Method: 开发了一种利用世界模型固有惊讶度来减少噪声影响的算法。引入了两种拒绝采样技术：多表示拒绝采样（针对多个故障传感器）和单表示拒绝采样（针对单个故障传感器）。该方法通过世界模型对输入的惊讶程度来识别和过滤噪声。

Result: 在多种噪声类型和水平的自动驾驶模拟环境（CARLA和Safety Gymnasium）中，该方法相对于基线保持了智能体性能。同时增强了两种不同架构的最先进世界模型（Cosmos和DreamerV3）的稳定性，证明了该方法在不同世界建模领域的鲁棒性。

Conclusion: 通过利用世界模型的惊讶度识别机制，可以有效减轻噪声对强化学习智能体的影响。该方法在多种环境和不同世界模型架构中都表现出良好的鲁棒性，为解决现实世界AI系统的噪声问题提供了有效途径。

Abstract: AI systems deployed in the real world must contend with distractions and out-of-distribution (OOD) noise that can destabilize their policies and lead to unsafe behavior. While robust training can reduce sensitivity to some forms of noise, it is infeasible to anticipate all possible OOD conditions. To mitigate this issue, we develop an algorithm that leverages a world model's inherent measure of surprise to reduce the impact of noise in world model--based reinforcement learning agents. We introduce both multi-representation and single-representation rejection sampling, enabling robustness to settings with multiple faulty sensors or a single faulty sensor. While the introduction of noise typically degrades agent performance, we show that our techniques preserve performance relative to baselines under varying types and levels of noise across multiple environments within self-driving simulation domains (CARLA and Safety Gymnasium). Furthermore, we demonstrate that our methods enhance the stability of two state-of-the-art world models with markedly different underlying architectures: Cosmos and DreamerV3. Together, these results highlight the robustness of our approach across world modeling domains. We release our code at https://github.com/Bluefin-Tuna/WISER .

中文标题: 通过惊讶识别增强世界模型的鲁棒性

中文摘要: 部署在现实世界中的AI系统必须应对可能破坏其策略稳定性并导致不安全行为的干扰和分布外噪声。虽然鲁棒训练可以减少对某些形式噪声的敏感性，但预测所有可能的OOD条件是不可行的。为了缓解这个问题，我们开发了一种算法，利用世界模型固有的惊讶度来减少基于世界模型的强化学习智能体中噪声的影响。我们引入了多表示和单表示拒绝采样，使其能够适应多个故障传感器或单个故障传感器的设置。虽然噪声的引入通常会降低智能体性能，但我们展示了在自动驾驶模拟领域（CARLA和Safety Gymnasium）的多个环境中，针对不同类型和水平的噪声，我们的技术相对于基线保持了性能。此外，我们证明了我们的方法增强了两种具有显著不同底层架构的最先进世界模型（Cosmos和DreamerV3）的稳定性。这些结果共同突显了我们的方法在世界建模领域的鲁棒性。我们在https://github.com/Bluefin-Tuna/WISER发布了代码。

</details>


### [403] [Open-Set Domain Adaptation Under Background Distribution Shift: Challenges and A Provably Efficient Solution](https://arxiv.org/abs/2512.01152)
*Shravan Chaudhari,Yoav Wald,Suchi Saria*

Main category: cs.LG

TL;DR: 本文提出了一种名为OURS的方法，用于解决在背景分布漂移情况下的开集识别问题，该方法在理论和实验上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器学习系统面临数据漂移的挑战，包括新类别的出现（开集识别）和已知类别分布的变化。现有开集识别方法大多假设背景分布固定，无法处理背景分布漂移的情况。

Method: 开发了OURS方法，该方法在背景分布漂移的情况下仍能保证解决开集识别问题。方法基于新类别与非新类别可分离的良性假设，并提供了理论保证。同时开发了使方法可扩展和鲁棒的技术。

Result: 在图像和文本数据上的综合实验表明，OURS在背景漂移情况下显著优于现有的开集识别方法。同时提供了关于新类别规模等因素如何影响性能的新见解。

Conclusion: OURS方法能够有效处理背景分布漂移下的开集识别问题，在理论和实践上都表现出色，为这一具有挑战性的问题提供了可行的解决方案。

Abstract: As we deploy machine learning systems in the real world, a core challenge is to maintain a model that is performant even as the data shifts. Such shifts can take many forms: new classes may emerge that were absent during training, a problem known as open-set recognition, and the distribution of known categories may change. Guarantees on open-set recognition are mostly derived under the assumption that the distribution of known classes, which we call \emph{the background distribution}, is fixed. In this paper we develop \ours{}, a method that is guaranteed to solve open-set recognition even in the challenging case where the background distribution shifts. We prove that the method works under benign assumptions that the novel class is separable from the non-novel classes, and provide theoretical guarantees that it outperforms a representative baseline in a simplified overparameterized setting. We develop techniques to make \ours{} scalable and robust, and perform comprehensive empirical evaluations on image and text data. The results show that \ours{} significantly outperforms existing open-set recognition methods under background shift. Moreover, we provide new insights into how factors such as the size of the novel class influences performance, an aspect that has not been extensively explored in prior work.

中文标题: 背景分布漂移下的开集域适应：挑战与可证明高效解决方案

中文摘要: 随着机器学习系统在现实世界中的部署，一个核心挑战是即使在数据漂移时也能保持模型的性能。这种漂移可以采取多种形式：训练期间不存在的新类别可能出现，这被称为开集识别；已知类别的分布也可能发生变化。开集识别的保证大多是在已知类别分布（我们称之为背景分布）固定的假设下推导出来的。在本文中，我们开发了OURS方法，即使在背景分布漂移的挑战性情况下，也能保证解决开集识别问题。我们证明了该方法在新类别与非新类别可分离的良性假设下有效，并在简化的过参数化设置中提供了优于代表性基线的理论保证。我们开发了使OURS可扩展和鲁棒的技术，并在图像和文本数据上进行了全面的实证评估。结果表明，在背景漂移情况下，OURS显著优于现有的开集识别方法。此外，我们提供了关于新类别规模等因素如何影响性能的新见解，这是先前工作中未广泛探索的方面。

</details>


### [404] [From Regression to Classification: Exploring the Benefits of Categorical Representations of Energy in MLIPs](https://arxiv.org/abs/2512.01160)
*Ahmad Ali*

Main category: cs.LG

TL;DR: 该论文探索了在机器学习原子间势能（MLIPs）中，将传统的标量回归方法改为多类别分类方法，通过预测能量/力的分类分布来提供更丰富的监督信号和不确定性量化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的MLIPs使用标量回归方法预测单个能量值，虽然计算成本低但缺乏不确定性量化能力。作者希望探索分类表示方法，通过预测能量/力的分类分布来提供更丰富的监督信号，并实现模型不确定性的量化。

Method: 提出多类别分类方法：预测能量/力的分类分布，将标量目标转换为直方图表示，使用交叉熵损失训练模型，通过预测分布的熵来量化认知不确定性。

Result: 分类方法在绝对误差性能上可与回归基线相媲美，同时能够通过预测分布的熵来量化认知不确定性，这是标量回归方法所不具备的能力。

Conclusion: 分类表示方法为MLIPs提供了有前景的替代方案，不仅保持了与回归方法相当的准确性，还增加了不确定性量化能力，为模型置信度评估提供了新途径。

Abstract: Density Functional Theory (DFT) is a widely used computational method for estimating the energy and behavior of molecules. Machine Learning Interatomic Potentials (MLIPs) are models trained to approximate DFT-level energies and forces at dramatically lower computational cost. Many modern MLIPs rely on a scalar regression formulation; given information about a molecule, they predict a single energy value and corresponding forces while minimizing absolute error with DFT's calculations. In this work, we explore a multi-class classification formulation that predicts a categorical distribution over energy/force values, providing richer supervision through multiple targets. Most importantly, this approach offers a principled way to quantify model uncertainty.
  In particular, our method predicts a histogram of the energy/force distribution, converts scalar targets into histograms, and trains the model using cross-entropy loss. Our results demonstrate that this categorical formulation can achieve absolute error performance comparable to regression baselines. Furthermore, this representation enables the quantification of epistemic uncertainty through the entropy of the predicted distribution, offering a measure of model confidence absent in scalar regression approaches.

中文标题: 从回归到分类：探索能量分类表示在机器学习原子间势能中的优势

中文摘要: 密度泛函理论（DFT）是一种广泛用于估计分子能量和行为的计算方法。机器学习原子间势能（MLIPs）是训练用于以显著降低的计算成本近似DFT级能量和力的模型。许多现代MLIPs依赖于标量回归公式；给定分子信息，它们预测单个能量值和相应的力，同时最小化与DFT计算的绝对误差。在这项工作中，我们探索了一种多类别分类公式，预测能量/力值的分类分布，通过多个目标提供更丰富的监督。最重要的是，这种方法提供了一种量化模型不确定性的原则性方法。

具体而言，我们的方法预测能量/力分布的直方图，将标量目标转换为直方图，并使用交叉熵损失训练模型。我们的结果表明，这种分类公式可以实现与回归基线相当的绝对误差性能。此外，这种表示通过预测分布的熵能够量化认知不确定性，提供了标量回归方法中缺乏的模型置信度度量。

</details>


### [405] [2D-ThermAl: Physics-Informed Framework for Thermal Analysis of Circuits using Generative AI](https://arxiv.org/abs/2512.01163)
*Soumyadeep Chandra,Sayeed Shafayet Chowdhury,Kaushik Roy*

Main category: cs.LG

TL;DR: ThermAl是一个基于生成式AI的物理信息框架，用于电路热分析，能够直接从活动配置文件快速准确地预测全芯片温度分布，比传统FEM方法快约200倍，精度达到0.71°C RMSE。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路中热分析变得日益重要，因为非均匀功耗和高晶体管密度会导致温度峰值和可靠性问题。传统FEM仿真虽然准确但计算成本过高，不适合早期设计阶段，导致需要多次迭代设计来解决热问题。需要一种快速准确的早期热分析方法。

Method: 提出ThermAl框架，采用混合U-Net架构，增强位置编码和玻尔兹曼正则化以保持物理一致性。使用COMSOL生成的大量功耗分布图数据集进行训练，涵盖从简单逻辑门到复杂设计的各种电路。

Result: ThermAl在大型电路上实现0.71°C的RMSE精度，比传统FEM工具快约200倍。在扩展温度范围（25-95°C）的交叉验证中保持高精度（<2.2%满量程RMSE），即使在高温应力场景下也表现良好。

Conclusion: ThermAl为早期电路设计阶段提供了一种快速准确的热分析方法，能够有效识别热点和学习热模式，适用于大规模EDA工作流程，在保持物理保真度的同时显著提高了分析效率。

Abstract: Thermal analysis is increasingly critical in modern integrated circuits, where non-uniform power dissipation and high transistor densities can cause rapid temperature spikes and reliability concerns. Traditional methods, such as FEM-based simulations offer high accuracy but computationally prohibitive for early-stage design, often requiring multiple iterative redesign cycles to resolve late-stage thermal failures. To address these challenges, we propose 'ThermAl', a physics-informed generative AI framework which effectively identifies heat sources and estimates full-chip transient and steady-state thermal distributions directly from input activity profiles. ThermAl employs a hybrid U-Net architecture enhanced with positional encoding and a Boltzmann regularizer to maintain physical fidelity. Our model is trained on an extensive dataset of heat dissipation maps, ranging from simple logic gates (e.g., inverters, NAND, XOR) to complex designs, generated via COMSOL. Experimental results demonstrate that ThermAl delivers precise temperature mappings for large circuits, with a root mean squared error (RMSE) of only 0.71°C, and outperforms conventional FEM tools by running up to ~200 times faster. We analyze performance across diverse layouts and workloads, and discuss its applicability to large-scale EDA workflows. While thermal reliability assessments often extend beyond 85°C for post-layout signoff, our focus here is on early-stage hotspot detection and thermal pattern learning. To ensure generalization beyond the nominal operating range 25-55°C, we additionally performed cross-validation on an extended dataset spanning 25-95°C maintaining a high accuracy (<2.2% full-scale RMSE) even under elevated temperature conditions representative of peak power and stress scenarios.

中文标题: 2D-ThermAl：基于生成式AI的电路热分析物理信息框架

中文摘要: 热分析在现代集成电路中变得越来越关键，非均匀功耗分布和高晶体管密度可能导致快速温度峰值和可靠性问题。传统方法如基于FEM的仿真虽然精度高，但在早期设计阶段计算成本过高，通常需要多次迭代重新设计来解决后期热故障。为解决这些挑战，我们提出了"ThermAl"，一个物理信息的生成式AI框架，能够直接从输入活动配置文件中有效识别热源并估计全芯片瞬态和稳态热分布。ThermAl采用混合U-Net架构，通过位置编码和玻尔兹曼正则化增强以保持物理保真度。我们的模型在广泛的功耗分布图数据集上进行训练，范围从简单逻辑门（如反相器、NAND、XOR）到复杂设计，通过COMSOL生成。实验结果表明，ThermAl能够为大型电路提供精确的温度映射，均方根误差仅为0.71°C，并且比传统FEM工具运行速度快约200倍。我们分析了不同布局和工作负载下的性能，并讨论了其在大规模EDA工作流程中的适用性。虽然热可靠性评估通常需要超过85°C进行布局后签核，但本文的重点是早期热点检测和热模式学习。为确保在标称工作范围25-55°C之外的泛化能力，我们还在扩展数据集（25-95°C）上进行了交叉验证，即使在代表峰值功率和应力场景的高温条件下仍保持高精度（<2.2%满量程RMSE）。

</details>


### [406] [A TinyML Reinforcement Learning Approach for Energy-Efficient Light Control in Low-Cost Greenhouse Systems](https://arxiv.org/abs/2512.01167)
*Mohamed Abdallah Salem,Manuel Cuevas Perez,Ahmed Harb Rabia*

Main category: cs.LG

TL;DR: 本研究提出了一种基于强化学习的自适应照明控制策略，使用低功耗微控制器实现。系统采用无模型Q学习算法，根据光敏电阻传感器的实时反馈动态调节LED亮度，能够在13个不同光照强度级别上稳定运行，实验验证了轻量级、设备端强化学习在节能照明控制中的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统温室照明控制系统通常采用固定或简单的控制策略，缺乏自适应性和能源效率。在低成本、资源受限的农业系统中，需要一种轻量级、智能的控制方案，能够根据环境条件动态调整照明，同时降低能耗和硬件成本。

Method: 采用基于无模型Q学习的强化学习算法，在低功耗微控制器上实现。系统使用光敏电阻传感器实时监测光照强度，通过Q学习算法动态调整LED亮度。状态空间包含64个状态，目标是在13个不同光照强度级别上稳定运行。进行了130次试验，每个目标级别10次，评估收敛速度、步数和达到目标状态所需时间。

Result: 实验结果表明，智能体能够有效学习在不同光照级别上稳定运行，具有最小的过冲和平滑收敛特性。即使在存在环境扰动的情况下，系统也能保持稳定。通过箱线图和直方图分析训练时间和学习效率的分布，验证了系统的可靠性和效率。

Conclusion: 本研究证明了轻量级、设备端强化学习在节能照明控制中的可行性，为资源受限农业系统中的多模态环境控制应用奠定了基础。该方法能够在低成本硬件上实现智能照明控制，提高能源效率并降低运营成本。

Abstract: This study presents a reinforcement learning (RL)-based control strategy for adaptive lighting regulation in controlled environments using a low-power microcontroller. A model-free Q-learning algorithm was implemented to dynamically adjust the brightness of a Light-Emitting Diode (LED) based on real-time feedback from a light-dependent resistor (LDR) sensor. The system was trained to stabilize at 13 distinct light intensity levels (L1 to L13), with each target corresponding to a specific range within the 64-state space derived from LDR readings. A total of 130 trials were conducted, covering all target levels with 10 episodes each. Performance was evaluated in terms of convergence speed, steps taken, and time required to reach target states. Box plots and histograms were generated to analyze the distribution of training time and learning efficiency across targets. Experimental validation demonstrated that the agent could effectively learn to stabilize at varying light levels with minimal overshooting and smooth convergence, even in the presence of environmental perturbations. This work highlights the feasibility of lightweight, on-device RL for energy-efficient lighting control and sets the groundwork for multi-modal environmental control applications in resource-constrained agricultural systems.

中文标题: 基于TinyML强化学习的低成本温室系统节能照明控制方法

中文摘要: 本研究提出了一种基于强化学习的控制策略，用于在受控环境中使用低功耗微控制器实现自适应照明调节。采用无模型Q学习算法，根据光敏电阻传感器的实时反馈动态调整发光二极管的亮度。系统被训练在13个不同的光照强度级别上稳定运行，每个目标对应从LDR读数导出的64状态空间中的特定范围。总共进行了130次试验，覆盖所有目标级别，每个级别10次。性能评估包括收敛速度、所需步数和达到目标状态所需时间。生成箱线图和直方图来分析训练时间和学习效率在不同目标间的分布。实验验证表明，智能体能够有效学习在不同光照级别上稳定运行，具有最小的过冲和平滑收敛特性，即使在存在环境扰动的情况下也是如此。这项工作突显了轻量级、设备端强化学习在节能照明控制中的可行性，并为资源受限农业系统中的多模态环境控制应用奠定了基础。

</details>


### [407] [Data assimilation and discrepancy modeling with shallow recurrent decoders](https://arxiv.org/abs/2512.01170)
*Yuxuan Bao,J. Nathan Kutz*

Main category: cs.LG

TL;DR: DA-SHRED是一个机器学习框架，通过浅层循环解码器将仿真模型与稀疏传感器数据结合，实现高维物理系统的数据同化，同时识别和恢复仿真模型中缺失的动态过程。


<details>
  <summary>Details</summary>
Motivation: 现代传感需求要求数据高效、实时处理，并在有限传感覆盖下部署。仿真模型往往忽略小尺度过程、对扰动敏感或过度简化参数，导致与传感器测量的现实存在偏差，因此需要将观测数据与仿真模型结合的数据同化方法。

Method: 提出DA-SHRED框架：1）利用SHRED从简化仿真模型学习潜在空间表示；2）使用真实传感器数据更新潜在变量以重建完整系统状态；3）在潜在空间中采用基于稀疏识别非线性动态的回归模型，识别仿真模型中缺失的动态函数。

Result: DA-SHRED成功弥合了仿真到现实的差距，能够准确重建高维物理系统的完整状态，并有效恢复复杂系统中缺失的动态过程，证明了高效时间编码与物理信息校正结合的有效性。

Conclusion: DA-SHRED框架通过结合浅层循环解码器和物理信息校正，实现了稳健的数据同化，不仅能够准确重建复杂物理系统的完整状态，还能识别和恢复仿真模型中缺失的动态，为解决仿真与现实的差距问题提供了有效方案。

Abstract: The requirements of modern sensing are rapidly evolving, driven by increasing demands for data efficiency, real-time processing, and deployment under limited sensing coverage. Complex physical systems are often characterized through the integration of a limited number of point sensors in combination with scientific computations which approximate the dominant, full-state dynamics. Simulation models, however, inevitably neglect small-scale or hidden processes, are sensitive to perturbations, or oversimplify parameter correlations, leading to reconstructions that often diverge from the reality measured by sensors. This creates a critical need for data assimilation, the process of integrating observational data with predictive simulation models to produce coherent and accurate estimates of the full state of complex physical systems. We propose a machine learning framework for Data Assimilation with a SHallow REcurrent Decoder (DA-SHRED) which bridges the simulation-to-real (SIM2REAL) gap between computational modeling and experimental sensor data. For real-world physics systems modeling high-dimensional spatiotemporal fields, where the full state cannot be directly observed and must be inferred from sparse sensor measurements, we leverage the latent space learned from a reduced simulation model via SHRED, and update these latent variables using real sensor data to accurately reconstruct the full system state. Furthermore, our algorithm incorporates a sparse identification of nonlinear dynamics based regression model in the latent space to identify functionals corresponding to missing dynamics in the simulation model. We demonstrate that DA-SHRED successfully closes the SIM2REAL gap and additionally recovers missing dynamics in highly complex systems, demonstrating that the combination of efficient temporal encoding and physics-informed correction enables robust data assimilation.

中文标题: 基于浅层循环解码器的数据同化和差异建模

中文摘要: 现代传感需求正在快速发展，这受到对数据效率、实时处理以及在有限传感覆盖下部署的日益增长需求的推动。复杂物理系统通常通过整合有限数量的点传感器与科学计算相结合来表征，这些计算近似了主导的完整状态动态。然而，仿真模型不可避免地忽略了小尺度或隐藏过程，对扰动敏感，或者过度简化了参数相关性，导致重建结果常常与传感器测量的现实情况存在偏差。这产生了对数据同化的关键需求，即将观测数据与预测仿真模型相结合，以产生复杂物理系统完整状态的连贯且准确的估计。我们提出了一个用于数据同化的机器学习框架，采用浅层循环解码器（DA-SHRED），该框架弥合了计算建模与实验传感器数据之间的仿真到现实（SIM2REAL）差距。对于建模高维时空场的现实世界物理系统，其中完整状态无法直接观测，必须从稀疏传感器测量中推断，我们利用通过SHRED从简化仿真模型中学到的潜在空间，并使用真实传感器数据更新这些潜在变量，以准确重建完整系统状态。此外，我们的算法在潜在空间中结合了基于稀疏识别非线性动态的回归模型，以识别与仿真模型中缺失动态相对应的函数。我们证明DA-SHRED成功弥合了SIM2REAL差距，并额外恢复了高度复杂系统中缺失的动态，表明高效时间编码和物理信息校正的结合能够实现稳健的数据同化。

</details>


### [408] [First On-Orbit Demonstration of a Geospatial Foundation Model](https://arxiv.org/abs/2512.01181)
*Andrew Du,Roberto Del Prete,Alejandro Mousist,Nick Manser,Fabrice Marre,Andrew Barton,Carl Seubert,Gabriele Meoni,Tat-Jun Chin*

Main category: cs.LG

TL;DR: 本文展示了首个在轨运行的地理空间基础模型，通过模型压缩和领域适应技术，使大型GeoFM能够在资源受限的航天硬件上部署，并在国际空间站上成功验证。


<details>
  <summary>Details</summary>
Motivation: 地理空间基础模型在遥感任务中具有广泛泛化能力，但其大尺寸成为在资源受限的航天硬件上部署的主要障碍。需要开发紧凑型模型以实现在轨AI处理。

Method: 开发了基于Vision Transformer的紧凑型地理空间基础模型变体，采用模型压缩和领域适应技术，在保持下游任务性能的同时减小模型尺寸和资源需求。

Result: 在五个下游任务中评估显示，压缩模型保持了高性能；在两个代表性飞行环境中验证成功；在国际空间站的IMAGIN-e载荷上实现了可靠的在轨推理。

Conclusion: 模型压缩和领域适应是减小地理空间基础模型尺寸和资源需求的关键，同时保持操作条件下的高性能。这为从大型GeoFM到飞行就绪、资源高效部署提供了可行路径。

Abstract: Geospatial foundation models (GeoFMs) promise broad generalisation capacity for Earth observation (EO) tasks, particularly under data-limited conditions. However, their large size poses a barrier to deployment on resource-constrained space hardware. To address this, we present compact variants of a Vision Transformer (ViT)-based GeoFM that preserve downstream task performance while enabling onboard execution. Evaluation across five downstream tasks and validation in two representative flight environments show that model compression and domain adaptation are critical to reducing size and resource demands while maintaining high performance under operational conditions. We further demonstrate reliable on-orbit inference with the IMAGIN-e payload aboard the International Space Station. These results establish a pathway from large GeoFMs to flight-ready, resource-efficient deployments, expanding the feasibility of onboard AI for EO missions.

中文标题: 地理空间基础模型首次在轨演示

中文摘要: 地理空间基础模型在地球观测任务中展现出广泛的泛化能力，特别是在数据有限条件下。然而，其大尺寸成为在资源受限的航天硬件上部署的障碍。为此，我们提出了基于视觉变换器的紧凑型地理空间基础模型变体，在保持下游任务性能的同时实现机载执行。通过对五个下游任务的评估和两个代表性飞行环境的验证表明，模型压缩和领域适应对于减小尺寸和资源需求、同时在操作条件下保持高性能至关重要。我们进一步在国际空间站的IMAGIN-e载荷上展示了可靠的在轨推理能力。这些结果为从大型地理空间基础模型到飞行就绪、资源高效部署建立了可行路径，扩展了机载AI在地球观测任务中的可行性。

</details>


### [409] [Teaching by Failure: Counter-Example-Driven Curricula for Transformer Self-Improvement](https://arxiv.org/abs/2512.01187)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: CEDC框架通过迭代识别模型自身失败案例并针对性训练，显著提升Transformer模型在长度外推和复杂结构任务上的鲁棒性，相比传统方法效率更高且无需人工难度标注。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在遇到比训练数据更长或结构更复杂的输入时，往往表现出脆弱的泛化能力，容易失败。现有方法如静态训练和标准课程学习需要人工设计难度启发式，效率低下且效果有限。

Method: 提出Counter-Example-Driven Curricula (CEDC)框架：1) 使用当前模型生成多样化候选问题；2) 通过快速可执行的验证器识别错误预测（反例）；3) 在包含这些失败案例的数据集上微调模型；4) 迭代重复此过程，形成自适应课程。

Result: 在整数加法、排序、Dyck-2语言识别和三个文本分类基准测试中，CEDC相比静态训练和标准课程学习基线：1) 长度外推能力提升高达30倍；2) 计算效率比均匀数据增强高3.75倍；3) 无需人工难度启发式；4) 反例分析显示课程能自适应地针对更复杂的错误模式。

Conclusion: 验证器引导的失败驱动学习是一个简单、强大且高效的范式，能显著增强Transformer模型的泛化能力。CEDC通过自动聚焦于模型自身失败案例，实现了鲁棒性的持续提升。

Abstract: Transformer models often exhibit brittle extrapolation, failing on inputs that are longer or structurally more complex than those seen during training. We introduce Counter-Example-Driven Curricula (CEDC), an automated framework that improves model robustness by iteratively focusing on its own failures. At each step, CEDC uses the current model to generate a diverse set of candidate problems, employs a fast, executable verifier to identify incorrect predictions (counter-examples), and then fine-tunes the model on a dataset enriched with these discovered failures. We evaluate CEDC on a suite of algorithmic and natural language tasks, including integer addition, sorting, Dyck-2 language recognition, and three text classification benchmarks. Compared to static training and standard curriculum learning baselines, CEDC achieves up to 30x greater length extrapolation, is 3.75x more computationally efficient than uniform data augmentation, and requires no manual difficulty heuristics. We provide a detailed analysis of the counter-examples, showing how the curriculum naturally adapts to target progressively more complex error modes. Our findings establish verifier-guided, failure-driven learning as a simple, powerful, and efficient paradigm for enhancing the generalization capabilities of Transformer models.

中文标题: 通过失败教学：基于反例驱动的Transformer自改进课程学习

中文摘要: Transformer模型通常表现出脆弱的泛化能力，在处理比训练数据更长或结构更复杂的输入时容易失败。我们提出了反例驱动课程学习(CEDC)，这是一个通过迭代聚焦于模型自身失败来提升模型鲁棒性的自动化框架。在每一步中，CEDC使用当前模型生成多样化候选问题，利用快速可执行的验证器识别错误预测（反例），然后在包含这些发现失败的数据集上微调模型。我们在算法和自然语言任务套件上评估CEDC，包括整数加法、排序、Dyck-2语言识别和三个文本分类基准。与静态训练和标准课程学习基线相比，CEDC实现了高达30倍的长度外推能力，计算效率比均匀数据增强高3.75倍，且无需人工难度启发式。我们对反例进行了详细分析，展示了课程如何自然地适应针对逐渐更复杂的错误模式。我们的研究确立了验证器引导的失败驱动学习作为增强Transformer模型泛化能力的简单、强大且高效的范式。

</details>


### [410] [LGDC: Latent Graph Diffusion via Spectrum-Preserving Coarsening](https://arxiv.org/abs/2512.01190)
*Nagham Osman,Keyue Jiang,Davide Buffelli,Xiaowen Dong,Laura Toni*

Main category: cs.LG

TL;DR: LGDC是一种混合图生成框架，结合自回归模型（擅长局部结构）和扩散模型（擅长全局模式）的优势，通过谱保持粗化-反粗化实现高效生成。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法存在局限性：自回归模型能很好捕捉局部结构但效率较低，而扩散模型擅长全局模式但可能忽略细节。需要一种能同时兼顾局部和全局属性且高效的生成方法。

Method: 提出LGDC框架：1）使用谱保持粗化将原始图映射到潜在空间；2）在潜在空间用扩散模型生成潜在图；3）通过反粗化将潜在图扩展回完整图。这种双向映射保留了图的谱特性。

Result: 实验表明LGDC在Tree数据集（局部结构重要）上性能与自回归模型相当，在Planar和Community-20数据集（全局结构重要）上性能与扩散模型相当，验证了混合方法的有效性。

Conclusion: LGDC成功结合了自回归和扩散模型的优势，通过谱保持粗化-反粗化机制实现了同时捕捉局部和全局图属性的高效生成，为图生成任务提供了新的混合范式。

Abstract: Graph generation is a critical task across scientific domains. Existing methods fall broadly into two categories: autoregressive models, which iteratively expand graphs, and one-shot models, such as diffusion, which generate the full graph at once. In this work, we provide an analysis of these two paradigms and reveal a key trade-off: autoregressive models stand out in capturing fine-grained local structures, such as degree and clustering properties, whereas one-shot models excel at modeling global patterns, such as spectral distributions. Building on this, we propose LGDC (latent graph diffusion via spectrum-preserving coarsening), a hybrid framework that combines strengths of both approaches. LGDC employs a spectrum-preserving coarsening-decoarsening to bidirectionally map between graphs and a latent space, where diffusion efficiently generates latent graphs before expansion restores detail. This design captures both local and global properties with improved efficiency. Empirically, LGDC matches autoregressive models on locally structured datasets (Tree) and diffusion models on globally structured ones (Planar, Community-20), validating the benefits of hybrid generation.

中文标题: LGDC：通过谱保持粗化的潜在图扩散

中文摘要: 图生成是跨科学领域的关键任务。现有方法大致分为两类：自回归模型，迭代扩展图；以及一次性模型，如扩散模型，一次性生成完整图。在这项工作中，我们对这两种范式进行分析，揭示了一个关键权衡：自回归模型在捕捉细粒度局部结构（如度和聚类属性）方面表现出色，而一次性模型在建模全局模式（如谱分布）方面表现优异。基于此，我们提出LGDC（通过谱保持粗化的潜在图扩散），这是一个结合两种方法优势的混合框架。LGDC采用谱保持粗化-反粗化技术，在图和潜在空间之间进行双向映射，其中扩散有效生成潜在图，然后通过扩展恢复细节。这种设计以更高的效率同时捕捉局部和全局属性。实证表明，LGDC在局部结构化数据集（Tree）上匹配自回归模型，在全局结构化数据集（Planar, Community-20）上匹配扩散模型，验证了混合生成的优势。

</details>


### [411] [Know Thyself by Knowing Others: Learning Neuron Identity from Population Context](https://arxiv.org/abs/2512.01199)
*Vinam Arora,Divyansha Lachi,Ian J. Knight,Mehdi Azabou,Blake Richards,Cole L. Hurwitz,Josh Siegle,Eva L. Dyer*

Main category: cs.LG

TL;DR: NuCLR是一个自监督学习框架，通过对比学习从神经活动数据中学习神经元身份表征，使用时空transformer整合群体上下文信息，在细胞类型和脑区解码任务上达到SOTA，并展示良好的跨动物泛化能力。


<details>
  <summary>Details</summary>
Motivation: 神经元处理信息的方式取决于其细胞类型、连接性和所在脑区，但从神经活动中推断这些因素仍然是一个重大挑战。需要构建通用表征来解析神经元身份信息。

Method: 提出NuCLR自监督框架，通过对比学习将同一神经元在不同时间和不同刺激下的观测表征拉近。使用时空transformer以置换等变方式整合群体上下文信息，不假设固定的神经元排序。

Result: 在多个电生理和钙成像数据集上，基于NuCLR表征的线性解码在细胞类型和脑区解码任务上达到新的SOTA，展示强大的零样本泛化到未见动物。首次系统分析了神经元级表征学习的缩放规律，显示增加预训练动物数量持续提升下游性能。学习到的表征具有标签效率，仅需少量标注样本即可达到竞争性能。

Conclusion: 大规模多样化神经数据集使模型能够恢复跨动物泛化的神经元身份信息，为神经科学中的表征学习提供了有效框架。

Abstract: Neurons process information in ways that depend on their cell type, connectivity, and the brain region in which they are embedded. However, inferring these factors from neural activity remains a significant challenge. To build general-purpose representations that allow for resolving information about a neuron's identity, we introduce NuCLR, a self-supervised framework that aims to learn representations of neural activity that allow for differentiating one neuron from the rest. NuCLR brings together views of the same neuron observed at different times and across different stimuli and uses a contrastive objective to pull these representations together. To capture population context without assuming any fixed neuron ordering, we build a spatiotemporal transformer that integrates activity in a permutation-equivariant manner. Across multiple electrophysiology and calcium imaging datasets, a linear decoding evaluation on top of NuCLR representations achieves a new state-of-the-art for both cell type and brain region decoding tasks, and demonstrates strong zero-shot generalization to unseen animals. We present the first systematic scaling analysis for neuron-level representation learning, showing that increasing the number of animals used during pretraining consistently improves downstream performance. The learned representations are also label-efficient, requiring only a small fraction of labeled samples to achieve competitive performance. These results highlight how large, diverse neural datasets enable models to recover information about neuron identity that generalize across animals. Code is available at https://github.com/nerdslab/nuclr.

中文标题: 通过了解他人认识自己：从群体上下文中学习神经元身份

中文摘要: 神经元处理信息的方式取决于其细胞类型、连接性以及它们所在的大脑区域。然而，从神经活动中推断这些因素仍然是一个重大挑战。为了构建能够解析神经元身份信息的通用表征，我们引入了NuCLR，这是一个自监督框架，旨在学习能够区分一个神经元与其余神经元的神经活动表征。NuCLR将同一神经元在不同时间和不同刺激下观测到的视图汇集在一起，并使用对比目标将这些表征拉近。为了在不假设任何固定神经元排序的情况下捕捉群体上下文，我们构建了一个时空transformer，以置换等变的方式整合活动。在多个电生理和钙成像数据集上，基于NuCLR表征的线性解码评估在细胞类型和大脑区域解码任务上达到了新的最先进水平，并展示了强大的零样本泛化到未见动物的能力。我们首次对神经元级表征学习进行了系统缩放分析，表明增加预训练期间使用的动物数量持续改善下游性能。学习到的表征也具有标签效率，仅需一小部分标记样本即可达到竞争性能。这些结果突显了大规模多样化神经数据集如何使模型能够恢复跨动物泛化的神经元身份信息。代码可在https://github.com/nerdslab/nuclr获取。

</details>


### [412] [Research on Milling Machine Predictive Maintenance Based on Machine Learning and SHAP Analysis in Intelligent Manufacturing Environment](https://arxiv.org/abs/2512.01205)
*Wen Zhao,Jiawen Ding,Xueting Huang,Yibo Zhang*

Main category: cs.LG

TL;DR: 基于AI4I 2020数据集，研究智能制造环境下铣床预测性维护，提出包含数据预处理、模型训练、评估、选择、SHAP分析和可视化的完整流程，发现XGBoost和随机森林等集成学习方法在故障预测中表现最佳，并通过SHAP分析识别出加工温度、扭矩和转速是关键故障影响因素。


<details>
  <summary>Details</summary>
Motivation: 在智能制造背景下，传统维护方式效率低下且成本高昂，需要利用人工智能技术实现预测性维护，提前发现设备故障，提高生产效率并降低维护成本。

Method: 基于AI4I 2020数据集，提出包含六个主要环节的完整预测性维护实验流程：1) 数据预处理；2) 模型训练；3) 模型评估；4) 模型选择；5) SHAP分析；6) 结果可视化。对比分析了八种机器学习模型。

Result: 集成学习方法（特别是XGBoost和随机森林）在铣床故障预测任务中表现最佳。通过SHAP分析发现加工温度、扭矩和转速是影响设备故障的关键因素。

Conclusion: 该研究结合人工智能与制造技术，为智能制造环境下的预测性维护实践提供了方法学参考，对推动制造业数字化转型、提高生产效率和降低维护成本具有实际意义。

Abstract: In the context of intelligent manufacturing, this paper conducts a series of experimental studies on the predictive maintenance of industrial milling machine equipment based on the AI4I 2020 dataset. This paper proposes a complete predictive maintenance experimental process combining artificial intelligence technology, including six main links: data preprocessing, model training, model evaluation, model selection, SHAP analysis, and result visualization. By comparing and analyzing the performance of eight machine learning models, it is found that integrated learning methods such as XGBoost and random forest perform well in milling machine fault prediction tasks. In addition, with the help of SHAP analysis technology, the influence mechanism of different features on equipment failure is deeply revealed, among which processing temperature, torque and speed are the key factors affecting failure. This study combines artificial intelligence and manufacturing technology, provides a methodological reference for predictive maintenance practice in an intelligent manufacturing environment, and has practical significance for promoting the digital transformation of the manufacturing industry, improving production efficiency and reducing maintenance costs.

中文标题: 智能制造环境下基于机器学习和SHAP分析的铣床预测性维护研究

中文摘要: 在智能制造背景下，本文基于AI4I 2020数据集对工业铣床设备的预测性维护进行了一系列实验研究。本文提出了结合人工智能技术的完整预测性维护实验流程，包括六个主要环节：数据预处理、模型训练、模型评估、模型选择、SHAP分析和结果可视化。通过对比分析八种机器学习模型的性能，发现集成学习方法如XGBoost和随机森林在铣床故障预测任务中表现良好。此外，借助SHAP分析技术，深入揭示了不同特征对设备故障的影响机制，其中加工温度、扭矩和转速是影响故障的关键因素。本研究结合人工智能与制造技术，为智能制造环境下的预测性维护实践提供了方法学参考，对推动制造业数字化转型、提高生产效率和降低维护成本具有实际意义。

</details>


### [413] [A Comparative Study of Machine Learning Algorithms for Electricity Price Forecasting with LIME-Based Interpretability](https://arxiv.org/abs/2512.01212)
*Xuanyi Zhao,Jiawen Ding,Xueting Huang,Yibo Zhang*

Main category: cs.LG

TL;DR: 本研究比较了八种机器学习算法在西班牙电力市场价格预测中的表现，发现KNN算法效果最佳（R²=0.865），并利用LIME方法揭示了气象因素和供需指标通过非线性关系对价格波动的重要影响。


<details>
  <summary>Details</summary>
Motivation: 随着电力市场的快速发展，价格波动性显著增加，准确预测对电力系统运行和市场决策至关重要。传统线性模型无法捕捉电力定价的复杂非线性特征，需要先进的机器学习方法。

Method: 本研究使用西班牙电力市场数据，整合消费、发电和气象变量，比较了八种机器学习模型：线性回归、岭回归、决策树、KNN、随机森林、梯度提升、SVR和XGBoost。为增强可解释性，采用LIME分析揭示影响因素。

Result: KNN算法表现最佳，R²达到0.865，MAE为3.556，RMSE为5.240。LIME分析显示气象因素和供需指标通过非线性关系显著影响价格波动。

Conclusion: 这项工作证明了机器学习模型在电力价格预测中的有效性，同时通过可解释性分析提高了决策透明度。

Abstract: With the rapid development of electricity markets, price volatility has significantly increased, making accurate forecasting crucial for power system operations and market decisions. Traditional linear models cannot capture the complex nonlinear characteristics of electricity pricing, necessitating advanced machine learning approaches. This study compares eight machine learning models using Spanish electricity market data, integrating consumption, generation, and meteorological variables. The models evaluated include linear regression, ridge regression, decision tree, KNN, random forest, gradient boosting, SVR, and XGBoost. Results show that KNN achieves the best performance with R^2 of 0.865, MAE of 3.556, and RMSE of 5.240. To enhance interpretability, LIME analysis reveals that meteorological factors and supply-demand indicators significantly influence price fluctuations through nonlinear relationships. This work demonstrates the effectiveness of machine learning models in electricity price forecasting while improving decision transparency through interpretability analysis.

中文标题: 基于LIME可解释性的机器学习算法在电力价格预测中的比较研究

中文摘要: 随着电力市场的快速发展，价格波动性显著增加，使得准确预测对电力系统运行和市场决策至关重要。传统线性模型无法捕捉电力定价的复杂非线性特征，需要先进的机器学习方法。本研究使用西班牙电力市场数据，整合消费、发电和气象变量，比较了八种机器学习模型。评估的模型包括线性回归、岭回归、决策树、KNN、随机森林、梯度提升、SVR和XGBoost。结果显示，KNN表现最佳，R²达到0.865，MAE为3.556，RMSE为5.240。为增强可解释性，LIME分析揭示了气象因素和供需指标通过非线性关系显著影响价格波动。这项工作证明了机器学习模型在电力价格预测中的有效性，同时通过可解释性分析提高了决策透明度。

</details>


### [414] [CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions](https://arxiv.org/abs/2512.01224)
*Ruixiang Feng,Zhenwei An,Yuntao Wen,Ran Le,Yiming Jia,Chen Yang,Zongchao Chen,Lisi Chen,Shen Gao,Shuo Shang,Yang Song,Tao Zhang*

Main category: cs.LG

TL;DR: CoSineVerifier是一个工具增强的答案验证器，专门用于计算导向的科学问题，通过外部执行器进行精确计算和符号简化，超越了简单的语义匹配。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的答案验证方法广泛应用于语言模型训练流程，但在计算导向的科学领域（如代数等价性检查和物理常数替换）仍面临重大挑战，需要超越简单语义匹配的鲁棒验证方法。

Method: 提出工具增强的验证器CoSineVerifier，采用两阶段训练流程：冷启动微调，然后进行多轮强化学习与工具集成，利用外部执行器进行精确计算和符号简化。

Result: 在STEM科目、通用QA和长形式推理任务上的广泛实验表明，CoSineVerifier在VerifyBench-Hard和SCI-Bench上达到最先进性能。作为RLVR的奖励模型，在AIME'24和AIME'25上持续优于基于评分标准和基于模型的验证器。

Conclusion: CoSineVerifier展示了强大的泛化能力，能够有效增强计算导向科学问题的答案验证，为提升LLM推理能力提供了有前景的解决方案。

Abstract: Answer verification methods are widely employed in language model training pipelines spanning data curation, evaluation, and reinforcement learning with verifiable rewards (RLVR). While prior work focus on developing unified verifiers applicable across multiple reasoning scenarios, significant challenges remain in computation-oriented scientific domains, such as algebraic equivalence checking and physical constant substitution. In this paper, we introduce \model, a tool-augmented verifier that leverages external executors to perform precise computations and symbolic simplifications. \model enables robust verification that goes beyond simple semantic matching. We propose a novel two-stage pipeline, which begin with cold-start fine-tuning and followed by multi-turn reinforcement learning with tool integration. Extensive experiments conducted on STEM subjects, general QA, and long-form reasoning tasks demonstrates strong generalization of \model. The results shows that the \model achieves state-of-the-art performance on VerifyBench-Hard and SCI-Bench. And we also employ our \model in RLVR as a reward model, the results show that it consistently outperforms both rubric-based and model-based verifiers on AIME'24 and AIME'25, demonstrating strong potential to enhance reasoning capabilities of LLM. Our model is released at \hyperlink{https://huggingface.co/Nanbeige/CoSineVerifier-Tool-4B}{https://huggingface.co/Nanbeige/CoSineVerifier-Tool-4B}.

中文标题: CoSineVerifier：面向计算导向科学问题的工具增强答案验证器

中文摘要: 答案验证方法广泛应用于语言模型训练流程，涵盖数据整理、评估和可验证奖励的强化学习（RLVR）。虽然先前工作专注于开发适用于多种推理场景的统一验证器，但在计算导向的科学领域（如代数等价性检查和物理常数替换）仍面临重大挑战。本文介绍CoSineVerifier，一种工具增强的验证器，利用外部执行器进行精确计算和符号简化。CoSineVerifier实现了超越简单语义匹配的鲁棒验证。我们提出了新颖的两阶段训练流程，从冷启动微调开始，随后进行多轮强化学习与工具集成。在STEM科目、通用QA和长形式推理任务上的广泛实验展示了CoSineVerifier的强大泛化能力。结果表明，CoSineVerifier在VerifyBench-Hard和SCI-Bench上达到最先进性能。我们还将CoSineVerifier作为奖励模型应用于RLVR，结果显示它在AIME'24和AIME'25上持续优于基于评分标准和基于模型的验证器，展示了增强LLM推理能力的强大潜力。我们的模型已在https://huggingface.co/Nanbeige/CoSineVerifier-Tool-4B发布。

</details>


### [415] [On the Tension Between Optimality and Adversarial Robustness in Policy Optimization](https://arxiv.org/abs/2512.01228)
*Haoran Li,Jiayu Lv,Congying Han,Zicheng Zhang,Anqi Li,Yan Liu,Tiande Guo,Nan Jiang*

Main category: cs.LG

TL;DR: 该论文探讨了强化学习中策略优化的最优性与对抗鲁棒性之间的张力，发现理论上的统一在实践中存在差距，并提出BARPO框架来缓解这一矛盾。


<details>
  <summary>Details</summary>
Motivation: 尽管CAR理论表明最优性与对抗鲁棒性可能统一，但实践中标准策略优化(SPO)倾向于收敛到脆弱但性能强的策略，而对抗鲁棒策略优化(ARPO)则牺牲性能换取鲁棒性，这种理论与实践的差距需要解决。

Method: 提出BARPO（Bilevel Adversarially Robust Policy Optimization）双层框架，通过调节对抗者强度来统一SPO和ARPO，既保持全局最优性又改善导航性，避免陷入欺骗性的粘滞FOSP。

Result: 实验结果表明BARPO在多个任务上一致优于原始ARPO，在保持鲁棒性的同时获得更好的性能，为理论和实践性能的调和提供了实用方法。

Conclusion: 最优性与对抗鲁棒性之间的张力源于最强对抗者对优化景观的重塑效应，BARPO通过调节对抗强度有效缓解了这一矛盾，为同时实现高性能和高鲁棒性提供了可行路径。

Abstract: Achieving optimality and adversarial robustness in deep reinforcement learning has long been regarded as conflicting goals. Nonetheless, recent theoretical insights presented in CAR suggest a potential alignment, raising the important question of how to realize this in practice. This paper first identifies a key gap between theory and practice by comparing standard policy optimization (SPO) and adversarially robust policy optimization (ARPO). Although they share theoretical consistency, a fundamental tension between robustness and optimality arises in practical policy gradient methods. SPO tends toward convergence to vulnerable first-order stationary policies (FOSPs) with strong natural performance, whereas ARPO typically favors more robust FOSPs at the expense of reduced returns. Furthermore, we attribute this tradeoff to the reshaping effect of the strongest adversary in ARPO, which significantly complicates the global landscape by inducing deceptive sticky FOSPs. This improves robustness but makes navigation more challenging. To alleviate this, we develop the BARPO, a bilevel framework unifying SPO and ARPO by modulating adversary strength, thereby facilitating navigability while preserving global optima. Extensive empirical results demonstrate that BARPO consistently outperforms vanilla ARPO, providing a practical approach to reconcile theoretical and empirical performance.

中文标题: 策略优化中最优性与对抗鲁棒性之间的张力研究

中文摘要: 在深度强化学习中实现最优性和对抗鲁棒性长期以来被视为相互冲突的目标。然而，CAR中提出的最新理论见解表明两者可能统一，这引发了一个重要问题：如何在实践中实现这种统一。本文首先通过比较标准策略优化(SPO)和对抗鲁棒策略优化(ARPO)，揭示了理论与实践之间的关键差距。尽管它们在理论上具有一致性，但在实际策略梯度方法中，鲁棒性与最优性之间存在根本性张力。SPO倾向于收敛到具有强大自然性能但脆弱的一阶平稳策略(FOSP)，而ARPO通常以降低回报为代价偏好更鲁棒的FOSP。此外，我们将这种权衡归因于ARPO中最强对抗者的重塑效应，它通过诱导欺骗性的粘滞FOSP显著复杂化了全局景观。这虽然提高了鲁棒性，但使导航更加困难。为缓解这一问题，我们开发了BARPO，这是一个通过调节对抗者强度来统一SPO和ARPO的双层框架，从而在保持全局最优的同时改善导航性。大量实证结果表明，BARPO一致优于原始ARPO，为调和理论与实证性能提供了实用方法。

</details>


### [416] [Efficient Training of Diffusion Mixture-of-Experts Models: A Practical Recipe](https://arxiv.org/abs/2512.01252)
*Yahui Liu,Yang Yue,Jingyuan Zhang,Chenxi Sun,Yang Zhou,Wencong Zeng,Ruiming Tang,Guorui Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种高效训练扩散专家混合模型的实用方案，通过系统探索架构配置（专家模块设计、中间宽度、专家数量、位置编码）而非仅关注路由机制，实现了在使用相同或更少激活参数的情况下超越强基线性能。


<details>
  <summary>Details</summary>
Motivation: 当前扩散MoE模型研究过度集中于路由机制创新，而忽视了基础架构配置空间的探索。作者观察到现有研究在架构设计方面存在明显不足，受LLM中MoE设计范式启发，认为系统研究架构配置对释放扩散MoE模型潜力至关重要。

Method: 1. 识别关键架构因素：DeepSeek风格专家模块、替代中间宽度、变化的专家数量、增强注意力位置编码；2. 系统研究这些配置对模型性能的影响；3. 提出适用于潜在空间和像素空间扩散框架的新架构；4. 开发高效训练方案，确保使用相等或更少激活参数。

Result: 通过精心调整架构配置，扩散MoE模型在性能上超越了强基线，且使用相等或更少的激活参数。实验表明架构优化带来的收益超过了仅通过路由创新获得的改进，验证了所提方案的有效性。

Conclusion: 扩散MoE模型的架构配置空间值得深入探索，精心调整架构因素比单纯改进路由机制更为重要。提出的实用训练方案为构建高效扩散MoE模型提供了有效指导，相关代码和模型已开源。

Abstract: Recent efforts on Diffusion Mixture-of-Experts (MoE) models have primarily focused on developing more sophisticated routing mechanisms. However, we observe that the underlying architectural configuration space remains markedly under-explored. Inspired by the MoE design paradigms established in large language models (LLMs), we identify a set of crucial architectural factors for building effective Diffusion MoE models--including DeepSeek-style expert modules, alternative intermediate widths, varying expert counts, and enhanced attention positional encodings. Our systematic study reveals that carefully tuning these configurations is essential for unlocking the full potential of Diffusion MoE models, often yielding gains that exceed those achieved by routing innovations alone. Through extensive experiments, we present novel architectures that can be efficiently applied to both latent and pixel-space diffusion frameworks, which provide a practical and efficient training recipe that enables Diffusion MoE models to surpass strong baselines while using equal or fewer activated parameters. All code and models are publicly available at: https://github.com/yhlleo/EfficientMoE.

中文标题: 高效训练扩散专家混合模型：实用方案

中文摘要: 近期关于扩散专家混合模型的研究主要集中于开发更复杂的路由机制。然而，我们观察到基础架构配置空间仍然明显未被充分探索。受大型语言模型中建立的MoE设计范式启发，我们识别出一组构建有效扩散MoE模型的关键架构因素——包括DeepSeek风格的专家模块、替代中间宽度、变化的专家数量以及增强的注意力位置编码。我们的系统研究表明，精心调整这些配置对于释放扩散MoE模型的全部潜力至关重要，通常能获得超过仅通过路由创新实现的收益。通过大量实验，我们提出了可高效应用于潜在空间和像素空间扩散框架的新颖架构，提供了一个实用且高效的训练方案，使扩散MoE模型在使用相等或更少激活参数的情况下超越强基线。所有代码和模型均已公开。

</details>


### [417] [Efficient Hyperparameter Search for Non-Stationary Model Training](https://arxiv.org/abs/2512.01258)
*Berivan Isik,Matthew Fahrbach,Dima Kuzmin,Nicolas Mayoraz,Emil Praun,Steffen Rendle,Raghavendra Vasudeva*

Main category: cs.LG

TL;DR: 提出两阶段超参数搜索框架，通过高效识别有潜力的配置再完整训练，在非平稳在线学习场景中降低10倍搜索成本


<details>
  <summary>Details</summary>
Motivation: 在线学习系统（如推荐和广告）需要持续适应数据分布变化，模型训练成本高昂，超参数搜索成本更是倍增。现有方法未能有效处理序列化、非平稳数据的挑战。

Method: 提出两阶段范式：第一阶段高效识别最有希望的配置，第二阶段仅训练选中的候选配置到完整状态。开发了针对非平稳数据的数据缩减和预测策略，克服传统超参数优化的局限性。

Result: 在Criteo 1TB数据集（最大的公开基准）上，总超参数搜索成本降低高达10倍；在规模大两个数量级的工业广告系统中，也实现了显著且经过验证的效率提升。

Conclusion: 通过专注于第一阶段准确识别而非达到峰值性能，可以采取激进的成本节约措施。该方法有效解决了非平稳数据场景下的超参数搜索效率问题，在实际工业应用中具有重要价值。

Abstract: Online learning is the cornerstone of applications like recommendation and advertising systems, where models continuously adapt to shifting data distributions. Model training for such systems is remarkably expensive, a cost that multiplies during hyperparameter search. We introduce a two-stage paradigm to reduce this cost: (1) efficiently identifying the most promising configurations, and then (2) training only these selected candidates to their full potential. Our core insight is that focusing on accurate identification in the first stage, rather than achieving peak performance, allows for aggressive cost-saving measures. We develop novel data reduction and prediction strategies that specifically overcome the challenges of sequential, non-stationary data not addressed by conventional hyperparameter optimization. We validate our framework's effectiveness through a dual evaluation: first on the Criteo 1TB dataset, the largest suitable public benchmark, and second on an industrial advertising system operating at a scale two orders of magnitude larger. Our methods reduce the total hyperparameter search cost by up to 10$\times$ on the public benchmark and deliver significant, validated efficiency gains in the industrial setting.

中文标题: 非平稳模型训练的高效超参数搜索

中文摘要: 在线学习是推荐和广告系统等应用的基石，其中模型需要持续适应变化的数据分布。这类系统的模型训练成本极高，而在超参数搜索期间成本更是倍增。我们引入了一个两阶段范式来降低这种成本：（1）高效识别最有希望的配置，然后（2）仅将这些选中的候选配置训练到其完整潜力。我们的核心洞见是，专注于第一阶段准确识别而非达到峰值性能，允许采取激进的成本节约措施。我们开发了新颖的数据缩减和预测策略，专门克服传统超参数优化未能解决的序列化、非平稳数据的挑战。我们通过双重评估验证了框架的有效性：首先在Criteo 1TB数据集（最大的合适公开基准）上，其次在规模大两个数量级的工业广告系统上。我们的方法在公开基准上将总超参数搜索成本降低了高达10倍，并在工业环境中实现了显著且经过验证的效率提升。

</details>


### [418] [Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding](https://arxiv.org/abs/2512.01278)
*Yilong Zhao,Jiaming Tang,Kan Zhu,Zihao Ye,Chi-Chih Chang,Chaofan Lin,Jongseok Park,Guangxuan Xiao,Mohamed S. Abdelfattah,Mingyu Gao,Baris Kasikci,Song Han,Ion Stoica*

Main category: cs.LG

TL;DR: SparseSpec是一个稀疏自推测解码框架，通过重用同一模型作为草稿和目标模型，结合创新的稀疏注意力机制和系统优化，显著提升推理模型在长文本生成时的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 推理语言模型在处理复杂任务时需要生成详细的思维链，导致生成文本变长，这使得推理瓶颈从计算密集型转向内存密集型。每次生成token都需要对之前所有token进行完整注意力计算，KV缓存越来越大，内存访问成为主要瓶颈。

Method: SparseSpec采用自推测解码框架，使用同一模型作为草稿和目标模型。核心创新包括：1) PillarAttn稀疏注意力机制，通过重用验证阶段信息准确选择关键token；2) 统一调度器批量处理token草稿和验证；3) 延迟验证实现CPU/GPU重叠；4) 动态KV缓存管理最大化内存利用率。

Result: 在各种模型和数据集上，SparseSpec优于现有最优解决方案，吞吐量提升最高达2.13倍。

Conclusion: SparseSpec通过创新的稀疏自推测解码框架，有效解决了推理模型在长文本生成时的内存瓶颈问题，显著提升了推理效率。

Abstract: Reasoning language models have demonstrated remarkable capabilities on challenging tasks by generating elaborate chain-of-thought (CoT) solutions. However, such lengthy generation shifts the inference bottleneck from compute-bound to memory-bound. To generate each token, the model applies full attention to all previously generated tokens, requiring memory access to an increasingly large KV-Cache. Consequently, longer generations demand more memory access for every step, leading to substantial pressure on memory bandwidth.
  To address this, we introduce SparseSpec, a speculative decoding framework that reuses the same model as the draft and target models (i.e., self-speculation). SparseSpec features a novel sparse attention mechanism, PillarAttn, as the draft model, which accurately selects critical tokens via elegantly reusing information from the verification stage. Furthermore, SparseSpec co-designs self-speculation with three system innovations: (1) a unified scheduler to batch token drafting and verification, (2) delayed verification for CPU/GPU overlap, and (3) dynamic KV-Cache management to maximize memory utilization. Across various models and datasets, SparseSpec outperforms state-of-the-art solutions, with an up to 2.13x throughput speedup.

中文标题: 通过稀疏自推测解码加速大规模推理模型推断

中文摘要: 推理语言模型通过在具有挑战性的任务上生成详细的思维链解决方案，展示了卓越的能力。然而，这种长文本生成将推理瓶颈从计算密集型转向内存密集型。为了生成每个token，模型需要对所有先前生成的token应用完整的注意力机制，需要访问越来越大的KV缓存。因此，更长的生成过程需要每个步骤更多的内存访问，给内存带宽带来巨大压力。
为了解决这个问题，我们引入了SparseSpec，这是一个推测解码框架，将同一模型重用为草稿模型和目标模型（即自推测）。SparseSpec采用了一种新颖的稀疏注意力机制PillarAttn作为草稿模型，通过优雅地重用验证阶段的信息来准确选择关键token。此外，SparseSpec将自推测与三个系统创新相结合：(1) 统一调度器批量处理token草稿和验证，(2) 延迟验证实现CPU/GPU重叠，(3) 动态KV缓存管理以最大化内存利用率。在各种模型和数据集上，SparseSpec优于现有最优解决方案，吞吐量提升最高达2.13倍。

</details>


### [419] [Directed evolution algorithm drives neural prediction](https://arxiv.org/abs/2512.01362)
*Yanlin Wang,Nancy M Young,Patrick C M Wong*

Main category: cs.LG

TL;DR: 本文提出了一种名为定向进化模型（DEM）的新方法，该模型模拟生物进化过程来解决神经预测中的领域偏移和标签稀缺问题，在人工耳蜗植入儿童的语言发展预测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 神经预测在个性化医疗中具有重要应用价值，但面临两大挑战：1）领域偏移问题（模型在不同数据集间泛化能力差）；2）标签稀缺问题（目标域标注数据不足）。特别是在人工耳蜗植入儿童的预后预测中，个体差异大且跨数据集预测困难。

Method: 提出定向进化模型（DEM），该模型模拟生物定向进化的试错过程，通过不确定性探索增强强化学习的泛化能力。具体方法包括：1）结合回放缓冲区技术；2）采用持续反向传播方法；3）在连续学习设置中优化利用与探索的平衡。

Result: 在四个不同的人工耳蜗植入儿童数据集上的实验表明：1）DEM能有效提高跨域神经预测性能；2）成功解决了目标域标签稀缺问题；3）相比传统方法，在跨数据集预测任务中表现更优。

Conclusion: 定向进化算法为神经预测提供了一种有效的计算框架，能够克服领域偏移和标签稀缺的挑战，在医学人工智能应用中具有重要潜力，特别是在需要跨域预测和个性化预后的场景中。

Abstract: Neural prediction offers a promising approach to forecasting the individual variability of neurocognitive functions and disorders and providing prognostic indicators for personalized invention. However, it is challenging to translate neural predictive models into medical artificial intelligent applications due to the limitations of domain shift and label scarcity. Here, we propose the directed evolution model (DEM), a novel computational model that mimics the trial-and-error processes of biological directed evolution to approximate optimal solutions for predictive modeling tasks. We demonstrated that the directed evolution algorithm is an effective strategy for uncertainty exploration, enhancing generalization in reinforcement learning. Furthermore, by incorporating replay buffer and continual backpropagate methods into DEM, we provide evidence of achieving better trade-off between exploitation and exploration in continuous learning settings. We conducted experiments on four different datasets for children with cochlear implants whose spoken language developmental outcomes vary considerably on the individual-child level. Preoperative neural MRI data has shown to accurately predict the post-operative outcome of these children within but not across datasets. Our results show that DEM can efficiently improve the performance of cross-domain pre-implantation neural predictions while addressing the challenge of label scarcity in target domain.

中文标题: 定向进化算法驱动神经预测

中文摘要: 神经预测为预测神经认知功能和障碍的个体变异性以及为个性化干预提供预后指标提供了一种有前景的方法。然而，由于领域偏移和标签稀缺性的限制，将神经预测模型转化为医学人工智能应用具有挑战性。在此，我们提出了定向进化模型（DEM），这是一种新颖的计算模型，模拟生物定向进化的试错过程，以逼近预测建模任务的最优解。我们证明了定向进化算法是一种有效的策略，可用于不确定性探索，增强强化学习中的泛化能力。此外，通过将回放缓冲区和持续反向传播方法整合到DEM中，我们提供了在连续学习设置中实现利用与探索之间更好权衡的证据。我们在四个不同的数据集上进行了实验，这些数据集来自人工耳蜗植入儿童，其口语发展结果在个体水平上存在显著差异。术前神经MRI数据已被证明可以准确预测这些儿童在数据集内而非跨数据集的术后结果。我们的结果表明，DEM能够有效提高跨域植入前神经预测的性能，同时解决目标域中标签稀缺的挑战。

</details>


### [420] [Consistency Flow Model Achieves One-step Denoising Error Correction Codes](https://arxiv.org/abs/2512.01389)
*Haoyu Lei,Chin Wa Lau,Kaiwen Zhou,Nian Guo,Farzan Farnia*

Main category: cs.LG

TL;DR: ECCFM是一个架构无关的训练框架，通过将去噪过程建模为概率流常微分方程，实现一步解码纠错码，在保持高准确率的同时大幅提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前基于去噪扩散的神经解码器虽然性能优越，但迭代采样过程限制了其在低延迟场景中的实用性，需要开发既能保持高精度又能实现高效推理的解码方法。

Method: 将反向去噪过程建模为概率流常微分方程，通过微分时间正则化强制平滑性，学习将噪声信号沿解码轨迹直接映射到原始码字，实现单步推理。

Result: 在多个解码基准测试中，ECCFM获得了比自回归和基于扩散的基线更低的误码率，特别是在长码上表现更优，推理速度比去噪扩散解码器快30-100倍。

Conclusion: ECCFM框架成功解决了神经解码器在准确性和计算效率之间的权衡问题，为低延迟通信系统提供了实用的高性能解码解决方案。

Abstract: Error Correction Codes (ECC) are fundamental to reliable digital communication, yet designing neural decoders that are both accurate and computationally efficient remains challenging. Recent denoising diffusion decoders with transformer backbones achieve state-of-the-art performance, but their iterative sampling limits practicality in low-latency settings. We introduce the Error Correction Consistency Flow Model (ECCFM), an architecture-agnostic training framework for high-fidelity one-step decoding. By casting the reverse denoising process as a Probability Flow Ordinary Differential Equation (PF-ODE) and enforcing smoothness through a differential time regularization, ECCFM learns to map noisy signals along the decoding trajectory directly to the original codeword in a single inference step. Across multiple decoding benchmarks, ECCFM attains lower bit-error rates (BER) than autoregressive and diffusion-based baselines, with notable improvements on longer codes, while delivering inference speeds up from 30x to 100x faster than denoising diffusion decoders.

中文标题: 一致性流模型实现一步去噪纠错码

中文摘要: 纠错码是可靠数字通信的基础，但设计既准确又计算高效的神经解码器仍然具有挑战性。最近基于Transformer骨干的去噪扩散解码器实现了最先进的性能，但其迭代采样限制了在低延迟设置中的实用性。我们引入了纠错一致性流模型，这是一个架构无关的训练框架，用于高保真的一步解码。通过将反向去噪过程建模为概率流常微分方程，并通过微分时间正则化强制平滑性，ECCFM学习将噪声信号沿解码轨迹直接映射到原始码字，实现单步推理。在多个解码基准测试中，ECCFM获得了比自回归和基于扩散的基线更低的误码率，在长码上表现尤为突出，同时推理速度比去噪扩散解码器快30-100倍。

</details>


### [421] [On Global Applicability and Location Transferability of Generative Deep Learning Models for Precipitation Downscaling](https://arxiv.org/abs/2512.01400)
*Paula Harder,Christian Lessig,Matthew Chantry,Francis Pelletier,David Rolnick*

Main category: cs.LG

TL;DR: 该研究评估了生成式深度学习模型在全球降水降尺度任务中的泛化能力，发现模型在不同地理区域间的迁移性能存在显著差异，揭示了区域特定模型在未见地区应用时的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数降水降尺度的深度学习模型都是针对特定区域开发的，这些模型在未见地理区域的泛化能力尚未得到充分探索。研究旨在评估生成式降尺度模型在全球范围内的适用性和位置迁移性，以了解模型在不同地理区域间的表现差异。

Method: 研究采用全球框架，使用ERA5再分析数据作为预测因子，IMERG降水估计（0.1°分辨率）作为目标。通过基于位置的层次化数据分割方法，系统评估了模型在全球15个不同区域的性能表现。

Result: 研究发现生成式降尺度模型在不同地理区域间的泛化性能存在显著差异。某些模型在训练区域表现良好，但在未见区域性能下降明显，表明当前区域特定模型在全球应用时存在局限性。

Conclusion: 生成式深度学习模型在降水降尺度任务中具有潜力，但其全球适用性和位置迁移性仍需改进。需要开发更具泛化能力的模型架构和训练策略，以实现真正全球可用的降水降尺度系统。

Abstract: Deep learning offers promising capabilities for the statistical downscaling of climate and weather forecasts, with generative approaches showing particular success in capturing fine-scale precipitation patterns. However, most existing models are region-specific, and their ability to generalize to unseen geographic areas remains largely unexplored. In this study, we evaluate the generalization performance of generative downscaling models across diverse regions. Using a global framework, we employ ERA5 reanalysis data as predictors and IMERG precipitation estimates at $0.1^\circ$ resolution as targets. A hierarchical location-based data split enables a systematic assessment of model performance across 15 regions around the world.

中文标题: 生成式深度学习模型在降水降尺度中的全球适用性和位置迁移性研究

中文摘要: 深度学习为气候和天气预报的统计降尺度提供了有前景的能力，其中生成式方法在捕捉精细尺度降水模式方面表现出特别的成功。然而，大多数现有模型都是区域特定的，它们在未见地理区域的泛化能力在很大程度上仍未得到探索。在本研究中，我们评估了生成式降尺度模型在不同区域的泛化性能。使用全球框架，我们采用ERA5再分析数据作为预测因子，IMERG降水估计（0.1°分辨率）作为目标。基于位置的层次化数据分割使得能够系统评估模型在全球15个区域的性能表现。

</details>


### [422] [Fantastic Features and Where to Find Them: A Probing Method to combine Features from Multiple Foundation Models](https://arxiv.org/abs/2512.01405)
*Benjamin Ramtoula,Pierre-Yves Lajoie,Paul Newman,Daniele De Martini*

Main category: cs.LG

TL;DR: ComBo是一种简单可扩展的探测方法，通过轻量级transformer整合多个基础模型的特征，无需数据集特定调优或反向传播，在VTAB-1k基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 不同基础模型具有互补优势，但现有方法无法有效整合多个模型的优势。传统微调方法专注于单个模型，而现有探测方法在大规模特征集上扩展性差且依赖数据集特定调优。

Method: ComBo将多个基础模型各层的激活压缩为紧凑令牌表示，使用轻量级transformer进行任务预测，无需反向传播。引入机制评估每个骨干的任务相关性，支持选择性适应。

Result: 在VTAB-1k的19个任务上，ComBo优于先前探测方法，匹配或超越基于蒸馏的模型合并等昂贵方案，能高效探测调优模型。

Conclusion: ComBo提供了一个实用通用的框架，能够有效结合多个基础模型的多样化表示，实现高效的多模型特征整合。

Abstract: Foundation models (FMs) trained with different objectives and data learn diverse representations, making some more effective than others for specific downstream tasks. Existing adaptation strategies, such as parameter-efficient fine-tuning, focus on individual models and do not exploit the complementary strengths across models. Probing methods offer a promising alternative by extracting information from frozen models, but current techniques do not scale well with large feature sets and often rely on dataset-specific hyperparameter tuning. We propose Combined backBones (ComBo), a simple and scalable probing-based adapter that effectively integrates features from multiple models and layers. ComBo compresses activations from layers of one or more FMs into compact token-wise representations and processes them with a lightweight transformer for task-specific prediction. Crucially, ComBo does not require dataset-specific tuning or backpropagation through the backbone models. However, not all models are equally relevant for all tasks. To address this, we introduce a mechanism that leverages ComBo's joint multi-backbone probing to efficiently evaluate each backbone's task-relevance, enabling both practical model comparison and improved performance through selective adaptation. On the 19 tasks of the VTAB-1k benchmark, ComBo outperforms previous probing methods, matches or surpasses more expensive alternatives, such as distillation-based model merging, and enables efficient probing of tuned models. Our results demonstrate that ComBo offers a practical and general-purpose framework for combining diverse representations from multiple FMs.

中文标题: 神奇特征何处寻：一种结合多个基础模型特征的探测方法

中文摘要: 具有不同目标和数据训练的基础模型学习到多样化的表示，使得某些模型在特定下游任务上比其他模型更有效。现有的适应策略，如参数高效微调，专注于单个模型，并未利用模型间的互补优势。探测方法通过从冻结模型中提取信息提供了有希望的替代方案，但现有技术在大规模特征集上扩展性不佳，且通常依赖于数据集特定的超参数调优。我们提出了Combined backBones (ComBo)，一种简单且可扩展的基于探测的适配器，能有效整合来自多个模型和层的特征。ComBo将来自一个或多个基础模型各层的激活压缩为紧凑的令牌级表示，并使用轻量级transformer进行任务特定预测。关键的是，ComBo不需要数据集特定的调优或通过骨干模型的反向传播。然而，并非所有模型对所有任务都同等相关。为解决此问题，我们引入了一种机制，利用ComBo的联合多骨干探测来高效评估每个骨干的任务相关性，实现实用的模型比较和通过选择性适应的性能提升。在VTAB-1k基准的19个任务上，ComBo优于先前的探测方法，匹配或超越了更昂贵的替代方案（如基于蒸馏的模型合并），并实现了对调优模型的高效探测。我们的结果表明，ComBo为结合多个基础模型的多样化表示提供了一个实用且通用的框架。

</details>


### [423] [Stay Unique, Stay Efficient: Preserving Model Personality in Multi-Task Merging](https://arxiv.org/abs/2512.01461)
*Kuangpu Guo,Yuhe Ding,Jian Liang,Zilei Wang,Ran He*

Main category: cs.LG

TL;DR: DTS是一种高效的模型合并方法，通过奇异值分解和智能阈值化保留任务特定信息，实现多任务能力同时最小化存储开销。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法在合并多个任务时会导致显著的性能下降，即使对于相似任务也是如此。这主要是因为合并过程中丢失了任务特定的关键信息。需要一种既能实现多任务能力又能保留模型"个性"（任务特定信息）的高效方法。

Method: 提出DTS框架：1）对任务特定信息进行奇异值分解，保留少量关键奇异值和向量；2）引入新颖的阈值化策略，将奇异向量元素分组并分配缩放因子；3）扩展为基于任务特征语义相似性的无数据融合变体，以处理未见任务。

Result: DTS在实验中始终优于现有最先进方法，同时每个任务仅需1%的额外存储开销。其变体在未见任务上展现出显著更好的泛化性能。

Conclusion: DTS框架成功解决了模型合并中的性能下降问题，通过高效保留任务特定信息实现了多任务能力，同时保持了存储效率。该方法为模型个性保留和高效多任务学习提供了有效解决方案。

Abstract: Model merging has emerged as a promising paradigm for enabling multi-task capabilities without additional training. However, existing methods often experience substantial performance degradation compared with individually fine-tuned models, even on similar tasks, underscoring the need to preserve task-specific information. This paper proposes Decomposition, Thresholding, and Scaling (DTS), an approximation-based personalized merging framework that preserves task-specific information with minimal storage overhead. DTS first applies singular value decomposition to the task-specific information and retains only a small subset of singular values and vectors. It then introduces a novel thresholding strategy that partitions singular vector elements into groups and assigns a scaling factor to each group. To enable generalization to unseen tasks, we further extend DTS with a variant that fuses task-specific information in a data-free manner based on the semantic similarity of task characteristics. Extensive experiments demonstrate that DTS consistently outperforms state-of-the-art baselines while requiring only 1\% additional storage per task. Furthermore, experiments on unseen tasks show that the DTS variant achieves significantly better generalization performance. Our code is available at https://github.com/krumpguo/DTS.

中文标题: 保持独特，保持高效：在多任务合并中保留模型个性

中文摘要: 模型合并已成为一种有前景的范式，可以在无需额外训练的情况下实现多任务能力。然而，现有方法通常相比单独微调的模型会出现显著的性能下降，即使在相似任务上也是如此，这突显了保留任务特定信息的必要性。本文提出了分解、阈值化和缩放（DTS），这是一种基于近似的个性化合并框架，能够以最小的存储开销保留任务特定信息。DTS首先对任务特定信息应用奇异值分解，仅保留一小部分奇异值和向量。然后引入一种新颖的阈值化策略，将奇异向量元素分组并为每组分配缩放因子。为了实现对未见任务的泛化，我们进一步扩展了DTS，提出了一种基于任务特征语义相似性以无数据方式融合任务特定信息的变体。大量实验表明，DTS在仅需每个任务1%额外存储的情况下，始终优于最先进的基线方法。此外，在未见任务上的实验显示，DTS变体实现了显著更好的泛化性能。我们的代码可在https://github.com/krumpguo/DTS获取。

</details>


### [424] [Multi-view diffusion geometry using intertwined diffusion trajectories](https://arxiv.org/abs/2512.01484)
*Gwendal Debaussart-Joniec,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架，通过交织多视图扩散轨迹（MDTs）构建多视图扩散几何，这是一种迭代结合多个数据视图随机游走算子的非均匀扩散过程。MDTs定义了具有清晰概率和几何解释的轨迹依赖扩散算子，捕捉视图间的相互作用。该框架包含现有多视图扩散模型，同时提供新的视图交互和融合自由度。


<details>
  <summary>Details</summary>
Motivation: 现有多视图扩散方法通常采用固定或简单的视图融合策略，缺乏灵活性和理论保证。需要一种统一框架，既能包含现有方法，又能提供更灵活的视图交互机制，同时具有坚实的理论基础。

Method: 提出交织多视图扩散轨迹（MDTs）框架，通过迭代组合多个视图的随机游走算子构建非均匀扩散过程。每个MDT定义轨迹依赖的扩散算子，建立理论性质（遍历性），推导MDT-based扩散距离和嵌入（通过奇异值分解），并提出在算子空间中学习MDT算子的策略。

Result: 建立了MDTs的理论性质（包括点向算子和过程本身的遍历性），推导了扩散距离和嵌入方法，提出了基于内部质量度量的学习策略。实验表明MDT算子在流形学习和数据聚类中具有实际影响，同时为评估扩散方法提供了中性基线。

Conclusion: MDTs提供了一个统一的多视图扩散几何框架，既包含现有方法，又提供新的视图交互自由度。该框架具有理论保证，支持灵活模型设计，并为评估扩散方法提供了中性基线，在流形学习和聚类任务中显示出实用价值。

Abstract: This paper introduces a comprehensive unified framework for constructing multi-view diffusion geometries through intertwined multi-view diffusion trajectories (MDTs), a class of inhomogeneous diffusion processes that iteratively combine the random walk operators of multiple data views. Each MDT defines a trajectory-dependent diffusion operator with a clear probabilistic and geometric interpretation, capturing over time the interplay between data views. Our formulation encompasses existing multi-view diffusion models, while providing new degrees of freedom for view interaction and fusion. We establish theoretical properties under mild assumptions, including ergodicity of both the point-wise operator and the process in itself. We also derive MDT-based diffusion distances, and associated embeddings via singular value decompositions. Finally, we propose various strategies for learning MDT operators within the defined operator space, guided by internal quality measures. Beyond enabling flexible model design, MDTs also offer a neutral baseline for evaluating diffusion-based approaches through comparison with randomly selected MDTs. Experiments show the practical impact of the MDT operators in a manifold learning and data clustering context.

中文标题: 使用交织扩散轨迹的多视图扩散几何

中文摘要: 本文引入了一个全面的统一框架，通过交织多视图扩散轨迹（MDTs）构建多视图扩散几何，这是一类迭代组合多个数据视图随机游走算子的非均匀扩散过程。每个MDT定义了一个具有清晰概率和几何解释的轨迹依赖扩散算子，随时间捕捉数据视图间的相互作用。我们的公式包含了现有的多视图扩散模型，同时为视图交互和融合提供了新的自由度。我们在温和假设下建立了理论性质，包括点向算子和过程本身的遍历性。我们还推导了基于MDT的扩散距离，以及通过奇异值分解的相关嵌入。最后，我们提出了在定义的算子空间内学习MDT算子的各种策略，由内部质量度量指导。除了支持灵活的模型设计外，MDTs还通过与随机选择的MDTs比较，为评估基于扩散的方法提供了中性基线。实验显示了MDT算子在流形学习和数据聚类背景下的实际影响。

</details>


### [425] [End-to-end Deep Reinforcement Learning for Stochastic Multi-objective Optimization in C-VRPTW](https://arxiv.org/abs/2512.01518)
*Abdo Abouelrous,Laurens Bliek,Yaoxin Wu,Yingqian Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种端到端深度强化学习模型，用于解决具有旅行时间不确定性和多目标（总旅行时间和路线完工时间）的车辆路径问题变体。该模型利用注意力机制和场景聚类训练，能够在可接受时间内生成高质量的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 实际车辆路径问题中存在旅行时间不确定性和多个冲突目标（如运营效率与劳动法规），传统方法难以同时处理这些复杂因素。基于学习的方法具有计算优势，但现有端到端模型难以直接处理旅行时间不确定性和多目标优化。

Method: 提出端到端深度强化学习模型，利用注意力机制处理路由问题结构。通过场景聚类技术减少训练时间，模型能够同时处理旅行时间随机性和多目标优化（总旅行时间和路线完工时间）。

Result: 与三个基线方法相比，所提模型能够在可接受的运行时间内构建出质量良好的帕累托前沿，有效平衡了多个冲突目标。

Conclusion: 该研究成功开发了一种能够同时处理随机性和多目标性的端到端深度强化学习模型，为复杂车辆路径问题提供了有效的解决方案，在计算效率和解决方案质量之间取得了良好平衡。

Abstract: In this work, we consider learning-based applications in routing to solve a Vehicle Routing variant characterized by stochasticity and multiple objectives. Such problems are representative of practical settings where decision-makers have to deal with uncertainty in the operational environment as well as multiple conflicting objectives due to different stakeholders. We specifically consider travel time uncertainty. We also consider two objectives, total travel time and route makespan, that jointly target operational efficiency and labor regulations on shift length, although different objectives could be incorporated. Learning-based methods offer earnest computational advantages as they can repeatedly solve problems with limited interference from the decision-maker. We specifically focus on end-to-end deep learning models that leverage the attention mechanism and multiple solution trajectories. These models have seen several successful applications in routing problems. However, since travel times are not a direct input to these models due to the large dimensions of the travel time matrix, accounting for uncertainty is a challenge, especially in the presence of multiple objectives. In turn, we propose a model that simultaneously addresses stochasticity and multi-objectivity and provide a refined training mechanism for this model through scenario clustering to reduce training time. Our results show that our model is capable of constructing a Pareto Front of good quality within acceptable run times compared to three baselines.

中文标题: 面向C-VRPTW随机多目标优化的端到端深度强化学习

中文摘要: 在本研究中，我们考虑将基于学习的方法应用于路由问题，以解决具有随机性和多目标特征的车辆路径问题变体。这类问题代表了实际场景，其中决策者必须在操作环境中处理不确定性，同时由于不同利益相关者而面临多个相互冲突的目标。我们特别考虑了旅行时间的不确定性。我们还考虑了两个目标：总旅行时间和路线完工时间，这两个目标共同针对操作效率和关于班次长度的劳动法规，尽管可以纳入不同的目标。基于学习的方法提供了显著的计算优势，因为它们可以在决策者有限干预的情况下重复解决问题。我们特别关注利用注意力机制和多个解决方案轨迹的端到端深度学习模型。这些模型在路由问题中已经看到了几个成功的应用。然而，由于旅行时间矩阵的维度较大，旅行时间不是这些模型的直接输入，因此考虑不确定性是一个挑战，尤其是在存在多个目标的情况下。因此，我们提出了一个同时处理随机性和多目标性的模型，并通过场景聚类为该模型提供了一种精细的训练机制，以减少训练时间。我们的结果表明，与三个基线相比，我们的模型能够在可接受的运行时间内构建出质量良好的帕累托前沿。

</details>


### [426] [TimePred: efficient and interpretable offline change point detection for high volume data - with application to industrial process monitoring](https://arxiv.org/abs/2512.01562)
*Simon Leszek*

Main category: cs.LG

TL;DR: TimePred是一个自监督框架，将高维多变量时间序列的变点检测简化为单变量均值漂移检测，通过预测样本的归一化时间索引实现高效离线检测，并支持可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 高维大规模时间序列的变点检测面临统计一致性、可扩展性和可解释性三大挑战。传统方法在处理工业过程监控等场景中的高容量数据时效率低下且难以解释。

Method: TimePred采用自监督学习框架，将多变量变点检测转化为单变量均值漂移检测。核心思想是训练模型预测每个样本的归一化时间索引，当时间序列发生变化时，预测误差会显著增加，从而指示变点位置。该方法支持集成XAI归因方法进行特征级解释。

Result: 实验表明TimePred在变点检测性能上具有竞争力，同时计算成本降低高达两个数量级。在工业制造案例研究中，展示了改进的检测精度和可解释变点洞察的实际价值。

Conclusion: TimePred为高容量数据的离线变点检测提供了一个高效且可解释的框架，特别适用于工业过程监控等需要处理大规模高维时间序列的应用场景。

Abstract: Change-point detection (CPD) in high-dimensional, large-volume time series is challenging for statistical consistency, scalability, and interpretability. We introduce TimePred, a self-supervised framework that reduces multivariate CPD to univariate mean-shift detection by predicting each sample's normalized time index. This enables efficient offline CPD using existing algorithms and supports the integration of XAI attribution methods for feature-level explanations. Our experiments show competitive CPD performance while reducing computational cost by up to two orders of magnitude. In an industrial manufacturing case study, we demonstrate improved detection accuracy and illustrate the practical value of interpretable change-point insights.

中文标题: TimePred：面向高容量数据的高效可解释离线变点检测方法及其在工业过程监控中的应用

中文摘要: 高维大规模时间序列的变点检测在统计一致性、可扩展性和可解释性方面面临挑战。我们提出了TimePred，这是一个自监督框架，通过预测每个样本的归一化时间索引，将多变量变点检测简化为单变量均值漂移检测。这使得能够利用现有算法进行高效的离线变点检测，并支持集成XAI归因方法进行特征级解释。我们的实验显示，在将计算成本降低高达两个数量级的同时，保持了具有竞争力的变点检测性能。在工业制造案例研究中，我们展示了改进的检测精度，并说明了可解释变点洞察的实际价值。

</details>


### [427] [Do Large Language Models Walk Their Talk? Measuring the Gap Between Implicit Associations, Self-Report, and Behavioral Altruism](https://arxiv.org/abs/2512.01568)
*Sandro Andric*

Main category: cs.LG

TL;DR: 研究发现LLMs存在"美德信号差距"：虽然模型知道利他是好的（隐性偏见），但自我报告的利他水平（77.5%）显著高于实际行为（65.6%），且隐性关联不能预测行为。只有少数模型能同时实现高亲社会行为和准确自我认知。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究大型语言模型是否真正具有利他主义倾向，以及模型的隐性认知、自我报告与实际行为之间是否存在一致性。这有助于理解LLMs的道德对齐程度和行为可预测性。

Method: 采用多方法研究设计，测试24个前沿LLMs：1）内隐联想测试测量隐性利他偏见；2）强制二元选择任务测量行为利他主义；3）自我评估量表测量显性利他信念。通过统计分析比较三者关系。

Result: 主要结果：1）所有模型都有强隐性亲利他偏见；2）行为利他水平高于随机但差异大；3）隐性关联与行为无显著相关；4）75%模型存在"美德信号差距"（自我报告显著高于实际行为）；5）仅12.5%模型实现高亲社会行为和准确自我认知的理想组合。

Conclusion: 结论：LLMs存在系统性自我高估问题，隐性认知不能预测行为。建议使用"校准差距"作为标准化对齐指标，校准良好的模型更具可预测性和行为一致性。这对AI安全和对齐研究有重要意义。

Abstract: We investigate whether Large Language Models (LLMs) exhibit altruistic tendencies, and critically, whether their implicit associations and self-reports predict actual altruistic behavior. Using a multi-method approach inspired by human social psychology, we tested 24 frontier LLMs across three paradigms: (1) an Implicit Association Test (IAT) measuring implicit altruism bias, (2) a forced binary choice task measuring behavioral altruism, and (3) a self-assessment scale measuring explicit altruism beliefs. Our key findings are: (1) All models show strong implicit pro-altruism bias (mean IAT = 0.87, p < .0001), confirming models "know" altruism is good. (2) Models behave more altruistically than chance (65.6% vs. 50%, p < .0001), but with substantial variation (48-85%). (3) Implicit associations do not predict behavior (r = .22, p = .29). (4) Most critically, models systematically overestimate their own altruism, claiming 77.5% altruism while acting at 65.6% (p < .0001, Cohen's d = 1.08). This "virtue signaling gap" affects 75% of models tested. Based on these findings, we recommend the Calibration Gap (the discrepancy between self-reported and behavioral values) as a standardized alignment metric. Well-calibrated models are more predictable and behaviorally consistent; only 12.5% of models achieve the ideal combination of high prosocial behavior and accurate self-knowledge.

中文标题: 大型语言模型是否言行一致？测量隐性关联、自我报告与行为利他主义之间的差距

中文摘要: 我们研究大型语言模型（LLMs）是否表现出利他主义倾向，并关键性地探讨其隐性关联和自我报告是否能预测实际的利他行为。借鉴人类社会心理学的方法，我们通过三种范式测试了24个前沿LLMs：（1）测量隐性利他偏好的内隐联想测试（IAT），（2）测量行为利他主义的强制二元选择任务，（3）测量显性利他信念的自我评估量表。主要发现包括：（1）所有模型都显示出强烈的隐性亲利他偏见（平均IAT = 0.87，p < .0001），证实模型"知道"利他是好的。（2）模型行为比随机更利他（65.6% vs. 50%，p < .0001），但存在显著差异（48-85%）。（3）隐性关联不能预测行为（r = .22，p = .29）。（4）最关键的是，模型系统性地高估了自己的利他主义，声称77.5%的利他行为而实际只有65.6%（p < .0001，Cohen's d = 1.08）。这种"美德信号差距"影响了75%的测试模型。基于这些发现，我们建议将校准差距（自我报告值与行为值之间的差异）作为标准化的对齐指标。校准良好的模型更具可预测性和行为一致性；只有12.5%的模型实现了高亲社会行为和准确自我认知的理想组合。

</details>


### [428] [Reconstructing Multi-Scale Physical Fields from Extremely Sparse Measurements with an Autoencoder-Diffusion Cascade](https://arxiv.org/abs/2512.01572)
*Letian Yi,Tingpeng Zhang,Mingyuan Zhou,Guannan Wang,Quanke Su,Zhilu Lai*

Main category: cs.LG

TL;DR: 提出Cas-Sensing框架，通过自编码器-扩散级联从极稀疏测量中重建多尺度物理场，先重建大尺度结构，再生成细节，显著缓解不适定问题。


<details>
  <summary>Details</summary>
Motivation: 从极稀疏随机测量重建完整物理场是一个长期存在的不适定逆问题。传统方法难以处理这种极端稀疏情况，需要一种能够有效利用数据层次结构的方法。

Method: 提出级联感知(Cas-Sensing)框架：1) 基于神经算子的函数自编码器从任意稀疏输入重建原始场的主导结构（大尺度分量和几何边界）；2) 使用掩码级联策略训练的扩散模型，基于大尺度结构生成精细尺度细节；3) 在生成过程中通过基于贝叶斯后验采样的流形约束梯度强制测量一致性。

Result: 在仿真和真实世界数据集上的实验表明，Cas-Sensing在不同传感器配置和几何边界下都具有良好的泛化能力，能够提供准确且鲁棒的重建结果。

Conclusion: Cas-Sensing通过层次化概率建模框架有效缓解了不适定问题，为科学和工程应用中的实际部署提供了有前景的工具。

Abstract: Reconstructing full fields from extremely sparse and random measurements is a longstanding ill-posed inverse problem. A powerful framework for addressing such challenges is hierarchical probabilistic modeling, where uncertainty is represented by intermediate variables and resolved through marginalization during inference. Inspired by this principle, we propose Cascaded Sensing (Cas-Sensing), a hierarchical reconstruction framework that integrates an autoencoder-diffusion cascade. First, a neural operator-based functional autoencoder reconstructs the dominant structures of the original field - including large-scale components and geometric boundaries - from arbitrary sparse inputs, serving as an intermediate variable. Then, a conditional diffusion model, trained with a mask-cascade strategy, generates fine-scale details conditioned on these large-scale structures. To further enhance fidelity, measurement consistency is enforced via the manifold constrained gradient based on Bayesian posterior sampling during the generation process. This cascaded pipeline substantially alleviates ill-posedness, delivering accurate and robust reconstructions. Experiments on both simulation and real-world datasets demonstrate that Cas-Sensing generalizes well across varying sensor configurations and geometric boundaries, making it a promising tool for practical deployment in scientific and engineering applications.

中文标题: 基于自编码器-扩散级联的极稀疏测量多尺度物理场重建

中文摘要: 从极稀疏随机测量中重建完整场是一个长期存在的不适定逆问题。解决此类挑战的强大框架是层次化概率建模，其中不确定性由中间变量表示，并在推理过程中通过边缘化解决。受此原理启发，我们提出了级联感知(Cas-Sensing)，这是一个集成自编码器-扩散级联的层次化重建框架。首先，基于神经算子的函数自编码器从任意稀疏输入重建原始场的主导结构——包括大尺度分量和几何边界——作为中间变量。然后，使用掩码级联策略训练的条件扩散模型，基于这些大尺度结构生成精细尺度细节。为了进一步增强保真度，在生成过程中通过基于贝叶斯后验采样的流形约束梯度强制测量一致性。这种级联流水线显著缓解了不适定性，提供了准确且鲁棒的重建。在仿真和真实世界数据集上的实验表明，Cas-Sensing在不同传感器配置和几何边界下都具有良好的泛化能力，使其成为科学和工程应用中实际部署的有前景工具。

</details>


### [429] [ICAD-LLM: One-for-All Anomaly Detection via In-Context Learning with Large Language Models](https://arxiv.org/abs/2512.01672)
*Zhongyuan Wu,Jingyuan Wang,Zexuan Cheng,Yilong Zhou,Weizhi Wang,Juhua Pu,Chao Li,Changqing Ma*

Main category: cs.LG

TL;DR: ICAD-LLM是一个基于大语言模型上下文学习的统一异常检测框架，能够处理多种数据模态（时间序列、日志、表格数据），通过定义异常为与正常样本参考集的不相似性，实现跨领域泛化而无需大量重新训练。


<details>
  <summary>Details</summary>
Motivation: 现代系统在快速变化的环境中运行，产生多种相互关联的数据模态，需要异常检测方法能够：1）在统一框架中处理异构数据格式；2）具备强大的泛化能力以适应新场景。现有方法通常专注于单一模态且缺乏跨领域灵活性，无法满足这些需求。

Method: 提出上下文异常检测（ICAD）范式，将异常定义为与相关正常样本参考集的不相似性。在此基础上开发ICAD-LLM框架，利用大语言模型的上下文学习能力，在单个模型中统一处理时间序列、系统日志、表格记录等多种异构数据模态。

Result: 实验表明ICAD-LLM在性能上与任务特定的异常检测方法相当，同时对先前未见任务展现出强大的泛化能力，显著降低了部署成本并实现了对新环境的快速适应。

Conclusion: ICAD-LLM是第一个能够跨不同领域和模态处理异常检测任务的模型，通过上下文学习范式和大语言模型的能力，实现了异构数据处理和快速适应新场景的目标，为异常检测领域提供了统一的解决方案。

Abstract: Anomaly detection (AD) is a fundamental task of critical importance across numerous domains. Current systems increasingly operate in rapidly evolving environments that generate diverse yet interconnected data modalities -- such as time series, system logs, and tabular records -- as exemplified by modern IT systems. Effective AD methods in such environments must therefore possess two critical capabilities: (1) the ability to handle heterogeneous data formats within a unified framework, allowing the model to process and detect multiple modalities in a consistent manner during anomalous events; (2) a strong generalization ability to quickly adapt to new scenarios without extensive retraining. However, most existing methods fall short of these requirements, as they typically focus on single modalities and lack the flexibility to generalize across domains. To address this gap, we introduce a novel paradigm: In-Context Anomaly Detection (ICAD), where anomalies are defined by their dissimilarity to a relevant reference set of normal samples. Under this paradigm, we propose ICAD-LLM, a unified AD framework leveraging Large Language Models' in-context learning abilities to process heterogeneous data within a single model. Extensive experiments demonstrate that ICAD-LLM achieves competitive performance with task-specific AD methods and exhibits strong generalization to previously unseen tasks, which substantially reduces deployment costs and enables rapid adaptation to new environments. To the best of our knowledge, ICAD-LLM is the first model capable of handling anomaly detection tasks across diverse domains and modalities.

中文标题: ICAD-LLM：基于大语言模型上下文学习的通用异常检测框架

中文摘要: 异常检测（AD）是跨多个领域具有关键重要性的基础任务。当前系统日益在快速变化的环境中运行，这些环境生成多样但相互关联的数据模态——例如时间序列、系统日志和表格记录——正如现代IT系统所展示的那样。在这种环境中，有效的AD方法必须具备两个关键能力：（1）能够在统一框架内处理异构数据格式，使模型在异常事件期间以一致的方式处理和检测多种模态；（2）强大的泛化能力，能够快速适应新场景而无需大量重新训练。然而，大多数现有方法未能满足这些要求，因为它们通常专注于单一模态，缺乏跨领域泛化的灵活性。为填补这一空白，我们引入了一种新范式：上下文异常检测（ICAD），其中异常被定义为与相关正常样本参考集的不相似性。在此范式下，我们提出了ICAD-LLM，这是一个利用大语言模型上下文学习能力的统一AD框架，可在单个模型中处理异构数据。大量实验表明，ICAD-LLM实现了与任务特定AD方法相竞争的性能，并展现出对先前未见任务的强大泛化能力，这显著降低了部署成本并实现了对新环境的快速适应。据我们所知，ICAD-LLM是第一个能够跨不同领域和模态处理异常检测任务的模型。

</details>


### [430] [A unified framework for geometry-independent operator learning in cardiac electrophysiology simulations](https://arxiv.org/abs/2512.01702)
*Bei Zhou,Cesare Corrado,Shuang Qian,Maximilian Balmus,Angela W. C. Lee,Cristobal Rodero,Marco J. W. Gotte,Luuk H. G. A. Hopman,Mengyun Qiao,Steven Niederer*

Main category: cs.LG

TL;DR: 提出几何无关算子学习框架，使用通用心房坐标系和视觉变换器骨干，在多种左心房解剖结构上快速准确预测局部激活时间场。


<details>
  <summary>Details</summary>
Motivation: 心房电激活图的准确获取对心律失常个性化治疗至关重要，但传统生物物理详细模拟计算成本高，难以用于实时临床或群体规模分析，需要开发快速准确的预测方法。

Method: 1) 创建包含308,700个模拟的大规模数据集，使用GPU加速求解器；2) 引入通用心房坐标系，将解剖和功能数据与网格拓扑解耦；3) 设计基于视觉变换器骨干的神经算子，学习从结构和电生理输入到局部激活时间场的映射。

Result: 模型在最大模拟时间455毫秒内平均预测误差为5.1毫秒，优于现有算子学习方法，推理速度达到每样本0.12毫秒，能够快速准确预测不同解剖结构的电激活模式。

Conclusion: 该框架建立了在可变解剖域中学习领域不变生物物理映射的通用策略，使计算电生理学能够集成到实时和大规模临床工作流程中，为心律失常个性化治疗提供实用工具。

Abstract: Accurate maps of atrial electrical activation are essential for personalised treatment of arrhythmias, yet biophysically detailed simulations remain computationally intensive for real-time clinical use or population-scale analyses. Here we introduce a geometry-independent operator-learning framework that predicts local activation time (LAT) fields across diverse left atrial anatomies with near-instantaneous inference. We generated a dataset of 308,700 simulations using a GPU-accelerated electrophysiology solver, systematically varying multiple pacing sites and physiologically varied conduction properties across 147 patient-specific geometries derived from two independent clinical cohorts. All anatomical and functional data are expressed in a Universal Atrium Coordinate system, providing a consistent representation that decouples electrophysiological patterns from mesh topology. Within this coordinate space, we designed a neural operator with a vision-transformer backbone to learn the mapping from structural and electrophysiological inputs to LAT fields. With a mean prediction error of 5.1 ms over a 455 ms maximum simulation time, the model outperforms established operator-learning approaches and performs inference in 0.12 ms per sample. Our framework establishes a general strategy for learning domain-invariant biophysical mappings across variable anatomical domains and enables integration of computational electrophysiology into real-time and large-scale clinical workflows.

中文标题: 心脏电生理模拟中几何无关算子学习的统一框架

中文摘要: 准确的心房电激活图对于心律失常的个性化治疗至关重要，然而生物物理详细的模拟在实时临床应用或群体规模分析中仍然计算密集。本文介绍了一种几何无关的算子学习框架，能够在不同的左心房解剖结构上预测局部激活时间场，并实现近乎即时的推理。我们使用GPU加速的电生理求解器生成了308,700个模拟的数据集，系统地在147个来自两个独立临床队列的患者特定几何结构上变化多个起搏位点和生理变化的传导特性。所有解剖和功能数据都在通用心房坐标系中表达，提供了与网格拓扑解耦的电生理模式的一致表示。在这个坐标空间中，我们设计了一个具有视觉变换器骨干的神经算子，用于学习从结构和电生理输入到LAT场的映射。在最大模拟时间455毫秒内，模型平均预测误差为5.1毫秒，优于已建立的算子学习方法，并以每样本0.12毫秒的速度进行推理。我们的框架建立了在可变解剖域中学习领域不变生物物理映射的通用策略，并使计算电生理学能够集成到实时和大规模临床工作流程中。

</details>


### [431] [Beyond Scaffold: A Unified Spatio-Temporal Gradient Tracking Method](https://arxiv.org/abs/2512.01732)
*Yan Huang,Jinming Xu,Jiming Chen,Karl Henrik Johansson*

Main category: cs.LG

TL;DR: 提出ST-GT算法，通过时空梯度跟踪解决分布式学习中数据异构和噪声问题，实现线性收敛和通信效率提升。


<details>
  <summary>Details</summary>
Motivation: 分布式和联邦学习中，为减少通信开销而进行的多次本地更新会导致本地模型偏离全局最优，主要原因是数据异构性和本地梯度噪声。现有方法如Scaffold虽能缓解此问题，但仍有改进空间。

Method: 从梯度跟踪角度重新审视Scaffold方法，提出统一的时空梯度跟踪算法ST-GT。该方法通过跟踪相邻节点间的全局梯度来缓解数据异构性，同时维护本地梯度的运行平均值来抑制噪声，适用于时变图上的分布式随机优化。

Result: ST-GT在不假设数据异构性有界的情况下，对强凸问题达到线性收敛速率，对非凸问题达到次线性速率。在强凸设置下首次实现关于本地更新次数τ的通信复杂度线性加速，将拓扑相关噪声从σ²降低到σ²/τ。

Conclusion: ST-GT算法通过统一的时空梯度跟踪框架，有效解决了分布式学习中的数据异构和噪声问题，显著提高了通信效率，为分布式优化提供了新的理论保证和实践指导。

Abstract: In distributed and federated learning algorithms, communication overhead is often reduced by performing multiple local updates between communication rounds. However, due to data heterogeneity across nodes and the local gradient noise within each node, this strategy can lead to the drift of local models away from the global optimum. To address this issue, we revisit the well-known federated learning method Scaffold (Karimireddy et al., 2020) under a gradient tracking perspective, and propose a unified spatio-temporal gradient tracking algorithm, termed ST-GT, for distributed stochastic optimization over time-varying graphs. ST-GT tracks the global gradient across neighboring nodes to mitigate data heterogeneity, while maintaining a running average of local gradients to substantially suppress noise, with slightly more storage overhead. Without assuming bounded data heterogeneity, we prove that ST-GT attains a linear convergence rate for strongly convex problems and a sublinear rate for nonconvex cases. Notably, ST-GT achieves the first linear speed-up in communication complexity with respect to the number of local updates per round $τ$ for the strongly-convex setting. Compared to traditional gradient tracking methods, ST-GT reduces the topology-dependent noise term from $σ^2$ to $σ^2/τ$, where $σ^2$ denotes the noise level, thereby improving communication efficiency.

中文标题: 超越Scaffold：一种统一的时空梯度跟踪方法

中文摘要: 在分布式和联邦学习算法中，通信开销通常通过在通信轮次之间执行多次本地更新来减少。然而，由于节点间的数据异构性和每个节点内的本地梯度噪声，这种策略可能导致本地模型偏离全局最优解。为解决这一问题，我们从梯度跟踪的角度重新审视著名的联邦学习方法Scaffold（Karimireddy等人，2020），并提出了一种统一的时空梯度跟踪算法，称为ST-GT，用于时变图上的分布式随机优化。ST-GT通过跟踪相邻节点间的全局梯度来缓解数据异构性，同时维护本地梯度的运行平均值以显著抑制噪声，仅需略微增加存储开销。在不假设数据异构性有界的情况下，我们证明了ST-GT对于强凸问题达到线性收敛速率，对于非凸情况达到次线性速率。值得注意的是，ST-GT在强凸设置下首次实现了关于每轮本地更新次数τ的通信复杂度的线性加速。与传统梯度跟踪方法相比，ST-GT将拓扑相关的噪声项从σ²降低到σ²/τ，其中σ²表示噪声水平，从而提高了通信效率。

</details>


### [432] [SA-ADP: Sensitivity-Aware Adaptive Differential Privacy for Large Language Models](https://arxiv.org/abs/2512.01748)
*Stella Etuk,Ashraf Matrawy*

Main category: cs.LG

TL;DR: SA-ADP是一种针对大语言模型的敏感度感知自适应差分隐私方法，根据个人可识别信息的敏感度差异分配噪声，在保持强大隐私保护的同时不降低模型效用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在训练过程中会记忆个人可识别信息，引发隐私泄露风险。传统DP-SGD方法对所有数据采用均匀加噪，虽然提供隐私保护，但会损害模型效用。需要一种更智能的方法，根据数据敏感度差异分配隐私保护资源。

Method: 提出SA-ADP方法，通过敏感度感知机制评估不同PII的隐私敏感度，根据敏感度差异自适应分配噪声。相比DP-SGD的均匀加噪，SA-ADP对高敏感信息施加更强保护，对低敏感信息减少噪声干扰。

Result: 在ABCD、CUSTOMERSIM、Wikitext-2和UNSW-NB15四个数据集上的实验表明，SA-ADP在隐私保护效果上与DP-SGD相当，同时在模型效用上达到与无隐私保护基线（No-DP）相当的水平。

Conclusion: SA-ADP成功解决了传统差分隐私方法中隐私保护与模型效用的权衡问题，通过敏感度感知的自适应噪声分配，实现了在不降低模型性能的前提下提供强大的隐私保护。

Abstract: Despite advances in the use of large language models (LLMs) in downstream tasks, their ability to memorize information has raised privacy concerns. Therefore, protecting personally identifiable information (PII) during LLM training remains a fundamental challenge. Conventional methods like Differential Privacy-Stochastic Gradient Descent (DP-SGD) provide robust privacy protection via uniform noising, protecting PII regardless of its distinct sensitivity. This comes at the expense of the model's utility, leading to a trade-off. In this paper, we propose SA-ADP, a sensitivity-aware approach that allocates noise based on the sensitivity of individual PII. We evaluated our method on four datasets (ABCD, CUSTOMERSIM, Wikitext-2, and UNSW-NB15 ). Our results show that SA-ADP achieves results comparable to the baseline (No-DP) and the conventional DP-SGD. This means that our method did not degrade the model's utility while still maintaining strong privacy protection.

中文标题: SA-ADP：面向大语言模型的敏感度感知自适应差分隐私

中文摘要: 尽管大语言模型（LLMs）在下游任务中的应用取得了进展，但其记忆信息的能力引发了隐私担忧。因此，在LLM训练过程中保护个人可识别信息（PII）仍然是一个基本挑战。传统方法如差分隐私-随机梯度下降（DP-SGD）通过均匀加噪提供强大的隐私保护，无论PII的敏感度如何都进行保护。但这以模型效用为代价，导致权衡问题。在本文中，我们提出了SA-ADP，一种敏感度感知方法，根据个体PII的敏感度分配噪声。我们在四个数据集（ABCD、CUSTOMERSIM、Wikitext-2和UNSW-NB15）上评估了我们的方法。结果显示，SA-ADP实现了与基线（No-DP）和传统DP-SGD相当的结果。这意味着我们的方法在保持强大隐私保护的同时，没有降低模型的效用。

</details>


### [433] [How Does RL Post-training Induce Skill Composition? A Case Study on Countdown](https://arxiv.org/abs/2512.01775)
*Simon Park,Simran Kaur,Sanjeev Arora*

Main category: cs.LG

TL;DR: 本文研究RL后训练如何促进技能组合，以Countdown任务为例，发现模型能组合子任务实现OOD泛化，但学习顺序受树结构影响：先掌握浅平衡树，右重结构学习困难。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习能提升大语言模型的推理能力，但其在促进组合泛化（从已知组件合成新技能）方面的作用常被与长度泛化混淆。本文旨在研究RL后训练如何教授技能组合，以及组合结构如何影响技能迁移。

Method: 聚焦Countdown任务（给定n个数字和目标值，构建表达式计算结果为目标值），将模型解决方案分析为表达式树，每个子树对应可重用的子任务（即"技能"）。通过跟踪训练过程中树形状及其成功率进行研究。

Result: 发现：(1) 对更大n和未见树形状的OOD泛化，表明子任务的组合重用；(2) 结构依赖的学习层次——模型先掌握浅平衡树（子任务间工作量平衡），后掌握深不平衡树，右重结构存在持续脆弱性（即使组合深度与某些左重结构相同）。

Conclusion: 诊断揭示了RL后训练学习的内容、顺序和泛化失败之处，阐明了仅通过RL后训练如何诱导超出标准指标（如pass@k）所能揭示的OOD泛化。

Abstract: While reinforcement learning (RL) successfully enhances reasoning in large language models, its role in fostering compositional generalization (the ability to synthesize novel skills from known components) is often conflated with mere length generalization. To this end, we study what RL post-training teaches about skill composition and how the structure of the composition affects the skill transfer. We focus on the Countdown task (given n numbers and a target, form an expression that evaluates to the target) and analyze model solutions as expression trees, where each subtree corresponds to a reusable subtask and thus can be viewed as a ``skill.'' Tracking tree shapes and their success rates over training, we find: (i) out-of-distribution (OOD) generalization to larger n and to unseen tree shapes, indicating compositional reuse of subtasks; (ii) a structure-dependent hierarchy of learnability -- models master shallow balanced trees (workload is balanced between subtasks) before deep unbalanced ones, with persistent fragility on right-heavy structures (even when the composition depth is the same as some left-heavy structures). Our diagnostic reveals what is learned, in what order, and where generalization fails, clarifying how RL-only post-training induces OOD generalization beyond what standard metrics such as pass@k reveal.

中文标题: RL后训练如何诱导技能组合？以Countdown任务为例的研究

中文摘要: 虽然强化学习成功增强了大型语言模型的推理能力，但其在促进组合泛化（从已知组件合成新技能的能力）方面的作用常常与单纯的长度泛化相混淆。为此，我们研究RL后训练如何教授技能组合，以及组合结构如何影响技能迁移。我们聚焦于Countdown任务（给定n个数字和一个目标值，构建计算结果为目标值的表达式），并将模型解决方案分析为表达式树，其中每个子树对应一个可重用的子任务，因此可被视为一种"技能"。通过跟踪训练过程中树形状及其成功率，我们发现：(i) 对更大n和未见树形状的分布外泛化，表明子任务的组合重用；(ii) 结构依赖的学习层次——模型先掌握浅平衡树（子任务间工作量平衡），后掌握深不平衡树，右重结构存在持续脆弱性（即使组合深度与某些左重结构相同）。我们的诊断揭示了学习的内容、顺序以及泛化失败之处，阐明了仅通过RL后训练如何诱导超出标准指标（如pass@k）所能揭示的OOD泛化。

</details>


### [434] [DeepCAVE: A Visualization and Analysis Tool for Automated Machine Learning](https://arxiv.org/abs/2512.01810)
*Sarah Segel,Helena Graf,Edward Bergman,Kristina Thieme,Marcel Wever,Alexander Tornede,Frank Hutter,Marius Lindauer*

Main category: cs.LG

TL;DR: DeepCAVE是一个用于自动机器学习（AutoML）中超参数优化（HPO）过程的可视化和分析工具，通过交互式仪表板帮助用户理解、调试和优化HPO过程。


<details>
  <summary>Details</summary>
Motivation: 超参数优化作为AutoML的核心范式，对充分发挥机器学习模型潜力至关重要，但其复杂性使得理解和调试优化过程变得困难。需要工具来提供对HPO过程的深入洞察。

Method: 开发了一个交互式可视化分析工具DeepCAVE，提供交互式仪表板，允许用户探索HPO过程的各个方面，识别问题、未开发的潜力以及被调优ML模型的新见解。

Result: DeepCAVE能够为研究人员、数据科学家和ML工程师提供可操作的洞察，帮助他们更好地理解HPO过程，识别优化中的问题和机会。

Conclusion: DeepCAVE通过提供可操作的洞察，在系统设计层面促进了HPO和ML的可解释性，旨在推动未来更稳健、更高效的方法论发展。

Abstract: Hyperparameter optimization (HPO), as a central paradigm of AutoML, is crucial for leveraging the full potential of machine learning (ML) models; yet its complexity poses challenges in understanding and debugging the optimization process. We present DeepCAVE, a tool for interactive visualization and analysis, providing insights into HPO. Through an interactive dashboard, researchers, data scientists, and ML engineers can explore various aspects of the HPO process and identify issues, untouched potentials, and new insights about the ML model being tuned. By empowering users with actionable insights, DeepCAVE contributes to the interpretability of HPO and ML on a design level and aims to foster the development of more robust and efficient methodologies in the future.

中文标题: DeepCAVE：自动机器学习的可视化与分析工具

中文摘要: 超参数优化（HPO）作为自动机器学习（AutoML）的核心范式，对于充分发挥机器学习（ML）模型的潜力至关重要；然而其复杂性给理解和调试优化过程带来了挑战。我们提出了DeepCAVE，一个用于交互式可视化和分析的工具，提供对HPO的深入洞察。通过交互式仪表板，研究人员、数据科学家和ML工程师可以探索HPO过程的各个方面，并识别问题、未开发的潜力以及被调优ML模型的新见解。通过为用户提供可操作的洞察，DeepCAVE在系统设计层面促进了HPO和ML的可解释性，旨在推动未来更稳健、更高效的方法论发展。

</details>


### [435] [Deconstructing Generative Diversity: An Information Bottleneck Analysis of Discrete Latent Generative Models](https://arxiv.org/abs/2512.01831)
*Yudi Wu,Wenhao Zhao,Dianbo Liu*

Main category: cs.LG

TL;DR: 本文提出基于信息瓶颈理论的诊断框架，分析离散潜在生成模型的生成多样性差异。该框架将生成过程建模为"压缩压力"（最小化码书熵）和"多样性压力"（最大化条件熵）之间的冲突，并将多样性分解为"路径多样性"（高层策略选择）和"执行多样性"（策略执行随机性）。通过三种零样本推理时干预技术，揭示了AR、MIM和Diffusion模型的三种不同策略模式。


<details>
  <summary>Details</summary>
Motivation: 不同离散潜在生成模型（如AR、MIM、Diffusion）的生成多样性存在显著差异，但缺乏系统性的理论框架来解释这些行为差异背后的策略机制。

Method: 基于信息瓶颈理论构建诊断框架，将生成过程建模为压缩压力与多样性压力的冲突。引入三种零样本推理时干预技术：路径干预、执行干预和混合干预，直接扰动潜在生成过程以揭示多样性分配机制。将该框架应用于代表性的AR、MIM和Diffusion系统进行分析。

Result: 分析揭示了三种不同的策略模式：1）MIM模型采用"多样性优先"策略，强调多样性压力；2）AR模型采用"压缩优先"策略，强调压缩压力；3）Diffusion模型采用"解耦"策略，将压缩和多样性压力分离到不同阶段。该分析为行为差异提供了理论解释，并启发了新的推理时多样性增强技术。

Conclusion: 信息瓶颈理论为分析离散潜在生成模型的多样性行为提供了原则性框架。多样性可分解为路径多样性和执行多样性，不同模型通过不同的压力平衡策略实现多样性。该框架不仅解释了现有模型的行为差异，还为设计新的多样性增强技术提供了理论基础。

Abstract: Generative diversity varies significantly across discrete latent generative models such as AR, MIM, and Diffusion. We propose a diagnostic framework, grounded in Information Bottleneck (IB) theory, to analyze the underlying strategies resolving this behavior. The framework models generation as a conflict between a 'Compression Pressure' - a drive to minimize overall codebook entropy - and a 'Diversity Pressure' - a drive to maximize conditional entropy given an input. We further decompose this diversity into two primary sources: 'Path Diversity', representing the choice of high-level generative strategies, and 'Execution Diversity', the randomness in executing a chosen strategy. To make this decomposition operational, we introduce three zero-shot, inference-time interventions that directly perturb the latent generative process and reveal how models allocate and express diversity. Application of this probe-based framework to representative AR, MIM, and Diffusion systems reveals three distinct strategies: "Diversity-Prioritized" (MIM), "Compression-Prioritized" (AR), and "Decoupled" (Diffusion). Our analysis provides a principled explanation for their behavioral differences and informs a novel inference-time diversity enhancement technique.

中文标题: 解构生成多样性：离散潜在生成模型的信息瓶颈分析

中文摘要: 生成多样性在AR、MIM和Diffusion等离散潜在生成模型之间存在显著差异。我们提出了一个基于信息瓶颈理论的诊断框架，用于分析导致这种行为差异的底层策略。该框架将生成过程建模为"压缩压力"（最小化整体码书熵的驱动力）和"多样性压力"（给定输入时最大化条件熵的驱动力）之间的冲突。我们进一步将这种多样性分解为两个主要来源："路径多样性"（代表高层生成策略的选择）和"执行多样性"（执行选定策略时的随机性）。为了使这种分解具有可操作性，我们引入了三种零样本推理时干预技术，直接扰动潜在生成过程，揭示模型如何分配和表达多样性。将该探测框架应用于代表性的AR、MIM和Diffusion系统，揭示了三种不同的策略："多样性优先"（MIM）、"压缩优先"（AR）和"解耦"（Diffusion）。我们的分析为它们的行为差异提供了原则性解释，并启发了新的推理时多样性增强技术。

</details>


### [436] [The Mean-Field Dynamics of Transformers](https://arxiv.org/abs/2512.01868)
*Philippe Rigollet*

Main category: cs.LG

TL;DR: 论文将Transformer注意力机制建模为粒子系统，研究其连续平均场极限，发现全局聚类现象：token在长时间亚稳态后最终会聚集成簇，并分析了聚类速率、归一化方案的影响以及长上下文注意力的相变。


<details>
  <summary>Details</summary>
Motivation: 为理解Transformer注意力机制的内在动力学特性，建立数学框架将其解释为相互作用粒子系统，研究其连续极限下的行为，揭示注意力机制中表示坍缩的机理以及保持多簇结构的条件。

Method: 将Transformer注意力理想化为球面上的连续系统，将其动力学与Wasserstein梯度流、Kuramoto同步模型和均值漂移聚类联系起来。采用可处理的等角约简方法获得精确聚类速率，分析常用归一化方案如何改变收缩速度，识别长上下文注意力的相变。

Result: 发现了全局聚类现象：token在长时间亚稳态后最终会聚集成簇。获得了精确的聚类速率，证明了归一化方案会影响收缩速度，识别了长上下文注意力的相变，揭示了表示坍缩的机制和保持多簇结构的条件。

Conclusion: Transformer注意力机制在连续极限下表现出全局聚类行为，其动力学特性与多种数学物理模型相关。理解这些机制有助于设计既能避免表示坍缩又能保持表达能力的深度注意力架构。

Abstract: We develop a mathematical framework that interprets Transformer attention as an interacting particle system and studies its continuum (mean-field) limits. By idealizing attention continuous on the sphere, we connect Transformer dynamics to Wasserstein gradient flows, synchronization models (Kuramoto), and mean-shift clustering. Central to our results is a global clustering phenomenon whereby tokens cluster asymptotically after long metastable states where they are arranged into multiple clusters. We further analyze a tractable equiangular reduction to obtain exact clustering rates, show how commonly used normalization schemes alter contraction speeds, and identify a phase transition for long-context attention. The results highlight both the mechanisms that drive representation collapse and the regimes that preserve expressive, multi-cluster structure in deep attention architectures.

中文标题: Transformer的平均场动力学

中文摘要: 我们开发了一个数学框架，将Transformer注意力解释为相互作用的粒子系统，并研究其连续（平均场）极限。通过将注意力理想化为球面上的连续系统，我们将Transformer动力学与Wasserstein梯度流、同步模型（Kuramoto）和均值漂移聚类联系起来。我们结果的核心是一个全局聚类现象，即token在长时间亚稳态后最终会聚集成簇，这些亚稳态中token被排列成多个簇。我们进一步分析了一个可处理的等角约简以获得精确的聚类速率，展示了常用归一化方案如何改变收缩速度，并识别了长上下文注意力的相变。这些结果既突出了驱动表示坍缩的机制，也指出了在深度注意力架构中保持表达性多簇结构的条件。

</details>


### [437] [Elastic Weight Consolidation for Knowledge Graph Continual Learning: An Empirical Evaluation](https://arxiv.org/abs/2512.01890)
*Gaganpreet Jhajj,Fuhua Lin*

Main category: cs.LG

TL;DR: EWC方法在知识图谱持续学习中能有效减少45.7%的灾难性遗忘，但任务划分策略对评估结果有显著影响


<details>
  <summary>Details</summary>
Motivation: 知识图谱需要持续更新，但神经嵌入模型在顺序学习新任务时会出现灾难性遗忘问题。本研究旨在评估弹性权重巩固（EWC）方法在知识图谱链接预测任务中缓解灾难性遗忘的效果。

Method: 使用TransE嵌入模型在FB15k-237数据集上进行知识图谱链接预测实验。采用EWC这种基于正则化的持续学习方法，通过多个实验（五个随机种子）评估其效果。比较了两种任务划分策略：基于关系的划分（按关系类型分组三元组）和随机划分。

Result: EWC将灾难性遗忘从12.62%降低到6.85%，减少了45.7%。任务划分策略显著影响遗忘程度：基于关系的划分比随机划分表现出更高的遗忘率（12.62% vs 2.81%），相差9.8个百分点。

Conclusion: EWC能有效缓解知识图谱持续学习中的灾难性遗忘问题，但评估协议的设计（特别是任务划分策略）对结果有重要影响。研究虽然局限于单一嵌入模型和数据集，但为知识图谱持续学习提供了实证基础。

Abstract: Knowledge graphs (KGs) require continual updates as new information emerges, but neural embedding models suffer from catastrophic forgetting when learning new tasks sequentially. We evaluate Elastic Weight Consolidation (EWC), a regularization-based continual learning method, on KG link prediction using TransE embeddings on FB15k-237. Across multiple experiments with five random seeds, we find that EWC reduces catastrophic forgetting from 12.62% to 6.85%, a 45.7% reduction compared to naive sequential training. We observe that the task partitioning strategy affects the magnitude of forgetting: relation-based partitioning (grouping triples by relation type) exhibits 9.8 percentage points higher forgetting than randomly partitioned tasks (12.62% vs 2.81%), suggesting that task construction influences evaluation outcomes. While focused on a single embedding model and dataset, our results demonstrate that EWC effectively mitigates catastrophic forgetting in KG continual learning and highlight the importance of evaluation protocol design.

中文标题: 弹性权重巩固在知识图谱持续学习中的实证评估

中文摘要: 随着新信息的出现，知识图谱需要持续更新，但神经嵌入模型在顺序学习新任务时会出现灾难性遗忘问题。我们评估了弹性权重巩固（EWC）这种基于正则化的持续学习方法在知识图谱链接预测中的应用，使用TransE嵌入在FB15k-237数据集上进行实验。通过五个随机种子的多次实验，我们发现EWC将灾难性遗忘从12.62%降低到6.85%，相比朴素顺序训练减少了45.7%。我们观察到任务划分策略影响遗忘程度：基于关系的划分（按关系类型分组三元组）比随机划分的任务表现出9.8个百分点的更高遗忘率（12.62% vs 2.81%），这表明任务构建方式影响评估结果。虽然研究集中于单一嵌入模型和数据集，但我们的结果表明EWC能有效缓解知识图谱持续学习中的灾难性遗忘，并强调了评估协议设计的重要性。

</details>


### [438] [Provably Safe Model Updates](https://arxiv.org/abs/2512.01899)
*Leo Elmecker-Plakolm,Pierre Fasterling,Philip Sosnin,Calvin Tsay,Matthew Wicker*

Main category: cs.LG

TL;DR: 提出一种可证明安全模型更新的框架，通过计算参数空间中的最大局部不变域来确保模型更新后仍满足性能规范，提供形式化安全保证。


<details>
  <summary>Details</summary>
Motivation: 安全关键环境中的机器学习模型需要持续更新以适应动态变化，但现有启发式方法无法提供形式化安全保证，参数更新可能导致灾难性遗忘或对齐漂移等意外后果。

Method: 将问题形式化为计算最大局部不变域（LID），采用参数化抽象域（正交体、zonotopes）的松弛方法，建立可处理的原对偶公式，通过投影更新到安全域实现高效认证。

Result: 在持续学习和基础模型微调基准测试中，该方法在避免遗忘方面匹配或超过启发式基线，同时提供形式化安全保证，支持多LID计算和前瞻数据缓冲区使用。

Conclusion: 该框架首次实现了可证明安全的模型更新，为动态环境中的机器学习部署提供了形式化安全保障，平衡了性能保持与安全认证的需求。

Abstract: Safety-critical environments are inherently dynamic. Distribution shifts, emerging vulnerabilities, and evolving requirements demand continuous updates to machine learning models. Yet even benign parameter updates can have unintended consequences, such as catastrophic forgetting in classical models or alignment drift in foundation models. Existing heuristic approaches (e.g., regularization, parameter isolation) can mitigate these effects but cannot certify that updated models continue to satisfy required performance specifications. We address this problem by introducing a framework for provably safe model updates. Our approach first formalizes the problem as computing the largest locally invariant domain (LID): a connected region in parameter space where all points are certified to satisfy a given specification. While exact maximal LID computation is intractable, we show that relaxing the problem to parameterized abstract domains (orthotopes, zonotopes) yields a tractable primal-dual formulation. This enables efficient certification of updates - independent of the data or algorithm used - by projecting them onto the safe domain. Our formulation further allows computation of multiple approximately optimal LIDs, incorporation of regularization-inspired biases, and use of lookahead data buffers. Across continual learning and foundation model fine-tuning benchmarks, our method matches or exceeds heuristic baselines for avoiding forgetting while providing formal safety guarantees.

中文标题: 可证明安全模型更新

中文摘要: 安全关键环境本质上是动态的。分布偏移、新出现的漏洞和不断变化的需求要求对机器学习模型进行持续更新。然而，即使是良性的参数更新也可能产生意想不到的后果，例如经典模型中的灾难性遗忘或基础模型中的对齐漂移。现有的启发式方法（例如正则化、参数隔离）可以缓解这些影响，但无法证明更新后的模型继续满足所需的性能规范。我们通过引入一个可证明安全模型更新的框架来解决这个问题。我们的方法首先将问题形式化为计算最大的局部不变域（LID）：参数空间中所有点都被证明满足给定规范的连通区域。虽然精确的最大LID计算是难以处理的，但我们表明，将问题放宽到参数化抽象域（正交体、zonotopes）会产生一个可处理的原对偶公式。这使得能够通过将更新投影到安全域上来高效地认证更新——独立于使用的数据或算法。我们的公式进一步允许计算多个近似最优的LID，结合正则化启发的偏差，以及使用前瞻数据缓冲区。在持续学习和基础模型微调基准测试中，我们的方法在避免遗忘方面匹配或超过了启发式基线，同时提供正式的安全保证。

</details>


### [439] [A Footprint-Aware, High-Resolution Approach for Carbon Flux Prediction Across Diverse Ecosystems](https://arxiv.org/abs/2512.01917)
*Jacob Searcy,Anish Dulal,Scott Bridgham,Ashley Cordes,Lillian Aoki,Brendan Bohannan,Qing Zhu,Lucas C. R. Silva*

Main category: cs.LG

TL;DR: 本文提出了一种名为Footprint-Aware Regression (FAR)的深度学习框架，首次同时预测空间足迹和像素级（30米尺度）碳通量估计，用于解决生态系统碳通量监测的挑战。


<details>
  <summary>Details</summary>
Motivation: 自然气候解决方案（NCS）为缓解二氧化碳排放提供了途径，但监测大地理区域内生态系统的碳吸收仍然具有挑战性。涡动协方差塔为基于卫星产品的预测"上尺度"模型提供了地面真实数据，但许多卫星现在产生的测量尺度小于通量塔的足迹范围。

Method: 本文引入了Footprint-Aware Regression (FAR)，这是一个首创的深度学习框架，同时预测空间足迹和像素级（30米尺度）的碳通量估计。该模型在AMERI-FAR25数据集上进行训练，该数据集结合了439个站点年的塔数据与相应的Landsat影像。

Result: 该模型产生高分辨率预测，在预测来自各种生态系统的测试站点的月净生态系统交换时，达到R² = 0.78的精度。

Conclusion: FAR框架为生态系统碳通量监测提供了一种高分辨率、足迹感知的新方法，能够有效解决卫星测量尺度与通量塔足迹不匹配的问题，为自然气候解决方案的实施提供了更好的监测工具。

Abstract: Natural climate solutions (NCS) offer an approach to mitigating carbon dioxide (CO2) emissions. However, monitoring the carbon drawdown of ecosystems over large geographic areas remains challenging. Eddy-flux covariance towers provide ground truth for predictive 'upscaling' models derived from satellite products, but many satellites now produce measurements on spatial scales smaller than a flux tower's footprint. We introduce Footprint-Aware Regression (FAR), a first-of-its-kind, deep-learning framework that simultaneously predicts spatial footprints and pixel-level (30 m scale) estimates of carbon flux. FAR is trained on our AMERI-FAR25 dataset which combines 439 site years of tower data with corresponding Landsat scenes. Our model produces high-resolution predictions and achieves R2 = 0.78 when predicting monthly net ecosystem exchange on test sites from a variety of ecosystems.

中文标题: 一种足迹感知、高分辨率的跨生态系统碳通量预测方法

中文摘要: 自然气候解决方案（NCS）为缓解二氧化碳（CO2）排放提供了一种途径。然而，监测大地理区域内生态系统的碳吸收仍然具有挑战性。涡动协方差塔为基于卫星产品的预测"上尺度"模型提供了地面真实数据，但许多卫星现在产生的测量尺度小于通量塔的足迹范围。我们引入了Footprint-Aware Regression (FAR)，这是一个首创的深度学习框架，同时预测空间足迹和像素级（30米尺度）的碳通量估计。FAR在我们的AMERI-FAR25数据集上进行训练，该数据集结合了439个站点年的塔数据与相应的Landsat影像。我们的模型产生高分辨率预测，在预测来自各种生态系统的测试站点的月净生态系统交换时，达到R² = 0.78的精度。

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [440] [MoLT: Mixture of Layer-Wise Tokens for Efficient Audio-Visual Learning](https://arxiv.org/abs/2512.00115)
*Kyeongha Rho,Hyeongkeun Lee,Jae Won Cho,Joon Son Chung*

Main category: cs.SD

TL;DR: MoLT是一种高效的视听学习适应框架，通过从后期Transformer层提取层间令牌进行并行融合，替代传统的逐层顺序适应，在保持参数效率的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统视听学习中的Transformer适应方法通常需要在每个层进行顺序处理，计算开销大且参数效率低。作者希望开发一种更高效的适应框架，减少计算负担同时保持性能。

Method: 1. 仅从后期Transformer层提取层间令牌，避免早期层特征的不稳定性；2. 使用两种适配器分别蒸馏模态特定信息和跨模态交互；3. 通过令牌融合模块动态融合层间令牌；4. 应用正交正则化防止令牌冗余。

Result: 在多个视听基准测试中表现优异：视听问答、视听分割和视听事件定位任务上均优于现有方法，同时保持参数和内存效率。

Conclusion: MoLT通过策略性地从后期层提取令牌并进行并行融合，实现了高效且有效的视听学习适应，为多模态学习提供了新的参数效率优化方案。

Abstract: In this paper, we propose Mixture of Layer-Wise Tokens (MoLT), a parameter- and memory-efficient adaptation framework for audio-visual learning. The key idea of MoLT is to replace conventional, computationally heavy sequential adaptation at every transformer layer with a parallel, lightweight scheme that extracts and fuses layer-wise tokens only from the late layers. We adopt two types of adapters to distill modality-specific information and cross-modal interaction into compact latent tokens in a layer-wise manner. A token fusion module then dynamically fuses these layer-wise tokens by taking into account their relative significance. To prevent the redundancy of latent tokens, we apply an orthogonality regularization between latent tokens during training. Through the systematic analysis of the position of adaptation in the pre-trained transformers, we extract latent tokens only from the late layers of the transformers. This strategic adaptation approach avoids error propagation from the volatile early-layer features, thereby maximizing the adaptation performance while maintaining parameter and memory efficiency. Through extensive experiments, we demonstrate that MoLT outperforms existing methods on diverse audio-visual benchmarks, including Audio-Visual Question Answering, Audio-Visual Segmentation, and Audio-Visual Event Localization.

中文标题: MoLT：用于高效视听学习的层间令牌混合方法

中文摘要: 本文提出了一种参数和内存高效的视听学习适应框架——层间令牌混合（MoLT）。MoLT的核心思想是用并行、轻量级的方案替代传统在每个Transformer层进行的计算密集型顺序适应，该方案仅从后期层提取和融合层间令牌。我们采用两种类型的适配器，以层间方式将模态特定信息和跨模态交互蒸馏到紧凑的潜在令牌中。然后，令牌融合模块通过考虑这些层间令牌的相对重要性来动态融合它们。为了防止潜在令牌的冗余，我们在训练期间对潜在令牌应用正交正则化。通过对预训练Transformer中适应位置的系统分析，我们仅从Transformer的后期层提取潜在令牌。这种策略性的适应方法避免了来自易变的早期层特征的错误传播，从而在保持参数和内存效率的同时最大化适应性能。通过大量实验，我们证明MoLT在多种视听基准测试中优于现有方法，包括视听问答、视听分割和视听事件定位。

</details>


### [441] [Art2Music: Generating Music for Art Images with Multi-modal Feeling Alignment](https://arxiv.org/abs/2512.00120)
*Jiaying Hong,Ting Zhu,Thanet Markchom,Huizhi Liang*

Main category: cs.SD

TL;DR: Art2Music是一个轻量级跨模态框架，通过两阶段方法从艺术图像和用户评论生成感知自然且情感对齐的音乐，使用伪情感对齐数据集ArtiCaps进行训练，在多项指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着AIGC的兴起，从多模态输入生成感知自然且情感对齐的音乐成为核心挑战。现有方法通常依赖需要昂贵标注的显式情感标签，因此需要更灵活的情感对齐方法。

Method: 构建ArtiCaps伪情感对齐图像-音乐-文本数据集；提出两阶段Art2Music框架：第一阶段使用OpenCLIP编码图像和文本，通过门控残差模块融合，双向LSTM解码为梅尔频谱图；第二阶段使用微调的HiFi-GAN声码器重建高质量音频波形。

Result: 在ArtiCaps数据集上，Mel-Cepstral Distortion、Frechet Audio Distance、Log-Spectral Distance和余弦相似度均有明显改善。基于小型LLM的评分研究进一步验证了跨模态情感对齐的一致性，并提供了可解释的匹配和不匹配分析。

Conclusion: Art2Music在感知自然度、频谱保真度和语义一致性方面均有改进，仅需5万训练样本即可保持稳健性能，为交互艺术、个性化声景和数字艺术展览中的情感对齐创意音频生成提供了可扩展解决方案。

Abstract: With the rise of AI-generated content (AIGC), generating perceptually natural and feeling-aligned music from multimodal inputs has become a central challenge. Existing approaches often rely on explicit emotion labels that require costly annotation, underscoring the need for more flexible feeling-aligned methods. To support multimodal music generation, we construct ArtiCaps, a pseudo feeling-aligned image-music-text dataset created by semantically matching descriptions from ArtEmis and MusicCaps. We further propose Art2Music, a lightweight cross-modal framework that synthesizes music from artistic images and user comments. In the first stage, images and text are encoded with OpenCLIP and fused using a gated residual module; the fused representation is decoded by a bidirectional LSTM into Mel-spectrograms with a frequency-weighted L1 loss to enhance high-frequency fidelity. In the second stage, a fine-tuned HiFi-GAN vocoder reconstructs high-quality audio waveforms. Experiments on ArtiCaps show clear improvements in Mel-Cepstral Distortion, Frechet Audio Distance, Log-Spectral Distance, and cosine similarity. A small LLM-based rating study further verifies consistent cross-modal feeling alignment and offers interpretable explanations of matches and mismatches across modalities. These results demonstrate improved perceptual naturalness, spectral fidelity, and semantic consistency. Art2Music also maintains robust performance with only 50k training samples, providing a scalable solution for feeling-aligned creative audio generation in interactive art, personalized soundscapes, and digital art exhibitions.

中文标题: Art2Music：通过多模态情感对齐为艺术图像生成音乐

中文摘要: 随着AI生成内容（AIGC）的兴起，从多模态输入生成感知自然且情感对齐的音乐已成为核心挑战。现有方法通常依赖需要昂贵标注的显式情感标签，这凸显了对更灵活情感对齐方法的需求。为支持多模态音乐生成，我们构建了ArtiCaps，这是一个通过语义匹配ArtEmis和MusicCaps描述创建的伪情感对齐图像-音乐-文本数据集。我们进一步提出了Art2Music，一个轻量级跨模态框架，可从艺术图像和用户评论合成音乐。在第一阶段，图像和文本通过OpenCLIP编码，并使用门控残差模块融合；融合表示通过双向LSTM解码为梅尔频谱图，采用频率加权L1损失以增强高频保真度。在第二阶段，微调的HiFi-GAN声码器重建高质量音频波形。在ArtiCaps上的实验显示，在Mel-Cepstral Distortion、Frechet Audio Distance、Log-Spectral Distance和余弦相似度方面均有明显改善。基于小型LLM的评分研究进一步验证了一致的跨模态情感对齐，并提供了跨模态匹配和不匹配的可解释说明。这些结果表明了改进的感知自然度、频谱保真度和语义一致性。Art2Music仅需5万训练样本即可保持稳健性能，为交互艺术、个性化声景和数字艺术展览中的情感对齐创意音频生成提供了可扩展解决方案。

</details>


### [442] [Explainable Multi-Modal Deep Learning for Automatic Detection of Lung Diseases from Respiratory Audio Signals](https://arxiv.org/abs/2512.00563)
*S M Asiful Islam Saky,Md Rashidul Islam,Md Saiful Arefin,Shahaba Alam*

Main category: cs.SD

TL;DR: 该研究开发了一个结合CNN-BiLSTM Attention和手工声学特征的多模态深度学习框架，用于从呼吸音频中自动检测肺部疾病，在哮喘数据集上达到91.21%准确率，并通过可解释性方法提供临床透明度。


<details>
  <summary>Details</summary>
Motivation: 传统听诊方法存在主观性、环境噪声干扰和临床医生间变异性等局限性，难以实现标准化、客观的呼吸疾病诊断。需要开发自动化的、可解释的深度学习系统来辅助临床决策。

Method: 提出多模态深度学习框架：1) 频谱-时间编码器使用CNN-BiLSTM Attention架构；2) 手工声学特征编码器提取MFCCs、频谱质心等生理相关特征；3) 后期融合整合两个分支；4) 采用重采样、归一化、噪声过滤、数据增强等预处理；5) 使用Grad-CAM、集成梯度和SHAP进行可解释性分析。

Result: 在哮喘检测数据集第二版上获得91.21%准确率、0.899宏F1分数和0.9866宏ROC-AUC，优于所有消融变体。消融研究显示时间建模、注意力机制和多模态融合对性能提升至关重要。

Conclusion: 该可解释多模态深度学习框架在呼吸疾病自动检测方面表现出色，结合了数据驱动学习和领域知识，为远程医疗、即时诊断和呼吸筛查提供了有前景的解决方案，同时通过可解释性方法增强了临床透明度。

Abstract: Respiratory diseases remain major global health challenges, and traditional auscultation is often limited by subjectivity, environmental noise, and inter-clinician variability. This study presents an explainable multimodal deep learning framework for automatic lung-disease detection using respiratory audio signals. The proposed system integrates two complementary representations: a spectral-temporal encoder based on a CNN-BiLSTM Attention architecture, and a handcrafted acoustic-feature encoder capturing physiologically meaningful descriptors such as MFCCs, spectral centroid, spectral bandwidth, and zero-crossing rate. These branches are combined through late-stage fusion to leverage both data-driven learning and domain-informed acoustic cues. The model is trained and evaluated on the Asthma Detection Dataset Version 2 using rigorous preprocessing, including resampling, normalization, noise filtering, data augmentation, and patient-level stratified partitioning. The study achieved strong generalization with 91.21% accuracy, 0.899 macro F1-score, and 0.9866 macro ROC-AUC, outperforming all ablated variants. An ablation study confirms the importance of temporal modeling, attention mechanisms, and multimodal fusion. The framework incorporates Grad-CAM, Integrated Gradients, and SHAP, generating interpretable spectral, temporal, and feature-level explanations aligned with known acoustic biomarkers to build clinical transparency. The findings demonstrate the framework's potential for telemedicine, point-of-care diagnostics, and real-world respiratory screening.

中文标题: 基于呼吸音频信号的多模态深度学习可解释性肺病自动检测

中文摘要: 呼吸系统疾病仍然是全球主要的健康挑战，传统听诊常受限于主观性、环境噪声和临床医生间的变异性。本研究提出了一种基于呼吸音频信号的可解释多模态深度学习框架，用于自动检测肺部疾病。该系统整合了两种互补的表示：基于CNN-BiLSTM Attention架构的频谱-时间编码器，以及捕捉生理意义描述符的手工声学特征编码器，如MFCCs、频谱质心、频谱带宽和过零率。这些分支通过后期融合相结合，以利用数据驱动学习和领域知识声学线索。该模型在哮喘检测数据集第二版上进行了训练和评估，采用了严格的预处理，包括重采样、归一化、噪声过滤、数据增强和患者级分层划分。研究实现了强大的泛化能力，达到91.21%的准确率、0.899的宏F1分数和0.9866的宏ROC-AUC，优于所有消融变体。消融研究证实了时间建模、注意力机制和多模态融合的重要性。该框架整合了Grad-CAM、集成梯度和SHAP，生成了与已知声学生物标志物一致的可解释频谱、时间和特征级解释，以建立临床透明度。研究结果表明该框架在远程医疗、即时诊断和现实世界呼吸筛查方面具有潜力。

</details>


### [443] [Melody or Machine: Detecting Synthetic Music with Dual-Stream Contrastive Learning](https://arxiv.org/abs/2512.00621)
*Arnesh Batra,Dev Sharma,Krish Thukral,Ruhani Bhatia,Naman Batra,Aditya Gautam*

Main category: cs.SD

TL;DR: 该论文提出了一个名为MoM的大规模基准数据集和一个名为CLAM的双流对比学习架构，用于检测AI生成的合成音乐。CLAM通过两个预训练音频编码器分别处理音频，并使用交叉聚合模块和对比三元组损失来捕捉人声与乐器元素之间的不一致性，在MoM基准上取得了0.925的F1分数。


<details>
  <summary>Details</summary>
Motivation: 随着端到端AI音乐生成的快速发展，对艺术真实性和版权的威胁日益加剧，需要能够跟上发展步伐的检测方法。现有模型在面对多样且快速发展的新生成器时表现不佳，特别是在分布外内容上性能显著下降，这凸显了需要更具挑战性的基准和更鲁棒的检测架构。

Method: 论文提出了CLAM双流检测架构，使用两个不同的预训练音频编码器（MERT和Wave2Vec2）创建音频的并行表示。通过可学习的交叉聚合模块融合这些表示，建模它们之间的相互依赖关系。模型采用双损失目标：用于分类的标准二元交叉熵损失，以及对比三元组损失，后者训练模型区分连贯和人工不匹配的流配对，增强对合成伪影的敏感性。

Result: CLAM在具有挑战性的MoM基准上取得了0.925的F1分数，建立了合成音乐取证的新最先进水平。MoM数据集包含超过130,000首歌曲（6,665小时），是迄今为止最多样化的数据集。

Conclusion: 该研究通过引入MoM基准和CLAM架构，显著推进了合成音乐检测领域。CLAM通过捕捉人声与乐器元素之间的微妙不一致性，有效识别AI生成的音乐，为保护艺术真实性和版权提供了有力工具。

Abstract: The rapid evolution of end-to-end AI music generation poses an escalating threat to artistic authenticity and copyright, demanding detection methods that can keep pace. While foundational, existing models like SpecTTTra falter when faced with the diverse and rapidly advancing ecosystem of new generators, exhibiting significant performance drops on out-of-distribution (OOD) content. This generalization failure highlights a critical gap: the need for more challenging benchmarks and more robust detection architectures. To address this, we first introduce Melody or Machine (MoM), a new large-scale benchmark of over 130,000 songs (6,665 hours). MoM is the most diverse dataset to date, built with a mix of open and closed-source models and a curated OOD test set designed specifically to foster the development of truly generalizable detectors. Alongside this benchmark, we introduce CLAM, a novel dual-stream detection architecture. We hypothesize that subtle, machine-induced inconsistencies between vocal and instrumental elements, often imperceptible in a mixed signal, offer a powerful tell-tale sign of synthesis. CLAM is designed to test this hypothesis by employing two distinct pre-trained audio encoders (MERT and Wave2Vec2) to create parallel representations of the audio. These representations are fused by a learnable cross-aggregation module that models their inter-dependencies. The model is trained with a dual-loss objective: a standard binary cross-entropy loss for classification, complemented by a contrastive triplet loss which trains the model to distinguish between coherent and artificially mismatched stream pairings, enhancing its sensitivity to synthetic artifacts without presuming a simple feature alignment. CLAM establishes a new state-of-the-art in synthetic music forensics. It achieves an F1 score of 0.925 on our challenging MoM benchmark.

中文标题: 旋律还是机器：基于双流对比学习的合成音乐检测

中文摘要: 端到端AI音乐生成的快速发展对艺术真实性和版权构成了日益严重的威胁，需要能够跟上发展步伐的检测方法。虽然现有模型如SpecTTTra具有基础性，但在面对多样且快速发展的新生成器生态系统时表现不佳，在分布外内容上表现出显著的性能下降。这种泛化失败突显了一个关键差距：需要更具挑战性的基准和更鲁棒的检测架构。为了解决这个问题，我们首先引入了Melody or Machine（MoM），一个包含超过130,000首歌曲（6,665小时）的新大规模基准。MoM是迄今为止最多样化的数据集，使用开源和闭源模型混合构建，并包含专门设计的分布外测试集，旨在促进真正可泛化检测器的开发。除了这个基准，我们还引入了CLAM，一种新颖的双流检测架构。我们假设人声和乐器元素之间微妙、机器引起的不一致性（通常在混合信号中难以察觉）提供了合成音乐的强大线索。CLAM通过采用两个不同的预训练音频编码器（MERT和Wave2Vec2）创建音频的并行表示来测试这一假设。这些表示通过可学习的交叉聚合模块融合，该模块建模它们之间的相互依赖关系。模型采用双损失目标训练：用于分类的标准二元交叉熵损失，辅以对比三元组损失，后者训练模型区分连贯和人工不匹配的流配对，增强其对合成伪影的敏感性，而不假设简单的特征对齐。CLAM在合成音乐取证中建立了新的最先进水平。它在我们的挑战性MoM基准上取得了0.925的F1分数。

</details>


### [444] [Q2D2: A Geometry-Aware Audio Codec Leveraging Two-Dimensional Quantization](https://arxiv.org/abs/2512.01537)
*Tal Shuster,Eliya Nachmani*

Main category: cs.SD

TL;DR: Q2D2是一种利用二维量化（如六边形、菱形或矩形网格）的几何感知音频编解码器，通过将特征对投影到结构化二维网格进行量化，相比传统量化方法（RVQ、VQ、FSQ）能更好地捕捉特征相关性，提高码本利用率和压缩效率，同时保持最先进的音频重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有神经音频编解码器通常依赖残差向量量化（RVQ）、向量量化（VQ）和有限标量量化（FSQ）等方法，但这些量化技术限制了潜在空间的几何结构，难以捕捉特征间的相关性，导致表示学习效率低下、码本利用率低和令牌率不理想。

Method: 提出二维量化（Q2D2）方案，将特征对投影到结构化二维网格（如六边形、菱形或矩形平铺）上，并量化到最近的网格值。这种方法产生由网格级别乘积定义的隐式码本，码本大小与传统方法相当，但通过几何结构更好地捕捉特征相关性。

Result: Q2D2在语音领域的广泛实验中，在各种客观和主观重建指标上实现了竞争性甚至优于最先进模型的性能。它提高了音频压缩效率，具有低令牌率和高码本利用率，同时保持最先进的重建质量。全面的消融研究进一步证实了设计选择的有效性。

Conclusion: 尽管几何公式简单，但Q2D2通过二维量化方案显著改进了音频压缩效率，在保持最先进重建质量的同时实现了低令牌率和高码本利用率，为神经音频编解码器的量化方法提供了新的有效途径。

Abstract: Recent neural audio codecs have achieved impressive reconstruction quality, typically relying on quantization methods such as Residual Vector Quantization (RVQ), Vector Quantization (VQ) and Finite Scalar Quantization (FSQ). However, these quantization techniques limit the geometric structure of the latent space, make it harder to capture correlations between features leading to inefficiency in representation learning, codebook utilization and token rate. In this paper we introduce Two Dimensional Quantization (Q2D2), a quantization scheme in which feature pairs are projected onto structured 2D grids such as hexagonal, rhombic, or rectangular tiling and quantized to the nearest grid values, yielding an implicit codebook defined by the product of grid levels, with codebook sizes comparable to conventional methods. Despite its simple geometric formulation, Q2D2 improves audio compression efficiency, with low token rates and high codebook utilization while maintaining state of the art reconstruction quality. Specifically, Q2D2 achieves competitive to superior performance in various objective and subjective reconstruction metrics, across extensive experiments in speech domain compared to state of the art models. Comprehensive ablation studies further confirm the effectiveness of our design choices.

中文标题: Q2D2：利用二维量化的几何感知音频编解码器

中文摘要: 最近的神经音频编解码器已经实现了令人印象深刻的重建质量，通常依赖于残差向量量化（RVQ）、向量量化（VQ）和有限标量量化（FSQ）等量化方法。然而，这些量化技术限制了潜在空间的几何结构，使得捕捉特征间的相关性更加困难，导致表示学习效率低下、码本利用率低和令牌率不理想。在本文中，我们引入了二维量化（Q2D2），这是一种量化方案，其中特征对被投影到结构化二维网格（如六边形、菱形或矩形平铺）上，并量化到最近的网格值，产生由网格级别乘积定义的隐式码本，码本大小与传统方法相当。尽管其几何公式简单，但Q2D2提高了音频压缩效率，具有低令牌率和高码本利用率，同时保持最先进的重建质量。具体而言，与最先进模型相比，在语音领域的广泛实验中，Q2D2在各种客观和主观重建指标上实现了竞争性甚至优于的性能。全面的消融研究进一步证实了我们设计选择的有效性。

</details>


### [445] [LLM2Fx-Tools: Tool Calling For Music Post-Production](https://arxiv.org/abs/2512.01559)
*Seungheon Doh,Junghyun Koo,Marco A. Martínez-Ramírez,Woosung Choi,Wei-Hsiang Liao,Qiyu Wu,Juhan Nam,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: LLM2Fx-Tools是一个多模态工具调用框架，利用大语言模型理解音频输入，通过思维链规划生成可执行的音频效果链，用于音乐后期制作。


<details>
  <summary>Details</summary>
Motivation: 传统音乐后期制作需要专业知识和经验，现有AI方法缺乏可解释性和可控性。本文旨在利用LLM的工具调用能力，为音频效果处理提供可解释、可控的自动化解决方案。

Method: 1. 提出LLM2Fx-Tools框架，使用LLM理解音频输入，通过思维链规划选择效果类型、确定顺序和估计参数；2. 创建LP-Fx数据集，包含结构化思维链标注和音频效果模块的工具调用；3. 采用自回归序列建模、工具调用和思维链推理；4. 在风格迁移场景中验证系统。

Result: 1. LLM2Fx-Tools能够从原始和处理后的音频对中推断效果链及其参数；2. 在风格迁移任务中成功将音频效果信息从参考源转移到新内容；3. LLM-as-a-judge评估表明系统能生成合适的思维链推理和音乐制作响应。

Conclusion: 这是首个将LLM工具调用应用于音频效果模块的工作，实现了可解释和可控的音乐制作。框架在效果链推断和风格迁移方面表现良好，为音乐后期制作自动化提供了新方向。

Abstract: This paper introduces LLM2Fx-Tools, a multimodal tool-calling framework that generates executable sequences of audio effects (Fx-chain) for music post-production. LLM2Fx-Tools uses a large language model (LLM) to understand audio inputs, select audio effects types, determine their order, and estimate parameters, guided by chain-of-thought (CoT) planning. We also present LP-Fx, a new instruction-following dataset with structured CoT annotations and tool calls for audio effects modules. Experiments show that LLM2Fx-Tools can infer an Fx-chain and its parameters from pairs of unprocessed and processed audio, enabled by autoregressive sequence modeling, tool calling, and CoT reasoning. We further validate the system in a style transfer setting, where audio effects information is transferred from a reference source and applied to new content. Finally, LLM-as-a-judge evaluation demonstrates that our approach generates appropriate CoT reasoning and responses for music production queries. To our knowledge, this is the first work to apply LLM-based tool calling to audio effects modules, enabling interpretable and controllable music production.

中文标题: LLM2Fx-Tools：音乐后期制作的工具调用

中文摘要: 本文介绍了LLM2Fx-Tools，一个多模态工具调用框架，用于为音乐后期制作生成可执行的音频效果序列（效果链）。LLM2Fx-Tools使用大语言模型理解音频输入，在思维链规划的指导下选择音频效果类型、确定其顺序并估计参数。我们还提出了LP-Fx，一个新的指令跟随数据集，包含结构化的思维链标注和音频效果模块的工具调用。实验表明，LLM2Fx-Tools能够通过自回归序列建模、工具调用和思维链推理，从原始和处理后的音频对中推断效果链及其参数。我们进一步在风格迁移场景中验证了该系统，其中音频效果信息从参考源转移到新内容。最后，LLM-as-a-judge评估表明，我们的方法为音乐制作查询生成了合适的思维链推理和响应。据我们所知，这是首个将基于LLM的工具调用应用于音频效果模块的工作，实现了可解释和可控的音乐制作。

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [446] [A Low-Cost Reliable Racetrack Cache Based on Data Compression](https://arxiv.org/abs/2512.01915)
*Elham Cheshmikhani,Fateme Shokouhinia,Hamed Farbeh*

Main category: cs.ET

TL;DR: 该论文提出了一种基于数据压缩的低成本可靠赛道存储器缓存方案，通过利用数值局部性压缩数据块，为强ECC释放存储空间，从而在不增加存储开销的情况下容忍多位错误。


<details>
  <summary>Details</summary>
Motivation: SRAM缓存面临纳米尺度下的可扩展性限制，而新兴的非易失性存储器技术如赛道存储器（RTM）具有高密度和接近SRAM的性能，适合用于末级缓存。但RTM面临可靠性挑战，其存储元件的随机行为和易错的数据移位导致多位错误概率高。传统纠错码要么无法容忍多位错误，要么需要大量额外存储空间。

Method: 提出利用数值局部性压缩数据块，释放大部分缓存块用于存储强ECC的冗余数据。通过数据压缩技术，在不增加存储开销的情况下为大多数缓存块提供强ECC保护，从而容忍多位错误。

Result: 使用gem5全系统模拟器评估显示，该方案将缓存的平均故障间隔时间平均提高了11.3倍，硬件和性能开销均低于1%。

Conclusion: 提出的基于数据压缩的可靠赛道存储器缓存方案能够有效解决RTM的可靠性问题，以极低的硬件和性能开销显著提高缓存可靠性，为RTM在末级缓存中的应用提供了可行的解决方案。

Abstract: SRAM-based cache memory faces several scalability limitations in deep nanoscale technologies, e.g., high leakage current, low cell stability, and low density. Emerging Non-Volatile Memory (NVM) technologies have received lots of attention in recent years, where Racetrack Memory (RTM) is among the most promising ones. RTM has the highest density among all NVMs and its access performance is comparable to SRAM technology. Therefore, RTM is a suitable alternative for SRAM in the Last-Level Caches (LLCs). Despite all its benefits, RTM confronts different reliability challenges due to the stochastic behavior of its storage element and highly error-prone data shifting, leading to a high probability of multiple-bit errors. Conventional Error-Correcting Codes (ECCs) are either incapable of tolerating multiple-bit errors or require a large amount of extra storage for check bits. This paper proposes taking advantage of value locality for compressing data blocks and freeing up a large fraction of cache blocks for storing data redundancy of strong ECCs. Utilizing the proposed scheme, a large majority of cache blocks are protected by strong ECCs to tolerate multiple-bit errors without any storage overhead. The evaluation using gem5 full-system simulator demonstrates that the proposed scheme enhances the mean-time-to-failure of the cache by an average of 11.3x with less than 1% hardware and performance overhead.

中文标题: 基于数据压缩的低成本可靠赛道存储器缓存

中文摘要: SRAM基缓存存储器在深纳米尺度技术中面临多个可扩展性限制，例如高漏电流、低单元稳定性和低密度。新兴非易失性存储器技术近年来受到广泛关注，其中赛道存储器是最有前景的技术之一。RTM在所有NVM中具有最高密度，其访问性能可与SRAM技术相媲美。因此，RTM是末级缓存中SRAM的合适替代品。尽管具有诸多优势，RTM由于其存储元件的随机行为和高度易错的数据移位而面临不同的可靠性挑战，导致多位错误的高概率。传统纠错码要么无法容忍多位错误，要么需要大量额外存储空间用于校验位。本文提出利用数值局部性压缩数据块，为强ECC释放大部分缓存块以存储数据冗余。利用所提出的方案，大多数缓存块通过强ECC保护以容忍多位错误，而无需任何存储开销。使用gem5全系统模拟器的评估表明，所提出的方案将缓存的平均故障间隔时间平均提高了11.3倍，硬件和性能开销均低于1%。

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [447] [PORTAL: Controllable Landscape Generator for Continuous Optimization-Part I: Framework](https://arxiv.org/abs/2512.00288)
*Danial Yazdani,Mai Peng,Delaram Yazdani,Shima F. Yazdi,Mohammad Nabi Omidvar,Yuan Sun,Trung Thanh Nguyen,Changhe Li,Xiaodong Li*

Main category: cs.NE

TL;DR: PORTAL是一个通用的连续优化基准测试生成器，提供对盆地曲率、条件数、变量交互和表面粗糙度的细粒度独立控制，支持系统性地创建多样化测试函数。


<details>
  <summary>Details</summary>
Motivation: 现有连续优化测试套件存在局限性：经典集合固定且僵化，先前生成器仅覆盖狭窄的景观家族，缺乏对细节的精细控制和可变性。需要一种能够提供细粒度控制、支持系统算法分析并促进元算法研究的通用基准生成器。

Method: PORTAL采用分层设计，从单个组件到多组件景观的块状组合，具有可控的部分可分离性和不平衡块贡献。提供对每个组件在每个维度和方向上形状的精确控制，支持通过元素级和耦合算子进行多样化变换模式。包含原则性中和机制防止组件主导，所有变换保持组件中心和局部二次结构。

Result: PORTAL能够以受控和系统的方式引入复杂的景观特征，如多模态性、不对称性和异质性粗糙度。支持特定挑战的隔离和渐进难度缩放，便于创建多样化数据集用于元算法研究、定制基准套件设计和交互式教育用途。

Conclusion: PORTAL解决了先前景观生成器的关键限制，提供了对连续优化基准测试的精细控制，支持系统算法分析，并为元算法研究、基准套件设计和教育应用提供了灵活平台。

Abstract: Benchmarking is central to optimization research, yet existing test suites for continuous optimization remain limited: classical collections are fixed and rigid, while previous generators cover only narrow families of landscapes with restricted variability and control over details. This paper introduces PORTAL (Platform for Optimization Research, Testing, Analysis, and Learning), a general benchmark generator that provides fine-grained, independent control over basin curvature, conditioning, variable interactions, and surface ruggedness. PORTAL's layered design spans from individual components to block-wise compositions of multi-component landscapes with controllable partial separability and imbalanced block contributions. It offers precise control over the shape of each component in every dimension and direction, and supports diverse transformation patterns through both element-wise and coupling operators with compositional sequencing. All transformations preserve component centers and local quadratic structure, ensuring stability and interpretability. A principled neutralization mechanism prevents unintended component domination caused by exponent or scale disparities, which addresses a key limitation of prior landscape generators. On this foundation, transformations introduce complex landscape characteristics, such as multimodality, asymmetry, and heterogeneous ruggedness, in a controlled and systematic way. PORTAL enables systematic algorithm analysis by supporting both isolation of specific challenges and progressive difficulty scaling. It also facilitates the creation of diverse datasets for meta-algorithmic research, tailored benchmark suite design, and interactive educational use. The complete Python and MATLAB source code for PORTAL is publicly available at [https://github.com/EvoMindLab/PORTAL].

中文标题: PORTAL：连续优化可控景观生成器 - 第一部分：框架

中文摘要: 基准测试是优化研究的核心，但现有的连续优化测试套件仍然有限：经典集合固定且僵化，而先前的生成器仅覆盖狭窄的景观家族，具有受限的可变性和对细节的控制。本文介绍了PORTAL（优化研究、测试、分析和学习平台），这是一个通用的基准生成器，提供对盆地曲率、条件数、变量交互和表面粗糙度的细粒度、独立控制。PORTAL的分层设计从单个组件扩展到具有可控部分可分离性和不平衡块贡献的多组件景观的块状组合。它提供对每个组件在每个维度和方向上形状的精确控制，并通过元素级和耦合算子支持多样化的变换模式，具有组合序列。所有变换保持组件中心和局部二次结构，确保稳定性和可解释性。一个原则性的中和机制防止由指数或尺度差异引起的意外组件主导，这解决了先前景观生成器的关键限制。在此基础上，变换以受控和系统的方式引入复杂的景观特征，如多模态性、不对称性和异质性粗糙度。PORTAL通过支持特定挑战的隔离和渐进难度缩放，实现系统算法分析。它还便于为元算法研究、定制基准套件设计和交互式教育用途创建多样化数据集。PORTAL的完整Python和MATLAB源代码可在[https://github.com/EvoMindLab/PORTAL]公开获取。

</details>


### [448] [A Novel Population Initialization Method via Adaptive Experience Transfer for General-Purpose Binary Evolutionary Optimization](https://arxiv.org/abs/2512.00341)
*Zhiyuan Wang,Shengcai Liu,Shaofeng Zhang,Ke Tang*

Main category: cs.NE

TL;DR: 提出了一种名为MPI的通用二进制进化优化种群初始化方法，通过自适应经验迁移利用已解决问题经验为新问题生成高质量初始种群，无需领域知识。


<details>
  <summary>Details</summary>
Motivation: 进化算法在有限函数评估次数下对初始种群质量敏感，但获取高质量初始种群且不依赖问题特定知识仍具挑战性。需要一种通用方法利用历史求解经验提升新问题初始化质量。

Method: MPI方法为二进制优化问题设计，通过经验混合机制表示、选择和迁移求解经验。从已解决问题中提取经验构建知识库，为新问题自适应选择相关经验并生成初始种群，整个过程无需问题特定知识。

Result: 在六个二进制优化问题类别（三个经典类别和三个复杂现实应用类别）上实验，仅基于经典类别构建经验库。结果显示MPI能有效将求解经验迁移到未见问题类别（复杂类别）和高维问题实例，显著优于现有通用种群初始化方法。

Conclusion: MPI是一种有效的通用种群初始化方法，能够通过自适应经验迁移显著提升进化算法在有限函数评估下的性能，特别是在未见问题类别和高维问题上表现优异。

Abstract: Evolutionary Algorithms (EAs) are widely used general-purpose optimization methods due to their domain independence. However, under a limited number of function evaluations (#FEs), the performance of EAs is quite sensitive to the quality of the initial population. Obtaining a high-quality initial population without problem-specific knowledge remains a significant challenge. To address this, this work proposes a general-purpose population initialization method, named mixture-of-experience for population initialization (MPI), for binary optimization problems where decision variables take values of 0 or 1. MPI leverages solving experiences from previously solved problems to generate high-quality initial populations for new problems using only a small number of FEs. Its main novelty lies in a general-purpose approach for representing, selecting, and transferring solving experiences without requiring problem-specific knowledge. Extensive experiments are conducted across six binary optimization problem classes, comprising three classic classes and three complex classes from real-world applications. The experience repository is constructed solely based on instances from the three classic classes, while the performance evaluation is performed across all six classes. The results demonstrate that MPI effectively transfers solving experiences to unseen problem classes (i.e., the complex ones) and higher-dimensional problem instances, significantly outperforming existing general-purpose population initialization methods.

中文标题: 基于自适应经验迁移的通用二进制进化优化新型种群初始化方法

中文摘要: 进化算法因其领域独立性而被广泛用作通用优化方法。然而，在有限函数评估次数下，进化算法的性能对初始种群质量相当敏感。在不依赖问题特定知识的情况下获取高质量初始种群仍然是一个重大挑战。为此，本文提出了一种名为经验混合种群初始化的通用种群初始化方法，适用于决策变量取值为0或1的二进制优化问题。MPI利用先前已解决问题的求解经验，仅使用少量函数评估次数为新问题生成高质量初始种群。其主要创新在于提供了一种无需问题特定知识的通用方法来表示、选择和迁移求解经验。我们在六个二进制优化问题类别上进行了广泛实验，包括三个经典类别和三个来自现实应用的复杂类别。经验库仅基于三个经典类别的实例构建，而性能评估在所有六个类别上进行。结果表明，MPI能够有效地将求解经验迁移到未见问题类别（即复杂类别）和高维问题实例，显著优于现有的通用种群初始化方法。

</details>


### [449] [Pascal-Weighted Genetic Algorithms: A Binomially-Structured Recombination Framework](https://arxiv.org/abs/2512.01249)
*Otman A. Basir*

Main category: cs.NE

TL;DR: 本文提出了一种基于帕斯卡（二项式）系数的多亲本重组算子PWR，通过二项式权重形成子代，强调中心遗传并抑制破坏性方差，在多个基准测试中表现优于标准重组算子。


<details>
  <summary>Details</summary>
Motivation: 传统遗传算法主要使用双亲本交叉算子，存在方差破坏性较大的问题。本文旨在开发一种基于帕斯卡系数的多亲本重组框架，通过结构化权重强调中心遗传，减少破坏性方差，提高算法性能。

Method: 提出帕斯卡加权重组（PWR）算子，使用归一化的帕斯卡（二项式）系数作为权重，将多个亲本进行结构化凸组合形成子代。建立了PWR的数学框架，推导了方差传递特性，并分析了其对模式生存的影响。该方法可扩展到实值、二进制/逻辑和排列表示。

Result: 在四个代表性基准测试中评估：PID控制器调优、FIR低通滤波器设计、无线功率调制优化和旅行商问题。PWR在所有测试中均表现出更平滑的收敛、更低的方差，相比标准重组算子获得9-22%的性能提升。

Conclusion: 帕斯卡加权重组算子为遗传算法提供了一种简单、算法无关的多亲本重组框架，通过二项式权重强调中心遗传并抑制破坏性方差，在各种应用中都能显著提升性能。

Abstract: This paper introduces a new family of multi-parent recombination operators for Genetic Algorithms (GAs), based on normalized Pascal (binomial) coefficients. Unlike classical two-parent crossover operators, Pascal-Weighted Recombination (PWR) forms offsprings as structured convex combination of multiple parents, using binomially shaped weights that emphasize central inheritance while suppressing disruptive variance. We develop a mathematical framework for PWR, derive variance-transfer properties, and analyze its effect on schema survival. The operator is extended to real-valued, binary/logit, and permutation representations.
  We evaluate the proposed method on four representative benchmarks: (i) PID controller tuning evaluated using the ITAE metric, (ii) FIR low-pass filter design under magnitude-response constraints, (iii) wireless power-modulation optimization under SINR coupling, and (iv) the Traveling Salesman Problem (TSP). We demonstrate how, across these benchmarks, PWR consistently yields smoother convergence, reduced variance, and achieves 9-22% performance gains over standard recombination operators. The approach is simple, algorithm-agnostic, and readily integrable into diverse GA architectures.

中文标题: 帕斯卡加权遗传算法：一种二项式结构的重组框架

中文摘要: 本文介绍了一种基于归一化帕斯卡（二项式）系数的新型多亲本重组算子家族，用于遗传算法。与经典的双亲本交叉算子不同，帕斯卡加权重组（PWR）使用二项式形状的权重将多个亲本形成结构化的凸组合子代，强调中心遗传同时抑制破坏性方差。我们开发了PWR的数学框架，推导了方差传递特性，并分析了其对模式生存的影响。该算子可扩展到实值、二进制/逻辑和排列表示。

我们在四个代表性基准测试中评估了所提出的方法：（i）使用ITAE指标评估的PID控制器调优，（ii）幅度响应约束下的FIR低通滤波器设计，（iii）SINR耦合下的无线功率调制优化，以及（iv）旅行商问题。我们展示了在这些基准测试中，PWR始终产生更平滑的收敛、更低的方差，并相比标准重组算子实现9-22%的性能提升。该方法简单、算法无关，易于集成到各种GA架构中。

</details>


### [450] [Current Challenges of Symbolic Regression: Optimization, Selection, Model Simplification, and Benchmarking](https://arxiv.org/abs/2512.01682)
*Guilherme Seidyo Imai Aldeia*

Main category: cs.NE

TL;DR: 该论文系统研究了符号回归面临的四大挑战：参数优化、选择机制、模型简化和基准测试，通过一系列创新方法提升了符号回归的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 符号回归结合了预测准确性和模型可解释性的优势，但面临参数优化困难、选择机制影响搜索、模型过度复杂以及缺乏有效基准测试等长期挑战，限制了其实际应用潜力。

Method: 1. 研究参数优化对预测准确性的影响；2. 探索ε-lexicase选择机制以提高优良后代的生成概率；3. 提出基于记忆化和局部敏感哈希的新简化方法以减少冗余；4. 将贡献集成到多目标进化符号回归库中；5. 改进大型符号回归基准测试套件并评估性能。

Result: 1. 参数优化提高了预测准确性，但在运行时间和表达式大小方面存在权衡；2. ε-lexicase选择能更有效地选择可能产生优良后代的父代；3. 新简化方法减少了冗余，产生了更简单、更准确的模型；4. 集成方法在真实世界和合成问题的基准测试中实现了帕累托最优性能；5. 改进的基准测试套件展示了论文方法在符号回归领域的优越性。

Conclusion: 该论文通过系统解决符号回归的关键挑战，显著提升了方法的性能和实用性。提出的多目标进化符号回归库在准确性和简洁性方面达到帕累托最优，超越了多种当代符号回归方法，为符号回归的实际应用提供了有力支持。

Abstract: Symbolic Regression (SR) is a regression method that aims to discover mathematical expressions that describe the relationship between variables, and it is often implemented through Genetic Programming, a metaphor for the process of biological evolution. Its appeal lies in combining predictive accuracy with interpretable models, but its promise is limited by several long-standing challenges: parameters are difficult to optimize, the selection of solutions can affect the search, and models often grow unnecessarily complex. In addition, current methods must be constantly re-evaluated to understand the SR landscape. This thesis addresses these challenges through a sequence of studies conducted throughout the doctorate, each focusing on an important aspect of the SR search process. First, I investigate parameter optimization, obtaining insights into its role in improving predictive accuracy, albeit with trade-offs in runtime and expression size. Next, I study parent selection, exploring $ε$-lexicase to select parents more likely to generate good performing offspring. The focus then turns to simplification, where I introduce a novel method based on memoization and locality-sensitive hashing that reduces redundancy and yields simpler, more accurate models. All of these contributions are implemented into a multi-objective evolutionary SR library, which achieves Pareto-optimal performance in terms of accuracy and simplicity on benchmarks of real-world and synthetic problems, outperforming several contemporary SR approaches. The thesis concludes by proposing changes to a famous large-scale symbolic regression benchmark suite, then running the experiments to assess the symbolic regression landscape, demonstrating that a SR method with the contributions presented in this thesis achieves Pareto-optimal performance.

中文标题: 符号回归的当前挑战：优化、选择、模型简化与基准测试

中文摘要: 符号回归是一种回归方法，旨在发现描述变量之间关系的数学表达式，通常通过遗传编程（生物进化过程的隐喻）实现。其吸引力在于结合了预测准确性和可解释模型，但其潜力受到几个长期挑战的限制：参数难以优化，解决方案的选择会影响搜索过程，模型往往变得不必要的复杂。此外，当前方法必须不断重新评估以理解符号回归的现状。本论文通过博士期间进行的一系列研究来解决这些挑战，每个研究都关注符号回归搜索过程的一个重要方面。首先，我研究了参数优化，获得了其在提高预测准确性方面的作用，尽管在运行时间和表达式大小方面存在权衡。接下来，我研究了父代选择，探索了ε-lexicase选择方法来选择更可能产生良好性能后代的父代。然后重点转向简化，我引入了一种基于记忆化和局部敏感哈希的新方法，减少了冗余并产生了更简单、更准确的模型。所有这些贡献都实现了一个多目标进化符号回归库，该库在真实世界和合成问题的基准测试中实现了准确性和简洁性方面的帕累托最优性能，超越了多种当代符号回归方法。论文最后提出了对一个著名的大规模符号回归基准测试套件的改进建议，然后运行实验来评估符号回归的现状，证明了采用本论文提出的贡献的符号回归方法实现了帕累托最优性能。

</details>
